{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56182010-5de3-494d-9dd7-220625e9e51e",
   "metadata": {},
   "source": [
    "# Βήμα 2: Προεπεξεργασία Κειμένου\n",
    "\n",
    "Σε αυτό το βήμα, θα προεπεξεργαστούμε τα άρθρα που συλλέξαμε από τη Wikipedia. Οι ενέργειες που θα πραγματοποιήσουμε περιλαμβάνουν:\n",
    "1. **Tokenization**: Διάσπαση του κειμένου σε λέξεις.\n",
    "2. **Stop-Word Removal**: Αφαίρεση λέξεων χωρίς σημασιολογική σημασία (π.χ., \"the\", \"is\").\n",
    "3. **Stemming/Lemmatization**: Επιστροφή των λέξεων στη βασική τους μορφή.\n",
    "4. **Αφαίρεση Ειδικών Χαρακτήρων**: Αφαίρεση σημείων στίξης και άλλων χαρακτήρων που δεν είναι απαραίτητοι.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4107d-5869-4273-a751-c1682d08dacf",
   "metadata": {},
   "source": [
    "## Κελί 1: Εισαγωγή βιβλιοθηκών"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17853ca6-dd7f-42a0-9027-53c80687cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4a252-c930-46dd-be52-785f9005d02b",
   "metadata": {},
   "source": [
    "## Κελί 2: Συνάρτηση για την προεπεξεργασία κειμένου"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84de2e10-9bf4-4478-877b-ba2fd72619b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Προεπεξεργάζεται το κείμενο:\n",
    "    - Καθαρισμός ειδικών χαρακτήρων.\n",
    "    - Tokenization.\n",
    "    - Αφαίρεση stop-words.\n",
    "    - Lemmatization.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Μετατροπή σε πεζά\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Αφαίρεση ειδικών χαρακτήρων\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    stop_words = set(stopwords.words('english'))  # Stop-words\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Αφαίρεση stop-words\n",
    "    lemmatizer = WordNetLemmatizer()  # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec579b-3b2f-4f04-ac1a-d274671a8ecd",
   "metadata": {},
   "source": [
    "## Κελί 3: Φόρτωση άρθρων από το αρχείο"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674d33a1-9d33-4085-8270-472874faabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 articles for preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Φόρτωση άρθρων από το αρχείο 'articles.json'\n",
    "with open('articles.json', 'r', encoding='utf-8') as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(articles)} articles for preprocessing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd5be9-7005-4bb3-9625-ba5bd155f89d",
   "metadata": {},
   "source": [
    "## Κελί 4: Εφαρμογή προεπεξεργασίας στα άρθρα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ebb3909-590b-4a27-b3bf-4432558c1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 100 articles.\n"
     ]
    }
   ],
   "source": [
    "# Εφαρμογή προεπεξεργασίας σε όλα τα άρθρα\n",
    "processed_articles = []\n",
    "for article in articles:\n",
    "    processed_content = preprocess_text(article['content'])\n",
    "    processed_articles.append({\"title\": article['title'], \"content\": processed_content})\n",
    "\n",
    "print(f\"Preprocessed {len(processed_articles)} articles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7fe09-fa9e-4a12-8e13-82fad3f1e5e7",
   "metadata": {},
   "source": [
    "## Κελί 5: Αποθήκευση επεξεργασμένων άρθρων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57703b0e-9220-47c9-829a-f43c3119feac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed articles saved to 'processed_articles.json'.\n"
     ]
    }
   ],
   "source": [
    "# Αποθήκευση των επεξεργασμένων άρθρων σε νέο αρχείο\n",
    "with open('processed_articles.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(processed_articles, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Preprocessed articles saved to 'processed_articles.json'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
