[
    {
        "title": "Main Page",
        "content": "William Robinson Brown (January 17, 1875 – August 4, 1955) was a corporate officer of the Brown Company and a breeder of Arabian horses. He advocated for sustainable forest management practices and his innovations became industry standards. He was influenced by the progressive movement, instituting employee benefits at the Brown Company that predated modern workers' compensation laws. He founded the Maynesboro Stud in 1912 with bloodstock from American breeders of Arabian horses, also importing horses from England, France, and Egypt. At its peak, Maynesboro was the largest Arabian horse farm in the United States. To prove the abilities of Arabians, he organized several endurance races, which his horses won three times. He served as the president of the Arabian Horse Club of America from 1918 until 1939. His 1929 book The Horse of the Desert is an authoritative work on Arabians. A Republican, he served as a presidential elector for New Hampshire in 1924. (Full article...)\n\nJanuary 17\n\nThe timeline of the Second Temple period in Jewish history begins with the end of the Babylonian captivity and the Persian conquest of the Babylonian Empire in 539 BCE. A new temple to replace the destroyed Solomon's Temple was built in Jerusalem by the returnees, and the Second Temple (model pictured) was finished around 516 BCE. Second Temple Judaism was centered around the religious leadership of the Second Temple, and lasted for six centuries. The Persians were largely tolerant of Judaism. Persian rule lasted for two centuries, but came to an end with the conquests of Macedonia under Alexander the Great in 332 BCE. Judea and the Eastern Mediterranean region came under Greek influence during the resulting Hellenistic period; Hellenistic Judaism blended both Greek and Jewish traditions. The Second Temple period came to an end with the First Jewish–Roman War of 66 to 73 CE. (Full list...)\n\nBenjamin Franklin (January 17, 1706 – April 17, 1790) was an American polymath: a writer, scientist, inventor, statesman, diplomat, printer, publisher and political philosopher. Among the most influential intellectuals of his time, Franklin was one of the Founding Fathers of the United States; a drafter and signer of the Declaration of Independence; and the first postmaster general. Franklin became a successful newspaper editor and printer in Philadelphia, the leading city in the colonies, publishing The Pennsylvania Gazette at age 23. He became wealthy publishing this and Poor Richard's Almanack, which he wrote under the pseudonym \"Richard Saunders\". As a scientist, his studies of electricity made him a major figure in the American Enlightenment and the history of physics. His inventions include the lightning rod, bifocals, glass harmonica and the Franklin stove. This 1778 portrait of Franklin was painted by Joseph Duplessis.\n\nPainting credit: Joseph Duplessis\nWikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:\n\nThis Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.\n"
    },
    {
        "title": "Mathematics",
        "content": "\n\nMathematics is a field of study that discovers and organizes methods, theories and theorems that are developed and proved for the needs of empirical sciences and mathematics itself. There are many areas of mathematics, which include number theory (the study of numbers), algebra (the study of formulas and related structures), geometry (the study of shapes and spaces that contain them), analysis (the study of continuous changes), and set theory (presently used as a foundation for all mathematics).\n\nMathematics involves the description and manipulation of abstract objects that consist of either abstractions from nature or—in modern mathematics—purely abstract entities that are stipulated to have certain properties, called axioms.  Mathematics uses pure reason to prove properties of objects, a proof consisting of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.[1]\n\nMathematics is essential in the natural sciences, engineering, medicine, finance, computer science, and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent of any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics) but often later find practical applications.[2][3]\n\nHistorically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements.[4] Since its beginning, mathematics was primarily divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra[a] and infinitesimal calculus were introduced as new fields. Since then, the interaction between mathematical innovations and scientific discoveries has led to a correlated increase in the development of both.[5] At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method,[6] which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than sixty first-level areas of mathematics.\n\n\nBefore the Renaissance, mathematics was divided into two main areas: arithmetic, regarding the manipulation of numbers, and geometry, regarding the study of shapes.[7] Some types of pseudoscience, such as numerology and astrology, were not then clearly distinguished from mathematics.[8]\n\nDuring the Renaissance, two more areas appeared. Mathematical notation led to algebra which, roughly speaking, consists of the study and the manipulation of formulas. Calculus, consisting of the two subfields differential calculus and integral calculus, is the study of continuous functions, which model the typically nonlinear relationships between varying quantities, as represented by variables. This division into four main areas—arithmetic, geometry, algebra, and calculus[9]—endured until the end of the 19th century. Areas such as celestial mechanics and solid mechanics were then studied by mathematicians, but now are considered as belonging to physics.[10] The subject of combinatorics has been studied for much of recorded history, yet did not become a separate branch of mathematics until the seventeenth century.[11]\n\nAt the end of the 19th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion of new areas of mathematics.[12][6] The 2020 Mathematics Subject Classification contains no less than sixty-three first-level areas.[13] Some of these areas correspond to the older division, as is true regarding number theory (the modern name for higher arithmetic) and geometry. Several other first-level areas have \"geometry\" in their names or are otherwise commonly considered part of geometry. Algebra and calculus do not appear as first-level areas but are respectively split into several first-level areas. Other first-level areas emerged during the 20th century or had not previously been considered as mathematics, such as mathematical logic and foundations.[14]\n\nNumber theory began with the manipulation of numbers, that is, natural numbers \n\n\n\n(\n\nN\n\n)\n,\n\n\n{\\displaystyle (\\mathbb {N} ),}\n\n and later expanded to integers \n\n\n\n(\n\nZ\n\n)\n\n\n{\\displaystyle (\\mathbb {Z} )}\n\n and rational numbers \n\n\n\n(\n\nQ\n\n)\n.\n\n\n{\\displaystyle (\\mathbb {Q} ).}\n\n Number theory was once called arithmetic, but nowadays this term is mostly used for numerical calculations.[15] Number theory dates back to ancient Babylon and probably China. Two prominent early number theorists were Euclid of ancient Greece and Diophantus of Alexandria.[16] The modern study of number theory in its abstract form is largely attributed to Pierre de Fermat and Leonhard Euler. The field came to full fruition with the contributions of Adrien-Marie Legendre and Carl Friedrich Gauss.[17]\n\nMany easily stated number problems have solutions that require sophisticated methods, often from across mathematics. A prominent example is Fermat's Last Theorem. This conjecture was stated in 1637 by Pierre de Fermat, but it was proved only in 1994 by Andrew Wiles, who used tools including scheme theory from algebraic geometry, category theory, and homological algebra.[18] Another example is Goldbach's conjecture, which asserts that every even integer greater than 2 is the sum of two prime numbers. Stated in 1742 by Christian Goldbach, it remains unproven despite considerable effort.[19]\n\nNumber theory includes several subareas, including analytic number theory, algebraic number theory, geometry of numbers (method oriented), diophantine equations, and transcendence theory (problem oriented).[14]\n\nGeometry is one of the oldest branches of mathematics. It started with empirical recipes concerning shapes, such as lines, angles and circles, which were developed mainly for the needs of surveying and architecture, but has since blossomed out into many other subfields.[20]\n\nA fundamental innovation was the ancient Greeks' introduction of the concept of proofs, which require that every assertion must be proved. For example, it is not sufficient to verify by measurement that, say, two lengths are equal; their equality must be proven via reasoning from previously accepted results (theorems) and a few basic statements. The basic statements are not subject to proof because they are self-evident (postulates), or are part of the definition of the subject of study (axioms). This principle, foundational for all mathematics, was first elaborated for geometry, and was systematized by Euclid around 300 BC in his book Elements.[21][22]\n\nThe resulting Euclidean geometry is the study of shapes and their arrangements constructed from lines, planes and circles in the Euclidean plane (plane geometry) and the three-dimensional Euclidean space.[b][20]\n\nEuclidean geometry was developed without change of methods or scope until the 17th century, when René Descartes introduced what is now called Cartesian coordinates. This constituted a major change of paradigm: Instead of defining real numbers as lengths of line segments (see number line), it allowed the representation of points using their coordinates, which are numbers. Algebra (and later, calculus) can thus be used to solve geometrical problems. Geometry was split into two new subfields: synthetic geometry, which uses purely geometrical methods, and analytic geometry, which uses coordinates systemically.[23]\n\nAnalytic geometry allows the study of curves unrelated to circles and lines. Such curves can be defined as the graph of functions, the study of which led to differential geometry. They can also be defined as implicit equations, often polynomial equations (which spawned algebraic geometry). Analytic geometry also makes it possible to consider Euclidean spaces of higher than three dimensions.[20]\n\nIn the 19th century, mathematicians discovered non-Euclidean geometries, which do not follow the parallel postulate. By questioning that postulate's truth, this discovery has been viewed as joining Russell's paradox in revealing the foundational crisis of mathematics. This aspect of the crisis was solved by systematizing the axiomatic method, and adopting that the truth of the chosen axioms is not a mathematical problem.[24][6] In turn, the axiomatic method allows for the study of various geometries obtained either by changing the axioms or by considering properties that do not change under specific transformations of the space.[25]\n\nToday's subareas of geometry include:[14]\n\nAlgebra is the art of manipulating equations and formulas. Diophantus (3rd century) and al-Khwarizmi (9th century) were the two main precursors of algebra.[27][28] Diophantus solved some equations involving unknown natural numbers by deducing new relations until he obtained the solution.[29] Al-Khwarizmi introduced systematic methods for transforming equations, such as moving a term from one side of an equation into the other side.[30] The term algebra is derived from the Arabic word al-jabr meaning 'the reunion of broken parts' that he used for naming one of these methods in the title of his main treatise.[31][32]\n\nAlgebra became an area in its own right only with François Viète (1540–1603), who introduced the use of variables for representing unknown or unspecified numbers.[33] Variables allow mathematicians to describe the operations that have to be done on the numbers represented using mathematical formulas.[34]\n\nUntil the 19th century, algebra consisted mainly of the study of linear equations (presently linear algebra), and polynomial equations in a single unknown, which were called algebraic equations (a term still in use, although it may be ambiguous). During the 19th century, mathematicians began to use variables to represent things other than numbers (such as matrices, modular integers, and geometric transformations), on which generalizations of arithmetic operations are often valid.[35] The concept of algebraic structure addresses this, consisting of a set whose elements are unspecified, of operations acting on the elements of the set, and rules that these operations must follow. The scope of algebra thus grew to include the study of algebraic structures. This object of algebra was called modern algebra or abstract algebra, as established by the influence and works of Emmy Noether.[36]\n\nSome types of algebraic structures have useful and often fundamental properties, in many areas of mathematics. Their study became autonomous parts of algebra, and include:[14]\n\nThe study of types of algebraic structures as mathematical objects is the purpose of universal algebra and category theory.[37] The latter applies to every mathematical structure (not only algebraic ones). At its origin, it was introduced, together with homological algebra for allowing the algebraic study of non-algebraic objects such as topological spaces; this particular area of application is called algebraic topology.[38]\n\nCalculus, formerly called infinitesimal calculus, was introduced independently and simultaneously by 17th-century mathematicians Newton and Leibniz.[39] It is fundamentally the study of the relationship of variables that depend on each other. Calculus was expanded in the 18th century by Euler with the introduction of the concept of a function and many other results.[40] Presently, \"calculus\" refers mainly to the elementary part of this theory, and \"analysis\" is commonly used for advanced parts.[41]\n\nAnalysis is further subdivided into real analysis, where variables represent real numbers, and complex analysis, where variables represent complex numbers. Analysis includes many subareas shared by other areas of mathematics which include:[14]\n\nDiscrete mathematics, broadly speaking, is the study of individual, countable mathematical objects. An example is the set of all integers.[42] Because the objects of study here are discrete, the methods of calculus and mathematical analysis do not directly apply.[c] Algorithms—especially their implementation and computational complexity—play a major role in discrete mathematics.[43]\n\nThe four color theorem and optimal sphere packing were two major problems of discrete mathematics solved in the second half of the 20th century.[44] The P versus NP problem, which remains open to this day, is also important for discrete mathematics, since its solution would potentially impact a large number of computationally difficult problems.[45]\n\nDiscrete mathematics includes:[14]\n\nThe two subjects of mathematical logic and set theory have belonged to mathematics since the end of the 19th century.[46][47] Before this period, sets were not considered to be mathematical objects, and logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians.[48]\n\nBefore Cantor's study of infinite sets, mathematicians were reluctant to consider actually infinite collections, and considered infinity to be the result of endless enumeration. Cantor's work offended many mathematicians not only by considering actually infinite sets[49] but by showing that this implies different sizes of infinity, per Cantor's diagonal argument. This led to the controversy over Cantor's set theory.[50] In the same period, various areas of mathematics concluded the former intuitive definitions of the basic mathematical objects were insufficient for ensuring mathematical rigour.[51]\n\nThis became the foundational crisis of mathematics.[52] It was eventually solved in mainstream mathematics by systematizing the axiomatic method inside a formalized set theory. Roughly speaking, each mathematical object is defined by the set of all similar objects and the properties that these objects must have.[12] For example, in Peano arithmetic, the natural numbers are defined by \"zero is a number\", \"each number has a unique successor\", \"each number but zero has a unique predecessor\", and some rules of reasoning.[53] This mathematical abstraction from reality is embodied in the modern philosophy of formalism, as founded by David Hilbert around 1910.[54]\n\nThe \"nature\" of the objects defined this way is a philosophical problem that mathematicians leave to philosophers, even if many mathematicians have opinions on this nature, and use their opinion—sometimes called \"intuition\"—to guide their study and proofs. The approach allows considering \"logics\" (that is, sets of allowed deducing rules), theorems, proofs, etc. as mathematical objects, and to prove theorems about them. For example, Gödel's incompleteness theorems assert, roughly speaking that, in every consistent formal system that contains the natural numbers, there are theorems that are true (that is provable in a stronger system), but not provable inside the system.[55] This approach to the foundations of mathematics was challenged during the first half of the 20th century by mathematicians led by Brouwer, who promoted intuitionistic logic, which explicitly lacks the law of excluded middle.[56][57]\n\nThese problems and debates led to a wide expansion of mathematical logic, with subareas such as model theory (modeling some logical theories inside other theories), proof theory, type theory, computability theory and computational complexity theory.[14] Although these aspects of mathematical logic were introduced before the rise of computers, their use in compiler design, formal verification, program analysis, proof assistants and other aspects of computer science, contributed in turn to the expansion of these logical theories.[58]\n\nThe field of statistics is a mathematical application that is employed for the collection and processing of data samples, using procedures based on mathematical methods especially probability theory. Statisticians generate data with random sampling or randomized experiments.[60]\n\nStatistical theory studies decision problems such as minimizing the risk (expected loss) of a statistical action, such as using a procedure in, for example, parameter estimation, hypothesis testing, and selecting the best. In these traditional areas of mathematical statistics, a statistical-decision problem is formulated by minimizing an objective function, like expected loss or cost, under specific constraints. For example, designing a survey often involves minimizing the cost of estimating a population mean with a given level of confidence.[61] Because of its use of optimization, the mathematical theory of statistics overlaps with other decision sciences, such as operations research, control theory, and mathematical economics.[62]\n\nComputational mathematics is the study of mathematical problems that are typically too large for human, numerical capacity.[63][64] Numerical analysis studies methods for problems in analysis using functional analysis and approximation theory; numerical analysis broadly includes the study of approximation and discretization with special focus on rounding errors.[65] Numerical analysis and, more broadly, scientific computing also study non-analytic topics of mathematical science, especially algorithmic-matrix-and-graph theory. Other areas of computational mathematics include computer algebra and symbolic computation.\n\nThe word mathematics comes from the Ancient Greek word máthēma (μάθημα), meaning 'something learned, knowledge, mathematics', and the derived expression mathēmatikḗ tékhnē (μαθηματικὴ τέχνη), meaning 'mathematical science'. It entered the English language during the Late Middle English period through French and Latin.[66]\n\nSimilarly, one of the two main schools of thought in Pythagoreanism was known as the mathēmatikoi (μαθηματικοί)—which at the time meant \"learners\" rather than \"mathematicians\" in the modern sense. The Pythagoreans were likely the first to constrain the use of the word to just the study of arithmetic and geometry. By the time of Aristotle (384–322 BC) this meaning was fully established.[67]\n\nIn Latin and English, until around 1700, the term mathematics more commonly meant \"astrology\" (or sometimes \"astronomy\") rather than \"mathematics\"; the meaning gradually changed to its present one from about 1500 to 1800. This change has resulted in several mistranslations: For example, Saint Augustine's warning that Christians should beware of mathematici, meaning \"astrologers\", is sometimes mistranslated as a condemnation of mathematicians.[68]\n\nThe apparent plural form in English goes back to the Latin neuter plural mathematica (Cicero), based on the Greek plural ta mathēmatiká (τὰ μαθηματικά) and means roughly \"all things mathematical\", although it is plausible that English borrowed only the adjective mathematic(al) and formed the noun mathematics anew, after the pattern of physics and metaphysics, inherited from Greek.[69] In English, the noun mathematics takes a singular verb. It is often shortened to maths[70] or, in North America, math.[71]\n\nIn addition to recognizing how to count physical objects, prehistoric peoples may have also known how to count abstract quantities, like time—days, seasons, or years.[72][73] Evidence for more complex mathematics does not appear until around 3000 BC, when the Babylonians and Egyptians began using arithmetic, algebra, and geometry for taxation and other financial calculations, for building and construction, and for astronomy.[74] The oldest mathematical texts from Mesopotamia and Egypt are from 2000 to 1800 BC.[75] Many early texts mention Pythagorean triples and so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical concept after basic arithmetic and geometry. It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time.[76]\n\nIn the 6th century BC, Greek mathematics began to emerge as a distinct discipline and some Ancient Greeks such as the Pythagoreans appeared to have considered it a subject in its own right.[77] Around 300 BC, Euclid organized mathematical knowledge by way of postulates and first principles, which evolved into the axiomatic method that is used in mathematics today, consisting of definition, axiom, theorem, and proof.[78] His book, Elements, is widely considered the most successful and influential textbook of all time.[79] The greatest mathematician of antiquity is often held to be Archimedes (c. 287 – c. 212 BC) of Syracuse.[80] He developed formulas for calculating the surface area and volume of solids of revolution and used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[81] Other notable achievements of Greek mathematics are conic sections (Apollonius of Perga, 3rd century BC),[82] trigonometry (Hipparchus of Nicaea, 2nd century BC),[83] and the beginnings of algebra (Diophantus, 3rd century AD).[84]\n\nThe Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics.[85] Other notable developments of Indian mathematics include the modern definition and approximation of sine and cosine, and an early form of infinite series.[86][87]\n\nDuring the Golden Age of Islam, especially during the 9th and 10th centuries, mathematics saw many important innovations building on Greek mathematics. The most notable achievement of Islamic mathematics was the development of algebra. Other achievements of the Islamic period include advances in spherical trigonometry and the addition of the decimal point to the Arabic numeral system.[88] Many notable mathematicians from this period were Persian, such as Al-Khwarizmi, Omar Khayyam and Sharaf al-Dīn al-Ṭūsī.[89] The Greek and Arabic mathematical texts were in turn translated to Latin during the Middle Ages and made available in Europe.[90]\n\nDuring the early modern period, mathematics began to develop at an accelerating pace in Western Europe, with innovations that revolutionized mathematics, such as the introduction of variables and symbolic notation by François Viète (1540–1603), the introduction of logarithms by John Napier in 1614, which greatly simplified numerical calculations, especially for astronomy and marine navigation, the introduction of coordinates by René Descartes (1596–1650) for reducing geometry to algebra, and the development of calculus by Isaac Newton (1643–1727) and Gottfried Leibniz (1646–1716). Leonhard Euler (1707–1783), the most notable mathematician of the 18th century, unified these innovations into a single corpus with a standardized terminology, and completed them with the discovery and the proof of numerous theorems.[91]\n\nPerhaps the foremost mathematician of the 19th century was the German mathematician Carl Gauss, who made numerous contributions to fields such as algebra, analysis, differential geometry, matrix theory, number theory, and statistics.[92] In the early 20th century, Kurt Gödel transformed mathematics by publishing his incompleteness theorems, which show in part that any consistent axiomatic system—if powerful enough to describe arithmetic—will contain true propositions that cannot be proved.[55]\n\nMathematics has since been greatly extended, and there has been a fruitful interaction between mathematics and science, to the benefit of both. Mathematical discoveries continue to be made to this very day. According to Mikhail B. Sevryuk, in the January 2006 issue of the Bulletin of the American Mathematical Society, \"The number of papers and books included in the Mathematical Reviews (MR) database since 1940 (the first year of operation of MR) is now more than 1.9 million, and more than 75 thousand items are added to the database each year. The overwhelming majority of works in this ocean contain new mathematical theorems and their proofs.\"[93]\n\nMathematical notation is widely used in science and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way. This notation consists of symbols used for representing operations, unspecified numbers, relations and any other mathematical objects, and then assembling them into expressions and formulas.[94] More precisely, numbers and other mathematical objects are represented by symbols called variables, which are generally Latin or Greek letters, and often include subscripts. Operation and relations are generally represented by specific symbols or glyphs,[95] such as + (plus), × (multiplication), \n\n\n\n∫\n\n\n{\\textstyle \\int }\n\n (integral), = (equal), and < (less than).[96] All these symbols are generally grouped according to specific rules to form expressions and formulas.[97] Normally, expressions and formulas do not appear alone, but are included in sentences of the current language, where expressions play the role of noun phrases and formulas play the role of clauses.\n\nMathematics has developed a rich terminology covering a broad range of fields that study the properties of various abstract, idealized objects and how they interact. It is based on rigorous definitions that provide a standard foundation for communication. An axiom or postulate is a mathematical statement that is taken to be true without need of proof. If a mathematical statement has yet to be proven (or disproven), it is termed a conjecture. Through a series of rigorous arguments employing deductive reasoning, a statement that is proven to be true becomes a theorem. A specialized theorem that is mainly used to prove another theorem is called a lemma. A proven instance that forms part of a more general finding is termed a corollary.[98]\n\nNumerous technical terms used in mathematics are neologisms, such as polynomial and homeomorphism.[99] Other technical terms are words of the common language that are used in an accurate meaning that may differ slightly from their common meaning. For example, in mathematics, \"or\" means \"one, the other or both\", while, in common language, it is either ambiguous or means \"one or the other but not both\" (in mathematics, the latter is called \"exclusive or\"). Finally, many mathematical terms are common words that are used with a completely different meaning.[100] This may lead to sentences that are correct and true mathematical assertions, but appear to be nonsense to people who do not have the required background. For example, \"every free module is flat\" and \"a field is always a ring\".\n\nMathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws.[101] The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model.[102] Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used.[103] For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model.[104]\n\nThere is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation.[105] In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied \"durch planmässiges Tattonieren\" (through systematic experimentation).[106] However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence.[107][108][109][110]\n\nUntil the 19th century, the development of mathematics in the West was mainly motivated by the needs of technology and science, and there was no clear distinction between pure and applied mathematics.[111] For example, the natural numbers and arithmetic were introduced for the need of counting, and geometry was motivated by surveying, architecture and astronomy. Later, Isaac Newton introduced infinitesimal calculus for explaining the movement of the planets with his law of gravitation. Moreover, most mathematicians were also scientists, and many scientists were also mathematicians.[112] However, a notable exception occurred with the tradition of pure mathematics in Ancient Greece.[113] The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.[114]\n\nIn the 19th century, mathematicians such as Karl Weierstrass and Richard Dedekind increasingly focused their research on internal problems, that is, pure mathematics.[111][115] This led to split mathematics into pure mathematics and applied mathematics, the latter being often considered as having a lower value among mathematical purists. However, the lines between the two are frequently blurred.[116]\n\nThe aftermath of World War II led to a surge in the development of applied mathematics in the US and elsewhere.[117][118] Many of the theories developed for applications were found interesting from the point of view of pure mathematics, and many results of pure mathematics were shown to have applications outside mathematics; in turn, the study of these applications may give new insights on the \"pure theory\".[119][120]\n\nAn example of the first case is the theory of distributions, introduced by Laurent Schwartz for validating computations done in quantum mechanics, which became immediately an important tool of (pure) mathematical analysis.[121] An example of the second case is the decidability of the first-order theory of the real numbers, a problem of pure mathematics that was proved true by Alfred Tarski, with an algorithm that is impossible to implement because of a computational complexity that is much too high.[122] For getting an algorithm that can be implemented and can solve systems of polynomial equations and inequalities, George Collins introduced the cylindrical algebraic decomposition that became a fundamental tool in real algebraic geometry.[123]\n\nIn the present day, the distinction between pure and applied mathematics is more a question of personal research aim of mathematicians than a division of mathematics into broad areas.[124][125] The Mathematics Subject Classification has a section for \"general applied mathematics\" but does not mention \"pure mathematics\".[14] However, these terms are still used in names of some university departments, such as at the Faculty of Mathematics at the University of Cambridge.\n\nThe unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner.[3] It is the fact that many mathematical theories (even the \"purest\") have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced.[126] Examples of unexpected applications of mathematical theories can be found in many areas of mathematics.\n\nA notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem.[127] A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It was almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses.[128]\n\nIn the 19th century, the internal development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of special relativity is a non-Euclidean space of dimension four, and spacetime of general relativity is a (curved) manifold of dimension four.[129][130]\n\nA striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon \n\n\n\n\nΩ\n\n−\n\n\n.\n\n\n{\\displaystyle \\Omega ^{-}.}\n\n In both cases, the equations of the theories had unexplained solutions, which led to conjecture of the existence of an unknown particle, and the search for these particles. In both cases, these particles were discovered a few years later by specific experiments.[131][132][133]\n\nMathematics and physics have influenced each other over their modern history. Modern physics uses mathematics abundantly,[134] and is also considered to be the motivation of major mathematical developments.[135]\n\nComputing is closely related to mathematics in several ways.[136] Theoretical computer science is considered to be mathematical in nature.[137] Communication technologies apply branches of mathematics that may be very old (e.g., arithmetic), especially with respect to transmission security, in cryptography and coding theory. Discrete mathematics is useful in many areas of computer science, such as complexity theory, information theory, and graph theory.[138] In 1998, the Kepler conjecture on sphere packing seemed to also be partially proven by computer.[139]\n\nBiology uses probability extensively in fields such as ecology or neurobiology.[140]  Most discussion of probability centers on the concept of evolutionary fitness.[140] Ecology heavily uses modeling to simulate population dynamics,[140][141] study ecosystems such as the predator-prey model, measure pollution diffusion,[142] or to assess climate change.[143] The dynamics of a population can be modeled by coupled differential equations, such as the Lotka–Volterra equations.[144]\n\nStatistical hypothesis testing, is run on data from clinical trials to determine whether a new treatment works.[145] Since the start of the 20th century, chemistry has used computing to model molecules in three dimensions.[146]\n\nStructural geology and climatology use probabilistic models to predict the risk of natural catastrophes.[147] Similarly, meteorology, oceanography, and planetology also use mathematics due to their heavy use of models.[148][149][150]\n\nAreas of mathematics used in the social sciences include probability/statistics and differential equations. These are used in linguistics, economics, sociology,[151] and psychology.[152]\n\nOften the fundamental postulate of mathematical economics is that of the rational individual actor – Homo economicus (lit. 'economic man').[153] In this model, the individual seeks to maximize their self-interest,[153] and always makes optimal choices using perfect information.[154] This atomistic view of economics allows it to relatively easily mathematize its thinking, because individual calculations are transposed into mathematical calculations. Such mathematical modeling allows one to probe economic mechanisms. Some reject or criticise the concept of Homo economicus. Economists note that real people have limited information, make poor choices and care about fairness, altruism, not just personal gain.[155]\n\nWithout mathematical modeling, it is hard to go beyond statistical observations or untestable speculation. Mathematical modeling allows economists to create structured frameworks to test hypotheses and analyze complex interactions. Models provide clarity and precision, enabling the translation of theoretical concepts into quantifiable predictions that can be tested against real-world data.[156]\n\nAt the start of the 20th century, there was a development to express historical movements in formulas. In 1922, Nikolai Kondratiev discerned the ~50-year-long Kondratiev cycle, which explains phases of economic growth or crisis.[157] Towards the end of the 19th century, mathematicians extended their analysis into geopolitics.[158] Peter Turchin developed cliodynamics since the 1990s.[159]\n\nMathematization of the social sciences is not without risk. In the controversial book Fashionable Nonsense (1997), Sokal and Bricmont denounced the unfounded or abusive use of scientific terminology, particularly from mathematics or physics, in the social sciences.[160] The study of complex systems (evolution of unemployment, business capital, demographic evolution of a population, etc.) uses mathematical knowledge. However, the choice of counting criteria, particularly for unemployment, or of models, can be subject to controversy.[161][162]\n\nThe connection between mathematics and material reality has led to philosophical debates since at least the time of Pythagoras. The ancient philosopher Plato argued that abstractions that reflect material reality have themselves a reality that exists outside space and time. As a result, the philosophical view that mathematical objects somehow exist on their own in abstraction is often referred to as Platonism. Independently of their possible philosophical opinions, modern mathematicians may be generally considered as Platonists, since they think of and talk of their objects of study as real objects.[163]\n\nArmand Borel summarized this view of mathematics reality as follows, and provided quotations of G. H. Hardy, Charles Hermite, Henri Poincaré and Albert Einstein that support his views.[131]\n\n Something becomes objective (as opposed to \"subjective\") as soon as we are convinced that it exists in the minds of others in the same form as it does in ours and that we can think about it and discuss it together.[164] Because the language of mathematics is so precise, it is ideally suited to defining concepts for which such a consensus exists. In my opinion, that is sufficient to provide us with a feeling of an objective existence, of a reality of mathematics ...\nNevertheless, Platonism and the concurrent views on abstraction do not explain the unreasonable effectiveness of mathematics.[165]\n\nThere is no general consensus about the definition of mathematics or its epistemological status—that is, its place inside knowledge. A great many professional mathematicians take no interest in a definition of mathematics, or consider it undefinable. There is not even consensus on whether mathematics is an art or a science. Some just say, \"mathematics is what mathematicians do\".[166][167] A common approach is to define mathematics by its object of study.[168][169][170][171]\n\nAristotle defined mathematics as \"the science of quantity\" and this definition prevailed until the 18th century. However, Aristotle also noted a focus on quantity alone may not distinguish mathematics from sciences like physics; in his view, abstraction and studying quantity as a property \"separable in thought\" from real instances set mathematics apart.[172] In the 19th century, when mathematicians began to address topics—such as infinite sets—which have no clear-cut relation to physical reality, a variety of new definitions were given.[173] With the large number of new areas of mathematics that have appeared since the beginning of the 20th century, defining mathematics by its object of study has become increasingly difficult.[174] For example, in lieu of a definition, Saunders Mac Lane in Mathematics, form and function summarizes the basics of several areas of mathematics, emphasizing their inter-connectedness, and observes:[175]\n\nthe development of Mathematics provides a tightly connected network of formal rules, concepts, and systems.  Nodes of this network are closely bound to procedures useful in human activities and to questions arising in science. The transition from activities to the formal Mathematical systems is guided by a variety of general insights and ideas.\nAnother approach for defining mathematics is to use its methods. For example, an area of study is often qualified as mathematics as soon as one can prove theorems—assertions whose validity relies on a proof, that is, a purely-logical deduction.[d][176][failed verification]\n\nMathematical reasoning requires rigor. This means that the definitions must be absolutely unambiguous and the proofs must be reducible to a succession of applications of inference rules,[e] without any use of empirical evidence and intuition.[f][177] Rigorous reasoning is not specific to mathematics, but, in mathematics, the standard of rigor is much higher than elsewhere. Despite mathematics' concision, rigorous proofs can require hundreds of pages to express, such as the 255-page Feit–Thompson theorem.[g] The emergence of computer-assisted proofs has allowed proof lengths to further expand.[h][178]  The result of this trend is a philosophy of the quasi-empiricist proof that can not be considered infallible, but has a probability attached to it.[6]\n\nThe concept of rigor in mathematics dates back to ancient Greece, where their society encouraged logical, deductive reasoning. However, this rigorous approach would tend to discourage exploration of new approaches, such as irrational numbers and concepts of infinity. The method of demonstrating rigorous proof was enhanced in the sixteenth century through the use of symbolic notation. In the 18th century, social transition led to mathematicians earning their keep through teaching, which led to more careful thinking about the underlying concepts of mathematics. This produced more rigorous approaches, while transitioning from geometric methods to algebraic and then arithmetic proofs.[6]\n\nAt the end of the 19th century, it appeared that the definitions of the basic concepts of mathematics were not accurate enough for avoiding paradoxes (non-Euclidean geometries and Weierstrass function) and contradictions (Russell's paradox). This was solved by the inclusion of axioms with the apodictic inference rules of mathematical theories; the re-introduction of axiomatic method pioneered by the ancient Greeks.[6] It results that \"rigor\" is no more a relevant concept in mathematics, as a proof is either correct or erroneous, and a \"rigorous proof\" is simply a pleonasm. Where a special concept of rigor comes into play is in the socialized aspects of a proof, wherein it may be demonstrably refuted by other mathematicians. After a proof has been accepted for many years or even decades, it can then be considered as reliable.[179]\n\nNevertheless, the concept of \"rigor\" may remain useful for teaching to beginners what is a mathematical proof.[180]\n\nMathematics has a remarkable ability to cross cultural boundaries and time periods. As a human activity, the practice of mathematics has a social side, which includes education, careers, recognition, popularization, and so on. In education, mathematics is a core part of the curriculum and forms an important element of the STEM academic disciplines. Prominent careers for professional mathematicians include math teacher or professor, statistician, actuary, financial analyst, economist, accountant, commodity trader, or computer consultant.[181]\n\nArchaeological evidence shows that instruction in mathematics occurred as early as the second millennium BCE in ancient Babylonia.[182] Comparable evidence has been unearthed for scribal mathematics training in the ancient Near East and then for the Greco-Roman world starting around 300 BCE.[183] The oldest known mathematics textbook is the Rhind papyrus, dated from c. 1650 BCE in Egypt.[184] Due to a scarcity of books, mathematical teachings in ancient India were communicated using memorized oral tradition since the Vedic period (c. 1500 – c. 500 BCE).[185] In Imperial China during the Tang dynasty (618–907 CE), a mathematics curriculum was adopted for the civil service exam to join the state bureaucracy.[186]\n\nFollowing the Dark Ages, mathematics education in Europe was provided by religious schools as part of the Quadrivium. Formal instruction in pedagogy began with Jesuit schools in the 16th and 17th century. Most mathematical curricula remained at a basic and practical level until the nineteenth century, when it began to flourish in France and Germany. The oldest journal addressing instruction in mathematics was L'Enseignement Mathématique, which began publication in 1899.[187] The Western advancements in science and technology led to the establishment of centralized education systems in many nation-states, with mathematics as a core component—initially for its military applications.[188] While the content of courses varies, in the present day nearly all countries teach mathematics to students for significant amounts of time.[189]\n\nDuring school, mathematical capabilities and positive expectations have a strong association with career interest in the field. Extrinsic factors such as feedback motivation by teachers, parents, and peer groups can influence the level of interest in mathematics.[190] Some students studying math may develop an apprehension or fear about their performance in the subject. This is known as math anxiety or math phobia, and is considered the most prominent of the disorders impacting academic performance. Math anxiety can develop due to various factors such as parental and teacher attitudes, social stereotypes, and personal traits. Help to counteract the anxiety can come from changes in instructional approaches, by interactions with parents and teachers, and by tailored treatments for the individual.[191]\n\nThe validity of a mathematical theorem relies only on the rigor of its proof, which could theoretically be done automatically by a computer program. This does not mean that there is no place for creativity in a mathematical work. On the contrary, many important mathematical results (theorems) are solutions of problems that other mathematicians failed to solve, and the invention of a way for solving them may be a fundamental way of the solving process.[192][193] An extreme example is Apery's theorem: Roger Apery provided only the ideas for a proof, and the formal proof was given only several months later by three other mathematicians.[194]\n\nCreativity and rigor are not the only psychological aspects of the activity of mathematicians. Some mathematicians can see their activity as a game, more specifically as solving puzzles.[195] This aspect of mathematical activity is emphasized in recreational mathematics.\n\nMathematicians can find an aesthetic value to mathematics. Like beauty, it is hard to define, it is commonly related to elegance, which involves qualities like simplicity, symmetry, completeness, and generality. G. H. Hardy in A Mathematician's Apology expressed the belief that the aesthetic considerations are, in themselves, sufficient to justify the study of pure mathematics. He also identified other criteria such as significance, unexpectedness, and inevitability, which contribute to mathematical aesthetics.[196] Paul Erdős expressed this sentiment more ironically by speaking of \"The Book\", a supposed divine collection of the most beautiful proofs. The 1998 book Proofs from THE BOOK, inspired by Erdős, is a collection of particularly succinct and revelatory mathematical arguments. Some examples of particularly elegant results included are Euclid's proof that there are infinitely many prime numbers and the fast Fourier transform for harmonic analysis.[197]\n\nSome feel that to consider mathematics a science is to downplay its artistry and history in the seven traditional liberal arts.[198] One way this difference of viewpoint plays out is in the philosophical debate as to whether mathematical results are created (as in art) or discovered (as in science).[131] The popularity of recreational mathematics is another sign of the pleasure many find in solving mathematical questions.\n\nNotes that sound well together to a Western ear are sounds whose fundamental frequencies of vibration are in simple ratios. For example, an octave doubles the frequency and a perfect fifth multiplies it by \n\n\n\n\n\n3\n2\n\n\n\n\n{\\displaystyle {\\frac {3}{2}}}\n\n.[199][200]\n\nHumans, as well as some other animals, find symmetric patterns to be more beautiful.[201] Mathematically, the symmetries of an object form a group known as the symmetry group.[202] For example, the group underlying mirror symmetry is the cyclic group of two elements, \n\n\n\n\nZ\n\n\n/\n\n2\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} /2\\mathbb {Z} }\n\n. A Rorschach test is a figure invariant by this symmetry,[203] as are butterfly and animal bodies more generally (at least on the surface).[204] Waves on the sea surface possess translation symmetry: moving one's viewpoint by the distance between wave crests does not change one's view of the sea.[205] Fractals possess self-similarity.[206][207]\n\nPopular mathematics is the act of presenting mathematics without technical terms.[208] Presenting mathematics may be hard since the general public suffers from mathematical anxiety and mathematical objects are highly abstract.[209] However, popular mathematics writing can overcome this by using applications or cultural links.[210] Despite this, mathematics is rarely the topic of popularization in printed or televised media.\n\nThe most prestigious award in mathematics is the Fields Medal,[211][212] established in 1936 and awarded every four years (except around World War II) to up to four individuals.[213][214] It is considered the mathematical equivalent of the Nobel Prize.[214]\n\nOther prestigious mathematics awards include:[215]\n\nA famous list of 23 open problems, called \"Hilbert's problems\", was compiled in 1900 by German mathematician David Hilbert.[223] This list has achieved great celebrity among mathematicians,[224] and at least thirteen of the problems (depending how some are interpreted) have been solved.[223]\n\nA new list of seven important problems, titled the \"Millennium Prize Problems\", was published in 2000. Only one of them, the Riemann hypothesis, duplicates one of Hilbert's problems. A solution to any of these problems carries a 1 million dollar reward.[225] To date, only one of these problems, the Poincaré conjecture, has been solved by the Russian mathematician Grigori Perelman.[226]\n"
    },
    {
        "title": "Mathematics (disambiguation)",
        "content": "Mathematics is a field of knowledge.\n\nMathematics may also refer to:\n"
    },
    {
        "title": "Math (disambiguation)",
        "content": "Math or maths  is an abbreviation of mathematics.\n\nMath or Maths may also refer to:\n"
    },
    {
        "title": "History of mathematics",
        "content": "\n\nThe history of mathematics deals with the origin of discoveries in mathematics and the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, followed closely by Ancient Egypt and the Levantine state of Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the field of astronomy to record time and formulate calendars.\n\nThe earliest mathematical texts available are from Mesopotamia and Egypt – Plimpton 322 (Babylonian c. 2000 – 1900 BC),[2] the Rhind Mathematical Papyrus (Egyptian c. 1800 BC)[3] and the Moscow Mathematical Papyrus (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples, so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.\n\nThe study of mathematics as a \"demonstrative discipline\" began in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek μάθημα (mathema), meaning \"subject of instruction\".[4] Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics.[5] The ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers.[6][7] The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī.[8][9] Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations.[10] Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.\n\nMany Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation.[11] Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century and following discoveries of German mathematicians like Carl Friedrich Gauss and David Hilbert.\n\nThe origins of mathematical thought lie in the concepts of number, patterns in nature, magnitude, and form.[12] Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the \"number\" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between \"one\", \"two\", and \"many\", but not of numbers larger than two.[12]\n\nThe Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a tally of the earliest known demonstration of sequences of prime numbers[13][failed verification] or a six-month lunar calendar.[14] Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that \"no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10.\"[15] The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.[16]\n\nPredynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design.[17] All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.[18]\n\nBabylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity.[19] The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period).[20] It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.\n\nIn contrast to the sparsity of sources in Egyptian mathematics, knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s.[21] Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.[22]\n\nThe earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC that was chiefly concerned with administrative/financial counting, such as grain allotments, workers, weights of silver, or even liquids, among other things.[23] From around 2500 BC onward, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.[24]\n\nBabylonian mathematics were written using a sexagesimal (base-60) numeral system.[21] From this derives the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 × 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is thought the sexagesimal system was initially used by Sumerian scribes because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30,[21] and for scribes (doling out the aforementioned grain allotments, recording weights of silver, etc.) being able to easily calculate by hand was essential, and so a sexagesimal system is pragmatically easier to calculate by hand with; however, there is the possibility that using a sexagesimal system was an ethno-linguistic phenomenon (that might not ever be known), and not a mathematical/practical decision.[25] Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different from multiplying integers, similar to modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computational accuracy; for example, the Babylonian tablet YBC 7289 gives an approximation of √2 accurate to five decimal places.[26] The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context.[20] By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions.[20] This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.[20]\n\nOther topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular numbers, and their reciprocal pairs.[27] The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time.[28] Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem.[29] However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.[22]\n\nEgyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars. Archaeological evidence has suggested that the Ancient Egyptian counting system had origins in Sub-Saharan Africa.[30] Also, fractal geometry designs which are widespread among Sub-Saharan African cultures are also found in Egyptian architecture and cosmological signs.[31]\n\nThe most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC.[32] It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge,[33] including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6).[34] It also shows how to solve first order linear equations[35] as well as arithmetic and geometric series.[36]\n\nAnother significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC.[37] It consists of what are today called word problems or story problems, which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).\n\nFinally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.[38]\n\nGreek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD.[39] Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.[40]\n\nGreek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.[41]\n\nGreek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.\n\nThales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed.[42] Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was \"All is number\".[43] It was the Pythagoreans who coined the term \"mathematics\", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem,[44] though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers.[45][46] Although he was preceded by the Babylonians, Indians and the Chinese,[47] the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum).[48] The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the mensa Pythagorica.[49]\n\nPlato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others.[50] His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus (c. 390 - c. 340 BC), came.[51] Plato also discussed the foundations of mathematics,[52] clarified some of the definitions (e.g. that of a line as \"breadthless length\"), and reorganized the assumptions.[53] The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.[51]\n\nEudoxus developed the method of exhaustion, a precursor of modern integration[54] and a theory of ratios that avoided the problem of incommensurable magnitudes.[55] The former allowed the calculations of areas and volumes of curvilinear figures,[56] while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.[57]\n\nIn the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria.[59] It was there that Euclid (c. 300 BC) taught, and wrote the Elements, widely considered the most successful and influential textbook of all time.[1] The Elements introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[60] The Elements was known to all educated people in the West up through the middle of the 20th century and its contents are still taught in geometry classes today.[61] In addition to the familiar theorems of Euclidean geometry, the Elements was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry,[60] including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.[62]\n\nArchimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity,[63] used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus.[64] He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, 3+⁠10/71⁠ < π < 3+⁠10/70⁠.[65] He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid),[64] and an ingenious method of exponentiation for expressing very large numbers.[66] While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles.[67] He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.[68]\n\nApollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone.[69] He also coined the terminology in use today for conic sections, namely parabola (\"place beside\" or \"comparison\"), \"ellipse\" (\"deficiency\"), and \"hyperbola\" (\"a throw beyond\").[70] His work Conics is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton.[71] While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.[72]\n\nAround the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers.[73] The 3rd century BC is generally regarded as the \"Golden Age\" of Greek mathematics, with advances in pure mathematics henceforth in relative decline.[74] Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers.[74] Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle.[75] Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots.[76] Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem.[77] The most complete and influential trigonometric work of antiquity is the Almagest of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years.[78] Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.[79]\n\nFollowing a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the \"Silver Age\" of Greek mathematics.[80] During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as \"Diophantine analysis\".[81] The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the Arithmetica, a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations.[82] The Arithmetica had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the Arithmetica (that of dividing a square into two squares).[83] Diophantus also made significant advances in notation, the Arithmetica being the first instance of algebraic symbolism and syncopation.[82]\n\nAmong the last great Greek mathematicians is Pappus of Alexandria (4th century AD). He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His Collection is a major source of knowledge on Greek mathematics as most of it has survived.[84] Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.\n\nThe first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library[citation needed] and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed.[85] Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius.[86] Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics. The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Hagia Sophia.[87] Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.[88]\n\nAlthough ethnic Greek mathematicians continued under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison.[89][90] Ancient Romans such as Cicero (106–43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks.[91] It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.[92]\n\nUsing calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury.[93] Siculus Flaccus, one of the Roman gromatici (i.e. land surveyor), wrote the Categories of Fields, which aided Roman surveyors in measuring the surface areas of allotted lands and territories.[94] Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns.[95] Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.[96][97]\n\nThe creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year.[98] In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February.[99] This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100–44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle.[100] This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (r. 1572–1585), virtually the same solar calendar used in modern times as the international standard calendar.[101]\n\nAt roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC – c. 15 BC).[102] The device was used at least until the reign of emperor Commodus (r. 177 – 192 AD), but its design seems to have been lost until experiments were made during the 15th century in Western Europe.[103] Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.[104]\n\nAn analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development.[105] The oldest extant mathematical text from China is the Zhoubi Suanjing (周髀算經), variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable.[106] However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.[47]\n\nOf particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called \"rod numerals\" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten.[107] Thus, the number 123 would be written using the symbol for \"1\", followed by the symbol for \"100\", then the symbol for \"2\" followed by the symbol for \"10\", followed by the symbol for \"3\". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system.[108] Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the suan pan, or Chinese abacus. The date of the invention of the suan pan is not certain, but the earliest written mention dates from AD 190, in Xu Yue's Supplementary Notes on the Art of Figures.\n\nThe oldest extant work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The Mo Jing described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well.[109] It also defined the concepts of circumference, diameter, radius, and volume.[110]\n\nIn 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is The Nine Chapters on the Mathematical Art, the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles.[106] It created mathematical proof for the Pythagorean theorem,[111] and a mathematical formula for Gaussian elimination.[112] The treatise also provides values of π,[106] which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78–139) approximated pi as 3.1724,[113] as well as 3.162 by taking the square root of 10.[114][115] Liu Hui commented on the Nine Chapters in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159).[116][117] Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (between 3.1415926 and 3.1415927), which remained the most accurate value of π for almost the next 1000 years.[116][118] He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.[119]\n\nThe high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960–1279), with the development of Chinese algebra. The most important text from that period is the Precious Mirror of the Four Elements by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method.[116] The Precious Mirror also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100.[120] The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).[120]\n\nEven after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.[120]\n\nJapanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere.[121] Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368–1644).[122] For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers.[123] Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.[124]\n\nThe mathematics that developed in Japan during the Edo period (1603-1887) is independent of Western mathematics; To this period belongs the mathematician Seki Takakazu, of great influence, for example, in the development of wasan (traditional Japanese mathematics), and whose discoveries (in areas such as integral calculus), are almost simultaneous with contemporary European mathematicians such as Gottfried Leibniz.\n\nJapanese mathematics of this period is inspired by Chinese mathematics and is oriented towards essentially geometric problems. On wooden tablets called sangaku, \"geometric enigmas\" are proposed and solved; That's where, for example, Soddy's hexlet theorem comes from.\n\nThe earliest civilization on the Indian subcontinent is the Indus Valley civilization (mature second phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.[126]\n\nThe oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD),[127] appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others.[128] As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual.[127] The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π.[129][130][a] In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem.[130] All of these results are present in Babylonian mathematics, indicating Mesopotamian influence.[127] It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.[127]\n\nPāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar.[131] His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion.[132] Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system.[133][134] His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called mātrāmeru).[135]\n\nThe next significant mathematical documents from India after the Sulba Sutras are the Siddhantas, astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence.[136] They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry.[137] Through a series of translation errors, the words \"sine\" and \"cosine\" derive from the Sanskrit \"jiya\" and \"kojiya\".[137]\n\nAround 500 AD, Aryabhata wrote the Aryabhatiya, a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology.[138] It is in the Aryabhatiya that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the Aryabhatiya as a \"mix of common pebbles and costly crystals\".[139]\n\nIn the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in Brahma-sphuta-siddhanta, he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system.[140] It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.[citation needed]\n\nIn the 12th century, Bhāskara II,[141] who lived in southern India, wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, the mean value theorem and the derivative of the sine function although he did not develop the notion of a derivative.[142][143] In the 14th century, Narayana Pandita completed his Ganita Kaumudi.[144]\n\nAlso in the 14th century, Madhava of Sangamagrama, the founder of the Kerala School of Mathematics, found the Madhava–Leibniz series and obtained from it a transformed series, whose first 21 terms he used to compute the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions.[145] In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the Yukti-bhāṣā.[146][147] It has been argued that certain ideas of calculus like infinite series and taylor series of some trigonometry functions, were transmitted to Europe in the 16th century[6] via Jesuit missionaries and traders who were active around the ancient port of Muziris at the time and, as a result, directly influenced later European developments in analysis and calculus.[148] However, other scholars argue that the Kerala School did not formulate a systematic theory of differentiation and integration, and that there is not any direct evidence of their results being transmitted outside Kerala.[149][150][151][152]\n\nThe Islamic Empire established across the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, they were not all written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time.[153]\n\nIn the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote an important book on the Hindu–Arabic numerals and one on methods for solving equations. His book On the Calculation with Hindu Numerals, written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word algorithm is derived from the Latinization of his name, Algoritmi, and the word algebra from the title of one of his works, Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala (The Compendious Book on Calculation by Completion and Balancing). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots,[154] and he was the first to teach algebra in an elementary form and for its own sake.[155] He also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as al-jabr.[156] His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" He also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"[157]\n\nIn Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions.[158] His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.\n\nFurther developments in algebra were made by Al-Karaji in his treatise al-Fakhri, where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes.[159] The historian of mathematics, F. Woepcke,[160] praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.[161]\n\nIn the late 11th century, Omar Khayyam wrote Discussions of the Difficulties in Euclid, a book about what he perceived as flaws in Euclid's Elements, especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.[162]\n\nIn the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating nth roots, which was a special case of the methods given many centuries later by Ruffini and Horner.\n\nOther achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.[163]\n\nDuring the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.\n\nIn the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics.[164] Maya numerals used a base of twenty, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures.[164] The Maya used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy.[164] While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Maya developed a standard symbol for it.[164]\n\nMedieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's Timaeus and the biblical passage (in the Book of Wisdom) that God had ordered all things in measure, and number, and weight.[165]\n\nBoethius provided a place for mathematics in the curriculum in the 6th century when he coined the term quadrivium to describe the study of arithmetic, geometry, astronomy, and music. He wrote De institutione arithmetica, a free translation from the Greek of Nicomachus's Introduction to Arithmetic; De institutione musica, also derived from Greek sources; and a series of excerpts from Euclid's Elements. His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.[166][167]\n\nIn the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's The Compendious Book on Calculation by Completion and Balancing, translated into Latin by Robert of Chester, and the complete text of Euclid's Elements, translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona.[168][169] These and other new sources sparked a renewal of mathematics.\n\nLeonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote Liber Abaci in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that)[170] which Fibonacci used as an unremarkable example.\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems.[171] One important contribution was development of mathematics of local motion.\n\nThomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:\nV = log (F/R).[172] Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.[173]\n\nOne of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed \"by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant\".[176]\n\nHeytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that \"a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]\".[177]\n\nNicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled.[178] In a later mathematical commentary on Euclid's Elements, Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.[179]\n\nDuring the Renaissance, the development of mathematics and of accounting were intertwined.[180] While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as abbaco in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.\n\nPiero della Francesca (c. 1415–1492) wrote books on solid geometry and linear perspective, including De Prospectiva Pingendi (On Perspective for Painting), Trattato d’Abaco (Abacus Treatise), and De quinque corporibus regularibus (On the Five Regular Solids).[181][182][183]\n\nLuca Pacioli's Summa de Arithmetica, Geometria, Proportioni et Proportionalità (Italian: \"Review of Arithmetic, Geometry, Ratio and Proportion\") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, \"Particularis de Computis et Scripturis\" (Italian: \"Details of Calculation and Recording\"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons.[184] In Summa Arithmetica, Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. Summa Arithmetica was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.\n\nIn Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book Ars Magna, together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his L'Algebra in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.\n\nSimon Stevin's De Thiende ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation in Europe, which influenced all later work on the real number system.[185][186]\n\nDriven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his Trigonometria in 1595. Regiomontanus's table of sines and cosines was published in 1533.[187]\n\nDuring the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that were involved, were studied intensely.[188]\n\nThe 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based Hans Lipperhey's. Tycho Brahe had gathered a large quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.[189]\nThe analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.\n\nBuilding on earlier work by many predecessors, Isaac Newton discovered the laws of physics that explain Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, developed calculus and much of the calculus notation still in use today. He also refined the binary number system, which is the foundation of nearly all digital (electronic, solid-state, discrete logic) computers, including the Von Neumann architecture, which is the standard design paradigm, or \"computer architecture\", followed from the second half of the 20th century, and into the 21st. Leibniz has been called the \"founder of computer science\".[190]\n\nScience and mathematics had become an international endeavor, which would soon spread over the entire world.[191]\n\nIn addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th and 19th centuries.\n\nThe most influential mathematician of the 18th century was arguably Leonhard Euler (1707–1783). His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol i, and he popularized the use of the Greek letter \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.\n\nOther important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Pierre-Simon Laplace, who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.\n\nThroughout the 19th century mathematics became increasingly abstract.[192] Carl Friedrich Gauss (1777–1855) epitomizes this trend.[citation needed] He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.[citation needed]\n\nThis century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.\nThe Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces, and set the mathematical foundations for the theory of general relativity.[193]\n\nThe 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra.[citation needed] The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in electrical engineering and computer science.[citation needed]\nAugustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.[citation needed]\n\nAlso, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem).[194] Other 19th-century mathematicians used this in their proofs that straight edge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle.[citation needed] Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks.[citation needed] On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.[citation needed]\n\nAbel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.[citation needed]\n\nIn the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.[citation needed]\n\nThe 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865,[195] the Société Mathématique de France in 1872,[196] the Circolo Matematico di Palermo in 1884,[197][198] the Edinburgh Mathematical Society in 1883,[199] and the American Mathematical Society in 1888.[200] The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.[201]\n\nIn 1897, Kurt Hensel introduced p-adic numbers.[202]\n\nThe 20th century saw mathematics become a major profession. By the end of the century, thousands of new Ph.D.s in mathematics were being awarded every year, and jobs were available in both teaching and industry.[203] An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.[204]\n\nIn a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics.[205] These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.[citation needed]\n\nNotable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so.[206] Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995.[207] Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory.[208] In 1998, Thomas Callister Hales proved the Kepler conjecture, also using a computer.[209]\n\nMathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the \"enormous theorem\"), whose proof between 1955 and 2004 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages.[210] A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym \"Nicolas Bourbaki\", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.[211]\n\nDifferential geometry came into its own when Albert Einstein used it in general relativity.[citation needed] Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods.[citation needed] All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc.[citation needed] As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory.[citation needed] Grothendieck and Serre recast algebraic geometry using sheaf theory.[citation needed] Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.[citation needed]\nMeasure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory.[citation needed] Knot theory greatly expanded.[citation needed] Quantum mechanics led to the development of functional analysis,[citation needed] a branch of mathematics that was greatly developed by Stefan Banach and his collaborators who formed the Lwów School of Mathematics.[212] Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals.[citation needed] Lie theory with its Lie groups and Lie algebras became one of the major areas of study.[213]\n\nNon-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities.[citation needed] An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.[citation needed]\n\nThe development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas–Lehmer primality test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research.[citation needed] In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation.[citation needed] Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the fast Fourier transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.[citation needed]\n\nAt the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved[by whom?] the truth or falsity of all statements formulated about the natural numbers plus either addition or multiplication (but not both), was decidable, i.e. could be determined by some algorithm.[citation needed] In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incomplete. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.[citation needed]\n\nOne of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact[214] who conjectured or proved over 3000 theorems[citation needed], including properties of highly composite numbers,[215] the partition function[214] and its asymptotics,[216] and mock theta functions.[214] He also made major investigations in the areas of gamma functions,[217][218] modular forms,[214] divergent series,[214] hypergeometric series[214] and prime number theory.[214]\n\nPaul Erdős published more papers than any other mathematician in history,[219] working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the \"collaborative distance\" between a person and Erdős, as measured by joint authorship of mathematical papers.[220][221]\n\nEmmy Noether has been described by many as the most important woman in the history of mathematics.[222] She studied the theories of rings, fields, and algebras.[223]\n\nAs in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century, there were hundreds of specialized areas in mathematics, and the Mathematics Subject Classification was dozens of pages long.[224] More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.[citation needed]\n\nIn 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems.[225] In 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).[226]\n\nMost mathematical journals now have online versions as well as print versions, and many online-only journals are launched.[227][228] There is an increasing drive toward open access publishing, first made popular by arXiv.[citation needed]\n\nThere are many observable trends in mathematics, the most notable being that the subject is growing ever larger as computers are ever more important and powerful; the volume of data being produced by science and industry, facilitated by computers, continues expanding exponentially. As a result, there is a corresponding growth in the demand for mathematics to help process and understand this big data.[229] Math science careers are also expected to continue to grow, with the US Bureau of Labor Statistics estimating (in 2018) that \"employment of mathematical science occupations is projected to grow 27.9 percent from 2016 to 2026.\"[230]\n"
    },
    {
        "title": "Lists of mathematics topics",
        "content": "\n\nLists of mathematics topics cover a variety of topics related to mathematics. Some of these lists link to hundreds of articles; some link only to a few. The template to the right includes links to alphabetical lists of all mathematical articles. This article brings together the same content organized in a manner better suited for browsing.\nLists cover aspects of basic and advanced mathematics, methodology, mathematical statements, integrals, general concepts, mathematical objects, and reference tables.\nThey also cover equations named after people, societies, mathematicians, journals, and meta-lists.\n\nThe purpose of this list is not similar to that of the Mathematics Subject Classification formulated by the American Mathematical Society. Many mathematics journals ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The subject codes so listed are used by the two major reviewing databases, Mathematical Reviews and Zentralblatt MATH. This list has some items that would not fit in such a classification, such as list of exponential topics and list of factorial and binomial topics, which may surprise the reader with the diversity of their coverage.\n\nThis branch is typically taught in secondary education or in the first year of university.\n\nAs a rough guide, this list is divided into pure and applied sections although in reality, these branches are overlapping and intertwined.\n\nAlgebra includes the study of algebraic structures, which are sets and operations defined on these sets satisfying certain axioms. The field of algebra is further divided according to which structure is studied; for instance, group theory concerns an algebraic structure called group.\n\nCalculus studies the computation of limits, derivatives, and integrals of functions of real numbers, and in particular studies instantaneous rates of change. Analysis evolved from calculus.\n\nGeometry is initially the study of spatial figures like circles and cubes, though it has been generalized considerably. Topology developed from geometry; it looks at those properties that do not change even when the figures are deformed by stretching and bending, like dimension.\n\nCombinatorics concerns the study of discrete (and usually finite) objects. Aspects include \"counting\" the objects satisfying certain criteria (enumerative combinatorics), deciding when the criteria can be met, and constructing and analyzing objects meeting the criteria (as in combinatorial designs and matroid theory), finding \"largest\", \"smallest\", or \"optimal\" objects (extremal combinatorics and combinatorial optimization), and finding algebraic structures these objects may have (algebraic combinatorics).\n\nLogic is the foundation that underlies mathematical logic and the rest of mathematics. It tries to formalize valid reasoning. In particular, it attempts to define what constitutes a proof.\n\nThe branch of mathematics deals with the properties and relationships of numbers, especially positive integers.\nNumber theory is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss said, \"Mathematics is the queen of the sciences—and number theory is the queen of mathematics.\"\nNumber theory also studies the natural, or whole, numbers. One of the central concepts in number theory is that of the prime number, and there are many questions about primes that appear simple but whose resolution continues to elude mathematicians.\n\nA differential equation is an equation involving an unknown function and its derivatives.\n\nIn a dynamical system, a fixed rule describes the time dependence of a point in a geometrical space. The mathematical models used to describe the swinging of a clock pendulum, the flow of water in a pipe, or the number of fish each spring in a lake are examples of dynamical systems.\n\nMathematical physics is concerned with \"the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories\".1\n\nThe fields of mathematics and computing intersect both in computer science, the study of algorithms and data structures, and in scientific computing, the study of algorithmic methods for solving problems in mathematics, science, and engineering.\n\nInformation theory is a branch of applied mathematics and social science involving the quantification of information. Historically, information theory was developed to find fundamental limits on compressing and reliably communicating data.\n\nSignal processing is the analysis, interpretation, and manipulation of signals. Signals of interest include sound, images, biological signals such as ECG, radar signals, and many others. Processing of such signals includes filtering, storage and reconstruction, separation of information from noise, compression, and feature extraction.\n\nProbability theory is the formalization and study of the mathematics of uncertain events or knowledge. The related field of mathematical statistics develops statistical theory with mathematics. Statistics, the science concerned with collecting and analyzing data, is an autonomous discipline (and not a subdiscipline of applied mathematics).\n\nGame theory is a branch of mathematics that uses models to study interactions with formalized incentive structures (\"games\"). It has applications in a variety of fields, including economics, anthropology, political science, social psychology and military strategy.\n\nOperations research is the study and use of mathematical models, statistics, and algorithms to aid in decision-making, typically with the goal of improving or optimizing the performance of real-world systems.\n\nA mathematical statement amounts to a proposition or assertion of some mathematical fact, formula, or construction. Such statements include axioms and the theorems that may be proved from them, conjectures that may be unproven or even unprovable, and also algorithms for computing the answers to questions that can be expressed mathematically.\n\nAmong mathematical objects are numbers, functions, sets, a great variety of things called \"spaces\" of one kind or another, algebraic structures such as rings, groups, or fields, and many other things.\n\nMathematicians study and research in all the different areas of mathematics. The publication of new discoveries in mathematics continues at an immense rate in hundreds of scientific journals, many of them devoted to mathematics and many devoted to subjects to which mathematics is applied (such as theoretical computer science and theoretical physics).\n\nIn calculus, the integral of a function is a generalization of area, mass, volume, sum, and total. The following pages list the integrals of many different functions.\n"
    },
    {
        "title": "Glossary of areas of mathematics",
        "content": "\n\nMathematics is a broad subject that is commonly divided in many areas  or branches that may be defined by their objects of study, by the used methods, or by both. For example, analytic number theory is a subarea of number theory devoted to the use of methods of analysis for the study of natural numbers.\n\nThis glossary is alphabetically sorted. This hides a large part of the relationships between areas. For the broadest areas of mathematics, see Mathematics § Areas of mathematics. The Mathematics Subject Classification is a hierarchical list of areas and subjects of study that has been elaborated by the community of mathematicians. It is used by most publishers for classifying mathematical articles and books.\n\nAlso called infinitesimal calculus\nAlso called absolute differential calculus."
    },
    {
        "title": "Number theory",
        "content": "Number theory is a branch of pure mathematics devoted primarily to the study of the integers and arithmetic functions. German mathematician Carl Friedrich Gauss (1777–1855) said, \"Mathematics is the queen of the sciences—and number theory is the queen of mathematics.\"[1] Number theorists study prime numbers as well as the properties of mathematical objects constructed from integers (for example, rational numbers), or defined as generalizations of the integers (for example, algebraic integers).\n\nIntegers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (for example, the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers; for example, as approximated by the latter (Diophantine approximation).\n\nThe older term for number theory is arithmetic. By the early twentieth century, it had been superseded by number theory.[note 1] (The word arithmetic is used by the general public to mean elementary calculations; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in floating-point arithmetic.) The use of the term arithmetic for number theory regained some ground in the second half of the twentieth century, arguably in part due to French influence.[note 2] In particular, arithmetical is commonly preferred as an adjective to number-theoretic.\n\nThe earliest historical find of an arithmetical nature is a fragment of a table: the broken clay tablet Plimpton 322 (Larsa, Mesopotamia, c. 1800 BC) contains a list of \"Pythagorean triples\", that is, integers \n\n\n\n(\na\n,\nb\n,\nc\n)\n\n\n{\\displaystyle (a,b,c)}\n\n such that \n\n\n\n\na\n\n2\n\n\n+\n\nb\n\n2\n\n\n=\n\nc\n\n2\n\n\n\n\n{\\displaystyle a^{2}+b^{2}=c^{2}}\n\n.\nThe triples are too many and too large to have been obtained by brute force. The heading over the first column reads: \"The takiltum of the diagonal which has been subtracted such that the width...\"[2]\n\nThe table's layout suggests[3] that it was constructed by means of what amounts, in modern language, to the identity\n\nwhich is implicit in routine Old Babylonian exercises.[4] If some other method was used,[5] the triples were first constructed and then reordered by \n\n\n\nc\n\n/\n\na\n\n\n{\\displaystyle c/a}\n\n, presumably for actual use as a \"table\", for example, with a view to applications.\n\nIt is not known what these applications may have been, or whether there could have been any; Babylonian astronomy, for example, truly came into its own only later. It has been suggested instead that the table was a source of numerical examples for school problems.[6][note 3]\n\nWhile evidence of Babylonian number theory is only survived by the Plimpton 322 tablet, some authors assert that Babylonian algebra was exceptionally well developed and included the foundations of modern elementary algebra.[7] Late Neoplatonic sources[8] state that Pythagoras learned mathematics from the Babylonians. Much earlier sources[9] state that Thales and Pythagoras traveled and studied in Egypt.\n\nIn book nine of Euclid's Elements, propositions 21–34 are very probably influenced by Pythagorean teachings;[10] it is very simple material (\"odd times even is even\", \"if an odd number measures [= divides] an even number, then it also measures [= divides] half of it\"), but it is all that is needed to prove that \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n\nis irrational.[11] Pythagorean mystics gave great importance to the odd and the even.[12]\nThe discovery that \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n is irrational is credited to the early Pythagoreans (pre-Theodorus).[13] By revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to Hippasus, who was expelled or split from the Pythagorean sect.[14] This forced a distinction between numbers (integers and the rationals—the subjects of arithmetic), on the one hand, and lengths and proportions (which may be identified with real numbers, whether rational or not), on the other hand.\n\nThe Pythagorean tradition spoke also of so-called polygonal or figurate numbers.[15] While square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums of triangular and pentagonal numbers would prove fruitful in the early modern period (seventeenth to early nineteenth centuries).\n\nThe Chinese remainder theorem appears as an exercise[16] in Sunzi Suanjing (between the third and fifth centuries).[17] (There is one important step glossed over in Sunzi's solution:[note 4] it is the problem that was later solved by Āryabhaṭa's Kuṭṭaka – see below.) The result was later generalized with a complete solution called Da-yan-shu (大衍術) in Qin Jiushao's 1247 Mathematical Treatise in Nine Sections [18] which was translated into English in early nineteenth century by British missionary Alexander Wylie.[19]\n\nThere is also some numerical mysticism in Chinese mathematics,[note 5] but, unlike that of the Pythagoreans, it seems to have led nowhere.\n\nAside from a few fragments, the mathematics of Classical Greece is known to us either through the reports of contemporary non-mathematicians or through mathematical works from the early Hellenistic period.[20] In the case of number theory, this means, by and large, Plato and Euclid, respectively.\n\nWhile Asian mathematics influenced Greek and Hellenistic learning, it seems to be the case that Greek mathematics is also an indigenous tradition.\n\nEusebius, PE X, chapter 4 mentions of Pythagoras:\n\n\"In fact the said Pythagoras, while busily studying the wisdom of each nation, visited Babylon, and Egypt, and all Persia, being instructed by the Magi and the priests: and in addition to these he is related to have studied under the Brahmans (these are Indian philosophers); and from some he gathered astrology, from others geometry, and arithmetic and music from others, and different things from different nations, and only from the wise men of Greece did he get nothing, wedded as they were to a poverty and dearth of wisdom: so on the contrary he himself became the author of instruction to the Greeks in the learning which he had procured from abroad.\"[21]\nAristotle claimed that the philosophy of Plato closely followed the teachings of the Pythagoreans,[22] and Cicero repeats this claim: Platonem ferunt didicisse Pythagorea omnia (\"They say Plato learned all things Pythagorean\").[23]\n\nPlato had a keen interest in mathematics, and distinguished clearly between arithmetic and calculation. (By arithmetic he meant, in part, theorising on number, rather than what arithmetic or number theory have come to mean.) It is through one of Plato's dialogues—namely, Theaetetus—that it is known that Theodorus had proven that \n\n\n\n\n\n3\n\n\n,\n\n\n5\n\n\n,\n…\n,\n\n\n17\n\n\n\n\n{\\displaystyle {\\sqrt {3}},{\\sqrt {5}},\\dots ,{\\sqrt {17}}}\n\n are irrational. Theaetetus was, like Plato, a disciple of Theodorus's; he worked on distinguishing different kinds of incommensurables, and was thus arguably a pioneer in the study of number systems. (Book X of Euclid's Elements is described by Pappus as being largely based on Theaetetus's work.)\n\nEuclid devoted part of his Elements to prime numbers and divisibility, topics that belong unambiguously to number theory and are basic to it (Books VII to IX of Euclid's Elements). In particular, he gave an algorithm for computing the greatest common divisor of two numbers (the Euclidean algorithm; Elements, Prop. VII.2) and the first known proof of the infinitude of primes (Elements, Prop. IX.20).\n\nIn 1773, Lessing published an epigram he had found in a manuscript during his work as a librarian; it claimed to be a letter sent by Archimedes to Eratosthenes.[24][25] The epigram proposed what has become known as\nArchimedes's cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamed Pell's equation). As far as it is known, such equations were first successfully treated by the Indian school. It is not known whether Archimedes himself had a method of solution.\n\nLittle is known about Diophantus of Alexandria; he probably lived in the third century AD, that is, about five hundred years after Euclid. Six out of the thirteen books of Diophantus's Arithmetica survive in the original Greek and four more survive in an Arabic translation. The Arithmetica is a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form \n\n\n\nf\n(\nx\n,\ny\n)\n=\n\nz\n\n2\n\n\n\n\n{\\displaystyle f(x,y)=z^{2}}\n\n or \n\n\n\nf\n(\nx\n,\ny\n,\nz\n)\n=\n\nw\n\n2\n\n\n\n\n{\\displaystyle f(x,y,z)=w^{2}}\n\n. In modern parlance, Diophantine equations are polynomial equations to which rational or integer solutions are sought.\n\nWhile Greek astronomy probably influenced Indian learning, to the point of introducing trigonometry,[26] it seems to be the case that Indian mathematics is otherwise an indigenous tradition;[27] in particular, there is no evidence that Euclid's Elements reached India before the eighteenth century.[28]\n\nĀryabhaṭa (476–550 AD) showed that pairs of simultaneous congruences \n\n\n\nn\n≡\n\na\n\n1\n\n\n\n\nmod\n\nm\n\n\n\n1\n\n\n\n\n{\\displaystyle n\\equiv a_{1}{\\bmod {m}}_{1}}\n\n, \n\n\n\nn\n≡\n\na\n\n2\n\n\n\n\nmod\n\nm\n\n\n\n2\n\n\n\n\n{\\displaystyle n\\equiv a_{2}{\\bmod {m}}_{2}}\n\n could be solved by a method he called kuṭṭaka, or pulveriser;[29] this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India.[30] Āryabhaṭa seems to have had in mind applications to astronomical calculations.[26]\n\nBrahmagupta (628 AD) started the systematic study of indefinite quadratic equations—in particular, the misnamed Pell equation, in which Archimedes may have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta's technical terminology. A general procedure (the chakravala, or \"cyclic method\") for solving Pell's equation was finally found by Jayadeva (cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears in Bhāskara II's Bīja-gaṇita (twelfth century).[31]\n\nIndian mathematics remained largely unknown in Europe until the late eighteenth century;[32] Brahmagupta and Bhāskara's work was translated into English in 1817 by Henry Colebrooke.[33]\n\nIn the early ninth century, the caliph Al-Ma'mun ordered translations of many Greek mathematical works and at least one Sanskrit work (the Sindhind, which may[34] or may not[35] be Brahmagupta's Brāhmasphuṭasiddhānta).\nDiophantus's main work, the Arithmetica, was translated into Arabic by Qusta ibn Luqa (820–912).\nPart of the treatise al-Fakhri (by al-Karajī, 953 – c. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī's contemporary Ibn al-Haytham knew[36] what would later be called Wilson's theorem.\n\nOther than a treatise on squares in arithmetic progression by Fibonacci—who traveled and studied in north Africa and Constantinople—no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus' Arithmetica.[37]\n\nPierre de Fermat (1607–1665) never published his writings; in particular, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes.[38] In his notes and letters, he scarcely wrote any proofs—he had no models in the area.[39]\n\nOver his lifetime, Fermat made the following contributions to the field:\n\nThe interest of Leonhard Euler (1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur[note 8] Goldbach, pointed him towards some of Fermat's work on the subject.[50][51] This has been called the \"rebirth\" of modern number theory,[52] after Fermat's relative lack of success in getting his contemporaries' attention for the subject.[53] Euler's work on number theory includes the following:[54]\n\nJoseph-Louis Lagrange (1736–1813) was the first to give full proofs of some of Fermat's and Euler's work and observations—for instance, the four-square theorem and the basic theory of the misnamed \"Pell's equation\" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studied quadratic forms in full generality (as opposed to \n\n\n\nm\n\nX\n\n2\n\n\n+\nn\n\nY\n\n2\n\n\n\n\n{\\displaystyle mX^{2}+nY^{2}}\n\n)—defining their equivalence relation, showing how to put them in reduced form, etc.\n\nAdrien-Marie Legendre (1752–1833) was the first to state the law of quadratic reciprocity. He also conjectured what amounts to the prime number theorem and Dirichlet's theorem on arithmetic progressions. He gave a full treatment of the equation \n\n\n\na\n\nx\n\n2\n\n\n+\nb\n\ny\n\n2\n\n\n+\nc\n\nz\n\n2\n\n\n=\n0\n\n\n{\\displaystyle ax^{2}+by^{2}+cz^{2}=0}\n\n[66] and worked on quadratic forms along the lines later developed fully by Gauss.[67] In his old age, he was the first to prove Fermat's Last Theorem for \n\n\n\nn\n=\n5\n\n\n{\\displaystyle n=5}\n\n (completing work by Peter Gustav Lejeune Dirichlet, and crediting both him and Sophie Germain).[68]\n\nIn his Disquisitiones Arithmeticae (1798), Carl Friedrich Gauss (1777–1855) proved the law of quadratic reciprocity and developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests.[69] The last section of the Disquisitiones established a link between roots of unity and number theory:\n\nThe theory of the division of the circle...which is treated in sec. 7 does not belong by itself to arithmetic, but its principles can only be drawn from higher arithmetic.[70]\nIn this way, Gauss arguably made a first foray towards both Évariste Galois's work and algebraic number theory.\n\nStarting early in the nineteenth century, the following developments gradually took place:\n\nAlgebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet's theorem on arithmetic progressions (1837),[72][73] whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable.[74] The first use of analytic ideas in number theory actually goes back to Euler (1730s),[75][76] who used formal power series and non-rigorous (or implicit) limiting arguments. The use of complex analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point;[77] Jacobi's four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).[78]\n\nThe history of each subfield is briefly addressed in its own section below; see the main article of each subfield for fuller treatments. Many of the most interesting questions in each area remain open and are being actively worked on.\n\nThe term elementary generally denotes a method that does not use complex analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erdős and Selberg.[79] The term is somewhat ambiguous: for example, proofs based on complex Tauberian theorems (for example, Wiener–Ikehara) are often seen as quite enlightening but not elementary, in spite of using Fourier analysis, rather than complex analysis as such. Here as elsewhere, an elementary proof may be longer and more difficult for most readers than a non-elementary one.\n\nNumber theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.[80]\n\nAnalytic number theory may be defined\n\nSome subjects generally considered to be part of analytic number theory, for example, sieve theory,[note 9] are better covered by the second rather than the first definition: some of sieve theory, for instance, uses little analysis,[note 10] yet it does belong to analytic number theory.\n\nThe following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture (or the twin prime conjecture, or the Hardy–Littlewood conjectures), the Waring problem and the Riemann hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.[82]\n\nOne may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject.[83] This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.[84]\n\nAn algebraic number is any complex number that is a solution to some polynomial equation \n\n\n\nf\n(\nx\n)\n=\n0\n\n\n{\\displaystyle f(x)=0}\n\n with rational coefficients; for example, every solution \n\n\n\nx\n\n\n{\\displaystyle x}\n\n of \n\n\n\n\nx\n\n5\n\n\n+\n(\n11\n\n/\n\n2\n)\n\nx\n\n3\n\n\n−\n7\n\nx\n\n2\n\n\n+\n9\n=\n0\n\n\n{\\displaystyle x^{5}+(11/2)x^{3}-7x^{2}+9=0}\n\n (say) is an algebraic number. Fields of algebraic numbers are also called algebraic number fields, or shortly number fields. Algebraic number theory studies algebraic number fields.[85] Thus, analytic and algebraic number theory can and do overlap: the former is defined by its methods, the latter by its objects of study.\n\nIt could be argued that the simplest kind of number fields (viz., quadratic fields) were already studied by Gauss, as the discussion of quadratic forms in Disquisitiones arithmeticae can be restated in terms of ideals and\nnorms in quadratic fields. (A quadratic field consists of all\nnumbers of the form \n\n\n\na\n+\nb\n\n\nd\n\n\n\n\n{\\displaystyle a+b{\\sqrt {d}}}\n\n, where\n\n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n are rational numbers and \n\n\n\nd\n\n\n{\\displaystyle d}\n\n\nis a fixed rational number whose square root is not rational.)\nFor that matter, the eleventh-century chakravala method amounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.\n\nThe grounds of the subject were set in the late nineteenth century, when ideal numbers, the theory of ideals and valuation theory were introduced; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals\nand \n\n\n\n\n\n−\n5\n\n\n\n\n{\\displaystyle {\\sqrt {-5}}}\n\n, the number \n\n\n\n6\n\n\n{\\displaystyle 6}\n\n can be factorised both as \n\n\n\n6\n=\n2\n⋅\n3\n\n\n{\\displaystyle 6=2\\cdot 3}\n\n and\n\n\n\n\n6\n=\n(\n1\n+\n\n\n−\n5\n\n\n)\n(\n1\n−\n\n\n−\n5\n\n\n)\n\n\n{\\displaystyle 6=(1+{\\sqrt {-5}})(1-{\\sqrt {-5}})}\n\n; all of \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n, \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n, \n\n\n\n1\n+\n\n\n−\n5\n\n\n\n\n{\\displaystyle 1+{\\sqrt {-5}}}\n\n and\n\n\n\n\n1\n−\n\n\n−\n5\n\n\n\n\n{\\displaystyle 1-{\\sqrt {-5}}}\n\n\nare irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws,[86] that is, generalisations of quadratic reciprocity.\n\nNumber fields are often studied as extensions of smaller number fields: a field L is said to be an extension of a field K if L contains K.\n(For example, the complex numbers C are an extension of the reals R, and the reals R are an extension of the rationals Q.)\nClassifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensions L of K such that the Galois group[note 11] Gal(L/K) of L over K is an abelian group—are relatively well understood.\nTheir classification was the object of the programme of class field theory, which was initiated in the late nineteenth century (partly by Kronecker and Eisenstein) and carried out largely in 1900–1950.\n\nAn example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.\n\nThe central problem of Diophantine geometry is to determine when a Diophantine equation has solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.\n\nFor example, an equation in two variables defines a curve in the plane. More generally, an equation or system of equations in two or more variables defines a curve, a surface, or some other such object in n-dimensional space. In Diophantine geometry, one asks whether there are any rational points (points all of whose coordinates are rationals) or\nintegral points (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is whether there are finitely\nor infinitely many rational points on a given curve or surface.\n\nAn example here may be helpful. Consider the Pythagorean equation \n\n\n\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n=\n1\n,\n\n\n{\\displaystyle x^{2}+y^{2}=1,}\n\n\none would like to know its rational solutions; that is, its solutions\n\n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n such that x and y are both rational. This is the same as asking for all integer solutions\nto \n\n\n\n\na\n\n2\n\n\n+\n\nb\n\n2\n\n\n=\n\nc\n\n2\n\n\n\n\n{\\displaystyle a^{2}+b^{2}=c^{2}}\n\n; any solution to the latter equation gives us a solution \n\n\n\nx\n=\na\n\n/\n\nc\n\n\n{\\displaystyle x=a/c}\n\n, \n\n\n\ny\n=\nb\n\n/\n\nc\n\n\n{\\displaystyle y=b/c}\n\n to the former. It is also the\nsame as asking for all points with rational coordinates on the curve described by \n\n\n\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n=\n1\n\n\n{\\displaystyle x^{2}+y^{2}=1}\n\n (a circle of radius 1 centered on the origin).\n\nThe rephrasing of questions on equations in terms of points on curves is felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve (that is, rational or integer solutions to an equation \n\n\n\nf\n(\nx\n,\ny\n)\n=\n0\n\n\n{\\displaystyle f(x,y)=0}\n\n, where \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is a polynomial in two variables) depends crucially on the genus of the curve.[note 12] A major achievement of this approach is Wiles's proof of Fermat's Last Theorem, for which other geometrical notions are just as crucial.\n\nThere is also the closely linked area of Diophantine approximations: given a number \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, determine how well it can be approximated by rational numbers. One seeks approximations that are good relative to the amount of space required to write the rational number: call \n\n\n\na\n\n/\n\nq\n\n\n{\\displaystyle a/q}\n\n (with \n\n\n\ngcd\n(\na\n,\nq\n)\n=\n1\n\n\n{\\displaystyle \\gcd(a,q)=1}\n\n) a good approximation to \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if \n\n\n\n\n|\n\nx\n−\na\n\n/\n\nq\n\n|\n\n<\n\n\n1\n\nq\n\nc\n\n\n\n\n\n\n{\\displaystyle |x-a/q|<{\\frac {1}{q^{c}}}}\n\n, where \n\n\n\nc\n\n\n{\\displaystyle c}\n\n is large. This question is of special interest if \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is an algebraic number. If \n\n\n\nx\n\n\n{\\displaystyle x}\n\n cannot be approximated well, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) are critical both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be approximated better than any algebraic number, then it is a transcendental number. It is by this argument that π and e have been shown to be transcendental.\n\nDiophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. Arithmetic geometry, however, is a contemporary term for much the same domain as that covered by the term Diophantine geometry. The term arithmetic geometry is arguably used most often when one wishes to emphasize the connections to modern algebraic geometry (for example, in Faltings's theorem) rather than to techniques in Diophantine approximations.\n\nThe areas below date from no earlier than the mid-twentieth century, even if they are based on older material. For example, as explained below, algorithms in number theory have a long history, arguably predating the formal concept of proof. However, the modern study of computability began only in the 1930s and 1940s, while computational complexity theory emerged in the 1970s.\n\nProbabilistic number theory starts with questions such as the following ones: Take an integer n at random between one and a million. How likely is it to be prime? (this is just another way of asking how many primes there are between one and a million). How many prime divisors will n have on average? What is the probability that it will have many more or many fewer divisors or prime divisors than the average?\n\nMuch of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutually independent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.\n\nIt is sometimes said that probabilistic combinatorics uses the fact that whatever happens with probability greater than \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.\n\nAt times, a non-rigorous, probabilistic approach leads to a number of heuristic algorithms and open problems, notably Cramér's conjecture.\n\nArithmetic combinatorics starts with questions like the following ones: Does a fairly \"thick\" infinite set \n\n\n\nA\n\n\n{\\displaystyle A}\n\n contain many elements in arithmetic progression: \n\n\n\na\n\n\n{\\displaystyle a}\n\n,\n\n\n\n\na\n+\nb\n,\na\n+\n2\nb\n,\na\n+\n3\nb\n,\n…\n,\na\n+\n10\nb\n\n\n{\\displaystyle a+b,a+2b,a+3b,\\ldots ,a+10b}\n\n, say? Should it be possible to write large integers as sums of elements of \n\n\n\nA\n\n\n{\\displaystyle A}\n\n?\n\nThese questions are characteristic of arithmetic combinatorics. This is a presently coalescing field; it subsumes additive number theory (which concerns itself with certain very specific sets \n\n\n\nA\n\n\n{\\displaystyle A}\n\n of arithmetic significance, such as the primes or the squares) and, arguably, some of the geometry of numbers, together with some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links with ergodic theory, finite group theory, model theory, and other fields. The term additive combinatorics is also used; however, the sets \n\n\n\nA\n\n\n{\\displaystyle A}\n\n being studied need not be sets of integers, but rather subsets of non-commutative groups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets of rings, in which case the growth of \n\n\n\nA\n+\nA\n\n\n{\\displaystyle A+A}\n\n and \n\n\n\nA\n\n\n{\\displaystyle A}\n\n·\n\n\n\nA\n\n\n{\\displaystyle A}\n\n may be compared.\n\nWhile the word algorithm goes back only to certain readers of al-Khwārizmī, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics—ancient Egyptian, Babylonian, Vedic, Chinese—whereas proofs appeared only with the Greeks of the classical period.\n\nAn early case is that of what is now called the Euclidean algorithm. In its basic form (namely, as an algorithm for computing the greatest common divisor) it appears as Proposition 2 of Book VII in Elements, together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equation \n\n\n\na\nx\n+\nb\ny\n=\nc\n\n\n{\\displaystyle ax+by=c}\n\n, or, what is the same, for finding the quantities whose existence is assured by the Chinese remainder theorem) it first appears in the works of Āryabhaṭa (fifth to sixth centuries) as an algorithm called kuṭṭaka (\"pulveriser\"), without a proof of correctness.\n\nThere are two main questions: \"Can this be computed?\" and \"Can it be computed rapidly?\" Anyone can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. Fast algorithms for testing primality are now known, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.\n\nThe difficulty of a computation can be useful: modern protocols for encrypting messages (for example, RSA) depend on functions that are known to all, but whose inverses are known only to a chosen few, and would take one too long a time to figure out on one's own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.\n\nSome things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution to Hilbert's tenth problem, that there is no Turing machine which can solve all Diophantine equations.[87] In particular, this means that, given a computably enumerable set of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (i.e., Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. It cannot be proven that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)\n\nThe number-theorist Leonard Dickson (1874–1954) said \"Thank God that number theory is unsullied by any application\". Such a view is no longer applicable to number theory.[88] In 1974, Donald Knuth said \"virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations\".[89]\nElementary number theory is taught in discrete mathematics courses for computer scientists. It also has applications to the continuous in numerical analysis.[90]\n\nNumber theory has now several modern applications spanning diverse areas such as:\n\nThe American Mathematical Society awards the Cole Prize in Number Theory. Moreover, number theory is one of the three mathematical subdisciplines rewarded by the Fermat Prize.\n\n[...] the question \"how was the tablet calculated?\" does not have to have the same answer as the question \"what problems does the tablet set?\" The first can be answered most satisfactorily by reciprocal pairs, as first suggested half a century ago, and the second by some sort of right-triangle problems (Robson 2001, p. 202).\nRobson takes issue with the notion that the scribe who produced Plimpton 322 (who had to \"work for a living\", and would not have belonged to a \"leisured middle class\") could have been motivated by his own \"idle curiosity\" in the absence of a \"market for new mathematics\".(Robson 2001, pp. 199–200)\n\n[26] Now there are an unknown number of things. If we count by threes, there is a remainder 2; if we count by fives, there is a remainder 3; if we count by sevens, there is a remainder 2. Find the number of things. Answer: 23.\n\nMethod: If we count by threes and there is a remainder 2, put down 140. If we count by fives and there is a remainder 3, put down 63. If we count by sevens and there is a remainder 2, put down 30. Add them to obtain 233 and subtract 210 to get the answer. If we count by threes and there is a remainder 1, put down 70. If we count by fives and there is a remainder 1, put down 21. If we count by sevens and there is a remainder 1, put down 15. When [a number] exceeds 106, the result is obtained by subtracting 105.\n[36] Now there is a pregnant woman whose age is 29. If the gestation period is 9 months, determine the sex of the unborn child. Answer: Male.\n\nMethod: Put down 49, add the gestation period and subtract the age. From the remainder take away 1 representing the heaven, 2 the earth, 3 the man, 4 the four seasons, 5 the five phases, 6 the six pitch-pipes, 7 the seven stars [of the Dipper], 8 the eight winds, and 9 the nine divisions [of China under Yu the Great]. If the remainder is odd, [the sex] is male and if the remainder is even, [the sex] is female.\nThis is the last problem in Sunzi's otherwise matter-of-fact treatise.\n\nTwo of the most popular introductions to the subject are:\n\nHardy and Wright's book is a comprehensive classic, though its clarity sometimes suffers due to the authors' insistence on elementary methods (Apostol 1981).\nVinogradov's main attraction consists in its set of problems, which quickly lead to Vinogradov's own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:\n\nPopular choices for a second textbook include:\n\n\n"
    },
    {
        "title": "Geometry",
        "content": "\n\nGeometry (from Ancient Greek  γεωμετρία (geōmetría) 'land measurement'; from  γῆ (gê) 'earth, land' and  μέτρον (métron) 'a measure')[1] is a branch of mathematics concerned with properties of space such as the distance, shape, size, and relative position of figures.[2] Geometry is, along with arithmetic, one of the oldest branches of mathematics. A mathematician who works in the field of geometry is called a geometer. Until the 19th century, geometry was almost exclusively devoted to Euclidean geometry,[a] which includes the notions of point, line, plane, distance, angle, surface, and curve, as fundamental concepts.[3]\n\nOriginally developed to model the physical world, geometry has applications in almost all sciences, and also in art, architecture, and other activities that are related to graphics.[4] Geometry also has applications in areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental in Wiles's proof of Fermat's Last Theorem, a problem that was stated in terms of elementary arithmetic, and remained unsolved for several centuries.\n\nDuring the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries is Carl Friedrich Gauss's Theorema Egregium (\"remarkable theorem\") that asserts roughly that the Gaussian curvature of a surface is independent from any specific embedding in a Euclidean space. This implies that surfaces can be studied intrinsically, that is, as stand-alone spaces, and has been expanded into the theory of manifolds and Riemannian geometry. Later in the 19th century, it appeared that geometries without the parallel postulate (non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underlies general relativity is a famous application of non-Euclidean geometry.\n\nSince the late 19th century, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry, algebraic geometry, computational geometry, algebraic topology, discrete geometry (also known as combinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometry that consider only alignment of points but not distance and parallelism, affine geometry that omits the concept of angle and distance, finite geometry that omits continuity, and others. This enlargement of the scope of geometry led to a change of meaning of the word \"space\", which originally referred to the three-dimensional space of the physical world and its model provided by Euclidean geometry; presently a geometric space, or simply a space is a mathematical structure on which some geometry is defined.\n\nThe earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC.[5][6] Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000–1800 BC) and Moscow Papyrus (c. 1890 BC), and the Babylonian clay tablets, such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum.[7] Later clay tablets (350–50 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter's position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries.[8] South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.[9][10]\n\nIn the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales's theorem.[11] Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem,[12] though the statement of the theorem has a long history.[13][14] Eudoxus (408–c. 355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures,[15] as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time,[16] introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[17] The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today.[18] Archimedes (c. 287–212 BC) of Syracuse, Italy used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of pi.[19] He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.\n\nIndian mathematicians also made many important contributions in geometry. The Shatapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras.[20] According to (Hayashi 2005, p. 363), the Śulba Sūtras contain \"the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples,[b] which are particular cases of Diophantine equations.[21]\nIn the Bakhshali manuscript, there are a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also \"employs a decimal place value system with a dot for zero.\"[22] Aryabhata's Aryabhatiya (499) includes the computation of areas and volumes.\nBrahmagupta wrote his astronomical work Brāhmasphuṭasiddhānta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: \"basic operations\" (including cube roots, fractions, ratio and proportion, and barter) and \"practical mathematics\" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain).[23] In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).[23]\n\nIn the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry.[24][25] Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra.[26] Thābit ibn Qurra (known as Thebit in Latin) (836–901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry.[27] Omar Khayyam (1048–1131) found geometric solutions to cubic equations.[28] The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were part of a line of research on the parallel postulate continued by later European geometers, including Vitello (c. 1230 – c. 1314), Gersonides (1288–1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri, that by the 19th century led to the discovery of hyperbolic geometry.[29]\n\nIn the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665).[30] This was a necessary precursor to the development of calculus and a precise quantitative science of physics.[31] The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661).[32] Projective geometry studies properties of shapes which are unchanged under projections and sections, especially as they relate to artistic perspective.[33]\n\nTwo developments in geometry in the 19th century changed the way it had been studied previously.[34] These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826–1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincaré, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of \"space\" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.[35]\n\nThe following are some of the most important concepts in geometry.[3][36]\n\nEuclid took an abstract approach to geometry in his Elements,[37] one of the most influential books ever written.[38] Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes.[39] He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry.[40] At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792–1856), János Bolyai (1802–1860), Carl Friedrich Gauss (1777–1855) and others[41] led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.[42]\n\nPoints are generally considered fundamental objects for building geometry. They may be defined by the properties that they must have, as in Euclid's definition as \"that which has no part\",[43] or in synthetic geometry. In modern mathematics, they are generally defined as elements of a set called space, which is itself axiomatically defined.\n\nWith these modern definitions, every geometric shape is defined as a set of points; this is not the case in synthetic geometry, where a line is another fundamental object that is not viewed as the set of the points through which it passes.\n\nHowever, there are modern geometries in which points are not primitive objects, or even without points.[44][45] One of the oldest such geometries is Whitehead's point-free geometry, formulated by Alfred North Whitehead in 1919–1920.\n\nEuclid described a line as \"breadthless length\" which \"lies equally with respect to the points on itself\".[43] In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation,[46] but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.[47] In differential geometry, a geodesic is a generalization of the notion of a line to curved spaces.[48]\n\nIn Euclidean geometry a plane is a flat, two-dimensional surface that extends infinitely;[43] the definitions for other types of geometries are generalizations of that. Planes are used in many areas of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles;[49] it can be studied as an affine space, where collinearity and ratios can be studied but not distances;[50] it can be studied as the complex plane using techniques of complex analysis;[51] and so on.\n\nA curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.[52]\n\nIn topology, a curve is defined by a function from an interval of the real numbers to another space.[49] In differential geometry, the same definition is used, but the defining function is required to be differentiable.[53] Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.[54]\n\nA surface is a two-dimensional object, such as a sphere or paraboloid.[55] In differential geometry[53] and topology,[49] surfaces are described by two-dimensional 'patches' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.[54]\n\nA solid is a three-dimensional object bounded by a closed surface; for example, a ball is the volume bounded by a sphere.\n\nA manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space.[49] In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.[53]\n\nManifolds are used extensively in physics, including in general relativity and string theory.[56]\n\nEuclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other.[43] In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.[57]\nThe size of an angle is formalized as an angular measure.\n\nIn Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right.[43] The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.[58]\n\nIn differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.[59][60]\n\nLength, area, and volume describe the size or extent of an object in one dimension, two dimension, and three dimensions respectively.[61]\n\nIn Euclidean geometry and analytic geometry, the length of a line segment can often be calculated by the Pythagorean theorem.[62]\n\nArea and volume can be defined as fundamental quantities separate from length, or they can be described and calculated in terms of lengths in a plane or 3-dimensional space.[61] Mathematicians have found many explicit formulas for area and formulas for volume of various geometric objects. In calculus, area and volume can be defined in terms of integrals, such as the Riemann integral[63] or the Lebesgue integral.[64]\n\nOther geometrical measures include the curvature and compactness.\n\nThe concept of length or distance can be generalized, leading to the idea of metrics.[65] For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.[66]\n\nIn a different direction, the concepts of length, area and volume are extended by measure theory, which studies methods of assigning a size or measure to sets, where the measures follow rules similar to those of classical area and volume.[67]\n\nCongruence and similarity are concepts that describe when two shapes have similar characteristics.[68] In Euclidean geometry, similarity is used to describe objects that have the same shape, while congruence is used to describe objects that are the same in both size and shape.[69] Hilbert, in his work on creating a more rigorous foundation for geometry, treated congruence as an undefined term whose properties are defined by axioms.\n\nCongruence and similarity are generalized in transformation geometry, which studies the properties of geometric objects that are preserved by different kinds of transformations.[70]\n\nClassical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments used in most geometric constructions are the compass and straightedge.[c] Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using neusis, parabolas and other curves, or mechanical devices, were found.\n\nThe geometrical concepts of rotation and orientation define part of the placement of objects embedded in the plane or in space.\n\nTraditional geometry allowed dimensions 1 (a line or curve), 2 (a plane or surface), and 3 (our ambient world conceived of as three-dimensional space). Furthermore, mathematicians and physicists have used higher dimensions for nearly two centuries.[71] One example of a mathematical use for higher dimensions is the configuration space of a physical system, which has a dimension equal to the system's degrees of freedom. For instance, the configuration of a screw can be described by five coordinates.[72]\n\nIn general topology, the concept of dimension has been extended from natural numbers, to infinite dimension (Hilbert spaces, for example) and positive real numbers (in fractal geometry).[73] In algebraic geometry, the dimension of an algebraic variety has received a number of apparently different definitions, which are all equivalent in the most common cases.[74]\n\nThe theme of symmetry in geometry is nearly as old as the science of geometry itself.[75] Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers[76] and were investigated in detail before the time of Euclid.[39] Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of Leonardo da Vinci, M. C. Escher, and others.[77] In the second half of the 19th century, the relationship between symmetry and geometry came under intense scrutiny. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is.[78] Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines.[79] However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' found its inspiration.[80] Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory,[81][82] the latter in Lie theory and Riemannian geometry.[83][84]\n\nA different type of symmetry is the principle of duality in projective geometry, among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and the result is an equally true theorem.[85] A similar and closely related form of duality exists between a vector space and its dual space.[86]\n\nEuclidean geometry is geometry in its classical sense.[87] As it models the space of the physical world, it is used in many scientific areas, such as mechanics, astronomy, crystallography,[88] and many technical fields, such as engineering,[89] architecture,[90] geodesy,[91] aerodynamics,[92] and navigation.[93] The mandatory educational curriculum of the majority of nations includes the study of Euclidean concepts such as points, lines, planes, angles, triangles, congruence, similarity, solid figures, circles, and analytic geometry.[94]\n\nEuclidean vectors are used for a myriad of applications in physics and engineering, such as position, displacement, deformation, velocity, acceleration, force, etc.\n\nDifferential geometry uses techniques of calculus and linear algebra to study problems in geometry.[95] It has applications in physics,[96] econometrics,[97] and bioinformatics,[98] among others.\n\nIn particular, differential geometry is of importance to mathematical physics due to Albert Einstein's general relativity postulation that the universe is curved.[99] Differential geometry can either be intrinsic (meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point) or extrinsic (where the object under study is a part of some ambient flat Euclidean space).[100]\n\n\n\nTopology is the field concerned with the properties of continuous mappings,[101] and can be considered a generalization of Euclidean geometry.[102] In practice, topology often means dealing with large-scale properties of spaces, such as connectedness and compactness.[49]\n\nThe field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms.[103] This has often been expressed in the form of the saying 'topology is rubber-sheet geometry'. Subfields of topology include geometric topology, differential topology, algebraic topology and general topology.[104]\n\nAlgebraic geometry is fundamentally the study by means of algebraic methods of some geometrical shapes, called algebraic sets, and defined as common zeros of multivariate polynomials.[105] Algebraic geometry became an autonomous subfield of geometry c. 1900, with a theorem called Hilbert's Nullstellensatz that establishes a strong correspondence between algebraic sets and ideals of polynomial rings. This led to a parallel development of algebraic geometry, and its algebraic counterpart, called commutative algebra.[106] From the late 1950s through the mid-1970s algebraic geometry had undergone major foundational development, with the introduction by Alexander Grothendieck of scheme theory, which allows using topological methods, including cohomology theories in a purely algebraic context.[106] Scheme theory allowed to solve many difficult problems not only in geometry, but also in number theory. Wiles' proof of Fermat's Last Theorem is a famous example of a long-standing problem of number theory whose solution uses scheme theory and its extensions such as stack theory. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.[107]\n\nAlgebraic geometry has applications in many areas, including cryptography[108] and string theory.[109]\n\nComplex geometry studies the nature of geometric structures modelled on, or arising out of, the complex plane.[110][111][112] Complex geometry lies at the intersection of differential geometry, algebraic geometry, and analysis of several complex variables, and has found applications to string theory and mirror symmetry.[113]\n\nComplex geometry first appeared as a distinct area of study in the work of Bernhard Riemann in his study of Riemann surfaces.[114][115][116] Work in the spirit of Riemann was carried out by the Italian school of algebraic geometry in the early 1900s. Contemporary treatment of complex geometry began with the work of Jean-Pierre Serre, who introduced the concept of sheaves to the subject, and illuminated the relations between complex geometry and algebraic geometry.[117][118]\nThe primary objects of study in complex geometry are complex manifolds, complex algebraic varieties, and complex analytic varieties, and holomorphic vector bundles and coherent sheaves over these spaces. Special examples of spaces studied in complex geometry include Riemann surfaces, and Calabi–Yau manifolds, and these spaces find uses in string theory. In particular, worldsheets of strings are modelled by Riemann surfaces, and superstring theory predicts that the extra 6 dimensions of 10 dimensional spacetime may be modelled by Calabi–Yau manifolds.\n\nDiscrete geometry is a subject that has close connections with convex geometry.[119][120][121] It is concerned mainly with questions of relative position of simple geometric objects, such as points, lines and circles. Examples include the study of sphere packings, triangulations, the Kneser-Poulsen conjecture, etc.[122][123] It shares many methods and principles with combinatorics.\n\nComputational geometry deals with algorithms and their implementations for manipulating geometrical objects. Important problems historically have included the travelling salesman problem, minimum spanning trees, hidden-line removal, and linear programming.[124]\n\nAlthough being a young area of geometry, it has many applications in computer vision, image processing, computer-aided design, medical imaging, etc.[125]\n\nGeometric group theory studies group actions on objects that are regarded as geometric (significantly, isometric actions on metric spaces) to study finitely generated groups, often involving large-scale geometric techniques.[126] It is closely connected to low-dimensional topology, such as in Grigori Perelman's proof of the Geometrization conjecture, which included the proof of the Poincaré conjecture, a Millennium Prize Problem.[127]\n\nGroup actions on their Cayley graphs are foundational examples of isometric group actions. Other major topics include quasi-isometries, Gromov-hyperbolic groups and their generalizations (relatively and acylindrically hyperbolic groups), free groups and their automorphisms, groups acting on trees, various notions of nonpositive curvature for groups (CAT(0) groups, Dehn functions, automaticity...), right angled Artin groups, and topics close to combinatorial group theory such as small cancellation theory and algorithmic problems (e.g. the word, conjugacy, and isomorphism problems). Other group-theoretic topics like mapping class groups, property (T), solvability, amenability and lattices in Lie groups are sometimes regarded as strongly geometric as well.[126][128][129][130]\n\nConvex geometry investigates convex shapes in the Euclidean space and its more abstract analogues, often using techniques of real analysis and discrete mathematics.[131] It has close connections to convex analysis, optimization and functional analysis and important applications in number theory.\n\nConvex geometry dates back to antiquity.[131] Archimedes gave the first known precise definition of convexity. The isoperimetric problem, a recurring concept in convex geometry, was studied by the Greeks as well, including Zenodorus. Archimedes, Plato, Euclid, and later Kepler and Coxeter all studied convex polytopes and their properties. From the 19th century on, mathematicians have studied other areas of convex mathematics, including higher-dimensional polytopes, volume and surface area of convex bodies, Gaussian curvature, algorithms, tilings and lattices.\n\nGeometry has found applications in many fields, some of which are described below.\n\nMathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.[132]\n\nArtists have long used concepts of proportion in design. Vitruvius developed a complicated theory of ideal proportions for the human figure.[133] These concepts have been used and adapted by artists from Michelangelo to modern comic book artists.[134]\n\nThe golden ratio is a particular proportion that has had a controversial role in art. Often claimed to be the most aesthetically pleasing ratio of lengths, it is frequently stated to be incorporated into famous works of art, though the most reliable and unambiguous examples were made deliberately by artists aware of this legend.[135]\n\nTilings, or tessellations, have been used in art throughout history. Islamic art makes frequent use of tessellations, as did the art of M. C. Escher.[136] Escher's work also made use of hyperbolic geometry.\n\nCézanne advanced the theory that all images can be built up from the sphere, the cone, and the cylinder. This is still used in art theory today, although the exact list of shapes varies from author to author.[137][138]\n\nGeometry has many applications in architecture. In fact, it has been said that geometry lies at the core of architectural design.[139][140] Applications of geometry to architecture include the use of projective geometry to create forced perspective,[141] the use of conic sections in constructing domes and similar objects,[90] the use of tessellations,[90] and the use of symmetry.[90]\n\nThe field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.[142]\n\nRiemannian geometry and pseudo-Riemannian geometry are used in general relativity.[143] String theory makes use of several variants of geometry,[144] as does quantum information theory.[145]\n\nCalculus was strongly influenced by geometry.[30] For instance, the introduction of coordinates by René Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century. Analytic geometry continues to be a mainstay of pre-calculus and calculus curriculum.[146][147]\n\nAnother important area of application is number theory.[148] In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths contradicted their philosophical views.[149] Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles's proof of Fermat's Last Theorem.[150]\n\n\"Three scientists, Ibn al-Haytham, Khayyam, and al-Tusi, had made the most considerable contribution to this branch of geometry whose importance came to be completely recognized only in the 19th century. In essence, their propositions concerning the properties of quadrangles which they considered, assuming that some of the angles of these figures were acute of obtuse, embodied the first few theorems of the hyperbolic and the elliptic geometries. Their other proposals showed that various geometric statements were equivalent to the Euclidean postulate V. It is extremely important that these scholars established the mutual connection between this postulate and the sum of the angles of a triangle and a quadrangle. By their works on the theory of parallel lines Arab mathematicians directly influenced the relevant investigations of their European counterparts. The first European attempt to prove the postulate on parallel lines—made by Witelo, the Polish scientists of the 13th century, while revising Ibn al-Haytham's Book of Optics (Kitab al-Manazir)—was undoubtedly prompted by Arabic sources. The proofs put forward in the 14th century by the Jewish scholar Levi ben Gerson, who lived in southern France, and by the above-mentioned Alfonso from Spain directly border on Ibn al-Haytham's demonstration. Above, we have demonstrated that Pseudo-Tusi's Exposition of Euclid had stimulated both J. Wallis's and G. Saccheri's studies of the theory of parallel lines.\""
    },
    {
        "title": "Algebra",
        "content": "\n\nAlgebra is the branch of mathematics that studies certain abstract systems, known as algebraic structures, and the manipulation of expressions within those systems. It is a generalization of arithmetic that introduces variables and algebraic operations other than the standard arithmetic operations, such as addition and multiplication.\n\nElementary algebra is the main form of algebra taught in schools. It examines mathematical statements using variables for unspecified values and seeks to determine for which values the statements are true. To do so, it uses different methods of transforming equations to isolate variables. Linear algebra is a closely related field that investigates linear equations and combinations of them called systems of linear equations. It provides methods to find the values that solve all equations in the system at the same time, and to study the set of these solutions.\n\nAbstract algebra studies algebraic structures, which consist of a set of mathematical objects together with one or several operations defined on that set. It is a generalization of elementary and linear algebra, since it allows mathematical objects other than numbers and non-arithmetic operations. It distinguishes between different types of algebraic structures, such as groups, rings, and fields, based on the number of operations they use and the laws they follow, called axioms. Universal algebra and category theory provide general frameworks to investigate abstract patterns that characterize different classes of algebraic structures.\n\nAlgebraic methods were first studied in the ancient period to solve specific problems in fields like geometry. Subsequent mathematicians examined general techniques to solve equations independent of their specific applications. They described equations and their solutions using words and abbreviations until the 16th and 17th centuries, when a rigorous symbolic formalism was developed. In the mid-19th century, the scope of algebra broadened beyond a theory of equations to cover diverse types of algebraic operations and structures. Algebra is relevant to many branches of mathematics, such as geometry, topology, number theory, and calculus, and other fields of inquiry, like logic and the empirical sciences.\n\nAlgebra is the branch of mathematics that studies algebraic structures and the operations they use.[1] An algebraic structure is a non-empty set of mathematical objects, such as the integers, together with algebraic operations defined on that set, like addition and multiplication.[2][a] Algebra explores the laws, general characteristics, and types of algebraic structures. Within certain algebraic structures, it examines the use of variables in equations and how to manipulate these equations.[4][b]\n\nAlgebra is often understood as a generalization of arithmetic.[8] Arithmetic studies operations like addition, subtraction, multiplication, and division, in a particular domain of numbers, such as the real numbers.[9] Elementary algebra constitutes the first level of abstraction. Like arithmetic, it restricts itself to specific types of numbers and operations. It generalizes these operations by allowing indefinite quantities in the form of variables in addition to numbers.[10] A higher level of abstraction is found in abstract algebra, which is not limited to a particular domain and examines algebraic structures such as groups and rings. It extends beyond typical arithmetic operations by also covering other types of operations.[11] Universal algebra is still more abstract in that it is not interested in specific algebraic structures but investigates the characteristics of algebraic structures in general.[12]\n\nThe term \"algebra\" is sometimes used in a more narrow sense to refer only to elementary algebra or only to abstract algebra.[14] When used as a countable noun, an algebra is a specific type of algebraic structure that involves a vector space equipped with a certain type of binary operation.[15] Depending on the context, \"algebra\" can also refer to other algebraic structures, like a Lie algebra or an associative algebra.[16]\n\nThe word algebra comes from the Arabic term الجبر (al-jabr), which originally referred to the surgical treatment of bonesetting. In the 9th century, the term received a mathematical meaning when the Persian mathematician Muhammad ibn Musa al-Khwarizmi employed it to describe a method of solving equations and used it in the title of a treatise on algebra, al-Kitāb al-Mukhtaṣar fī Ḥisāb al-Jabr wal-Muqābalah [The Compendious Book on Calculation by Completion and Balancing] which was translated into Latin as Liber Algebrae et Almucabola.[c] The word entered the English language in the 16th century from Italian, Spanish, and medieval Latin.[18] Initially, its meaning was restricted to the theory of equations, that is, to the art of manipulating polynomial equations in view of solving them. This changed in the 19th century[d] when the scope of algebra broadened to cover the study of diverse types of algebraic operations and structures together with their underlying axioms, the laws they follow.[21]\n\nElementary algebra, also called school algebra, college algebra, and classical algebra,[22] is the oldest and most basic form of algebra. It is a generalization of arithmetic that relies on variables and examines how mathematical statements may be transformed.[23]\n\nArithmetic is the study of numerical operations and investigates how numbers are combined and transformed using the arithmetic operations of addition, subtraction, multiplication, division, exponentiation, extraction of roots, and logarithm. For example, the operation of addition combines two numbers, called the addends, into a third number, called the sum, as in \n\n\n\n2\n+\n5\n=\n7\n\n\n{\\displaystyle 2+5=7}\n\n.[9]\n\nElementary algebra relies on the same operations while allowing variables in addition to regular numbers. Variables are symbols for unspecified or unknown quantities. They make it possible to state relationships for which one does not know the exact values and to express general laws that are true, independent of which numbers are used. For example, the equation \n\n\n\n2\n×\n3\n=\n3\n×\n2\n\n\n{\\displaystyle 2\\times 3=3\\times 2}\n\n belongs to arithmetic and expresses an equality only for these specific numbers. By replacing the numbers with variables, it is possible to express a general law that applies to any possible combination of numbers, like the commutative property of multiplication, which is expressed in the equation \n\n\n\na\n×\nb\n=\nb\n×\na\n\n\n{\\displaystyle a\\times b=b\\times a}\n\n.[23]\n\nAlgebraic expressions are formed by using arithmetic operations to combine variables and numbers. By convention, the lowercase letters \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, \n\n\n\ny\n\n\n{\\displaystyle y}\n\n, and \n\n\n\nz\n\n\n{\\displaystyle z}\n\n represent variables. In some cases, subscripts are added to distinguish variables, as in \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n, \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, and \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n. The lowercase letters \n\n\n\na\n\n\n{\\displaystyle a}\n\n, \n\n\n\nb\n\n\n{\\displaystyle b}\n\n, and \n\n\n\nc\n\n\n{\\displaystyle c}\n\n are usually used for constants and coefficients.[e] The expression \n\n\n\n5\nx\n+\n3\n\n\n{\\displaystyle 5x+3}\n\n is an algebraic expression created by multiplying the number 5 with the variable \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and adding the number 3 to the result. Other examples of algebraic expressions are \n\n\n\n32\nx\ny\nz\n\n\n{\\displaystyle 32xyz}\n\n and \n\n\n\n64\n\nx\n\n1\n\n\n2\n\n\n+\n7\n\nx\n\n2\n\n\n−\nc\n\n\n{\\displaystyle 64x_{1}^{2}+7x_{2}-c}\n\n.[25]\n\nSome algebraic expressions take the form of statements that relate two expressions to one another. An equation is a statement formed by comparing two expressions, saying that they are equal. This can be expressed using the equals sign (\n\n\n\n=\n\n\n{\\displaystyle =}\n\n), as in \n\n\n\n5\n\nx\n\n2\n\n\n+\n6\nx\n=\n3\ny\n+\n4\n\n\n{\\displaystyle 5x^{2}+6x=3y+4}\n\n. Inequations involve a different type of comparison, saying that the two sides are different. This can be expressed using symbols such as the less-than sign (\n\n\n\n<\n\n\n{\\displaystyle <}\n\n), the greater-than sign (\n\n\n\n>\n\n\n{\\displaystyle >}\n\n), and the inequality sign (\n\n\n\n≠\n\n\n{\\displaystyle \\neq }\n\n). Unlike other expressions, statements can be true or false, and their truth value usually depends on the values of the variables. For example, the statement \n\n\n\n\nx\n\n2\n\n\n=\n4\n\n\n{\\displaystyle x^{2}=4}\n\n is true if \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is either 2 or −2 and false otherwise.[26] Equations with variables can be divided into identity equations and conditional equations. Identity equations are true for all values that can be assigned to the variables, such as the equation \n\n\n\n2\nx\n+\n5\nx\n=\n7\nx\n\n\n{\\displaystyle 2x+5x=7x}\n\n. Conditional equations are only true for some values. For example, the equation \n\n\n\nx\n+\n4\n=\n9\n\n\n{\\displaystyle x+4=9}\n\n is only true if \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is 5.[27]\n\nThe main goal of elementary algebra is to determine the values for which a statement is true. This can be achieved by transforming and manipulating statements according to certain rules. A key principle guiding this process is that whatever operation is applied to one side of an equation also needs to be done to the other side. For example, if one subtracts 5 from the left side of an equation one also needs to subtract 5 from the right side to balance both sides. The goal of these steps is usually to isolate the variable one is interested in on one side, a process known as solving the equation for that variable. For example, the equation \n\n\n\nx\n−\n7\n=\n4\n\n\n{\\displaystyle x-7=4}\n\n can be solved for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n by adding 7 to both sides, which isolates \n\n\n\nx\n\n\n{\\displaystyle x}\n\n on the left side and results in the equation \n\n\n\nx\n=\n11\n\n\n{\\displaystyle x=11}\n\n.[28]\n\nThere are many other techniques used to solve equations. Simplification is employed to replace a complicated expression with an equivalent simpler one. For example, the expression \n\n\n\n7\nx\n−\n3\nx\n\n\n{\\displaystyle 7x-3x}\n\n can be replaced with the expression \n\n\n\n4\nx\n\n\n{\\displaystyle 4x}\n\n since \n\n\n\n7\nx\n−\n3\nx\n=\n(\n7\n−\n3\n)\nx\n=\n4\nx\n\n\n{\\displaystyle 7x-3x=(7-3)x=4x}\n\n by the distributive property.[29] For statements with several variables, substitution is a common technique to replace one variable with an equivalent expression that does not use this variable. For example, if one knows that \n\n\n\ny\n=\n3\nx\n\n\n{\\displaystyle y=3x}\n\n then one can simplify the expression \n\n\n\n7\nx\ny\n\n\n{\\displaystyle 7xy}\n\n to arrive at \n\n\n\n21\n\nx\n\n2\n\n\n\n\n{\\displaystyle 21x^{2}}\n\n. In a similar way, if one knows the value of one variable one may be able to use it to determine the value of other variables.[30]\n\nAlgebraic equations can be interpreted geometrically to describe spatial figures in the form of a graph. To do so, the different variables in the equation are understood as coordinates and the values that solve the equation are interpreted as points of a graph. For example, if \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is set to zero in the equation \n\n\n\ny\n=\n0.5\nx\n−\n1\n\n\n{\\displaystyle y=0.5x-1}\n\n, then \n\n\n\ny\n\n\n{\\displaystyle y}\n\n must be −1 for the equation to be true. This means that the \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n-pair \n\n\n\n(\n0\n,\n−\n1\n)\n\n\n{\\displaystyle (0,-1)}\n\n is part of the graph of the equation. The \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n-pair \n\n\n\n(\n0\n,\n7\n)\n\n\n{\\displaystyle (0,7)}\n\n, by contrast, does not solve the equation and is therefore not part of the graph. The graph encompasses the totality of \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n-pairs that solve the equation.[31]\n\nA polynomial is an expression consisting of one or more terms that are added or subtracted from each other, like \n\n\n\n\nx\n\n4\n\n\n+\n3\nx\n\ny\n\n2\n\n\n+\n5\n\nx\n\n3\n\n\n−\n1\n\n\n{\\displaystyle x^{4}+3xy^{2}+5x^{3}-1}\n\n. Each term is either a constant, a variable, or a product of a constant and variables. Each variable can be raised to a positive-integer power. A monomial is a polynomial with one term while two- and three-term polynomials are called binomials and trinomials. The degree of a polynomial is the maximal value (among its terms) of the sum of the exponents of the variables (4 in the above example).[32] Polynomials of degree one are called linear polynomials. Linear algebra studies systems of linear polynomials.[33] A polynomial is said to be univariate or multivariate, depending on whether it uses one or more variables.[34]\n\nFactorization is a method used to simplify polynomials, making it easier to analyze them and determine the values for which they evaluate to zero. Factorization consists in rewriting a polynomial as a product of several factors. For example, the polynomial \n\n\n\n\nx\n\n2\n\n\n−\n3\nx\n−\n10\n\n\n{\\displaystyle x^{2}-3x-10}\n\n can be factorized as \n\n\n\n(\nx\n+\n2\n)\n(\nx\n−\n5\n)\n\n\n{\\displaystyle (x+2)(x-5)}\n\n. The polynomial as a whole is zero if and only if one of its factors is zero, i.e., if \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is either −2 or 5.[35] Before the 19th century, much of algebra was devoted to polynomial equations, that is equations obtained by equating a polynomial to zero. The first attempts for solving polynomial equations were to express the solutions in terms of nth roots. The solution of a second-degree polynomial equation of the form \n\n\n\na\n\nx\n\n2\n\n\n+\nb\nx\n+\nc\n=\n0\n\n\n{\\displaystyle ax^{2}+bx+c=0}\n\n is given by the quadratic formula[36]\n\n\n\n\nx\n=\n\n\n\n−\nb\n±\n\n\n\nb\n\n2\n\n\n−\n4\na\nc\n \n\n\n\n\n2\na\n\n\n\n.\n\n\n{\\displaystyle x={\\frac {-b\\pm {\\sqrt {b^{2}-4ac\\ }}}{2a}}.}\n\n\nSolutions for the degrees 3 and 4 are given by the cubic and quartic formulas. There are no general solutions for higher degrees, as proven in the 19th century by the Abel–Ruffini theorem.[37] Even when general solutions do not exist, approximate solutions can be found by numerical tools like the Newton–Raphson method.[38]\n\nThe fundamental theorem of algebra asserts that every univariate polynomial equation of positive degree with real or complex coefficients has at least one complex solution. Consequently, every polynomial of a positive degree can be factorized into linear polynomials. This theorem was proved at the beginning of the 19th century, but this does not close the problem since the theorem does not provide any way for computing the solutions.[39]\n\nLinear algebra starts with the study of systems of linear equations.[40] An equation is linear if it can be expressed in the form \n\n\n\n\na\n\n1\n\n\n\nx\n\n1\n\n\n+\n\na\n\n2\n\n\n\nx\n\n2\n\n\n+\n.\n.\n.\n+\n\na\n\nn\n\n\n\nx\n\nn\n\n\n=\nb\n\n\n{\\displaystyle a_{1}x_{1}+a_{2}x_{2}+...+a_{n}x_{n}=b}\n\n where \n\n\n\n\na\n\n1\n\n\n\n\n{\\displaystyle a_{1}}\n\n, \n\n\n\n\na\n\n2\n\n\n\n\n{\\displaystyle a_{2}}\n\n, ..., \n\n\n\n\na\n\nn\n\n\n\n\n{\\displaystyle a_{n}}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n are constants. Examples are \n\n\n\n\nx\n\n1\n\n\n−\n7\n\nx\n\n2\n\n\n+\n3\n\nx\n\n3\n\n\n=\n0\n\n\n{\\displaystyle x_{1}-7x_{2}+3x_{3}=0}\n\n and \n\n\n\n\n\n1\n4\n\n\nx\n−\ny\n=\n4\n\n\n{\\textstyle {\\frac {1}{4}}x-y=4}\n\n. A system of linear equations is a set of linear equations for which one is interested in common solutions.[41]\n\nMatrices are rectangular arrays of values that have been originally introduced for having a compact and synthetic notation for systems of linear equations.[42] For example, the system of equations\n\n\n\n\n\n\n\n\n9\n\nx\n\n1\n\n\n+\n3\n\nx\n\n2\n\n\n−\n13\n\nx\n\n3\n\n\n\n\n\n=\n0\n\n\n\n\n2.3\n\nx\n\n1\n\n\n+\n7\n\nx\n\n3\n\n\n\n\n\n=\n9\n\n\n\n\n−\n5\n\nx\n\n1\n\n\n−\n17\n\nx\n\n2\n\n\n\n\n\n=\n−\n3\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}9x_{1}+3x_{2}-13x_{3}&=0\\\\2.3x_{1}+7x_{3}&=9\\\\-5x_{1}-17x_{2}&=-3\\end{aligned}}}\n\n\ncan be written as \n\n\n\n\nA\nX\n=\nB\n,\n\n\n{\\displaystyle AX=B,}\n\n\nwhere \n\n\n\nA\n,\nX\n\n\n{\\displaystyle A,X}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n are the matrices\n\n\n\n\nA\n=\n\n\n[\n\n\n\n9\n\n\n3\n\n\n−\n13\n\n\n\n\n2.3\n\n\n0\n\n\n7\n\n\n\n\n−\n5\n\n\n−\n17\n\n\n0\n\n\n\n]\n\n\n,\n\nX\n=\n\n\n[\n\n\n\n\nx\n\n1\n\n\n\n\n\n\n\nx\n\n2\n\n\n\n\n\n\n\nx\n\n3\n\n\n\n\n\n]\n\n\n,\n\nB\n=\n\n\n[\n\n\n\n0\n\n\n\n\n9\n\n\n\n\n−\n3\n\n\n\n]\n\n\n.\n\n\n{\\displaystyle A={\\begin{bmatrix}9&3&-13\\\\2.3&0&7\\\\-5&-17&0\\end{bmatrix}},\\quad X={\\begin{bmatrix}x_{1}\\\\x_{2}\\\\x_{3}\\end{bmatrix}},\\quad B={\\begin{bmatrix}0\\\\9\\\\-3\\end{bmatrix}}.}\n\n\n\nUnder some conditions on the number of rows and columns, matrices can be added, multiplied, and sometimes inverted. All methods for solving linear systems may be expressed as matrix manipulations using these operations. For example, solving the above system consists of computing an inverted matrix \n\n\n\n\nA\n\n−\n1\n\n\n\n\n{\\displaystyle A^{-1}}\n\n such that \n\n\n\n\nA\n\n−\n1\n\n\nA\n=\nI\n,\n\n\n{\\displaystyle A^{-1}A=I,}\n\n where \n\n\n\nI\n\n\n{\\displaystyle I}\n\n is the identity matrix. Then, multiplying on the left both members of the above matrix equation by \n\n\n\n\nA\n\n−\n1\n\n\n,\n\n\n{\\displaystyle A^{-1},}\n\n one gets the solution of the system of linear equations as[43]\n\n\n\n\nX\n=\n\nA\n\n−\n1\n\n\nB\n.\n\n\n{\\displaystyle X=A^{-1}B.}\n\n\n\nMethods of solving systems of linear equations range from the introductory, like substitution[44] and elimination,[45] to more advanced techniques using matrices, such as Cramer's rule, the Gaussian elimination, and LU decomposition.[46] Some systems of equations are inconsistent, meaning that no solutions exist because the equations contradict each other.[47][f] Consistent systems have either one unique solution or an infinite number of solutions.[48][g]\n\nThe study of vector spaces and linear maps form a large part of linear algebra. A vector space is an algebraic structure formed by a set with an addition that makes it an abelian group and a scalar multiplication that is compatible with addition (see vector space for details). A linear map is a function between vector spaces that is compatible with addition and scalar multiplication. In the case of finite-dimensional vector spaces, vectors and linear maps can be represented by matrices. It follows that the theories of matrices and finite-dimensional vector spaces are essentially the same. In particular, vector spaces provide a third way for expressing and manipulating systems of linear equations.[49] From this perspective, a matrix is a representation of a linear map: if one chooses a particular basis to describe the vectors being transformed, then the entries in the matrix give the results of applying the linear map to the basis vectors.[50]\n\nSystems of equations can be interpreted as geometric figures. For systems with two variables, each equation represents a line in two-dimensional space. The point where the two lines intersect is the solution of the full system because this is the only point that solves both the first and the second equation. For inconsistent systems, the two lines run parallel, meaning that there is no solution since they never intersect. If two equations are not independent then they describe the same line, meaning that every solution of one equation is also a solution of the other equation. These relations make it possible to seek solutions graphically by plotting the equations and determining where they intersect.[51] The same principles also apply to systems of equations with more variables, with the difference being that the equations do not describe lines but higher dimensional figures. For instance, equations with three variables correspond to planes in three-dimensional space, and the points where all planes intersect solve the system of equations.[52]\n\nAbstract algebra, also called modern algebra,[53] is the study of algebraic structures. An algebraic structure is a framework for understanding operations on mathematical objects, like the addition of numbers. While elementary algebra and linear algebra work within the confines of particular algebraic structures, abstract algebra takes a more general approach that compares how algebraic structures differ from each other and what types of algebraic structures there are, such as groups, rings, and fields.[54] The key difference between these types of algebraic structures lies in the number of operations they use and the laws they obey.[55] In mathematics education, abstract algebra refers to an advanced undergraduate course that mathematics majors take after completing courses in linear algebra.[56]\n\nOn a formal level, an algebraic structure is a set[h] of mathematical objects, called the underlying set, together with one or several operations.[i] Abstract algebra is primarily interested in binary operations,[j] which take any two objects from the underlying set as inputs and map them to another object from this set as output.[60] For example, the algebraic structure \n\n\n\n⟨\n\nN\n\n,\n+\n⟩\n\n\n{\\displaystyle \\langle \\mathbb {N} ,+\\rangle }\n\n has the natural numbers (\n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n) as the underlying set and addition (\n\n\n\n+\n\n\n{\\displaystyle +}\n\n) as its binary operation.[58] The underlying set can contain mathematical objects other than numbers, and the operations are not restricted to regular arithmetic operations.[61] For instance, the underlying set of the symmetry group of a geometric object is made up of geometric transformations, such as rotations, under which the object remains unchanged. Its binary operation is function composition, which takes two transformations as input and has the transformation resulting from applying the first transformation followed by the second as its output.[62]\n\nAbstract algebra classifies algebraic structures based on the laws or axioms that its operations obey and the number of operations it uses. One of the most basic types is a group, which has one operation and requires that this operation is associative and has an identity element and inverse elements. An operation is associative if the order of several applications does not matter, i.e., if \n\n\n\n(\na\n∘\nb\n)\n∘\nc\n\n\n{\\displaystyle (a\\circ b)\\circ c}\n\n[k] is the same as \n\n\n\na\n∘\n(\nb\n∘\nc\n)\n\n\n{\\displaystyle a\\circ (b\\circ c)}\n\n for all elements. An operation has an identity element or a neutral element if one element e exists that does not change the value of any other element, i.e., if \n\n\n\na\n∘\ne\n=\ne\n∘\na\n=\na\n\n\n{\\displaystyle a\\circ e=e\\circ a=a}\n\n. An operation has inverse elements if for any element \n\n\n\na\n\n\n{\\displaystyle a}\n\n there exists a reciprocal element \n\n\n\n\na\n\n−\n1\n\n\n\n\n{\\displaystyle a^{-1}}\n\n that undoes \n\n\n\na\n\n\n{\\displaystyle a}\n\n. If an element operates on its inverse then the result is the neutral element e, expressed formally as \n\n\n\na\n∘\n\na\n\n−\n1\n\n\n=\n\na\n\n−\n1\n\n\n∘\na\n=\ne\n\n\n{\\displaystyle a\\circ a^{-1}=a^{-1}\\circ a=e}\n\n. Every algebraic structure that fulfills these requirements is a group.[64] For example, \n\n\n\n⟨\n\nZ\n\n,\n+\n⟩\n\n\n{\\displaystyle \\langle \\mathbb {Z} ,+\\rangle }\n\n is a group formed by the set of integers together with the operation of addition. The neutral element is 0 and the inverse element of any number \n\n\n\na\n\n\n{\\displaystyle a}\n\n is \n\n\n\n−\na\n\n\n{\\displaystyle -a}\n\n.[65] The natural numbers with addition, by contrast, do not form a group since they contain only positive integers and therefore lack inverse elements.[66]\n\nGroup theory examines the nature of groups, with basic theorems such as the fundamental theorem of finite abelian groups and the Feit–Thompson theorem.[67] The latter was a key early step in one of the most important mathematical achievements of the 20th century: the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 2004, that culminated in a complete classification of finite simple groups.[68]\n\nA ring is an algebraic structure with two operations that work similarly to addition and multiplication of numbers and are named and generally denoted similarly. A ring is a commutative group under addition: the addition of the ring is associative, commutative, and has an identity element and inverse elements. The multiplication is associative and distributive with respect to addition; that is, \n\n\n\na\n(\nb\n+\nc\n)\n=\na\nb\n+\na\nc\n\n\n{\\displaystyle a(b+c)=ab+ac}\n\n and \n\n\n\n(\nb\n+\nc\n)\na\n=\nb\na\n+\nc\na\n.\n\n\n{\\displaystyle (b+c)a=ba+ca.}\n\n Moreover, multiplication is associative and has an identity element generally denoted as 1.[69][l] Multiplication needs not to be commutative; if it is commutative, one has a commutative ring.[71] The ring of integers (\n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n) is one of the simplest commutative rings.[72]\n\nA field is a commutative ring such that ⁠\n\n\n\n1\n≠\n0\n\n\n{\\displaystyle 1\\neq 0}\n\n⁠ and each nonzero element has a multiplicative inverse.[73] The ring of integers does not form a field because it lacks multiplicative inverses. For example, the multiplicative inverse of \n\n\n\n7\n\n\n{\\displaystyle 7}\n\n is \n\n\n\n\n\n\n1\n7\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{7}}}\n\n, which is not an integer. The rational numbers, the real numbers, and the complex numbers each form a field with the operations of addition and multiplication.[74]\n\nRing theory is the study of rings, exploring concepts such as subrings, quotient rings, polynomial rings, and ideals as well as theorems such as Hilbert's basis theorem.[75] Field theory is concerned with fields, examining field extensions, algebraic closures, and finite fields.[76] Galois theory explores the relation between field theory and group theory, relying on the fundamental theorem of Galois theory.[77]\n\nBesides groups, rings, and fields, there are many other algebraic structures studied by algebra. They include magmas, semigroups, monoids, abelian groups, commutative rings, modules, lattices, vector spaces, algebras over a field, and associative and non-associative algebras. They differ from each other in regard to the types of objects they describe and the requirements that their operations fulfill. Many are related to each other in that a basic structure can be turned into a more advanced structure by adding additional requirements.[55] For example, a magma becomes a semigroup if its operation is associative.[78]\n\nHomomorphisms are tools to examine structural features by comparing two algebraic structures.[79] A homomorphism is a function from the underlying set of one algebraic structure to the underlying set of another algebraic structure that preserves certain structural characteristics. If the two algebraic structures use binary operations and have the form \n\n\n\n⟨\nA\n,\n∘\n⟩\n\n\n{\\displaystyle \\langle A,\\circ \\rangle }\n\n and \n\n\n\n⟨\nB\n,\n⋆\n⟩\n\n\n{\\displaystyle \\langle B,\\star \\rangle }\n\n then the function \n\n\n\nh\n:\nA\n→\nB\n\n\n{\\displaystyle h:A\\to B}\n\n is a homomorphism if it fulfills the following requirement: \n\n\n\nh\n(\nx\n∘\ny\n)\n=\nh\n(\nx\n)\n⋆\nh\n(\ny\n)\n\n\n{\\displaystyle h(x\\circ y)=h(x)\\star h(y)}\n\n. The existence of a homomorphism reveals that the operation \n\n\n\n⋆\n\n\n{\\displaystyle \\star }\n\n in the second algebraic structure plays the same role as the operation \n\n\n\n∘\n\n\n{\\displaystyle \\circ }\n\n does in the first algebraic structure.[80] Isomorphisms are a special type of homomorphism that indicates a high degree of similarity between two algebraic structures. An isomorphism is a bijective homomorphism, meaning that it establishes a one-to-one relationship between the elements of the two algebraic structures. This implies that every element of the first algebraic structure is mapped to one unique element in the second structure without any unmapped elements in the second structure.[81]\n\nAnother tool of comparison is the relation between an algebraic structure and its subalgebra.[82] The algebraic structure and its subalgebra use the same operations,[m] which follow the same axioms. The only difference is that the underlying set of the subalgebra is a subset of the underlying set of the algebraic structure.[n] All operations in the subalgebra are required to be closed in its underlying set, meaning that they only produce elements that belong to this set.[82] For example, the set of even integers together with addition is a subalgebra of the full set of integers together with addition. This is the case because the sum of two even numbers is again an even number. But the set of odd integers together with addition is not a subalgebra because it is not closed: adding two odd numbers produces an even number, which is not part of the chosen subset.[83]\n\nUniversal algebra is the study of algebraic structures in general. As part of its general perspective, it is not concerned with the specific elements that make up the underlying sets and considers operations with more than two inputs, such as ternary operations. It provides a framework for investigating what structural features different algebraic structures have in common.[85][o] One of those structural features concerns the identities that are true in different algebraic structures. In this context, an identity is a universal equation or an equation that is true for all elements of the underlying set. For example, commutativity is a universal equation that states that \n\n\n\na\n∘\nb\n\n\n{\\displaystyle a\\circ b}\n\n is identical to \n\n\n\nb\n∘\na\n\n\n{\\displaystyle b\\circ a}\n\n for all elements.[87] A variety is a class of all algebraic structures that satisfy certain identities. For example, if two algebraic structures satisfy commutativity then they are both part of the corresponding variety.[88][p][q]\n\nCategory theory examines how mathematical objects are related to each other using the concept of categories. A category is a collection of objects together with a collection of morphisms or \"arrows\" between those objects. These two collections must satisfy certain conditions. For example, morphisms can be joined, or composed: if there exists a morphism from object \n\n\n\na\n\n\n{\\displaystyle a}\n\n to object \n\n\n\nb\n\n\n{\\displaystyle b}\n\n, and another morphism from object \n\n\n\nb\n\n\n{\\displaystyle b}\n\n to object \n\n\n\nc\n\n\n{\\displaystyle c}\n\n, then there must also exist one from object \n\n\n\na\n\n\n{\\displaystyle a}\n\n to object \n\n\n\nc\n\n\n{\\displaystyle c}\n\n. Composition of morphisms is required to be associative, and there must be an \"identity morphism\" for every object.[92] Categories are widely used in contemporary mathematics since they provide a unifying framework to describe and analyze many fundamental mathematical concepts. For example, sets can be described with the category of sets, and any group can be regarded as the morphisms of a category with just one object.[93]\n\nThe origin of algebra lies in attempts to solve mathematical problems involving arithmetic calculations and unknown quantities. These developments happened in the ancient period in Babylonia, Egypt, Greece, China, and India. One of the earliest documents on algebraic problems is the Rhind Mathematical Papyrus from ancient Egypt, which was written around 1650 BCE.[r] It discusses solutions to linear equations, as expressed in problems like \"A quantity; its fourth is added to it. It becomes fifteen. What is the quantity?\" Babylonian clay tablets from around the same time explain methods to solve linear and quadratic polynomial equations, such as the method of completing the square.[95]\n\nMany of these insights found their way to the ancient Greeks. Starting in the 6th century BCE, their main interest was geometry rather than algebra, but they employed algebraic methods to solve geometric problems. For example, they studied geometric figures while taking their lengths and areas as unknown quantities to be determined, as exemplified in Pythagoras' formulation of the difference of two squares method and later in Euclid's Elements.[96] In the 3rd century CE, Diophantus provided a detailed treatment of how to solve algebraic equations in a series of books called Arithmetica. He was the first to experiment with symbolic notation to express polynomials.[97] Diophantus's work influenced Arab development of algebra with many of his methods reflected in the concepts and techniques used in medieval Arabic algebra.[98] In ancient China, The Nine Chapters on the Mathematical Art, a book composed over the period spanning from the 10th century BCE to the 2nd century CE,[99] explored various techniques for solving algebraic equations, including the use of matrix-like constructs.[100]\n\nThere is no unanimity as to whether these early developments are part of algebra or only precursors. They offered solutions to algebraic problems but did not conceive them in an abstract and general manner, focusing instead on specific cases and applications.[101] This changed with the Persian mathematician al-Khwarizmi,[s] who published his The Compendious Book on Calculation by Completion and Balancing in 825 CE. It presents the first detailed treatment of general methods that can be used to manipulate linear and quadratic equations by \"reducing\" and \"balancing\" both sides.[103] Other influential contributions to algebra came from the Arab mathematician Thābit ibn Qurra also in the 9th century and the Persian mathematician Omar Khayyam in the 11th and 12th centuries.[104]\n\nIn India, Brahmagupta investigated how to solve quadratic equations and systems of equations with several variables in the 7th century CE. Among his innovations were the use of zero and negative numbers in algebraic equations.[105] The Indian mathematicians Mahāvīra in the 9th century and Bhāskara II in the 12th century further refined Brahmagupta's methods and concepts.[106] In 1247, the Chinese mathematician Qin Jiushao wrote the Mathematical Treatise in Nine Sections, which includes an algorithm for the numerical evaluation of polynomials, including polynomials of higher degrees.[107]\n\nThe Italian mathematician Fibonacci brought al-Khwarizmi's ideas and techniques to Europe in books including his Liber Abaci.[108] In 1545, the Italian polymath Gerolamo Cardano published his book Ars Magna, which covered many topics in algebra, discussed imaginary numbers, and was the first to present general methods for solving cubic and quartic equations.[109] In the 16th and 17th centuries, the French mathematicians François Viète and René Descartes introduced letters and symbols to denote variables and operations, making it possible to express equations in an abstract and concise manner. Their predecessors had relied on verbal descriptions of problems and solutions.[110] Some historians see this development as a key turning point in the history of algebra and consider what came before it as the prehistory of algebra because it lacked the abstract nature based on symbolic manipulation.[111]\n\nIn the 17th and 18th centuries, many attempts were made to find general solutions to polynomials of degree five and higher. All of them failed.[37] At the end of the 18th century, the German mathematician Carl Friedrich Gauss proved the fundamental theorem of algebra, which describes the existence of zeros of polynomials of any degree without providing a general solution.[19] At the beginning of the 19th century, the Italian mathematician Paolo Ruffini and the Norwegian mathematician Niels Henrik Abel were able to show that no general solution exists for polynomials of degree five and higher.[37] In response to and shortly after their findings, the French mathematician Évariste Galois developed what came later to be known as Galois theory, which offered a more in-depth analysis of the solutions of polynomials while also laying the foundation of group theory.[20] Mathematicians soon realized the relevance of group theory to other fields and applied it to disciplines like geometry and number theory.[112]\n\nStarting in the mid-19th century, interest in algebra shifted from the study of polynomials associated with elementary algebra towards a more general inquiry into algebraic structures, marking the emergence of abstract algebra. This approach explored the axiomatic basis of arbitrary algebraic operations.[113] The invention of new algebraic systems based on different operations and elements accompanied this development, such as Boolean algebra, vector algebra, and matrix algebra.[114] Influential early developments in abstract algebra were made by the German mathematicians David Hilbert, Ernst Steinitz, and Emmy Noether as well as the Austrian mathematician Emil Artin. They researched different forms of algebraic structures and categorized them based on their underlying axioms into types, like groups, rings, and fields.[115]\n\nThe idea of the even more general approach associated with universal algebra was conceived by the English mathematician Alfred North Whitehead in his 1898 book A Treatise on Universal Algebra. Starting in the 1930s, the American mathematician Garrett Birkhoff expanded these ideas and developed many of the foundational concepts of this field.[116] The invention of universal algebra led to the emergence of various new areas focused on the algebraization of mathematics—that is, the application of algebraic methods to other branches of mathematics. Topological algebra arose in the early 20th century, studying algebraic structures such as topological groups and Lie groups.[117] In the 1940s and 50s, homological algebra emerged, employing algebraic techniques to study homology.[118] Around the same time, category theory was developed and has since played a key role in the foundations of mathematics.[119] Other developments were the formulation of model theory and the study of free algebras.[120]\n\nThe influence of algebra is wide-reaching, both within mathematics and in its applications to other fields.[121] The algebraization of mathematics is the process of applying algebraic methods and principles to other branches of mathematics, such as geometry, topology, number theory, and calculus. It happens by employing symbols in the form of variables to express mathematical insights on a more general level, allowing mathematicians to develop formal models describing how objects interact and relate to each other.[122]\n\nOne application, found in geometry, is the use of algebraic statements to describe geometric figures. For example, the equation \n\n\n\ny\n=\n3\nx\n−\n7\n\n\n{\\displaystyle y=3x-7}\n\n describes a line in two-dimensional space while the equation \n\n\n\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n+\n\nz\n\n2\n\n\n=\n1\n\n\n{\\displaystyle x^{2}+y^{2}+z^{2}=1}\n\n corresponds to a sphere in three-dimensional space. Of special interest to algebraic geometry are algebraic varieties,[t] which are solutions to systems of polynomial equations that can be used to describe more complex geometric figures.[124] Algebraic reasoning can also solve geometric problems. For example, one can determine whether and where the line described by \n\n\n\ny\n=\nx\n+\n1\n\n\n{\\displaystyle y=x+1}\n\n intersects with the circle described by \n\n\n\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n=\n25\n\n\n{\\displaystyle x^{2}+y^{2}=25}\n\n by solving the system of equations made up of these two equations.[125] Topology studies the properties of geometric figures or topological spaces that are preserved under operations of continuous deformation. Algebraic topology relies on algebraic theories such as group theory to classify topological spaces. For example, homotopy groups classify topological spaces based on the existence of loops or holes in them.[126]\n\nNumber theory is concerned with the properties of and relations between integers. Algebraic number theory applies algebraic methods and principles to this field of inquiry. Examples are the use of algebraic expressions to describe general laws, like Fermat's Last Theorem, and of algebraic structures to analyze the behavior of numbers, such as the ring of integers.[127] The related field of combinatorics uses algebraic techniques to solve problems related to counting, arrangement, and combination of discrete objects. An example in algebraic combinatorics is the application of group theory to analyze graphs and symmetries.[128] The insights of algebra are also relevant to calculus, which uses mathematical expressions to examine rates of change and accumulation. It relies on algebra, for instance, to understand how these expressions can be transformed and what role variables play in them.[129] Algebraic logic employs the methods of algebra to describe and analyze the structures and patterns that underlie logical reasoning,[130] exploring both the relevant mathematical structures themselves and their application to concrete problems of logic.[131] It includes the study of Boolean algebra to describe propositional logic[132] as well as the formulation and analysis of algebraic structures corresponding to more complex systems of logic.[133]\n\nAlgebraic methods are also commonly employed in other areas, like the natural sciences. For example, they are used to express scientific laws and solve equations in physics, chemistry, and biology.[135] Similar applications are found in fields like economics, geography, engineering (including electronics and robotics), and computer science to express relationships, solve problems, and model systems.[136] Linear algebra plays a central role in artificial intelligence and machine learning, for instance, by enabling the efficient processing and analysis of large datasets.[137] Various fields rely on algebraic structures investigated by abstract algebra. For example, physical sciences like crystallography and quantum mechanics make extensive use of group theory,[138] which is also employed to study puzzles such as Sudoku and Rubik's cubes,[139] and origami.[140] Both coding theory and cryptology rely on abstract algebra to solve problems associated with data transmission, like avoiding the effects of noise and ensuring data security.[141]\n\nAlgebra education mostly focuses on elementary algebra, which is one of the reasons why elementary algebra is also called school algebra. It is usually not introduced until secondary education since it requires mastery of the fundamentals of arithmetic while posing new cognitive challenges associated with abstract reasoning and generalization.[143] It aims to familiarize students with the formal side of mathematics by helping them understand mathematical symbolism, for example, how variables can be used to represent unknown quantities. An additional difficulty for students lies in the fact that, unlike arithmetic calculations, algebraic expressions are often difficult to solve directly. Instead, students need to learn how to transform them according to certain laws, often with the goal of determining an unknown quantity.[144]\n\nSome tools to introduce students to the abstract side of algebra rely on concrete models and visualizations of equations, including geometric analogies, manipulatives including sticks or cups, and \"function machines\" representing equations as flow diagrams. One method uses balance scales as a pictorial approach to help students grasp basic problems of algebra. The mass of some objects on the scale is unknown and represents variables. Solving an equation corresponds to adding and removing objects on both sides in such a way that the sides stay in balance until the only object remaining on one side is the object of unknown mass.[145] Word problems are another tool to show how algebra is applied to real-life situations. For example, students may be presented with a situation in which Naomi's brother has twice as many apples as Naomi. Given that both together have twelve apples, students are then asked to find an algebraic equation that describes this situation (\n\n\n\n2\nx\n+\nx\n=\n12\n\n\n{\\displaystyle 2x+x=12}\n\n) and to determine how many apples Naomi has (\n\n\n\nx\n=\n4\n\n\n{\\displaystyle x=4}\n\n).[146]\n\nAt the university level, mathematics students encounter advanced algebra topics from linear and abstract algebra. Initial undergraduate courses in linear algebra focus on matrices, vector spaces, and linear maps. Upon completing them, students are usually introduced to abstract algebra, where they learn about algebraic structures like groups, rings, and fields, as well as the relations between them. The curriculum typically also covers specific instances of algebraic structures, such as the systems of the rational numbers, the real numbers, and the polynomials.[147]\n"
    },
    {
        "title": "Calculus",
        "content": "\n\nCalculus is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations.\n\nOriginally called infinitesimal calculus or \"the calculus of infinitesimals\", it has two major branches, differential calculus and integral calculus. The former concerns instantaneous rates of change, and the slopes of curves, while the latter concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus. They make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] It is the \"mathematical backbone\" for dealing with problems where variables change with time or another reference variable.[2]\n\nInfinitesimal calculus was formulated separately in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[3][4] Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus is widely used in science, engineering, biology, and even has applications in social science and other branches of math.[5][6]\n\nIn mathematics education, calculus is an abbreviation of both infinitesimal calculus and integral calculus, which denotes courses of elementary mathematical analysis. \n\nIn Latin, the word calculus means “small pebble”, (the diminutive of calx, meaning \"stone\"), a meaning which still persists in medicine. Because such pebbles were used for counting out distances,[7] tallying votes, and doing abacus arithmetic, the word came to be the Latin word for calculation. In this sense, it was used in English at least as early as 1672, several years before the publications of Leibniz and Newton, who wrote their mathematical texts in Latin.[8]\n\nIn addition to differential calculus and integral calculus, the term is also used for naming specific methods of computation or theories that imply some sort of computation. Examples of this usage include propositional calculus, Ricci calculus, calculus of variations, lambda calculus, sequent calculus, and process calculus. Furthermore, the term \"calculus\" has variously been applied in ethics and philosophy, for such systems as Bentham's felicific calculus, and the ethical calculus.\n\nModern calculus was developed in 17th-century Europe by Isaac Newton and Gottfried Wilhelm Leibniz (independently of each other, first publishing around the same time) but elements of it first appeared in ancient Egypt and later Greece, then in China and the Middle East, and still later again in medieval Europe and India.\n\nCalculations of volume and area, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (c. 1820 BC), but the formulae are simple instructions, with no indication as to how they were obtained.[9][10]\n\nLaying the foundations for integral calculus and foreshadowing the concept of the limit, ancient Greek mathematician Eudoxus of Cnidus (c. 390–337 BC) developed  the method of exhaustion to prove the formulas for cone and pyramid volumes.\n\nDuring the Hellenistic period, this method was further developed by Archimedes (c. 287 – c. 212 BC), who combined it with a concept of the indivisibles—a precursor to infinitesimals—allowing him to solve several problems now treated by integral calculus. In The Method of Mechanical Theorems he describes, for example, calculating the center of gravity of a solid hemisphere, the center of gravity of a frustum of a circular paraboloid, and the area of a region bounded by a parabola and one of its secant lines.[11]\n\nThe method of exhaustion was later discovered independently in China by Liu Hui in the 3rd century AD to find the area of a circle.[12][13] In the 5th century AD, Zu Gengzhi, son of Zu Chongzhi, established a method[14][15] that would later be called Cavalieri's principle to find the volume of a sphere.\n\nIn the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen (c. 965 – c. 1040 AD) derived a formula for the sum of fourth powers. He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.[16]\n\nBhāskara II (c. 1114–1185) was acquainted with some ideas of differential calculus and suggested that the \"differential coefficient\" vanishes at an extremum value of the function.[17] In his astronomical work, he gave a procedure that looked like a precursor to infinitesimal methods. Namely, if \n\n\n\nx\n≈\ny\n\n\n{\\displaystyle x\\approx y}\n\n then \n\n\n\nsin\n⁡\n(\ny\n)\n−\nsin\n⁡\n(\nx\n)\n≈\n(\ny\n−\nx\n)\ncos\n⁡\n(\ny\n)\n.\n\n\n{\\displaystyle \\sin(y)-\\sin(x)\\approx (y-x)\\cos(y).}\n\n This can be interpreted as the discovery that cosine is the derivative of sine.[18] In the 14th century, Indian mathematicians gave a non-rigorous method, resembling differentiation, applicable to some trigonometric functions. Madhava of Sangamagrama and the Kerala School of Astronomy and Mathematics stated components of calculus, but according to Victor J. Katz they were not able to \"combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the great problem-solving tool we have today\".[16]\n\nJohannes Kepler's work Stereometria Doliorum (1615) formed the basis of integral calculus.[19] Kepler developed a method to calculate the area of an ellipse by adding up the lengths of many radii drawn from a focus of the ellipse.[20]\n\nSignificant work was a treatise, the origin being Kepler's methods,[20] written by Bonaventura Cavalieri, who argued that volumes and areas should be computed as the sums of the volumes and areas of infinitesimally thin cross-sections. The ideas were similar to Archimedes' in The Method, but this treatise is believed to have been lost in the 13th century and was only rediscovered in the early 20th century, and so would have been unknown to Cavalieri. Cavalieri's work was not well respected since his methods could lead to erroneous results, and the infinitesimal quantities he introduced were disreputable at first.\n\nThe formal study of calculus brought together Cavalieri's infinitesimals with the calculus of finite differences developed in Europe at around the same time. Pierre de Fermat, claiming that he borrowed from Diophantus, introduced the concept of adequality, which represented equality up to an infinitesimal error term.[21] The combination was achieved by John Wallis, Isaac Barrow, and James Gregory, the latter two proving predecessors to the second fundamental theorem of calculus around 1670.[22][23]\n\nThe product rule and chain rule,[24] the notions of higher derivatives and Taylor series,[25] and of analytic functions[26] were used by Isaac Newton in an idiosyncratic notation which he applied to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his Principia Mathematica (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.[27]\n\nThese ideas were arranged into a true calculus of infinitesimals by Gottfried Wilhelm Leibniz, who was originally accused of plagiarism by Newton.[28] He is now regarded as an independent inventor of and contributor to calculus. His contribution was to provide a clear set of rules for working with infinitesimal quantities, allowing the computation of second and higher derivatives, and providing the product rule and chain rule, in their differential and integral forms. Unlike Newton, Leibniz put painstaking effort into his choices of notation.[29]\n\nToday, Leibniz and Newton are usually both given credit for independently inventing and developing calculus. Newton was the first to apply calculus to general physics. Leibniz developed much of the notation used in calculus today.[30]: 51–52  The basic insights that both Newton and Leibniz provided were the laws of differentiation and integration, emphasizing that differentiation and integration are inverse processes, second and higher derivatives, and the notion of an approximating polynomial series.\n\nWhen Newton and Leibniz first published their results, there was great controversy over which mathematician (and therefore which country) deserved credit. Newton derived his results first (later to be published in his Method of Fluxions), but Leibniz published his \"Nova Methodus pro Maximis et Minimis\" first. Newton claimed Leibniz stole ideas from his unpublished notes, which Newton had shared with a few members of the Royal Society. This controversy divided English-speaking mathematicians from continental European mathematicians for many years, to the detriment of English mathematics.[31] A careful examination of the papers of Leibniz and Newton shows that they arrived at their results independently, with Leibniz starting first with integration and Newton with differentiation. It is Leibniz, however, who gave the new discipline its name. Newton called his calculus \"the science of fluxions\", a term that endured in English schools into the 19th century.[32]: 100  The first complete treatise on calculus to be written in English and use the Leibniz notation was not published until 1815.[33]\n\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.[34][35]\n\nIn calculus, foundations refers to the rigorous development of the subject from axioms and definitions.  In early calculus, the use of infinitesimal quantities was thought unrigorous and was fiercely criticized by several authors, most notably Michel Rolle and Bishop Berkeley. Berkeley famously described infinitesimals as the ghosts of departed quantities in his book The Analyst in 1734.  Working out a rigorous foundation for calculus occupied mathematicians for much of the century following Newton and Leibniz, and is still to some extent an active area of research today.[36]\n\nSeveral mathematicians, including Maclaurin, tried to prove the soundness of using infinitesimals, but it would not be until 150 years later when, due to the work of Cauchy and Weierstrass, a way was finally found to avoid mere \"notions\" of infinitely small quantities.[37] The foundations of differential and integral calculus had been laid. In Cauchy's Cours d'Analyse, we find a broad range of foundational approaches, including a definition of continuity in terms of infinitesimals, and a (somewhat imprecise) prototype of an (ε, δ)-definition of limit in the definition of differentiation.[38] In his work, Weierstrass formalized the concept of limit and eliminated infinitesimals (although his definition can validate nilsquare infinitesimals). Following the work of Weierstrass, it eventually became common to base calculus on limits instead of infinitesimal quantities, though the subject is still occasionally called \"infinitesimal calculus\". Bernhard Riemann used these ideas to give a precise definition of the integral.[39] It was also during this period that the ideas of calculus were generalized to the complex plane with the development of complex analysis.[40]\n\nIn modern mathematics, the foundations of calculus are included in the field of real analysis, which contains full definitions and proofs of the theorems of calculus. The reach of calculus has also been greatly extended. Henri Lebesgue invented measure theory, based on earlier developments by Émile Borel, and used it to define integrals of all but the most pathological functions.[41] Laurent Schwartz introduced distributions, which can be used to take the derivative of any function whatsoever.[42]\n\nLimits are not the only rigorous approach to the foundation of calculus. Another way is to use Abraham Robinson's non-standard analysis. Robinson's approach, developed in the 1960s, uses technical machinery from mathematical logic to augment the real number system with infinitesimal and infinite numbers, as in the original Newton-Leibniz conception. The resulting numbers are called hyperreal numbers, and they can be used to give a Leibniz-like development of the usual rules of calculus.[43] There is also smooth infinitesimal analysis, which differs from non-standard analysis in that it mandates neglecting higher-power infinitesimals during derivations.[36] Based on the ideas of F. W. Lawvere and employing the methods of category theory, smooth infinitesimal analysis views all functions as being continuous and incapable of being expressed in terms of discrete entities. One aspect of this formulation is that the law of excluded middle does not hold.[36] The law of excluded middle is also rejected in constructive mathematics, a branch of mathematics that insists that proofs of the existence of a number, function, or other mathematical object should give a construction of the object. Reformulations of calculus in a constructive framework are generally part of the subject of constructive analysis.[36]\n\nWhile many of the ideas of calculus had been developed earlier in Greece, China, India, Iraq, Persia, and Japan, the use of calculus began in Europe, during the 17th century, when Newton and Leibniz built on the work of earlier mathematicians to introduce its basic principles.[13][27][44] The Hungarian polymath John von Neumann wrote of this work,\n\nThe calculus was the first achievement of modern mathematics and it is difficult to overestimate its importance. I think it defines more unequivocally than anything else the inception of modern mathematics, and the system of mathematical analysis, which is its logical development, still constitutes the greatest technical advance in exact thinking.[45]\nApplications of differential calculus include computations involving velocity and acceleration, the slope of a curve, and optimization.[46]: 341–453  Applications of integral calculus include computations involving area, volume, arc length, center of mass, work, and pressure.[46]: 685–700  More advanced applications include power series and Fourier series.\n\nCalculus is also used to gain a more precise understanding of the nature of space, time, and motion. For centuries, mathematicians and philosophers wrestled with paradoxes involving division by zero or sums of infinitely many numbers. These questions arise in the study of motion and area. The ancient Greek philosopher Zeno of Elea gave several famous examples of such paradoxes. Calculus provides tools, especially the limit and the infinite series, that resolve the paradoxes.[47]\n\nCalculus is usually developed by working with very small quantities. Historically, the first method of doing so was by infinitesimals. These are objects which can be treated like real numbers but which are, in some sense, \"infinitely small\".  For example, an infinitesimal number could be greater than 0, but less than any number in the sequence 1, 1/2, 1/3, ... and thus less than any positive real number. From this point of view, calculus is a collection of techniques for manipulating infinitesimals. The symbols \n\n\n\nd\nx\n\n\n{\\displaystyle dx}\n\n and \n\n\n\nd\ny\n\n\n{\\displaystyle dy}\n\n were taken to be infinitesimal, and the derivative \n\n\n\nd\ny\n\n/\n\nd\nx\n\n\n{\\displaystyle dy/dx}\n\n was their ratio.[36]\n\nThe infinitesimal approach fell out of favor in the 19th century because it was difficult to make the notion of an infinitesimal precise. In the late 19th century, infinitesimals were replaced within academia by the epsilon, delta approach to limits. Limits describe the behavior of a function at a certain input in terms of its values at nearby inputs. They capture small-scale behavior using the intrinsic structure of the real number system (as a metric space with the least-upper-bound property). In this treatment, calculus is a collection of techniques for manipulating certain limits. Infinitesimals get replaced by sequences of smaller and smaller numbers, and the infinitely small behavior of a function is found by taking the limiting behavior for these sequences. Limits were thought to provide a more rigorous foundation for calculus, and for this reason, they became the standard approach during the 20th century. However, the infinitesimal concept was revived in the 20th century with the introduction of non-standard analysis and smooth infinitesimal analysis, which provided solid foundations for the manipulation of infinitesimals.[36]\n\nDifferential calculus is the study of the definition, properties, and applications of the derivative of a function. The process of finding the derivative is called differentiation. Given a function and a point in the domain, the derivative at that point is a way of encoding the small-scale behavior of the function near that point. By finding the derivative of a function at every point in its domain, it is possible to produce a new function, called the derivative function or just the derivative of the original function. In formal terms, the derivative is a linear operator which takes a function as its input and produces a second function as its output. This is more abstract than many of the processes studied in elementary algebra, where functions usually input a number and output another number. For example, if the doubling function is given the input three, then it outputs six, and if the squaring function is given the input three, then it outputs nine. The derivative, however, can take the squaring function as an input. This means that the derivative takes all the information of the squaring function—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to produce another function. The function produced by differentiating the squaring function turns out to be the doubling function.[30]: 32 \n\nIn more explicit terms the \"doubling function\" may be denoted by g(x) = 2x and the \"squaring function\" by f(x) = x2. The \"derivative\" now takes the function f(x), defined by the expression \"x2\", as an input, that is all the information—such as that two is sent to four, three is sent to nine, four is sent to sixteen, and so on—and uses this information to output another function, the function g(x) = 2x, as will turn out.\n\nIn Lagrange's notation, the symbol for a derivative is an apostrophe-like mark called a prime. Thus, the derivative of a function called f is denoted by f′, pronounced \"f prime\" or \"f dash\". For instance, if f(x) = x2 is the squaring function, then f′(x) = 2x is its derivative (the doubling function g from above).\n\nIf the input of the function represents time, then the derivative represents change concerning time. For example, if f is a function that takes time as input and gives the position of a ball at that time as output, then the derivative of f is how the position is changing in time, that is, it is the velocity of the ball.[30]: 18–20 \n\nIf a function is linear (that is if the graph of the function is a straight line), then the function can be written as y = mx + b, where x is the independent variable, y is the dependent variable, b is the y-intercept, and:\n\nThis gives an exact value for the slope of a straight line.[48]: 6  If the graph of the function is not a straight line, however, then the change in y divided by the change in x varies. Derivatives give an exact meaning to the notion of change in output concerning change in input. To be concrete, let f be a function, and fix a point a in the domain of f. (a, f(a)) is a point on the graph of the function. If h is a number close to zero, then a + h is a number close to a. Therefore, (a + h, f(a + h)) is close to (a, f(a)). The slope between these two points is\n\nThis expression is called a difference quotient. A line through two points on a curve is called a secant line, so m is the slope of the secant line between (a, f(a)) and (a + h, f(a + h)). The second line is only an approximation to the behavior of the function at the point  a because it does not account for what happens between  a and  a + h. It is not possible to discover the behavior at  a by setting h to zero because this would require dividing by zero, which is undefined. The derivative is defined by taking the limit as h tends to zero, meaning that it considers the behavior of f for all small values of h and extracts a consistent value for the case when h equals zero:\n\nGeometrically, the derivative is the slope of the tangent line to the graph of f at  a. The tangent line is a limit of secant lines just as the derivative is a limit of difference quotients. For this reason, the derivative is sometimes called the slope of the function f.[48]: 61–63 \n\nHere is a particular example, the derivative of the squaring function at the input 3. Let f(x) = x2 be the squaring function.\n\nThe slope of the tangent line to the squaring function at the point (3, 9) is 6, that is to say, it is going up six times as fast as it is going to the right. The limit process just described can be performed for any point in the domain of the squaring function. This defines the derivative function of the squaring function or just the derivative of the squaring function for short. A computation similar to the one above shows that the derivative of the squaring function is the doubling function.[48]: 63 \n\nA common notation, introduced by Leibniz, for the derivative in the example above is\n\nIn an approach based on limits, the symbol ⁠dy/ dx⁠ is to be interpreted not as the quotient of two numbers but as a shorthand for the limit computed above.[48]: 74  Leibniz, however, did intend it to represent the quotient of two infinitesimally small numbers, dy being the infinitesimally small change in y caused by an infinitesimally small change  dx applied to x. We can also think of ⁠d/ dx⁠ as a differentiation operator, which takes a function as an input and gives another function, the derivative, as the output. For example:\n\nIn this usage, the dx in the denominator is read as \"with respect to x\".[48]: 79  Another example of correct notation could be:\n\nEven when calculus is developed using limits rather than infinitesimals, it is common to manipulate symbols like  dx and dy as if they were real numbers; although it is possible to avoid such manipulations, they are sometimes notationally convenient in expressing operations such as the total derivative.\n\nIntegral calculus is the study of the definitions, properties, and applications of two related concepts, the indefinite integral and the definite integral. The process of finding the value of an integral is called integration.[46]: 508  The indefinite integral, also known as the antiderivative, is the inverse operation to the derivative.[48]: 163–165  F is an indefinite integral of f when f is a derivative of F.  (This use of lower- and upper-case letters for a function and its indefinite integral is common in calculus.) The definite integral inputs a function and outputs a number, which gives the algebraic sum of areas between the graph of the input and the x-axis. The technical definition of the definite integral involves the limit of a sum of areas of rectangles, called a Riemann sum.[49]: 282 \n\nA motivating example is the distance traveled in a given time.[48]: 153  If the speed is constant, only multiplication is needed:\n\nBut if the speed changes, a more powerful method of finding the distance is necessary. One such method is to approximate the distance traveled by breaking up the time into many short intervals of time, then multiplying the time elapsed in each interval by one of the speeds in that interval, and then taking the sum (a Riemann sum) of the approximate distance traveled in each interval. The basic idea is that if only a short time elapses, then the speed will stay more or less the same. However, a Riemann sum only gives an approximation of the distance traveled. We must take the limit of all such Riemann sums to find the exact distance traveled.\n\nWhen velocity is constant, the total distance traveled over the given time interval can be computed by multiplying velocity and time.  For example, traveling a steady 50 mph for 3 hours results in a total distance of 150 miles.  Plotting the velocity as a function of time yields a rectangle with a height equal to the velocity and a width equal to the time elapsed.  Therefore, the product of velocity and time also calculates the rectangular area under the (constant) velocity curve.[46]: 535   This connection between the area under a curve and the distance traveled can be extended to any irregularly shaped region exhibiting a fluctuating velocity over a given period. If f(x) represents speed as it varies over time, the distance traveled between the times represented by  a and b is the area of the region between f(x) and the x-axis, between x = a and x = b.\n\nTo approximate that area, an intuitive method would be to divide up the distance between  a and b into several equal segments, the length of each segment represented by the symbol Δx. For each small segment, we can choose one value of the function f(x). Call that value h. Then the area of the rectangle with base Δx and height h gives the distance (time Δx multiplied by speed h) traveled in that segment.   Associated with each segment is the average value of the function above it, f(x) = h. The sum of all such rectangles gives an approximation of the area between the axis and the curve, which is an approximation of the total distance traveled. A smaller value for Δx will give more rectangles and in most cases a better approximation, but for an exact answer, we need to take a limit as Δx approaches zero.[46]: 512–522 \n\nThe symbol of integration is \n\n\n\n∫\n\n\n{\\displaystyle \\int }\n\n, an elongated S chosen to suggest summation.[46]: 529  The definite integral is written as:\n\nand is read \"the integral from a to b of f-of-x with respect to x.\" The Leibniz notation  dx is intended to suggest dividing the area under the curve into an infinite number of rectangles so that their width Δx becomes the infinitesimally small  dx.[30]: 44 \n\nThe indefinite integral, or antiderivative, is written:\n\nFunctions differing by only a constant have the same derivative, and it can be shown that the antiderivative of a given function is a family of functions differing only by a constant.[49]: 326  Since the derivative of the function y = x2 + C, where C is any constant, is y′ = 2x, the antiderivative of the latter is given by:\n\nThe unspecified constant  C present in the indefinite integral or antiderivative is known as the constant of integration.[50]: 135 \n\nThe fundamental theorem of calculus states that differentiation and integration are inverse operations.[49]: 290  More precisely, it relates the values of antiderivatives to definite integrals. Because it is usually easier to compute an antiderivative than to apply the definition of a definite integral, the fundamental theorem of calculus provides a practical way of computing definite integrals. It can also be interpreted as a precise statement of the fact that differentiation is the inverse of integration.\n\nThe fundamental theorem of calculus states: If a function f is continuous on the interval [a, b] and if F is a function whose derivative is f on the interval (a, b), then\n\nFurthermore, for every x in the interval (a, b),\n\nThis realization, made by both Newton and Leibniz, was key to the proliferation of analytic results after their work became known. (The extent to which Newton and Leibniz were influenced by immediate predecessors, and particularly what Leibniz may have learned from the work of Isaac Barrow, is difficult to determine because of the priority dispute between them.[51]) The fundamental theorem provides an algebraic method of computing many definite integrals—without performing limit processes—by finding formulae for antiderivatives. It is also a prototype solution of a differential equation. Differential equations relate an unknown function to its derivatives and are ubiquitous in the sciences.[52]: 351–352 \n\nCalculus is used in every branch of the physical sciences,[53]: 1  actuarial science, computer science, statistics, engineering, economics, business, medicine, demography, and in other fields wherever a problem can be mathematically modeled and an optimal solution is desired.[54] It allows one to go from (non-constant) rates of change to the total change or vice versa, and many times in studying a problem we know one and are trying to find the other.[55] Calculus can be used in conjunction with other mathematical disciplines. For example, it can be used with linear algebra to find the \"best fit\" linear approximation for a set of points in a domain. Or, it can be used in probability theory to determine the expectation value of a continuous random variable given a probability density function.[56]: 37  In analytic geometry, the study of graphs of functions, calculus is used to find high points and low points (maxima and minima), slope, concavity and inflection points. Calculus is also used to find approximate solutions to equations; in practice, it is the standard way to solve differential equations and do root finding in most applications. Examples are methods such as Newton's method, fixed point iteration, and linear approximation. For instance, spacecraft use a variation of the Euler method to approximate curved courses within zero-gravity environments.\n\nPhysics makes particular use of calculus; all concepts in classical mechanics and electromagnetism are related through calculus. The mass of an object of known density, the moment of inertia of objects, and the potential energies due to gravitational and electromagnetic forces can all be found by the use of calculus. An example of the use of calculus in mechanics is Newton's second law of motion, which states that the derivative of an object's momentum concerning time equals the net force upon it. Alternatively, Newton's second law can be expressed by saying that the net force equals the object's mass times its acceleration, which is the time derivative of velocity and thus the second time derivative of spatial position. Starting from knowing how an object is accelerating, we use calculus to derive its path.[57]\n\nMaxwell's theory of electromagnetism and Einstein's theory of general relativity are also expressed in the language of differential calculus.[58][59]: 52–55  Chemistry also uses calculus in determining reaction rates[60]: 599  and in studying radioactive decay.[60]: 814  In biology, population dynamics starts with reproduction and death rates to model population changes.[61][62]: 631 \n\nGreen's theorem, which gives the relationship between a line integral around a simple closed curve C and a double integral over the plane region D bounded by C, is applied in an instrument known as a planimeter, which is used to calculate the area of a flat surface on a drawing.[63] For example, it can be used to calculate the amount of area taken up by an irregularly shaped flower bed or swimming pool when designing the layout of a piece of property.\n\nIn the realm of medicine, calculus can be used to find the optimal branching angle of a blood vessel to maximize flow.[64] Calculus can be applied to understand how quickly a drug is eliminated from a body or how quickly a cancerous tumor grows.[65]\n\nIn economics, calculus allows for the determination of maximal profit by providing a way to easily calculate both marginal cost and marginal revenue.[66]: 387 \n"
    },
    {
        "title": "Mathematical analysis",
        "content": "\n\nAnalysis is the branch of mathematics dealing with continuous functions, limits, and related theories, such as differentiation, integration, measure, infinite sequences, series, and analytic functions.[1][2]\n\nThese theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.\nAnalysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).\n\nMathematical analysis formally developed in the 17th century during the Scientific Revolution,[3] but many of its ideas can be traced back to earlier mathematicians. Early results in analysis were implicitly present in the early days of ancient Greek mathematics. For instance, an infinite geometric sum is implicit in Zeno's paradox of the dichotomy.[4] (Strictly speaking, the point of the paradox is to deny that the infinite sum exists.) Later, Greek mathematicians such as Eudoxus and Archimedes made more explicit, but informal, use of the concepts of limits and convergence when they used the method of exhaustion to compute the area and volume of regions and solids.[5] The explicit use of infinitesimals appears in Archimedes' The Method of Mechanical Theorems, a work rediscovered in the 20th century.[6] In Asia, the Chinese mathematician Liu Hui used the method of exhaustion in the 3rd century CE to find the area of a circle.[7] From Jain literature, it appears that Hindus were in possession of the formulae for the sum of the arithmetic and geometric series as early as the 4th century BCE.[8]\nĀcārya Bhadrabāhu uses the sum of a geometric series in his Kalpasūtra in 433 BCE.[9]\n\nZu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere in the 5th century.[10] In the 12th century, the Indian mathematician Bhāskara II used infinitesimal and used what is now known as Rolle's theorem.[11]\n\nIn the 14th century, Madhava of Sangamagrama developed infinite series expansions, now called Taylor series, of functions such as sine, cosine, tangent and arctangent.[12] Alongside his development of Taylor series of trigonometric functions, he also estimated the magnitude of the error terms resulting of truncating these series, and gave a rational approximation of some infinite series.  His followers at the Kerala School of Astronomy and Mathematics further expanded his works, up to the 16th century.\n\nThe modern foundations of mathematical analysis were established in 17th century Europe.[3] This began when Fermat and Descartes developed analytic geometry, which is the precursor to modern calculus. Fermat's method of adequality allowed him to determine the maxima and minima of functions and the tangents of curves.[13] Descartes's publication of La Géométrie in 1637, which introduced the Cartesian coordinate system, is considered to be the establishment of mathematical analysis. It would be a few decades later that Newton and Leibniz independently developed infinitesimal calculus, which grew, with the stimulus of applied work that continued through the 18th century, into analysis topics such as the calculus of variations, ordinary and partial differential equations, Fourier analysis, and generating functions. During this period, calculus techniques were applied to approximate discrete problems by continuous ones.\n\nIn the 18th century, Euler introduced the notion of a mathematical function.[14] Real analysis began to emerge as an independent subject when Bernard Bolzano introduced the modern definition of continuity in 1816,[15] but Bolzano's work did not become widely known until the 1870s. In 1821, Cauchy began to put calculus on a firm logical foundation by rejecting the principle of the generality of algebra widely used in earlier work, particularly by Euler.  Instead, Cauchy formulated calculus in terms of geometric ideas and infinitesimals.  Thus, his definition of continuity required an infinitesimal change in x to correspond to an infinitesimal change in y.  He also introduced the concept of the Cauchy sequence, and started the formal theory of complex analysis. Poisson, Liouville, Fourier and others studied partial differential equations and harmonic analysis.  The contributions of these mathematicians and others, such as Weierstrass, developed the (ε, δ)-definition of limit approach, thus founding the modern field of mathematical analysis. Around the same time, Riemann introduced his theory of integration, and made significant advances in complex analysis.\n\nTowards the end of the 19th century, mathematicians started worrying that they were assuming the existence of a continuum of real numbers without proof. Dedekind then constructed the real numbers by Dedekind cuts, in which irrational numbers are formally defined, which serve to fill the \"gaps\" between rational numbers, thereby creating a complete set: the continuum of real numbers, which had already been developed by Simon Stevin in terms of decimal expansions. Around that time, the attempts to refine the theorems of Riemann integration led to the study of the \"size\" of the set of discontinuities of real functions.\n\nAlso, various pathological objects, (such as nowhere continuous functions, continuous but nowhere differentiable functions, and space-filling curves), commonly known as \"monsters\", began to be investigated. In this context, Jordan developed his theory of measure, Cantor developed what is now called naive set theory, and Baire proved the Baire category theorem. In the early 20th century, calculus was formalized using an axiomatic set theory. Lebesgue greatly improved measure theory, and introduced his own theory of integration, now known as Lebesgue integration, which proved to be a big improvement over Riemann's. Hilbert introduced Hilbert spaces to solve integral equations. The idea of normed vector space was in the air, and in the 1920s Banach created functional analysis.\n\nIn mathematics, a metric space is a set where a notion of distance (called a metric) between elements of the set is defined.\n\nMuch of analysis happens in some metric space; the most commonly used are the real line, the complex plane, Euclidean space, other vector spaces, and the integers. Examples of analysis without a metric include measure theory (which describes size rather than distance) and functional analysis (which studies topological vector spaces that need not have any sense of distance).\n\nFormally, a metric space is an ordered pair \n\n\n\n(\nM\n,\nd\n)\n\n\n{\\displaystyle (M,d)}\n\n where \n\n\n\nM\n\n\n{\\displaystyle M}\n\n is a set and \n\n\n\nd\n\n\n{\\displaystyle d}\n\n is a metric on \n\n\n\nM\n\n\n{\\displaystyle M}\n\n, i.e., a function\n\nsuch that for any \n\n\n\nx\n,\ny\n,\nz\n∈\nM\n\n\n{\\displaystyle x,y,z\\in M}\n\n, the following holds:\n\nBy taking the third property and letting \n\n\n\nz\n=\nx\n\n\n{\\displaystyle z=x}\n\n, it can be shown that \n\n\n\nd\n(\nx\n,\ny\n)\n≥\n0\n\n\n{\\displaystyle d(x,y)\\geq 0}\n\n     (non-negative).\n\nA sequence is an ordered list. Like a set, it contains members (also called elements, or terms). Unlike a set, order matters, and exactly the same elements can appear multiple times at different positions in the sequence. Most precisely, a sequence can be defined as a function whose domain is a countable totally ordered set, such as the natural numbers.\n\nOne of the most important properties of a sequence is convergence. Informally, a sequence converges if it has a limit. Continuing informally, a (singly-infinite) sequence has a limit if it approaches some point x, called the limit, as n becomes very large. That is, for an abstract sequence (an) (with n running from 1 to infinity understood) the distance between an and x approaches 0 as n → ∞, denoted\n\nReal analysis (traditionally, the \"theory of functions of a real variable\") is a branch of mathematical analysis dealing with the real numbers and real-valued functions of a real variable.[16][17] In particular, it deals with the analytic properties of real functions and sequences, including convergence and limits of sequences of real numbers, the calculus of the real numbers, and continuity, smoothness and related properties of real-valued functions.\n\nComplex analysis (traditionally known as the \"theory of functions of a complex variable\") is the branch of mathematical analysis that investigates functions of complex numbers.[18] It is useful in many branches of mathematics, including algebraic geometry, number theory, applied mathematics; as well as in physics, including hydrodynamics, thermodynamics, mechanical engineering, electrical engineering, and particularly, quantum field theory.\n\nComplex analysis is particularly concerned with the analytic functions of complex variables (or, more generally, meromorphic functions). Because the separate real and imaginary parts of any analytic function must satisfy Laplace's equation, complex analysis is widely applicable to two-dimensional problems in physics.\n\nFunctional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear operators acting upon these spaces and respecting these structures in a suitable sense.[19][20] The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces.  This point of view turned out to be particularly useful for the study of differential and integral equations.\n\nHarmonic analysis is a branch of mathematical analysis concerned with the representation of functions and signals as the superposition of basic waves. This includes the study of the notions of Fourier series and Fourier transforms (Fourier analysis), and of their generalizations. Harmonic analysis has applications in areas as diverse as music theory, number theory, representation theory, signal processing, quantum mechanics, tidal analysis, and neuroscience.\n\nA differential equation is a mathematical equation for an unknown function of one or several variables that relates the values of the function itself and its derivatives of various orders.[21][22][23] Differential equations play a prominent role in engineering, physics, economics, biology, and other disciplines.\n\nDifferential equations arise in many areas of science and technology, specifically whenever a deterministic relation involving some continuously varying quantities (modeled by functions) and their rates of change in space or time (expressed as derivatives) is known or postulated. This is illustrated in classical mechanics, where the motion of a body is described by its position and velocity as the time value varies. Newton's laws allow one (given the position, velocity, acceleration and various forces acting on the body) to express these variables dynamically as a differential equation for the unknown position of the body as a function of time. In some cases, this differential equation (called an equation of motion) may be solved explicitly.\n\nA measure on a set is a systematic way to assign a number to each suitable subset of that set, intuitively interpreted as its size.[24] In this sense, a measure is a generalization of the concepts of length, area, and volume. A particularly important example is the Lebesgue measure on a Euclidean space, which assigns the conventional length, area, and volume of Euclidean geometry to suitable subsets of the \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-dimensional Euclidean space \n\n\n\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{n}}\n\n. For instance, the Lebesgue measure of the interval \n\n\n\n\n[\n\n0\n,\n1\n\n]\n\n\n\n{\\displaystyle \\left[0,1\\right]}\n\n in the real numbers is its length in the everyday sense of the word – specifically, 1.\n\nTechnically, a measure is a function that assigns a non-negative real number or +∞ to (certain) subsets of a set \n\n\n\nX\n\n\n{\\displaystyle X}\n\n. It must assign 0 to the empty set and be (countably) additive: the measure of a 'large' subset that can be decomposed into a finite (or countable) number of 'smaller' disjoint subsets, is the sum of the measures of the \"smaller\" subsets. In general, if one wants to associate a consistent size to each subset of a given set while satisfying the other axioms of a measure, one only finds trivial examples like the counting measure. This problem was resolved by defining measure only on a sub-collection of all subsets; the so-called measurable subsets, which are required to form a \n\n\n\nσ\n\n\n{\\displaystyle \\sigma }\n\n-algebra. This means that the empty set, countable unions, countable intersections and complements of measurable subsets are measurable. Non-measurable sets in a Euclidean space, on which the Lebesgue measure cannot be defined consistently, are necessarily complicated in the sense of being badly mixed up with their complement. Indeed, their existence is a non-trivial consequence of the axiom of choice.\n\nNumerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).[25]\n\nModern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\n\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century, the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\n\nVector analysis, also called vector calculus, is a branch of mathematical analysis dealing with vector-valued functions.[26]\n\nScalar analysis is a branch of mathematical analysis dealing with values related to scale as opposed to direction. Values such as temperature are scalar because they describe the magnitude of a value without regard to direction, force, or displacement that value may or may not have.\n\nTechniques from analysis are also found in other areas such as:\n\nThe vast majority of classical mechanics, relativity, and quantum mechanics is based on applied analysis, and differential equations in particular. Examples of important differential equations include Newton's second law, the Schrödinger equation, and the Einstein field equations.\n\nFunctional analysis is also a major factor in quantum mechanics.\n\nWhen processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate individual components of a compound waveform, concentrating them for easier detection or removal.  A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.[27]\n\nTechniques from analysis are used in many areas of mathematics, including:\n"
    },
    {
        "title": "Discrete mathematics",
        "content": "Discrete mathematics is the study of mathematical structures that can be considered \"discrete\" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than \"continuous\" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic.[1][2][3] By contrast, discrete mathematics excludes topics in \"continuous mathematics\" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets[4] (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics\".[5]\n\nThe set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.\n\nResearch in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in \"discrete\" steps and store data in \"discrete\" bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems.\n\nAlthough the main objects of study in discrete mathematics are discrete objects, analytic methods from \"continuous\" mathematics are often employed as well.\n\nIn university curricula, discrete mathematics appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well.[6][7] Some high-school-level discrete mathematics textbooks have appeared as well.[8] At this level, discrete mathematics is sometimes seen as a preparatory course, like precalculus in this respect.[9]\n\nThe Fulkerson Prize is awarded for outstanding papers in discrete mathematics.\n\nTheoretical computer science includes areas of discrete mathematics relevant to computing. It draws heavily on graph theory and mathematical logic. Included within theoretical computer science is the study of algorithms and data structures. Computability studies what can be computed in principle, and has close ties to logic, while complexity studies the time, space, and other resources taken by computations. Automata theory and formal language theory are closely related to computability. Petri nets and process algebras are used to model computer systems, and methods from discrete mathematics are used in analyzing VLSI electronic circuits. Computational geometry applies algorithms to geometrical problems and representations of geometrical objects, while computer image analysis applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.\n\nInformation theory involves the quantification of information. Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: analog signals, analog coding, analog encryption.\n\nLogic is the study of the principles of valid reasoning and inference, as well as of consistency, soundness, and completeness. For example, in most systems of logic (but not in intuitionistic logic) Peirce's law (((P→Q)→P)→P) is a theorem. For classical logic, it can be easily verified with a truth table. The study of mathematical proof is particularly important in logic, and has accumulated to automated theorem proving and formal verification of software.\n\nLogical formulas are discrete structures, as are proofs, which form finite trees[10] or, more generally, directed acyclic graph structures[11][12] (with each inference step combining one or more premise branches to give a single conclusion). The truth values of logical formulas usually form a finite set, generally restricted to two values: true and false, but logic can also be continuous-valued, e.g., fuzzy logic. Concepts such as infinite proof trees or infinite derivation trees have also been studied,[13] e.g. infinitary logic.\n\nSet theory is the branch of mathematics that studies sets, which are collections of objects, such as {blue, white, red} or the (infinite) set of all prime numbers. Partially ordered sets and sets with other relations have applications in several areas.\n\nIn discrete mathematics, countable sets (including finite sets) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor's work distinguishing between different kinds of infinite set, motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics.\n\nCombinatorics studies the ways in which discrete structures can be combined or arranged.\nEnumerative combinatorics concentrates on counting the number of certain combinatorial objects - e.g. the twelvefold way provides a unified framework for counting permutations, combinations and partitions.\nAnalytic combinatorics concerns the enumeration (i.e., determining the number) of combinatorial structures using tools from complex analysis and probability theory. In contrast with enumerative combinatorics which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.\nTopological combinatorics concerns the use of techniques from topology and algebraic topology/combinatorial topology in combinatorics.\nDesign theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties.\nPartition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials. Originally a part of number theory and analysis, partition theory is now considered a part of combinatorics or an independent field.\nOrder theory is the study of partially ordered sets, both finite and infinite.\n\nGraph theory, the study of graphs and networks, is often considered part of combinatorics, but has grown large enough and distinct enough, with its own kind of problems, to be regarded as a subject in its own right.[14] Graphs are one of the prime objects of study in discrete mathematics. They are among the most ubiquitous models of both natural and human-made structures. They can model many types of relations and process dynamics in physical, biological and social systems. In computer science, they can represent networks of communication, data organization, computational devices, the flow of computation, etc. In mathematics, they are useful in geometry and certain parts of topology, e.g. knot theory. Algebraic graph theory has close links with group theory and topological graph theory has close links to topology. There are also continuous graphs; however, for the most part, research in graph theory falls within the domain of discrete mathematics.\n\nNumber theory is concerned with the properties of numbers in general, particularly integers. It has applications to cryptography and cryptanalysis, particularly with regard to modular arithmetic, diophantine equations, linear and quadratic congruences, prime numbers and primality testing. Other discrete aspects of number theory include geometry of numbers. In analytic number theory, techniques from continuous mathematics are also used. Topics that go beyond discrete objects include transcendental numbers, diophantine approximation, p-adic analysis and function fields.\n\nAlgebraic structures occur as both discrete examples and continuous examples. Discrete algebras include: Boolean algebra used in logic gates and programming; relational algebra used in databases; discrete and finite versions of groups, rings and fields are important in algebraic coding theory; discrete semigroups and monoids appear in the theory of formal languages.\n\nThere are many concepts and theories in continuous mathematics which have discrete versions, such as discrete calculus, discrete Fourier transforms, discrete geometry, discrete logarithms, discrete differential geometry, discrete exterior calculus, discrete Morse theory, discrete optimization, discrete probability theory, discrete probability distribution, difference equations, discrete dynamical systems, and discrete vector measures.\n\nIn discrete calculus and the calculus of finite differences, a function defined on an interval of the integers is usually called a sequence. A sequence could be a finite sequence from a data source or an infinite sequence from a discrete dynamical system. Such a discrete function could be defined explicitly by a list (if its domain is finite), or by a formula for its general term, or it could be given implicitly by a recurrence relation or difference equation. Difference equations are similar to differential equations, but replace differentiation by taking the difference between adjacent terms; they can be used to approximate differential equations or (more often) studied in their own right. Many questions and methods concerning differential equations have counterparts for difference equations. For instance, where there are integral transforms in harmonic analysis for studying continuous functions or analogue signals, there are discrete transforms for discrete functions or digital signals. As well as discrete metric spaces, there are more general discrete topological spaces, finite metric spaces, finite topological spaces.\n\nThe time scale calculus is a unification of the theory of difference equations with that of differential equations, which has applications to fields requiring simultaneous modelling of discrete and continuous data. Another way of modeling such a situation is the notion of hybrid dynamical systems.\n\nDiscrete geometry and combinatorial geometry are about combinatorial properties of discrete collections of geometrical objects. A long-standing topic in discrete geometry is tiling of the plane.\n\nIn algebraic geometry, the concept of a curve can be extended to discrete geometries by taking the spectra of polynomial rings over finite fields to be models of the affine spaces over that field, and letting subvarieties or spectra of other rings provide the curves that lie in that space. Although the space in which the curves appear has a finite number of points, the curves are not so much sets of points as analogues of curves in continuous settings. For example, every point of the form \n\n\n\nV\n(\nx\n−\nc\n)\n⊂\nSpec\n⁡\nK\n[\nx\n]\n=\n\n\nA\n\n\n1\n\n\n\n\n{\\displaystyle V(x-c)\\subset \\operatorname {Spec} K[x]=\\mathbb {A} ^{1}}\n\n for \n\n\n\nK\n\n\n{\\displaystyle K}\n\n a field can be studied either as \n\n\n\nSpec\n⁡\nK\n[\nx\n]\n\n/\n\n(\nx\n−\nc\n)\n≅\nSpec\n⁡\nK\n\n\n{\\displaystyle \\operatorname {Spec} K[x]/(x-c)\\cong \\operatorname {Spec} K}\n\n, a point, or as the spectrum \n\n\n\nSpec\n⁡\nK\n[\nx\n\n]\n\n(\nx\n−\nc\n)\n\n\n\n\n{\\displaystyle \\operatorname {Spec} K[x]_{(x-c)}}\n\n of the local ring at (x-c), a point together with a neighborhood around it. Algebraic varieties also have a well-defined notion of tangent space called the Zariski tangent space, making many features of calculus applicable even in finite settings.\n\nIn applied mathematics, discrete modelling is the discrete analogue of continuous modelling. In discrete modelling, discrete formulae are fit to data. A common method in this form of modelling is to use recurrence relation. Discretization concerns the process of transferring continuous models and equations into discrete counterparts, often for the purposes of making calculations easier by using approximations. Numerical analysis provides an important example.\n\nThe history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field. In graph theory, much research was motivated by attempts to prove the four color theorem, first stated in 1852, but not proved until 1976 (by Kenneth Appel and Wolfgang Haken, using substantial computer assistance).[15]\n\nIn logic, the second problem on David Hilbert's list of open problems presented in 1900 was to prove that the axioms of arithmetic are consistent. Gödel's second incompleteness theorem, proved in 1931, showed that this was not possible – at least not within arithmetic itself. Hilbert's tenth problem was to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution. In 1970, Yuri Matiyasevich proved that this could not be done.\n\nThe need to break German codes in World War II led to advances in cryptography and theoretical computer science, with the first programmable digital electronic computer being developed at England's Bletchley Park with the guidance of Alan Turing and his seminal work, On Computable Numbers.[16] The Cold War meant that cryptography remained important, with fundamental advances such as public-key cryptography being developed in the following decades. The telecommunications industry has also motivated advances in discrete mathematics, particularly in graph theory and information theory. Formal verification of statements in logic has been necessary for software development of safety-critical systems, and advances in automated theorem proving have been driven by this need.\n\nComputational geometry has been an important part of the computer graphics incorporated into modern video games and computer-aided design tools.\n\nSeveral fields of discrete mathematics, particularly theoretical computer science, graph theory, and combinatorics, are important in addressing the challenging bioinformatics problems associated with understanding the tree of life.[17]\n\nCurrently, one of the most famous open problems in theoretical computer science is the P = NP problem, which involves the relationship between the complexity classes P and NP. The Clay Mathematics Institute has offered a $1 million USD prize for the first correct proof, along with prizes for six other mathematical problems.[18]\n"
    },
    {
        "title": "Mathematical logic",
        "content": "\n\nMathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory (also known as computability theory). Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.\n\nSince its inception, mathematical logic has both contributed to and been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.\n\nThe Handbook of Mathematical Logic[1] in 1977 makes a rough division of contemporary mathematical logic into four areas:\n\nAdditionally, sometimes the field of computational complexity theory is also included together with mathematical logic.[2][3] Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp.  Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.\n\nThe mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.\n\nMathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics.[4] Mathematical logic, also called 'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the nineteenth century with the aid of an artificial notation and a rigorously deductive method.[5] Before this emergence, logic was studied with rhetoric, with calculationes,[6] through the syllogism, and with philosophy. The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.\n\nTheories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world.  Greek methods, particularly Aristotelian logic (or term logic) as found in the Organon, found wide application and acceptance in Western science and mathematics for millennia.[7] The Stoics, especially Chrysippus, began the development of propositional logic. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.\n\nIn the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic.  Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics.[8] In 1847, Vatroslav Bertić made substantial work on algebraization of logic, independently from Boole.[9] Charles Sanders Peirce later built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\n\nGottlob Frege presented an independent development of logic with quantifiers in his Begriffsschrift, published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century.  The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.\n\nFrom 1890 to 1905, Ernst Schröder published Vorlesungen über die Algebra der Logik in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\n\nConcerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.\n\nIn logic, the term arithmetic refers to the theory of the natural numbers. Giuseppe Peano[10] published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind proposed a different characterization, which lacked the formal logical character of Peano's axioms.[11] Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the  recursive definitions of addition and multiplication from the successor function and mathematical induction.\n\nIn the mid-19th century, flaws in Euclid's axioms for geometry became known.[12]  In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826,[13] mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert[14] developed a complete set of axioms for geometry, building on previous work by Pasch.[15]  The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line.  This would prove to be a major area of research in the first half of the 20th century.\n\nThe 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate.  Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (ε, δ)-definition of limit and continuous functions was already developed by Bolzano in 1817,[16] but remained relatively unknown.\nCauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34).  In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers, a definition still employed in contemporary texts.[17]\n\nGeorg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities.[18] Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895.[19]\n\nIn the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.\n\nIn 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's Entscheidungsproblem, posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.\n\nErnst Zermelo gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain.[20] To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof.[21] This paper led to the general acceptance of the axiom of choice in the mathematics community.\n\nSkepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti[22] was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard  discovered Richard's paradox.[23]\n\nZermelo provided the first set of axioms for set theory.[24] These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF). Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.\n\nIn 1910, the first volume of Principia Mathematica by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. Principia Mathematica is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics.[25]\n\nFraenkel[26] proved that the axiom of choice cannot be proved from the axioms of Zermelo's set theory with urelements. Later work by Paul Cohen[27] showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.[28]\n\nLeopold Löwenheim[29] and Thoralf Skolem[30] obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem's paradox.\n\nIn his doctoral thesis, Kurt Gödel proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic.[31] Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.\n\nIn 1931, Gödel published On Formally Undecidable Propositions of Principia Mathematica and Related Systems, which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as Gödel's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic.  Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.[a]\n\nGödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction.[32] Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory.  Gödel gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intuitionistic arithmetic in higher types.[33]\n\nThe first textbook on symbolic logic for the layman was written by Lewis Carroll,[34] author of Alice's Adventures in Wonderland, in 1896.[35]\n\nAlfred Tarski developed the basics of model theory.\n\nBeginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish Éléments de mathématique, a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words bijection, injection, and surjection, and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.\n\nThe study of computability came to be known as recursion theory or computability theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions.[b] When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept – the computable function – had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.\n\nNumerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene[36] introduced the concepts of relative computability, foreshadowed by Turing,[37] and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Georg Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.\n\nAt its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language.  The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties.[c]  Stronger classical logics such as second-order logic or infinitary logic are also studied, along with Non-classical logics such as intuitionistic logic.\n\nFirst-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.\n\nEarly results from formal logic established limitations of first-order logic. The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.\n\nGödel's completeness theorem established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic.[31] It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.\n\nGödel's incompleteness theorems establish additional limits on first-order axiomatizations.[38] The first incompleteness theorem states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic, there exists a statement that is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system). For example, in every logical system capable of expressing the Peano axioms, the Gödel sentence holds for the natural numbers but cannot be proved.\n\nHere a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called \"sufficiently strong.\" When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be reached.\n\nMany logics besides first-order logic are studied.  These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.\n\nThe most well studied infinitary logic is \n\n\n\n\nL\n\n\nω\n\n1\n\n\n,\nω\n\n\n\n\n{\\displaystyle L_{\\omega _{1},\\omega }}\n\n. In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of \n\n\n\n\nL\n\n\nω\n\n1\n\n\n,\nω\n\n\n\n\n{\\displaystyle L_{\\omega _{1},\\omega }}\n\n such as\n\nHigher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type.  The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.\n\nAnother type of logics are fixed-point logics that allow inductive definitions, like one writes for primitive recursive functions.\n\nOne can formally define an extension of first-order logic — a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic.\n\nLindström's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the downward Löwenheim–Skolem theorem is first-order logic.\n\nModal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability[39] and set-theoretic forcing.[40]\n\nIntuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true.  Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.\n\nAlgebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.\n\nSet theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo,[24] was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.\n\nOther formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF).  Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.\n\nTwo famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo,[20] was proved independent of ZF by Fraenkel,[26] but has come to be widely accepted by mathematicians.  It states that given a collection of nonempty sets there is a single set C that contains exactly one element from each set in the collection. The set C is said to \"choose\" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size.[41] This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.\n\nThe continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory.[27] This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear.[42]\n\nContemporary research in set theory includes the study of large cardinals and determinacy.  Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC.  Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line.  Determinacy refers to the possible existence of winning strategies for certain two-player games (the games are said to be determined). The existence of these strategies implies structural properties of the real line and other Polish spaces.\n\nModel theory studies the models of various formal theories.  Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.\n\nThe set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.\n\nThe method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable.[43] He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic. A modern subfield developing from this is concerned with o-minimal structures.\n\nMorley's categoricity theorem, proved by Michael D. Morley,[44] states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.\n\nA trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis.  Many special cases of this conjecture have been established.\n\nRecursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability.  Recursion theory also includes the study of generalized computability and definability.  Recursion theory grew from the work of Rózsa Péter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.[45]\n\nClassical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems.  More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.\n\nGeneralized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.\n\nContemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.\n\nAn important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.\n\nThere are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959.  The busy beaver problem, developed by Tibor Radó in 1962, is another well-known example.\n\nHilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970.[46]\n\nProof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.  Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.\n\nThe study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems.  An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods.[47]\n\nBecause proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability.   The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.  Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or translate) classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.\n\nRecent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.\n\n\"Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski).  Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls).\"[48] \"Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas).\"[48]\n\nThe study of computability theory in computer science is closely related to the study of computability in mathematical logic.  There is a difference of emphasis, however.  Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.\n\nThe theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard correspondence between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.\n\nComputer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.\n\nDescriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.\n\nIn the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.\n\nCantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated \"God made the integers; all else is the work of man,\" endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying \"No one shall expel us from the Paradise that Cantor has created.\"\n\nMathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.  In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining point to mean a point on a fixed sphere and line to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.\n\nWith the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term finitary to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.\n\nA second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of constructive. At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. \n\nIn the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a part of philosophy of mathematics. This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to intuit the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true. Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Kleene and Kreisel would later study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.\n\n\"Die Ausführung dieses Vorhabens hat eine wesentliche Verzögerung dadurch erfahren, daß in einem Stadium, in dem die Darstellung schon ihrem Abschuß nahe war, durch das Erscheinen der Arbeiten von Herbrand und von Gödel eine veränderte Situation im Gebiet der Beweistheorie entstand, welche die Berücksichtigung neuer Einsichten zur Aufgabe machte. Dabei ist der Umfang des Buches angewachsen, so daß eine Teilung in zwei Bände angezeigt erschien.\"\n\"Carrying out this plan [by Hilbert for an exposition on proof theory for mathematical logic] has experienced an essential delay because, at the stage at which the exposition was already near to its conclusion, there occurred an altered situation in the area of proof theory due to the appearance of works by Herbrand and Gödel, which necessitated the consideration of new insights. Thus the scope of this book has grown, so that a division into two volumes seemed advisable.\"\nBochenski, Jozef Maria, ed. (1959). A Precis of Mathematical Logic. Synthese Library, Vol. 1. Translated by Otto Bird. Dordrecht: Springer. doi:10.1007/978-94-017-0592-9. ISBN 9789048183296.\n\nCantor, Georg (1874). \"Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen\" (PDF). Journal für die Reine und Angewandte Mathematik. 1874 (77): 258–262. doi:10.1515/crll.1874.77.258. S2CID 199545885.\nCarroll, Lewis (1896). Symbolic Logic. Kessinger Legacy Reprints. ISBN 9781163444955.\n\nSoare, Robert Irving (22 December 2011). \"Computability Theory and Applications: The Art of Classical Computability\" (PDF). Department of Mathematics. University of Chicago. Retrieved 23 August 2017.\nSwineshead, Richard (1498). Calculationes Suiseth Anglici (in Lithuanian). Papie: Per Franciscum Gyrardengum.\n"
    },
    {
        "title": "Set theory",
        "content": "\n\nSet theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects. Although objects of any kind can be collected into a set, set theory – as a branch of mathematics – is mostly concerned with those that are relevant to mathematics as a whole.\n\nThe modern study of set theory was initiated by the German mathematicians Richard Dedekind and Georg Cantor in the 1870s. In particular, Georg Cantor is commonly considered the founder of set theory. The non-formalized systems investigated during this early stage go under the name of naive set theory. After the discovery of paradoxes within naive set theory (such as Russell's paradox, Cantor's paradox and the Burali-Forti paradox), various axiomatic systems were proposed in the early twentieth century, of which Zermelo–Fraenkel set theory (with or without the axiom of choice) is still the best-known and most studied.\n\nSet theory is commonly employed as a foundational system for the whole of mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice. Besides its foundational role, set theory also provides the framework to develop a mathematical theory of infinity, and has various applications in computer science (such as in the theory of relational algebra), philosophy, formal semantics, and evolutionary dynamics. Its foundational appeal, together with its paradoxes, and its implications for the concept of infinity and its multiple applications have made set theory an area of major interest for logicians and philosophers of mathematics. Contemporary research into set theory covers a vast array of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.\n\nThe basic notion of grouping objects has existed since at least the emergence of numbers, and the notion of treating sets as their own objects has existed since at least the Tree of Porphyry, 3rd-century AD. The simplicity and ubiquity of sets makes it hard to determine the origin of sets as now used in mathematics, however, Bernard Bolzano's Paradoxes of the Infinite (Paradoxien des Unendlichen, 1851) is generally considered the first rigorous introduction of sets to mathematics. In his work, he (among other things) expanded on Galileo's paradox, and introduced one-to-one correspondence of infinite sets, for example between the intervals \n\n\n\n[\n0\n,\n5\n]\n\n\n{\\displaystyle [0,5]}\n\n and \n\n\n\n[\n0\n,\n12\n]\n\n\n{\\displaystyle [0,12]}\n\n by the relation \n\n\n\n5\ny\n=\n12\nx\n\n\n{\\displaystyle 5y=12x}\n\n. However, he resisted saying these sets were equinumerous, and his work is generally considered to have been uninfluential in mathematics of his time.[1][2]\n\nBefore mathematical set theory, basic concepts of infinity were considered to be solidly in the domain of philosophy (see: Infinity (philosophy) and Infinity § History). Since the 5th century BC, beginning with Greek philosopher Zeno of Elea in the West (and early Indian mathematicians in the East, mathematicians had struggled with the concept of infinity. With the development of calculus in the late 17th century, philosophers began to generally distingush between actual and potential infinity, wherein mathematics was only considered in the latter. [3] Carl Friedrich Gauss famously stated: \"Infinity is nothing more than a figure of speech which helps us talk about limits. The notion of a completed infinity doesn't belong in mathematics.\"[4]\n\nDevelopment of mathematical set theory was motivated by several mathematicians. Bernhard Riemann's lecture On the Hypotheses which lie at the Foundations of Geometry (1854) proposed new ideas about topology, and about basing mathematics (especially geometry) in terms of sets or manifolds in the sense of a class (which he called Mannigfaltigkeit) now called point-set topology. The lecture was published by Richard Dedekind in 1868, along with Riemann’s paper on trigonometric series (which presented the Riemann integral), The latter was a starting point a movement in real analysis for the study of “seriously” discontinuous functions. A young Georg Cantor entered into this area, which led him to the study of point-sets. Around 1871, influenced by Riemann, Dedekind began working with sets in his publications, which dealt very clearly and precisely with equivalence relations, partitions of sets, and homomorphisms. Thus, many of the usual set-theoretic procedures of twentieth-century mathematics go back to his work. However, he did not publish a formal explanation of his set theory until 1888. \n\nSet theory, as understood by modern mathematicians, is generally considered to be founded by a single paper in 1874 by Georg Cantor titled On a Property of the Collection of All Real Algebraic Numbers.[5][6][7] In his paper, he developed the notion of cardinality, comparing the sizes of two sets by setting them in one-to-one correspondence. His \"revolutionary discovery\" was that the set of all real numbers is uncountable, that is, one cannot put all real numbers in a list. This theorem is proved using Cantor's first uncountability proof, which differs from the more familiar proof using his diagonal argument. \n\nCantor introduced fundamental constructions in set theory, such as the power set of a set A, which is the set of all possible subsets of A. He later proved that the size of the power set of A is strictly larger than the size of A, even when A is an infinite set; this result soon became known as Cantor's theorem. Cantor developed a theory of transfinite numbers, called cardinals and ordinals, which extended the arithmetic of the natural numbers. His notation for the cardinal numbers was the Hebrew letter \n\n\n\nℵ\n\n\n{\\displaystyle \\aleph }\n\n (ℵ, aleph) with a natural number subscript; for the ordinals he employed the Greek letter \n\n\n\nω\n\n\n{\\displaystyle \\omega }\n\n (ω, omega).\n\nSet theory was beginning to become an essential ingredient of the new “modern” approach to mathematics. Originally, Cantor's theory of transfinite numbers was regarded as counter-intuitive – even shocking. This caused it to encounter resistance from mathematical contemporaries such as Leopold Kronecker and Henri Poincaré and later from Hermann Weyl and L. E. J. Brouwer, while Ludwig Wittgenstein raised philosophical objections (see: Controversy over Cantor's theory).[a] Dedekind’s algebraic style only began to find followers in the 1890s\n\nDespite the controversy, Cantor's set theory gained remarkable ground around the turn of the 20th century with the work of several notable mathematicians and philosophers. Richard Dedekind, around the same time, began working with sets in his publications, and famously constructing the real numbers using Dedekind cuts. He also worked with Giuseppe Peano in developing the Peano axioms, which formalized natural-number arithmetic, using set-theoretic ideas, which also introduced the epsilon symbol for set membership. Possibly most prominently, Gottlob Frege began to develop his Foundations of Aritmetic. \n\nIn his work, Frege tries to ground all mathematics in terms of logical axioms using Cantor's cardinality. For example, the sentence \"the number of horses in the barn is four\" means that four objects fall under the concept horse in the barn. Frege attempted to explain our grasp of numbers through cardinality ('the number of...', or \n\n\n\nN\nx\n:\nF\nx\n\n\n{\\displaystyle Nx:Fx}\n\n), relying on Hume's principle. \n\nHowever, Frege's work was short-lived, as it was found by Bertrand Russell that his axioms lead to a contradiction. Specifically, Frege's Basic Law V (now known as the axiom schema of unrestricted comprehension). According to Basic Law V, for any sufficiently well-defined property, there is the set of all and only the objects that have that property. The contradiction, called Russell's paradox, is shown as follows: \n\nLet R be the set of all sets that are not members of themselves. (This set is sometimes called \"the Russell set\".)  If R is not a member of itself, then its definition entails that it is a member of itself; yet, if it is a member of itself, then it is not a member of itself, since it is the set of all sets that are not members of themselves. The resulting contradiction is Russell's paradox. In symbols:\n\nThis came around a time of several paradoxes or counter-intuitive results. For example, that the parallel postulate cannot be proved, the existence of mathematical objects that cannot be computed or explicitly described, and the existence of theorems of arithmetic that cannot be proved with Peano arithmetic. The result was a foundational crisis of mathematics.\n\nSet theory begins with a fundamental binary relation between an object o and a set A. If o is a member (or element) of A, the notation o ∈ A is used. A set is described by listing elements separated by commas, or by a characterizing property of its elements, within braces { }.[8] Since sets are objects, the membership relation can relate sets as well, i.e., sets themselves can be members of other sets.\n\nA derived binary relation between two sets is the subset relation, also called set inclusion. If all the members of set A are also members of set B, then A is a subset of B, denoted A ⊆ B. For example, {1, 2} is a subset of {1, 2, 3}, and so is {2} but {1, 4} is not. As implied by this definition, a set is a subset of itself. For cases where this possibility is unsuitable or would make sense to be rejected, the term proper subset is defined. A is called a proper subset of B if and only if A is a subset of B, but A is not equal to B. Also, 1, 2, and 3 are members (elements) of the set {1, 2, 3}, but are not subsets of it; and in turn, the subsets, such as {1}, are not members of the set {1, 2, 3}. More complicated relations can exist; for example, the set {1} is both a member and a proper subset of the set {1, {1}}.\n\nJust as arithmetic features binary operations on numbers, set theory features binary operations on sets.[9] The following is a partial list of them:\n\nSome basic sets of central importance are the set of natural numbers, the set of real numbers and the empty set – the unique set containing no elements. The empty set is also occasionally called the null set,[11] though this name is ambiguous and can lead to several interpretations.\n\nThe power set of a set A, denoted \n\n\n\n\n\nP\n\n\n(\nA\n)\n\n\n{\\displaystyle {\\mathcal {P}}(A)}\n\n, is the set whose members are all of the possible subsets of A. For example, the power set of {1, 2} is { {}, {1}, {2}, {1, 2} }. Notably, \n\n\n\n\n\nP\n\n\n(\nA\n)\n\n\n{\\displaystyle {\\mathcal {P}}(A)}\n\n contains both A and the empty set.\n\nA set is pure if all of its members are sets, all members of its members are sets, and so on. For example, the set containing only the empty set is a nonempty pure set. In modern set theory, it is common to restrict attention to the von Neumann universe of pure sets, and many systems of axiomatic set theory are designed to axiomatize the pure sets only. There are many technical advantages to this restriction, and little generality is lost, because essentially all mathematical concepts can be modeled by pure sets. Sets in the von Neumann universe are organized into a cumulative hierarchy, based on how deeply their members, members of members, etc. are nested. Each set in this hierarchy is assigned (by transfinite recursion) an ordinal number \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n, known as its rank. The rank of a pure set \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is defined to be the least ordinal that is strictly greater than the rank of any of its elements. For example, the empty set is assigned rank 0, while the set  {{}}  containing only the empty set is assigned rank 1. For each ordinal \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n, the set \n\n\n\n\nV\n\nα\n\n\n\n\n{\\displaystyle V_{\\alpha }}\n\n is defined to consist of all pure sets with rank less than \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n. The entire von Neumann universe is denoted \n\n\n\nV\n\n\n{\\displaystyle V}\n\n.\n\n\nElementary set theory can be studied informally and intuitively, and so can be taught in primary schools using Venn diagrams. The intuitive approach tacitly assumes that a set may be formed from the class of all objects satisfying any particular defining condition. This assumption gives rise to paradoxes, the simplest and best known of which are Russell's paradox and the Burali-Forti paradox. Axiomatic set theory was originally devised to rid set theory of such paradoxes.[note 1]\n\nThe most widely studied systems of axiomatic set theory imply that all sets form a cumulative hierarchy.[b] Such systems come in two flavors, those whose ontology consists of:\n\nThe above systems can be modified to allow urelements, objects that can be members of sets but that are not themselves sets and do not have any members.\n\nThe New Foundations systems of NFU (allowing urelements) and NF (lacking them), associate with Willard Van Orman Quine, are not based on a cumulative hierarchy. NF and NFU include a \"set of everything\", relative to which every set has a complement. In these systems urelements matter, because NF, but not NFU, produces sets for which the axiom of choice does not hold. Despite NF's ontology not reflecting the traditional cumulative hierarchy and violating well-foundedness, Thomas Forster has argued that it does reflect an iterative conception of set.[12]\n\nSystems of constructive set theory, such as CST, CZF, and IZF, embed their set axioms in intuitionistic instead of classical logic. Yet other systems accept classical logic but feature a nonstandard membership relation. These include rough set theory and fuzzy set theory, in which the value of an atomic formula embodying the membership relation is not simply True or False. The Boolean-valued models of ZFC are a related subject.\n\nAn enrichment of ZFC called internal set theory was proposed by Edward Nelson in 1977.[13]\n\nMany mathematical concepts can be defined precisely using only set theoretic concepts. For example, mathematical structures as diverse as graphs, manifolds, rings, vector spaces, and relational algebras can all be defined as sets satisfying various (axiomatic) properties. Equivalence and order relations are ubiquitous in mathematics, and the theory of mathematical relations can be described in set theory.[14][15]\n\nSet theory is also a promising foundational system for much of mathematics. Since the publication of the first volume of Principia Mathematica, it has been claimed that most (or even all) mathematical theorems can be derived using an aptly designed set of axioms for set theory, augmented with many definitions, using first or second-order logic. For example, properties of the natural and real numbers can be derived within set theory, as each of these number systems can be defined by representing their elements as sets of specific forms.[16]\n\nSet theory as a foundation for mathematical analysis, topology, abstract algebra, and discrete mathematics is likewise uncontroversial; mathematicians accept (in principle) that theorems in these areas can be derived from the relevant definitions and the axioms of set theory. However, it remains that few full derivations of complex mathematical theorems from set theory have been formally verified, since such formal derivations are often much longer than the natural language proofs mathematicians commonly present. One verification project, Metamath, includes human-written, computer-verified derivations of more than 12,000 theorems starting from ZFC set theory, first-order logic and propositional logic.[17]\n\nSet theory is a major area of research in mathematics with many interrelated subfields:\n\nCombinatorial set theory concerns extensions of finite combinatorics to infinite sets. This includes the study of cardinal arithmetic and the study of extensions of Ramsey's theorem such as the Erdős–Rado theorem.\n\nDescriptive set theory is the study of subsets of the real line and, more generally, subsets of Polish spaces. It begins with the study of pointclasses in the Borel hierarchy and extends to the study of more complex hierarchies such as the projective hierarchy and the Wadge hierarchy. Many properties of Borel sets can be established in ZFC, but proving these properties hold for more complicated sets requires additional axioms related to determinacy and large cardinals.\n\nThe field of effective descriptive set theory is between set theory and recursion theory. It includes the study of lightface pointclasses, and is closely related to hyperarithmetical theory. In many cases, results of classical descriptive set theory have effective versions; in some cases, new results are obtained by proving the effective version first and then extending (\"relativizing\") it to make it more broadly applicable.\n\nA recent area of research concerns Borel equivalence relations and more complicated definable equivalence relations. This has important applications to the study of invariants in many fields of mathematics.\n\nIn set theory as Cantor defined and Zermelo and Fraenkel axiomatized, an object is either a member of a set or not. In fuzzy set theory this condition was relaxed by Lotfi A. Zadeh so an object has a degree of membership in a set, a number between 0 and 1. For example, the degree of membership of a person in the set of \"tall people\" is more flexible than a simple yes or no answer and can be a real number such as 0.75.\n\nAn inner model of Zermelo–Fraenkel set theory (ZF) is a transitive class that includes all the ordinals and satisfies all the axioms of ZF. The canonical example is the constructible universe L developed by Gödel.\nOne reason that the study of inner models is of interest is that it can be used to prove consistency results. For example, it can be shown that regardless of whether a model V of ZF satisfies the continuum hypothesis or the axiom of choice, the inner model L constructed inside the original model will satisfy both the generalized continuum hypothesis and the axiom of choice. Thus the assumption that ZF is consistent (has at least one model) implies that ZF together with these two principles is consistent.\n\nThe study of inner models is common in the study of determinacy and large cardinals, especially when considering axioms such as the axiom of determinacy that contradict the axiom of choice. Even if a fixed model of set theory satisfies the axiom of choice, it is possible for an inner model to fail to satisfy the axiom of choice. For example, the existence of sufficiently large cardinals implies that there is an inner model satisfying the axiom of determinacy (and thus not satisfying the axiom of choice).[18]\n\nA large cardinal is a cardinal number with an extra property. Many such properties are studied, including inaccessible cardinals, measurable cardinals, and many more. These properties typically imply the cardinal number must be very large, with the existence of a cardinal with the specified property unprovable in Zermelo–Fraenkel set theory.\n\nDeterminacy refers to the fact that, under appropriate assumptions, certain two-player games of perfect information are determined from the start in the sense that one player must have a winning strategy. The existence of these strategies has important consequences in descriptive set theory, as the assumption that a broader class of games is determined often implies that a broader class of sets will have a topological property. The axiom of determinacy (AD) is an important object of study; although incompatible with the axiom of choice, AD implies that all subsets of the real line are well behaved (in particular, measurable and with the perfect set property). AD can be used to prove that the Wadge degrees have an elegant structure.\n\nPaul Cohen invented the method of forcing while searching for a model of ZFC in which the continuum hypothesis fails, or a model of ZF in which the axiom of choice fails. Forcing adjoins to some given model of set theory additional sets in order to create a larger model with properties determined (i.e. \"forced\") by the construction and the original model. For example, Cohen's construction adjoins additional subsets of the natural numbers without changing any of the cardinal numbers of the original model. Forcing is also one of two methods for proving relative consistency by finitistic methods, the other method being Boolean-valued models.\n\nA cardinal invariant is a property of the real line measured by a cardinal number. For example, a well-studied invariant is the smallest cardinality of a collection of meagre sets of reals whose union is the entire real line. These are invariants in the sense that any two isomorphic models of set theory must give the same cardinal for each invariant. Many cardinal invariants have been studied, and the relationships between them are often complex and related to axioms of set theory.\n\nSet-theoretic topology studies questions of general topology that are set-theoretic in nature or that require advanced methods of set theory for their solution. Many of these theorems are independent of ZFC, requiring stronger axioms for their proof. A famous problem is the normal Moore space question, a question in general topology that was the subject of intense research. The answer to the normal Moore space question was eventually proved to be independent of ZFC.\n\nFrom set theory's inception, some mathematicians have objected to it as a foundation for mathematics. The most common objection to set theory, one Kronecker voiced in set theory's earliest years, starts from the constructivist view that mathematics is loosely related to computation. If this view is granted, then the treatment of infinite sets, both in naive and in axiomatic set theory, introduces into mathematics methods and objects that are not computable even in principle. The feasibility of constructivism as a substitute foundation for mathematics was greatly increased by Errett Bishop's influential book Foundations of Constructive Analysis.[19]\n\nA different objection put forth by Henri Poincaré is that defining sets using the axiom schemas of specification and replacement, as well as the axiom of power set, introduces impredicativity, a type of circularity, into the definitions of mathematical objects. The scope of predicatively founded mathematics, while less than that of the commonly accepted Zermelo–Fraenkel theory, is much greater than that of constructive mathematics, to the point that Solomon Feferman has said that \"all of scientifically applicable analysis can be developed [using predicative methods]\".[20]\n\nLudwig Wittgenstein condemned set theory philosophically for its connotations of mathematical platonism.[21]  He wrote that \"set theory is wrong\", since it builds on the \"nonsense\" of fictitious symbolism, has \"pernicious idioms\", and that it is nonsensical to talk about \"all numbers\".[22]  Wittgenstein identified mathematics with algorithmic human deduction;[23] the need for a secure foundation for mathematics seemed, to him, nonsensical.[24]  Moreover, since human effort is necessarily finite, Wittgenstein's philosophy required an ontological commitment to radical constructivism and finitism.  Meta-mathematical statements – which, for Wittgenstein, included any statement quantifying over infinite domains, and thus almost all modern set theory – are not mathematics.[25]  Few modern philosophers have adopted Wittgenstein's views after a spectacular blunder in Remarks on the Foundations of Mathematics: Wittgenstein attempted to refute Gödel's incompleteness theorems after having only read the abstract.  As reviewers Kreisel, Bernays, Dummett, and Goodstein all pointed out, many of his critiques did not apply to the paper in full.  Only recently have philosophers such as Crispin Wright begun to rehabilitate Wittgenstein's arguments.[26]\n\nCategory theorists have proposed topos theory as an alternative to traditional axiomatic set theory. Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory.[27][28] Topoi also give a natural setting for forcing and discussions of the independence of choice from ZF, as well as providing the framework for pointless topology and Stone spaces.[29]\n\nAn active area of research is the univalent foundations and related to it homotopy type theory. Within homotopy type theory, a set may be regarded as a homotopy 0-type, with universal properties of sets arising from the inductive and recursive properties of higher inductive types. Principles such as the axiom of choice and the law of the excluded middle can be formulated in a manner corresponding to the classical formulation in set theory or perhaps in a spectrum of distinct ways unique to type theory. Some of these principles may be proven to be a consequence of other principles. The variety of formulations of these axiomatic principles allows for a detailed analysis of the formulations required in order to derive various mathematical results.[30][31]\n\nAs set theory gained popularity as a foundation for modern mathematics, there has been support for the idea of introducing the basics of naive set theory early in mathematics education.\n\nIn the US in the 1960s, the New Math experiment aimed to teach basic set theory, among other abstract concepts, to primary school students, but was met with much criticism. The math syllabus in European schools followed this trend, and currently includes the subject at different levels in all grades. Venn diagrams are widely employed to explain basic set-theoretic relationships to primary school students (even though John Venn originally devised them as part of a procedure to assess the validity of inferences in term logic).\n\nSet theory is used to introduce students to logical operators (NOT, AND, OR), and semantic or rule description (technically intensional definition[32]) of sets (e.g. \"months starting with the letter A\"), which may be useful when learning computer programming, since Boolean logic is used in various programming languages. Likewise, sets and other collection-like objects, such as multisets and lists, are common datatypes in computer science and programming.\n\nIn addition to that, sets are commonly referred to in mathematical teaching when talking about different types of numbers (the sets \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n of natural numbers, \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n of integers, \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n of real numbers, etc.), and when defining a mathematical function as a relation from one set (the domain) to another set (the range).\n"
    },
    {
        "title": "Probability",
        "content": "\n\nProbability is the branch of mathematics and statistics concerning events and numerical descriptions of how likely they are to occur.  The probability of an event is a number between 0 and 1; the larger the probability, the more likely an event is to occur.[note 1][1][2] This number is often expressed as a percentage (%), ranging from 0% to 100%. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is 1/2 (which could also be written as 0.5 or 50%).\n\nThese concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.[3]\n\nThe word probability derives from the Latin probabilitas, which can also mean \"probity\", a measure of the authority of a witness in a legal case in Europe, and often correlated with the witness's nobility. In a sense, this differs much from the modern meaning of probability, which in contrast is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference.[4]\n\nWhen dealing with random experiments – i.e., experiments that are random and well-defined – in a purely theoretical setting (like tossing a coin), probabilities can be numerically described by the number of desired outcomes, divided by the total number of all outcomes. This is referred to as theoretical probability (in contrast to empirical probability, dealing with probabilities in the context of real experiments). For example, tossing a coin twice will yield \"head-head\", \"head-tail\", \"tail-head\", and \"tail-tail\" outcomes. The probability of getting an outcome of \"head-head\" is 1 out of 4 outcomes, or, in numerical terms, 1/4, 0.25 or 25%. However, when it comes to practical application, there are two major competing categories of probability interpretations, whose adherents hold different views about the fundamental nature of probability:\n\nThe scientific study of probability is a modern development of mathematics. Gambling shows that there has been an interest in quantifying the ideas of probability throughout history, but exact mathematical descriptions arose much later. There are reasons for the slow development of the mathematics of probability. Whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues [note 2] are still obscured by superstitions.[11]\n\nAccording to Richard Jeffrey, \"Before the middle of the seventeenth century, the term 'probable' (Latin probabilis) meant approvable, and was applied in that sense, univocally, to opinion and to action. A probable action or opinion was one such as sensible people would undertake or hold, in the circumstances.\"[12] However, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence.[13]\n\nThe sixteenth-century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes[14]).\nAside from the elementary work by Cardano, the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal (1654). Christiaan Huygens (1657) gave the earliest known scientific treatment of the subject.[15] Jakob Bernoulli's Ars Conjectandi (posthumous, 1713) and Abraham de Moivre's Doctrine of Chances (1718) treated the subject as a branch of mathematics.[16] See Ian Hacking's The Emergence of Probability[4] and James Franklin's The Science of Conjecture[17] for histories of the early development of the very concept of mathematical probability.\n\nThe theory of errors may be traced back to Roger Cotes's Opera Miscellanea (posthumous, 1722), but a memoir prepared by Thomas Simpson in 1755 (printed 1756) first applied the theory to the discussion of errors of observation.[18] The reprint (1757) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. Simpson also discusses continuous errors and describes a probability curve.\n\nThe first two laws of error that were proposed both originated with Pierre-Simon Laplace. The first law was published in 1774, and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error – disregarding sign. The second law of error was proposed in 1778 by Laplace, and stated that the frequency of the error is an exponential function of the square of the error.[19] The second law of error is called the normal distribution or the Gauss law. \"It is difficult historically to attribute that law to Gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old.\"[19]\n\nDaniel Bernoulli (1778) introduced the principle of the maximum product of the probabilities of a system of concurrent errors.\n\nAdrien-Marie Legendre (1805) developed the method of least squares, and introduced it in his Nouvelles méthodes pour la détermination des orbites des comètes (New Methods for Determining the Orbits of Comets).[20] In ignorance of Legendre's contribution, an Irish-American writer, Robert Adrain, editor of \"The Analyst\" (1808), first deduced the law of facility of error,\n\n\n\n\n\nϕ\n(\nx\n)\n=\nc\n\ne\n\n−\n\nh\n\n2\n\n\n\nx\n\n2\n\n\n\n\n\n\n{\\displaystyle \\phi (x)=ce^{-h^{2}x^{2}}}\n\n\n\nwhere \n\n\n\nh\n\n\n{\\displaystyle h}\n\n is a constant depending on precision of observation, and \n\n\n\nc\n\n\n{\\displaystyle c}\n\n is a scale factor ensuring that the area under the curve equals 1. He gave two proofs, the second being essentially the same as John Herschel's (1850).[citation needed] Gauss gave the first proof that seems to have been known in Europe (the third after Adrain's) in 1809. Further proofs were given by Laplace (1810, 1812), Gauss (1823), James Ivory (1825, 1826), Hagen (1837), Friedrich Bessel (1838), W.F. Donkin (1844, 1856), and Morgan Crofton (1870). Other contributors were Ellis (1844), De Morgan (1864), Glaisher (1872), and Giovanni Schiaparelli (1875). Peters's (1856) formula[clarification needed] for r, the probable error of a single observation, is well known.\n\nIn the nineteenth century, authors on the general theory included Laplace, Sylvestre Lacroix (1816), Littrow (1833), Adolphe Quetelet (1853), Richard Dedekind (1860), Helmert (1872), Hermann Laurent (1873), Liagre, Didion and Karl Pearson. Augustus De Morgan and George Boole improved the exposition of the theory.\n\nIn 1906, Andrey Markov introduced[21] the notion of Markov chains, which played an important role in stochastic processes theory and its applications. The modern theory of probability based on measure theory was developed by Andrey Kolmogorov in 1931.[22]\n\nOn the geometric side, contributors to The Educational Times included Miller, Crofton, McColl, Wolstenholme, Watson, and Artemas Martin.[23] See integral geometry for more information.\n\nLike other theories, the theory of probability is a representation of its concepts in formal terms – that is, in terms that can be considered separately from their meaning. These formal terms are manipulated by the rules of mathematics and logic, and any results are interpreted or translated back into the problem domain.\n\nThere have been at least two successful attempts to formalize probability, namely the Kolmogorov formulation and the Cox formulation. In Kolmogorov's formulation (see also probability space), sets are interpreted as events and probability as a measure on a class of sets. In Cox's theorem, probability is taken as a primitive (i.e., not further analyzed), and the emphasis is on constructing a consistent assignment of probability values to propositions. In both cases, the laws of probability are the same, except for technical details.\n\nThere are other methods for quantifying uncertainty, such as the Dempster–Shafer theory or possibility theory, but those are essentially different and not compatible with the usually-understood laws of probability.\n\nProbability theory is applied in everyday life in risk assessment and modeling. The insurance industry and markets use actuarial science to determine pricing and make trading decisions. Governments apply probabilistic methods in environmental regulation, entitlement analysis, and financial regulation.\n\nAn example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices, which have ripple effects in the economy as a whole. An assessment by a commodity trader that a war is more likely can send that commodity's prices up or down, and signals other traders of that opinion. Accordingly, the probabilities are neither assessed independently nor necessarily rationally. The theory of behavioral finance emerged to describe the effect of such groupthink on pricing, on policy, and on peace and conflict.[24]\n\nIn addition to financial assessment, probability can be used to analyze trends in biology (e.g., disease spread) as well as ecology (e.g., biological Punnett squares).[25] As with finance, risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring, and can assist with implementing protocols to avoid encountering such circumstances. Probability is used to design games of chance so that casinos can make a guaranteed profit, yet provide payouts to players that are frequent enough to encourage continued play.[26]\n\nAnother significant application of probability theory in everyday life is reliability. Many consumer products, such as automobiles and consumer electronics, use reliability theory in product design to reduce the probability of failure. Failure probability may influence a manufacturer's decisions on a product's warranty.[27]\n\nThe cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory.\n\nConsider an experiment that can produce a number of results. The collection of all possible results is called the sample space of the experiment, sometimes denoted as \n\n\n\nΩ\n\n\n{\\displaystyle \\Omega }\n\n. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a die can produce six possible results. One collection of possible results gives an odd number on the die. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called \"events\". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.\n\nA probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that for any collection of mutually exclusive events (events with no common results, such as the events {1,6}, {3}, and {2,4}), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.[28]\n\nThe probability of an event A is written as \n\n\n\nP\n(\nA\n)\n\n\n{\\displaystyle P(A)}\n\n,[29] \n\n\n\np\n(\nA\n)\n\n\n{\\displaystyle p(A)}\n\n, or \n\n\n\n\nPr\n\n(\nA\n)\n\n\n{\\displaystyle {\\text{Pr}}(A)}\n\n.[30] This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.\n\nThe opposite or complement of an event A is the event [not A] (that is, the event of A not occurring), often denoted as \n\n\n\n\nA\n′\n\n,\n\nA\n\nc\n\n\n\n\n{\\displaystyle A',A^{c}}\n\n, \n\n\n\n\n\nA\n¯\n\n\n,\n\nA\n\n∁\n\n\n,\n¬\nA\n\n\n{\\displaystyle {\\overline {A}},A^{\\complement },\\neg A}\n\n, or \n\n\n\n\n∼\n\nA\n\n\n{\\displaystyle {\\sim }A}\n\n; its probability is given by P(not A) = 1 − P(A).[31] As an example, the chance of not rolling a six on a six-sided die is 1 – (chance of rolling a six) = 1 − ⁠1/6⁠ = ⁠5/6⁠. For a more comprehensive treatment, see Complementary event.\n\nIf two events A and B occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as \n\n\n\nP\n(\nA\n∩\nB\n)\n.\n\n\n{\\displaystyle P(A\\cap B).}\n\n\n\nIf two events, A and B are independent then the joint probability is[29]\n\n\n\n\n\nP\n(\nA\n\n\n and \n\n\nB\n)\n=\nP\n(\nA\n∩\nB\n)\n=\nP\n(\nA\n)\nP\n(\nB\n)\n.\n\n\n{\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=P(A)P(B).}\n\n\n\nFor example, if two coins are flipped, then the chance of both being heads is \n\n\n\n\n\n\n1\n2\n\n\n\n×\n\n\n\n1\n2\n\n\n\n=\n\n\n\n1\n4\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {1}{2}}\\times {\\tfrac {1}{2}}={\\tfrac {1}{4}}.}\n\n[32]\n\nIf either event A or event B can occur but never both simultaneously, then they are called mutually exclusive events.\n\nIf two events are mutually exclusive, then the probability of both occurring is denoted as \n\n\n\nP\n(\nA\n∩\nB\n)\n\n\n{\\displaystyle P(A\\cap B)}\n\n and\n\n\n\nP\n(\nA\n\n\n and \n\n\nB\n)\n=\nP\n(\nA\n∩\nB\n)\n=\n0\n\n\n{\\displaystyle P(A{\\mbox{ and }}B)=P(A\\cap B)=0}\n\nIf two events are mutually exclusive, then the probability of either occurring is denoted as \n\n\n\nP\n(\nA\n∪\nB\n)\n\n\n{\\displaystyle P(A\\cup B)}\n\n and\n\n\n\nP\n(\nA\n\n\n or \n\n\nB\n)\n=\nP\n(\nA\n∪\nB\n)\n=\nP\n(\nA\n)\n+\nP\n(\nB\n)\n−\nP\n(\nA\n∩\nB\n)\n=\nP\n(\nA\n)\n+\nP\n(\nB\n)\n−\n0\n=\nP\n(\nA\n)\n+\nP\n(\nB\n)\n\n\n{\\displaystyle P(A{\\mbox{ or }}B)=P(A\\cup B)=P(A)+P(B)-P(A\\cap B)=P(A)+P(B)-0=P(A)+P(B)}\n\n\n\nFor example, the chance of rolling a 1 or 2 on a six-sided die is \n\n\n\nP\n(\n1\n\n\n or \n\n\n2\n)\n=\nP\n(\n1\n)\n+\nP\n(\n2\n)\n=\n\n\n\n1\n6\n\n\n\n+\n\n\n\n1\n6\n\n\n\n=\n\n\n\n1\n3\n\n\n\n.\n\n\n{\\displaystyle P(1{\\mbox{ or }}2)=P(1)+P(2)={\\tfrac {1}{6}}+{\\tfrac {1}{6}}={\\tfrac {1}{3}}.}\n\n\n\nIf the events are not (necessarily) mutually exclusive then\n\n\n\nP\n\n(\n\nA\n\n\n or \n\n\nB\n\n)\n\n=\nP\n(\nA\n∪\nB\n)\n=\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n−\nP\n\n(\n\nA\n\n\n and \n\n\nB\n\n)\n\n.\n\n\n{\\displaystyle P\\left(A{\\hbox{ or }}B\\right)=P(A\\cup B)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A{\\mbox{ and }}B\\right).}\n\nRewritten,\n\n\n\nP\n\n(\n\nA\n∪\nB\n\n)\n\n=\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n\n\n{\\displaystyle P\\left(A\\cup B\\right)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)}\n\n\n\nFor example, when drawing a card from a deck of cards, the chance of getting a heart or a face card (J, Q, K) (or both) is \n\n\n\n\n\n\n13\n52\n\n\n\n+\n\n\n\n12\n52\n\n\n\n−\n\n\n\n3\n52\n\n\n\n=\n\n\n\n11\n26\n\n\n\n,\n\n\n{\\displaystyle {\\tfrac {13}{52}}+{\\tfrac {12}{52}}-{\\tfrac {3}{52}}={\\tfrac {11}{26}},}\n\n since among the 52 cards of a deck, 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the \"3 that are both\" are included in each of the \"13 hearts\" and the \"12 face cards\", but should only be counted once.\n\nThis can be expanded further for multiple not (necessarily) mutually exclusive events. For three events, this proceeds as follows:\n\n\n\n\n\n\n\nP\n\n(\n\nA\n∪\nB\n∪\nC\n\n)\n\n=\n\n\nP\n\n(\n\n\n(\n\nA\n∪\nB\n\n)\n\n∪\nC\n\n)\n\n\n\n\n\n=\n\n\nP\n\n(\n\nA\n∪\nB\n\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\n\n(\n\nA\n∪\nB\n\n)\n\n∩\nC\n\n)\n\n\n\n\n\n=\n\n\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\n\n(\n\nA\n∩\nC\n\n)\n\n∪\n\n(\n\nB\n∩\nC\n\n)\n\n\n)\n\n\n\n\n\n=\n\n\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n−\n\n(\n\nP\n\n(\n\nA\n∩\nC\n\n)\n\n+\nP\n\n(\n\nB\n∩\nC\n\n)\n\n−\nP\n\n(\n\n\n(\n\nA\n∩\nC\n\n)\n\n∩\n\n(\n\nB\n∩\nC\n\n)\n\n\n)\n\n\n)\n\n\n\n\n\nP\n\n(\n\nA\n∪\nB\n∪\nC\n\n)\n\n=\n\n\nP\n\n(\nA\n)\n\n+\nP\n\n(\nB\n)\n\n+\nP\n\n(\nC\n)\n\n−\nP\n\n(\n\nA\n∩\nB\n\n)\n\n−\nP\n\n(\n\nA\n∩\nC\n\n)\n\n−\nP\n\n(\n\nB\n∩\nC\n\n)\n\n+\nP\n\n(\n\nA\n∩\nB\n∩\nC\n\n)\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}P\\left(A\\cup B\\cup C\\right)=&P\\left(\\left(A\\cup B\\right)\\cup C\\right)\\\\=&P\\left(A\\cup B\\right)+P\\left(C\\right)-P\\left(\\left(A\\cup B\\right)\\cap C\\right)\\\\=&P\\left(A\\right)+P\\left(B\\right)-P\\left(A\\cap B\\right)+P\\left(C\\right)-P\\left(\\left(A\\cap C\\right)\\cup \\left(B\\cap C\\right)\\right)\\\\=&P\\left(A\\right)+P\\left(B\\right)+P\\left(C\\right)-P\\left(A\\cap B\\right)-\\left(P\\left(A\\cap C\\right)+P\\left(B\\cap C\\right)-P\\left(\\left(A\\cap C\\right)\\cap \\left(B\\cap C\\right)\\right)\\right)\\\\P\\left(A\\cup B\\cup C\\right)=&P\\left(A\\right)+P\\left(B\\right)+P\\left(C\\right)-P\\left(A\\cap B\\right)-P\\left(A\\cap C\\right)-P\\left(B\\cap C\\right)+P\\left(A\\cap B\\cap C\\right)\\end{aligned}}}\n\nIt can be seen, then, that this pattern can be repeated for any number of events.\n\nConditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written \n\n\n\nP\n(\nA\n∣\nB\n)\n\n\n{\\displaystyle P(A\\mid B)}\n\n, and is read \"the probability of A, given B\". It is defined by[33]\n\n\n\n\n\nP\n(\nA\n∣\nB\n)\n=\n\n\n\nP\n(\nA\n∩\nB\n)\n\n\nP\n(\nB\n)\n\n\n\n\n\n\n{\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}\\,}\n\n\n\nIf \n\n\n\nP\n(\nB\n)\n=\n0\n\n\n{\\displaystyle P(B)=0}\n\n then \n\n\n\nP\n(\nA\n∣\nB\n)\n\n\n{\\displaystyle P(A\\mid B)}\n\n is formally undefined by this expression. In this case \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n are independent, since \n\n\n\nP\n(\nA\n∩\nB\n)\n=\nP\n(\nA\n)\nP\n(\nB\n)\n=\n0.\n\n\n{\\displaystyle P(A\\cap B)=P(A)P(B)=0.}\n\n However, it is possible to define a conditional probability for some zero-probability events, for example by using a σ-algebra of such events (such as those arising from a continuous random variable).[34]\n\nFor example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is \n\n\n\n1\n\n/\n\n2\n;\n\n\n{\\displaystyle 1/2;}\n\n however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken. For example, if a red ball was taken, then the probability of picking a red ball again would be \n\n\n\n1\n\n/\n\n3\n,\n\n\n{\\displaystyle 1/3,}\n\n since only 1 red and 2 blue balls would have been remaining. And if a blue ball was taken previously, the probability of taking a red ball will be \n\n\n\n2\n\n/\n\n3.\n\n\n{\\displaystyle 2/3.}\n\n\n\nIn probability theory and applications, Bayes' rule relates the odds of event \n\n\n\n\nA\n\n1\n\n\n\n\n{\\displaystyle A_{1}}\n\n to event \n\n\n\n\nA\n\n2\n\n\n,\n\n\n{\\displaystyle A_{2},}\n\n before (prior to) and after (posterior to) conditioning on another event \n\n\n\nB\n.\n\n\n{\\displaystyle B.}\n\n The odds on \n\n\n\n\nA\n\n1\n\n\n\n\n{\\displaystyle A_{1}}\n\n to event \n\n\n\n\nA\n\n2\n\n\n\n\n{\\displaystyle A_{2}}\n\n is simply the ratio of the probabilities of the two events. When arbitrarily many events \n\n\n\nA\n\n\n{\\displaystyle A}\n\n are of interest, not just two, the rule can be rephrased as posterior is proportional to prior times likelihood, \n\n\n\nP\n(\nA\n\n|\n\nB\n)\n∝\nP\n(\nA\n)\nP\n(\nB\n\n|\n\nA\n)\n\n\n{\\displaystyle P(A|B)\\propto P(A)P(B|A)}\n\n where the proportionality symbol means that the left hand side is proportional to (i.e., equals a constant times) the right hand side as \n\n\n\nA\n\n\n{\\displaystyle A}\n\n varies, for fixed or given \n\n\n\nB\n\n\n{\\displaystyle B}\n\n (Lee, 2012; Bertsch McGrayne, 2012). In this form it goes back to Laplace (1774) and to Cournot (1843); see Fienberg (2005).\n\nIn a deterministic universe, based on Newtonian concepts, there would be no probability if all conditions were known (Laplace's demon) (but there are situations in which sensitivity to initial conditions exceeds our ability to measure them, i.e. know them).  In the case of a roulette wheel, if the force of the hand and the period of that force are known, the number on which the ball will stop would be a certainty (though as a practical matter, this would likely be true only of a roulette wheel that had not been exactly levelled – as Thomas A. Bass' Newtonian Casino revealed).  This also assumes knowledge of inertia and friction of the wheel, weight, smoothness, and roundness of the ball, variations in hand speed during the turning, and so forth. A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel. Physicists face the same situation in the kinetic theory of gases, where the system, while deterministic in principle, is so complex (with the number of molecules typically the order of magnitude of the Avogadro constant 6.02×1023) that only a statistical description of its properties is feasible.[35]\n\nProbability theory is required to describe quantum phenomena.[36] A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics. The objective wave function evolves deterministically but, according to the Copenhagen interpretation, it deals with probabilities of observing, the outcome being explained by a wave function collapse when an observation is made. However, the loss of determinism for the sake of instrumentalism did not meet with universal approval. Albert Einstein famously remarked in a letter to Max Born: \"I am convinced that God does not play dice\".[37] Like Einstein, Erwin Schrödinger, who discovered the wave function, believed quantum mechanics is a statistical approximation of an underlying deterministic reality.[38] In some modern interpretations of the statistical mechanics of measurement, quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes.\n"
    },
    {
        "title": "Statistics",
        "content": "Statistics (from German: Statistik, orig. \"description of a state, a country\"[1]) is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.[2] In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.[3]\n\nWhen census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\n\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences made using mathematical statistics employ the framework of probability theory, which deals with the analysis of random phenomena.\n\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is rejected when it is in fact true, giving a \"false positive\") and Type II errors (null hypothesis fails to be rejected when it is in fact false, giving a \"false negative\"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[4]\n\nStatistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n\n\"Statistics is both the science of uncertainty and the technology of extracting information from data.\" - featured in the International Encyclopedia of Statistical Science.[5]\nStatistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.[6][7][5]\n\nStatistics is regarded as a body of science[8] or a branch of mathematics.[9] It is based on probability, a branch of mathematics that studies random events. Statistics is considered the science of uncertainty. This arises from the ways to cope with measurement and sampling error as well as dealing with uncertanties in modelling. Although probability and statistics was once paired together as a single subject, they are conceptually distinct from one another. The former is based on deducing answers to specific situations from a general theory of probability, meanwhile statistics induces statements about a population based on a data set. Statistics serves to bridge the gap between probability and applied mathematical fields.[10][5][11]\n\nSome consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty.[12][13] Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification.[14] Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.[15]\n\nThe word statistics ultimately comes from the Latin word Status, meaning \"situation\" or \"condition\" in society, which in late Latin adopted the meaning \"state\". Derived from this, political scientist Gottfried Achenwall, coined the German word statistik (a summary of how things stand). In 1770, the term entered the English language through German and referred to the study of political arrangements. The term gained its modern meaning in the 1790s in John Sinclair's works.[16][17] In modern German, the term statistik is synonymous with mathematical statistics. The term statistic, in singular form, is used to describe a function that returns its value of the same name.[18]\n\nWhen full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting through statistical models.\n\nTo use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\n\nSampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.\n\nA common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements with different levels using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data—like natural experiments and observational studies[19]—for which a statistician would use a modified, more structured estimation method (e.g., difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.\n\nThe basic steps of a statistical experiment are:\n\nExperiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[20]\n\nAn example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a cohort study, and then look for the number of cases of lung cancer in each group.[21] A case-control study is another type of observational study in which people with and without the outcome of interest (e.g. lung cancer) are invited to participate and their exposure histories are collected.\n\nVarious attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one (injective) transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.\n\nBecause variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating-point arithmetic. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.\n\nOther categorizations have been proposed. For example, Mosteller and Tukey (1977)[22] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[23] described continuous counts, continuous ratios, count ratios, and categorical modes of data. (See also: Chrisman (1998),[24] van den Berg (1991).[25])\n\nThe issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\"[26]: 82 \n\nA descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information,[27] while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.[28]\n\nStatistical inference is the process of using data analysis to deduce properties of an underlying probability distribution.[29] Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.[30]\n\nConsider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[31] The population being examined is described by a probability distribution that may have unknown parameters.\n\nA statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\n\nA random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots include the z-score, the chi square statistic and Student's t-value.\n\nBetween two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\n\nOther desirable properties for estimators include: UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.\n\nThis still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.\n\nInterpretation of statistical information can often involve the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[32][33]\n\nThe best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence \"beyond a reasonable doubt\". However, \"failure to reject H0\" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not \"prove\" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.\n\nWhat statisticians call an alternative hypothesis is simply a hypothesis that contradicts the null hypothesis.\n\nWorking from a null hypothesis, two broad categories of error are recognized:\n\nStandard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.\n\nA statistical error is the amount by which an observation differs from its expected value. A residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).\n\nMean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.\n\nMany statistical methods seek to minimize the residual sum of squares, and these are called \"methods of least squares\" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.\n\nMeasurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.[34]\n\nMost studies only sample part of a population, so results do not fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would include the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable.  Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by \"probability\", that is as a Bayesian probability.\n\nIn principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n\nStatistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).\n\nThe standard approach[31] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator does not belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.\n\nReferring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\n\nAlthough in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.\n\nSome problems are usually associated with this framework (See criticism of hypothesis testing):\n\nSome well-known statistical tests and procedures are:\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\nMathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[1][7] All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.[8]\n\nFormal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels.[36] Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187–1268) later made an important contribution on the use of sample size in frequency analysis.[36]\n\nAlthough the term statistic was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science.[37][38] The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[39] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n\nThe mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli's posthumous work Ars Conjectandi.[40] This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis.[41] The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.[42]\n\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages.[43] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others.[44] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[45] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[46] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[47]\n\nThe second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[48][49][50] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[51] He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[52][53] In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle[54] (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway,[55][56][57][58][59][60] a concept in sexual selection about a positive feedback runaway effect found in evolution.\n\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[61]\n\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.[62]\n\nApplied statistics, sometimes referred to as Statistical science,[63] comprises descriptive statistics and the application of inferential statistics.[64][65] Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\n\nStatistical consultants can help organizations and companies that do not have in-house expertise relevant to their particular questions.\n\nMachine learning models are statistical and probabilistic models that capture patterns in the data through use of computational algorithms.\n\nStatistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research.[66] A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student's t-test, linear regression, Pearson's correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon's diversity index, Tukey's range test, cluster analysis, Spearman's rank correlation coefficient and principal component analysis.[67]\n\nA typical statistics course covers descriptive statistics, probability, binomial and normal distributions, test of hypotheses and confidence intervals, linear regression, and correlation.[68] Modern fundamental statistical courses for undergraduate students focus on correct test selection, results interpretation, and use of free statistics software.[67]\n\nThe rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\n\nIncreased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.\n\nIn business, \"statistics\" is a widely used management- and decision support tool. It is particularly applied in financial management, marketing management, and production, services and operations management.[69][70] Statistics is also heavily used in management accounting and auditing. The discipline of Management Science formalizes the use of statistics, and other mathematics, in business. (Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)\n\nA typical \"Business Statistics\" course is intended for business majors, and covers[71] descriptive statistics (collection, description, analysis, and summary of data), probability (typically the binomial and normal distributions), test of hypotheses and confidence intervals, linear regression, and correlation; (follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally. Professional certification programs, such as the CFA, often include topics in statistics.\n\nTraditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This tradition has changed with the use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n\nStatistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines include:\n\nIn addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:\n\nStatistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions.\n\nMisuse of statistics can produce subtle but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\n\nEven when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\n\nThere is a general perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[74] A mistrust and misunderstanding of statistics is associated with the quotation, \"There are three kinds of lies: lies, damned lies, and statistics\". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics,[74] by Darrell Huff, outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[75]\n\nWays to avoid misuse of statistics include using proper diagrams and avoiding bias.[76] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[77] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[76] Most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[77] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[78] According to Huff, \"The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism.\"[79]\n\nTo assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[74]\n\nThe concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death, might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables.\n"
    },
    {
        "title": "Decision theory",
        "content": "Decision theory or the theory of rational choice is a branch of probability, economics, and analytic philosophy that uses the tools of expected utility and probability to model how individuals would behave rationally under uncertainty.[1][2] It differs from the cognitive and behavioral sciences in that it is mainly prescriptive and concerned with identifying optimal decisions for a rational agent, rather than describing how people actually make decisions. Despite this, the field is important to the study of real human behavior by social scientists, as it lays the foundations to mathematically model and analyze individuals in fields such as sociology, economics, criminology, cognitive science, moral philosophy and political science.\n\nNormative decision theory is concerned with identification of optimal decisions where optimality is often determined by considering an ideal decision maker who is able to calculate with perfect accuracy and is in some sense fully rational. The practical application of this prescriptive approach (how people ought to make decisions) is called decision analysis and is aimed at finding tools, methodologies, and software (decision support systems) to help people make better decisions.[3][4]\n\nIn contrast, descriptive decision theory is concerned with describing observed behaviors often under the assumption that those making decisions are behaving under some consistent rules. These rules may, for instance, have a procedural framework (e.g. Amos Tversky's elimination by aspects model) or an axiomatic framework (e.g. stochastic transitivity axioms), reconciling the Von Neumann-Morgenstern axioms with behavioral violations of the expected utility hypothesis, or they may explicitly give a functional form for time-inconsistent utility functions (e.g. Laibson's quasi-hyperbolic discounting).[3][4]\n\nPrescriptive decision theory is concerned with predictions about behavior that positive decision theory produces to allow for further tests of the kind of decision-making that occurs in practice. In recent decades, there has also been increasing interest in \"behavioral decision theory\", contributing to a re-evaluation of what useful decision-making requires.[5][6]\n\nThe area of choice under uncertainty represents the heart of decision theory. Known from the 17th century (Blaise Pascal invoked it in his famous wager, which is contained in his Pensées, published in 1670), the idea of expected value is that, when faced with a number of actions, each of which could give rise to more than one possible outcome with different probabilities, the rational procedure is to identify all possible outcomes, determine their values (positive or negative) and the probabilities that will result from each course of action, and multiply the two to give an \"expected value\", or the average expectation for an outcome; the action to be chosen should be the one that gives rise to the highest total expected value. In 1738, Daniel Bernoulli published an influential paper entitled Exposition of a New Theory on the Measurement of Risk, in which he uses the St. Petersburg paradox to show that expected value theory must be normatively wrong. He gives an example in which a Dutch merchant is trying to decide whether to insure a cargo being sent from Amsterdam to St. Petersburg in winter. In his solution, he defines a utility function and computes expected utility rather than expected financial value.[7]\n\nIn the 20th century, interest was reignited by Abraham Wald's 1939 paper pointing out that the two central procedures of sampling-distribution-based statistical-theory, namely hypothesis testing and parameter estimation, are special cases of the general decision problem.[8] Wald's paper renewed and synthesized many concepts of statistical theory, including loss functions, risk functions, admissible decision rules, antecedent distributions, Bayesian procedures, and minimax procedures. The phrase \"decision theory\" itself was used in 1950 by E. L. Lehmann.[9]\n\nThe revival of subjective probability theory, from the work of Frank Ramsey, Bruno de Finetti, Leonard Savage and others, extended the scope of expected utility theory to situations where subjective probabilities can be used. At the time, von Neumann and Morgenstern's theory of expected utility[10] proved that expected utility maximization followed from basic postulates about rational behavior.\n\nThe work of Maurice Allais and Daniel Ellsberg showed that human behavior has systematic and sometimes important departures from expected-utility maximization (Allais paradox and Ellsberg paradox).[11] The prospect theory of Daniel Kahneman and Amos Tversky renewed the empirical study of economic behavior with less emphasis on rationality presuppositions. It describes a way by which people make decisions when all of the outcomes carry a risk.[12] Kahneman and Tversky found three regularities – in actual human decision-making, \"losses loom larger than gains\"; people focus more on changes in their utility-states than they focus on absolute utilities; and the estimation of subjective probabilities is severely biased by anchoring.\n\nIntertemporal choice is concerned with the kind of choice where different actions lead to outcomes that are realized at different stages over time.[13] It is also described as cost-benefit decision making since it involves the choices between rewards that vary according to magnitude and time of arrival.[14] If someone received a windfall of several thousand dollars, they could spend it on an expensive holiday, giving them immediate pleasure, or they could invest it in a pension scheme, giving them an income at some time in the future. What is the optimal thing to do? The answer depends partly on factors such as the expected rates of interest and inflation, the person's life expectancy, and their confidence in the pensions industry. However even with all those factors taken into account, human behavior again deviates greatly from the predictions of prescriptive decision theory, leading to alternative models in which, for example, objective interest rates are replaced by subjective discount rates.\n\nSome decisions are difficult because of the need to take into account how other people in the situation will respond to the decision that is taken. The analysis of such social decisions is often treated under decision theory, though it involves mathematical methods. In the emerging field of socio-cognitive engineering, the research is especially focused on the different types of distributed decision-making in human organizations, in normal and abnormal/emergency/crisis situations.[15]\n\nOther areas of decision theory are concerned with decisions that are difficult simply because of their complexity, or the complexity of the organization that has to make them. Individuals making decisions are limited in resources (i.e. time and intelligence) and are therefore boundedly rational; the issue is thus, more than the deviation between real and optimal behavior, the difficulty of determining the optimal behavior in the first place. Decisions are also affected by whether options are framed together or separately; this is known as the distinction bias.\n\nHeuristics are procedures for making a decision without working out the consequences of every option. Heuristics decrease the amount of evaluative thinking required for decisions, focusing on some aspects of the decision while ignoring others.[16] While quicker than step-by-step processing, heuristic thinking is also more likely to involve fallacies or inaccuracies.[17]\n\nOne example of a common and erroneous thought process that arises through heuristic thinking is the gambler's fallacy — believing that an isolated random event is affected by previous isolated random events. For example, if flips of a fair coin give repeated tails, the coin still has the same probability (i.e., 0.5) of tails in future turns, though intuitively it might seems that heads becomes more likely.[18] In the long run, heads and tails should occur equally often; people commit the gambler's fallacy when they use this heuristic to predict that a result of heads is \"due\" after a run of tails.[19] Another example is that decision-makers may be biased towards preferring moderate alternatives to extreme ones. The compromise effect operates under a mindset that the most moderate option carries the most benefit. In an incomplete information scenario, as in most daily decisions, the moderate option will look more appealing than either extreme, independent of the context, based only on the fact that it has characteristics that can be found at either extreme.[20]\n\nA highly controversial issue is whether one can replace the use of probability in decision theory with something else.\n\nAdvocates for the use of probability theory point to:\n\nThe proponents of fuzzy logic, possibility theory, Dempster–Shafer theory, and info-gap decision theory maintain that probability is only one of many alternatives and point to many examples where non-standard alternatives have been implemented with apparent success. Notably, probabilistic decision theory can sometimes be sensitive to assumptions about the probabilities of various events, whereas non-probabilistic rules, such as minimax, are robust in that they do not make such assumptions.\n\nA general criticism of decision theory based on a fixed universe of possibilities is that it considers the \"known unknowns\", not the \"unknown unknowns\":[21] it focuses on expected variations, not on unforeseen events, which some argue have outsized impact and must be considered – significant events may be \"outside model\". This line of argument, called the ludic fallacy, is that there are inevitable imperfections in modeling the real world by particular models, and that unquestioning reliance on models blinds one to their limits.\n\nThe roots of decision theory lie in probability theory, developed by Blaise Pascal and Pierre de Fermat in the 17th century, which was later refined by others like Christiaan Huygens. These developments provided a framework for understanding risk and uncertainty, which are central to decision-making.\n\nIn the 18th century, Daniel Bernoulli introduced the concept of \"expected utility\" in the context of gambling, which was later formalized by John von Neumann and Oskar Morgenstern in the 1940s. Their work on Game Theory and Expected Utility Theory helped establish a rational basis for decision-making under uncertainty.\n\nAfter World War II, decision theory expanded into economics, particularly with the work of economists like Milton Friedman and others, who applied it to market behavior and consumer choice theory. This era also saw the development of Bayesian decision theory, which incorporates Bayesian probability into decision-making models.\n\nBy the late 20th century, scholars like Daniel Kahneman and Amos Tversky challenged the assumptions of rational decision-making. Their work in behavioral economics highlighted cognitive biases and heuristics that influence real-world decisions, leading to the development of prospect theory, which modified expected utility theory by accounting for psychological factors.\n"
    },
    {
        "title": "Mathematical physics",
        "content": "Mathematical physics refers to the development of mathematical methods for application to problems in physics. The Journal of Mathematical Physics defines the field as \"the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories\".[1] An alternative definition would also include those mathematics that are inspired by physics, known as physical mathematics.[2]\n\nThere are several distinct branches of mathematical physics, and these roughly correspond to particular historical parts of our world.\n\nApplying the techniques of mathematical physics to classical mechanics typically involves the rigorous, abstract, and advanced reformulation of Newtonian mechanics in terms of Lagrangian mechanics and Hamiltonian mechanics (including both approaches in the presence of constraints). Both formulations are embodied in analytical mechanics and lead to an understanding of the deep interplay between the notions of symmetry and conserved quantities during the dynamical evolution of mechanical systems, as embodied within the most elementary formulation of Noether's theorem. These approaches and ideas have been extended to other areas of physics, such as statistical mechanics, continuum mechanics, classical field theory, and quantum field theory. Moreover, they have provided multiple examples and ideas in differential geometry (e.g., several notions in symplectic geometry and vector bundles).\n\nWithin mathematics proper, the theory of partial differential equation, variational calculus, Fourier analysis, potential theory, and vector analysis are perhaps most closely associated with mathematical physics. These fields were developed intensively from the second half of the 18th century (by, for example, D'Alembert, Euler, and Lagrange) until the 1930s. Physical applications of these developments include hydrodynamics, celestial mechanics, continuum mechanics, elasticity theory, acoustics, thermodynamics, electricity, magnetism, and aerodynamics.\n\nThe theory of atomic spectra (and, later, quantum mechanics) developed almost concurrently with some parts of the mathematical fields of linear algebra, the spectral theory of operators, operator algebras and, more broadly, functional analysis. Nonrelativistic quantum mechanics includes Schrödinger operators, and it has connections to atomic and molecular physics. Quantum information theory is another subspecialty.\n\nThe special and general theories of relativity require a rather different type of mathematics. This was group theory, which played an important role in both quantum field theory and differential geometry. This was, however, gradually supplemented by topology and functional analysis  in the mathematical description of cosmological as well as quantum field theory phenomena. In the mathematical description of these physical areas, some concepts in homological algebra and category theory[3] are also important.\n\nStatistical mechanics forms a separate field, which includes the theory of phase transitions. It relies upon the Hamiltonian mechanics (or its quantum version) and it is closely related with the more mathematical ergodic theory and some parts of probability theory. There are increasing interactions between combinatorics and physics, in particular statistical physics.\n\nThe usage of the term \"mathematical physics\" is sometimes idiosyncratic. Certain parts of mathematics that initially arose from the development of physics are not, in fact, considered parts of mathematical physics, while other closely related fields are. For example, ordinary differential equations and symplectic geometry are generally viewed as purely mathematical disciplines, whereas dynamical systems and Hamiltonian mechanics belong to mathematical physics. John Herapath used the term for the title of his 1847 text on \"mathematical principles of natural philosophy\", the scope at that time being \n\"the causes of heat, gaseous elasticity, gravitation, and other great phenomena of nature\".[4]\n\nThe term \"mathematical physics\" is sometimes used to denote research aimed at studying and solving problems in physics or thought experiments within a mathematically rigorous framework. In this sense, mathematical physics covers a very broad academic realm distinguished only by the blending of some mathematical aspect and theoretical physics aspect. Although related to theoretical physics,[5] mathematical physics in this sense emphasizes the mathematical rigour of the similar type as found in mathematics.\n\nOn the other hand, theoretical physics emphasizes the links to observations and experimental physics, which often requires theoretical physicists (and mathematical physicists in the more general sense) to use heuristic, intuitive, or approximate arguments.[6] Such arguments are not considered rigorous by mathematicians.\n\nSuch mathematical physicists primarily expand and elucidate physical theories. Because of the required level of mathematical rigour, these researchers often deal with questions that theoretical physicists have considered to be already solved. However, they can sometimes show that the previous solution was incomplete, incorrect, or simply too naïve. Issues about attempts to infer the second law of thermodynamics from statistical mechanics are examples.[citation needed] Other examples concern the subtleties involved with synchronisation procedures in special and general relativity (Sagnac effect and Einstein synchronisation).\n\nThe effort to put physical theories on a mathematically rigorous footing not only developed physics but also has influenced developments of some mathematical areas. For example, the development of quantum mechanics and some aspects of functional analysis parallel each other in many ways. The mathematical study of quantum mechanics, quantum field theory, and quantum statistical mechanics has motivated results in operator algebras. The attempt to construct a rigorous mathematical formulation of quantum field theory has also brought about some progress in fields such as representation theory.\n\nThere is a tradition of mathematical analysis of nature that goes back to the ancient Greeks; examples include Euclid (Optics), Archimedes (On the Equilibrium of Planes, On Floating Bodies), and Ptolemy (Optics, Harmonics).[7][8] Later, Islamic and Byzantine scholars built on these works, and these ultimately were reintroduced or became available to the West in the 12th century and during the Renaissance.\n\nIn the first decade of the 16th century, amateur astronomer Nicolaus Copernicus proposed heliocentrism, and published a treatise on it in 1543. He retained the Ptolemaic idea of epicycles, and merely sought to simplify astronomy by constructing simpler sets of epicyclic orbits. Epicycles consist of circles upon circles. According to Aristotelian physics, the circle was the perfect form of motion, and was the intrinsic motion of Aristotle's fifth element—the quintessence or universal essence known in Greek as aether for the English pure air—that was the pure substance beyond the sublunary sphere, and thus was celestial entities' pure composition. The German Johannes Kepler [1571–1630], Tycho Brahe's assistant, modified Copernican orbits to ellipses, formalized in the equations of Kepler's laws of planetary motion.\n\nAn enthusiastic atomist, Galileo Galilei in his 1623 book The Assayer asserted that the \"book of nature is written in mathematics\".[9] His 1632 book, about his telescopic observations, supported heliocentrism.[10] Having made use of experimentation, Galileo then refuted geocentric cosmology by refuting Aristotelian physics itself. Galileo's 1638 book Discourse on Two New Sciences established the law of equal free fall as well as the principles of inertial motion, two central concepts of what today is known as classical mechanics.[10] By the Galilean law of inertia as well as the principle of Galilean invariance, also called Galilean relativity, for any object experiencing inertia, there is empirical justification for knowing only that it is at relative rest or relative motion—rest or motion with respect to another object.\n\nRené Descartes developed a complete system of heliocentric cosmology anchored on the principle of vortex motion, Cartesian physics, whose widespread acceptance helped bring the demise of Aristotelian physics. Descartes used mathematical reasoning as a model for science, and developed analytic geometry, which in time allowed the plotting of locations in 3D space (Cartesian coordinates) and marking their progressions along the flow of time.[11]\n\nChristiaan Huygens, a talented mathematician and physicist and older contemporary of Newton, was the first to successfully idealize a physical problem by a set of mathematical parameters in Horologium Oscillatorum (1673), and the first to fully mathematize a mechanistic explanation of an unobservable physical phenomenon in Traité de la Lumière (1690). He is thus considered a forerunner of theoretical physics and one of the founders of modern mathematical physics.[12][13]\n\nThe prevailing framework for science in the 16th and early 17th centuries was one borrowed from Ancient Greek mathematics, where geometrical shapes formed the building blocks to describe and think about space, and time was often thought as a separate entity. With the introduction of algebra into geometry, and with it the idea of a coordinate system, time and space could now be thought as axes belonging to the same plane. This essential mathematical framework is at the base of all modern physics and used in all further mathematical frameworks developed in next centuries. \n\nBy the middle of the 17th century, important concepts such as the fundamental theorem of calculus (proved in 1668 by Scottish mathematician James Gregory) and finding extrema and minima of functions via differentiation using Fermat's theorem (by French mathematician Pierre de Fermat) were already known before Leibniz and Newton.[14] Isaac Newton (1642–1727) developed calculus (although Gottfried Wilhelm Leibniz developed similar concepts outside the context of physics) and Newton's method to solve problems in mathematics and physics. He was extremely successful in his application of calculus and other methods to the study of motion. Newton's theory of motion, culminating in his Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy) in 1687, modeled three Galilean laws of motion along with Newton's law of universal gravitation on a framework of absolute space—hypothesized by Newton as a physically real entity of Euclidean geometric structure extending infinitely in all directions—while presuming absolute time, supposedly justifying knowledge of absolute motion, the object's motion with respect to absolute space.[15] The principle of Galilean invariance/relativity was merely implicit in Newton's theory of motion. Having ostensibly reduced the Keplerian celestial laws of motion as well as Galilean terrestrial laws of motion to a unifying force, Newton achieved great mathematical rigor, but with theoretical laxity.[16]\n\nIn the 18th century, the Swiss Daniel Bernoulli (1700–1782) made contributions to fluid dynamics, and vibrating strings. The Swiss Leonhard Euler (1707–1783) did special work in variational calculus, dynamics, fluid dynamics, and other areas. Also notable was the Italian-born Frenchman, Joseph-Louis Lagrange (1736–1813) for work in analytical mechanics: he formulated Lagrangian mechanics) and variational methods. A major contribution to the formulation of Analytical Dynamics called Hamiltonian dynamics was also made by the Irish physicist, astronomer and mathematician, William Rowan Hamilton (1805–1865). Hamiltonian dynamics had played an important role in the formulation of modern theories in physics, including field theory and quantum mechanics. The French mathematical physicist Joseph Fourier (1768 – 1830) introduced the notion of Fourier series to solve the heat equation, giving rise to a new approach to solving partial differential equations by means of integral transforms.\n\nInto the early 19th century, following mathematicians in France, Germany and England had contributed to mathematical physics. The French Pierre-Simon Laplace (1749–1827) made paramount contributions to mathematical astronomy, potential theory. Siméon Denis Poisson (1781–1840) worked in analytical mechanics and potential theory. In Germany, Carl Friedrich Gauss (1777–1855) made key contributions to the theoretical foundations of electricity, magnetism, mechanics, and fluid dynamics. In England, George Green (1793–1841) published An Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism in 1828, which in addition to its significant contributions to mathematics made early progress towards laying down the mathematical foundations of electricity and magnetism.\n\nA couple of decades ahead of Newton's publication of a particle theory of light, the Dutch Christiaan Huygens (1629–1695) developed the wave theory of light, published in 1690. By 1804, Thomas Young's double-slit experiment revealed an interference pattern, as though light were a wave, and thus Huygens's wave theory of light, as well as Huygens's inference that light waves were vibrations of the luminiferous aether, was accepted. Jean-Augustin Fresnel modeled hypothetical behavior of the aether. The English physicist Michael Faraday introduced the theoretical concept of a field—not action at a distance. Mid-19th century, the Scottish James Clerk Maxwell (1831–1879) reduced electricity and magnetism to Maxwell's electromagnetic field theory, whittled down by others to the four Maxwell's equations. Initially, optics was found consequent of[clarification needed] Maxwell's field. Later, radiation and then today's known electromagnetic spectrum were found also consequent of[clarification needed] this electromagnetic field.\n\nThe English physicist Lord Rayleigh [1842–1919] worked on sound. The Irishmen William Rowan Hamilton (1805–1865), George Gabriel Stokes (1819–1903) and Lord Kelvin (1824–1907) produced several major works: Stokes was a leader in optics and fluid dynamics; Kelvin made substantial discoveries in thermodynamics; Hamilton did notable work on analytical mechanics, discovering a new and powerful approach nowadays known as Hamiltonian mechanics. Very relevant contributions to this approach are due to his German colleague mathematician Carl Gustav Jacobi (1804–1851) in particular referring to canonical transformations. The German Hermann von Helmholtz (1821–1894) made substantial contributions in the fields of electromagnetism, waves, fluids, and sound. In the United States, the pioneering work of Josiah Willard Gibbs (1839–1903) became the basis for statistical mechanics. Fundamental theoretical results in this area were achieved by the German Ludwig Boltzmann (1844–1906). Together, these individuals laid the foundations of electromagnetic theory, fluid dynamics, and statistical mechanics.\n\nBy the 1880s, there was a prominent paradox that an observer within Maxwell's electromagnetic field measured it at approximately constant speed, regardless of the observer's speed relative to other objects within the electromagnetic field. Thus, although the observer's speed was continually lost[clarification needed] relative to the electromagnetic field, it was preserved relative to other objects in the electromagnetic field. And yet no violation of Galilean invariance within physical interactions among objects was detected. As Maxwell's electromagnetic field was modeled as oscillations of the aether, physicists inferred that motion within the aether resulted in aether drift, shifting the electromagnetic field, explaining the observer's missing speed relative to it. The Galilean transformation had been the mathematical process used to translate the positions in one reference frame to predictions of positions in another reference frame, all plotted on Cartesian coordinates, but this process was replaced by Lorentz transformation, modeled by the Dutch Hendrik Lorentz [1853–1928].\n\nIn 1887, experimentalists Michelson and Morley failed to detect aether drift, however. It was hypothesized that motion into the aether prompted aether's shortening, too, as modeled in the Lorentz contraction. It was hypothesized that the aether thus kept Maxwell's electromagnetic field aligned with the principle of Galilean invariance across all inertial frames of reference, while Newton's theory of motion was spared.\n\nAustrian theoretical physicist and philosopher Ernst Mach criticized Newton's postulated absolute space.  Mathematician Jules-Henri Poincaré (1854–1912) questioned even absolute time. In 1905, Pierre Duhem published a devastating criticism of the foundation of Newton's theory of motion.[16] Also in 1905, Albert Einstein (1879–1955) published his special theory of relativity, newly explaining both the electromagnetic field's invariance and Galilean invariance by discarding all hypotheses concerning aether, including the existence of aether itself.  Refuting the framework of Newton's theory—absolute space and absolute time—special relativity refers to relative space and relative time, whereby length contracts and time dilates along the travel pathway of an object.\n\nCartesian coordinates arbitrarily used rectilinear coordinates. Gauss, inspired by Descartes' work, introduced the curved geometry, replacing rectilinear axis by curved ones. Gauss also introduced another key tool of modern physics, the curvature. Gauss's work was limited to two dimensions. Extending it to three or more dimensions introduced a lot of complexity, with the need of the (not yet invented) tensors. It was Riemman the one in charge to extend curved geometry to N dimensions. In 1908, Einstein's former mathematics professor Hermann Minkowski, applied the curved geometry construction to model 3D space together with the 1D axis of time by treating the temporal axis like a fourth spatial dimension—altogether 4D spacetime—and declared the imminent demise of the separation of space and time. [17] Einstein initially called this \"superfluous learnedness\", but later used Minkowski spacetime with great elegance in his general theory of relativity,[18] extending invariance to all reference frames—whether perceived as inertial or as accelerated—and credited this to Minkowski, by then deceased. General relativity replaces Cartesian coordinates with Gaussian coordinates, and replaces Newton's claimed empty yet Euclidean space traversed instantly by Newton's vector of hypothetical gravitational force—an instant action at a distance—with a gravitational field. The gravitational field is Minkowski spacetime itself, the 4D topology of Einstein aether modeled on a Lorentzian manifold that \"curves\" geometrically, according to the Riemann curvature tensor. The concept of Newton's gravity: \"two masses attract each other\" replaced by the geometrical argument: \"mass transform curvatures of spacetime and free falling particles with mass move along a geodesic curve in the spacetime\" (Riemannian geometry already existed before the 1850s, by mathematicians Carl Friedrich Gauss and Bernhard Riemann in search for intrinsic geometry and non-Euclidean geometry.), in the vicinity of either mass or energy. (Under special relativity—a special case of general relativity—even massless energy exerts gravitational effect by its mass equivalence locally \"curving\" the geometry of the four, unified dimensions of space and time.)\n\nAnother revolutionary development of the 20th century was quantum theory, which emerged from the seminal contributions of Max Planck (1856–1947) (on black-body radiation) and Einstein's work on the photoelectric effect.  In 1912, a mathematician Henri Poincare published Sur la théorie des quanta.[19][20] He introduced the first non-naïve definition of quantization in this paper. The development of early quantum physics followed by a heuristic framework devised by Arnold Sommerfeld (1868–1951) and Niels Bohr (1885–1962), but this was soon replaced by the quantum mechanics developed by Max Born (1882–1970), Louis de Broglie (1892–1987), Werner Heisenberg (1901–1976),  Paul Dirac (1902–1984), Erwin Schrödinger (1887–1961), Satyendra Nath Bose (1894–1974), and Wolfgang Pauli (1900–1958). This revolutionary theoretical framework is based on a probabilistic interpretation of states, and evolution and measurements in terms of self-adjoint operators on an infinite-dimensional vector space. That is called Hilbert space (introduced by mathematicians David Hilbert (1862–1943), Erhard Schmidt (1876–1959) and Frigyes Riesz (1880–1956) in search of generalization of Euclidean space and study of integral equations), and rigorously defined within the axiomatic modern version by John von Neumann in his celebrated book Mathematical Foundations of Quantum Mechanics, where he built up a relevant part of modern functional analysis on Hilbert spaces, the spectral theory (introduced by David Hilbert who investigated quadratic forms with infinitely many variables. Many years later, it had been revealed that his spectral theory is associated with the spectrum of the hydrogen atom. He was surprised by this application.) in particular. Paul Dirac used algebraic constructions to produce a relativistic model for the electron, predicting its magnetic moment and the existence of its antiparticle, the positron.\n\nProminent contributors to the 20th century's mathematical physics include (ordered by birth date):\n"
    },
    {
        "title": "Mathematical chemistry",
        "content": "Mathematical chemistry[1] is the area of research engaged in novel applications of mathematics to chemistry; it concerns itself principally with the mathematical modeling of chemical phenomena.[2] Mathematical chemistry has also sometimes been called computer chemistry, but should not be confused with computational chemistry.\n\nMajor areas of research in mathematical chemistry include chemical graph theory, which deals with topology such as the mathematical study of isomerism and the development of topological descriptors or indices which find application in quantitative structure-property relationships; and chemical aspects of group theory, which finds applications in stereochemistry and quantum chemistry. Another important area is molecular knot theory and circuit topology that describe the topology of folded linear molecules such as proteins and nucleic acids.\n\nThe history of the approach may be traced back to the 19th century. Georg Helm published a treatise titled \"The Principles of Mathematical Chemistry: The Energetics of Chemical Phenomena\" in 1894.[3] Some of the more contemporary periodical publications specializing in the field are MATCH Communications in Mathematical and in Computer Chemistry, first published in 1975, and the Journal of Mathematical Chemistry, first published in 1987. In 1986 a series of annual conferences MATH/CHEM/COMP taking place in Dubrovnik was initiated by the late Ante Graovac.\n\nThe basic models for mathematical chemistry are molecular graph and topological index.\n\nIn 2005 the International Academy of Mathematical Chemistry (IAMC) was founded in Dubrovnik (Croatia) by Milan Randić. The Academy has 82 members (2009) from all over the world, including six scientists awarded with a Nobel Prize.\n"
    },
    {
        "title": "Geomathematics",
        "content": "Geomathematics (also: mathematical geosciences, mathematical geology, mathematical geophysics) is the application of mathematical methods to solve problems in geosciences, including geology and geophysics, and particularly geodynamics and seismology.\n\nGeophysical fluid dynamics develops the theory of fluid dynamics for the atmosphere, ocean and Earth's interior.[1] Applications include geodynamics and the theory of the geodynamo.\n\nGeophysical inverse theory is concerned with analyzing geophysical data to get model parameters.[2][3] It is concerned with the question: What can be known about the Earth's interior from measurements on the surface? Generally there are limits on what can be known even in the ideal limit of exact data.[4]\n\nThe goal of inverse theory is to determine the spatial distribution of some variable (for example, density or seismic wave velocity). The distribution determines the values of an observable at the surface (for example, gravitational acceleration for density). There must be a forward model predicting the surface observations given the distribution of this variable.\n\nApplications include geomagnetism, magnetotellurics and seismology.\n\nMany geophysical data sets have spectra that follow a power law, meaning that the frequency of an observed magnitude varies as some power of the magnitude. An example is the distribution of earthquake magnitudes; small earthquakes are far more common than large earthquakes. This is often an indicator that the data sets have an underlying fractal geometry. Fractal sets have a number of common features, including structure at many scales, irregularity, and self-similarity (they can be split into parts that look much like the whole). The manner in which these sets can be divided determine the Hausdorff dimension of the set, which is generally different from the more familiar topological dimension. Fractal phenomena are associated with chaos, self-organized criticality and turbulence.[5] Fractal Models in the Earth Sciences by Gabor Korvin was one of the earlier books on the application of Fractals in the Earth Sciences.[6]\n\nData assimilation combines numerical models of geophysical systems with observations that may be irregular in space and time. Many of the applications involve geophysical fluid dynamics. Fluid dynamic models are governed by a set of partial differential equations. For these equations to make good predictions, accurate initial conditions are needed. However, often the initial conditions are not very well known. Data assimilation methods allow the models to incorporate later observations to improve the initial conditions. Data assimilation plays an increasingly important role in weather forecasting.[7]\n\nSome statistical problems come under the heading of mathematical geophysics, including model validation and quantifying uncertainty.\n\nAn important research area that utilises inverse methods is\nseismic tomography, a technique for imaging the subsurface of the Earth using seismic waves. Traditionally seismic waves produced by earthquakes or anthropogenic seismic sources (e.g., explosives, marine air guns) were used.\n\nCrystallography is one of the traditional areas of geology that use mathematics. Crystallographers make use of linear algebra by using the Metrical Matrix. The Metrical Matrix uses the basis vectors of the unit cell dimensions to find the volume of a unit cell, d-spacings, the angle between two planes, the angle between atoms, and the bond length.[8] Miller's Index is also helpful in the application of the Metrical Matrix. Brag's equation is also useful when using an electron microscope to be able to show relationship between light diffraction angles, wavelength, and the d-spacings within a sample.[8]\n\nGeophysics is one of the most math heavy disciplines of Earth Science. There are many applications which include gravity, magnetic, seismic, electric, electromagnetic, resistivity, radioactivity, induced polarization, and well logging.[9] Gravity and magnetic methods share similar characteristics because they're measuring small changes in the gravitational field based on the density of the rocks in that area.[9] While similar gravity fields tend to be more uniform and smooth compared to magnetic fields. Gravity is used often for oil exploration and seismic can also be used, but it is often significantly more expensive.[9] Seismic is used more than most geophysics techniques because of its ability to penetrate, its resolution, and its accuracy.\n\nMany applications of mathematics in geomorphology are related to water. In the soil aspect things like Darcy's law, Stokes' law, and porosity are used.\n\nMathematics in Glaciology consists of theoretical, experimental, and modeling. It usually covers glaciers, sea ice, waterflow, and the land under the glacier.\n\nPolycrystalline ice deforms slower than single crystalline ice, due to the stress being on the basal planes that are already blocked by other ice crystals.[13] It can be mathematically modeled with Hooke's Law to show the elastic characteristics while using Lamé constants.[13] Generally the ice has its linear elasticity constants averaged over one dimension of space to simplify the equations while still maintaining accuracy.[13]\n\nViscoelastic polycrystalline ice is considered to have low amounts of stress usually below one bar.[13] This type of ice system is where one would test for creep or vibrations from the tension on the ice. One of the more important equations to this area of study is called the relaxation function.[13] Where it's a stress-strain relationship independent of time.[13] This area is usually applied to transportation or building onto floating ice.[13]\n\nShallow-Ice approximation is useful for glaciers that have variable thickness, with a small amount of stress and variable velocity.[13] One of the main goals of the mathematical work is to be able to predict the stress and velocity. Which can be affected by changes in the properties of the ice and temperature. This is an area in which the basal shear-stress formula can be used.[13]\n"
    },
    {
        "title": "Computational mathematics",
        "content": "\n\nComputational mathematics is the study of the interaction between mathematics and calculations done by a computer.[1]\n\nA large part of computational mathematics consists roughly of using mathematics for allowing and improving computer computation in areas of science and engineering where mathematics are useful. This involves in particular algorithm design, computational complexity, numerical methods and computer algebra.\n\nComputational mathematics refers also to the use of computers for mathematics itself. This includes mathematical experimentation for establishing conjectures (particularly in number theory), the use of computers for proving theorems (for example the four color theorem), and the design and use of proof assistants.\n\nComputational mathematics emerged as a distinct part of applied mathematics by the early 1950s. Currently, computational mathematics can refer to or include:\n\nJournals that publish contributions from computational mathematics include\n"
    },
    {
        "title": "Mathematical and theoretical biology",
        "content": "Mathematical and theoretical biology, or biomathematics, is a branch of biology which employs theoretical analysis, mathematical models and abstractions of living organisms to investigate the principles that govern the structure, development and behavior of the systems, as opposed to experimental biology which deals with the conduction of experiments to test scientific theories.[1] The field is sometimes called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side.[2] Theoretical biology focuses more on the development of theoretical principles for biology while mathematical biology focuses on the use of mathematical tools to study biological systems, even though the two terms are sometimes interchanged.[3][4]\n\nMathematical biology aims at the mathematical representation and modeling of biological processes, using techniques and tools of applied mathematics. It can be useful in both theoretical and practical research. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter. This requires precise mathematical models.\n\nBecause of the complexity of the living systems, theoretical biology employs several fields of mathematics,[5] and has contributed to the development of new techniques.\n\nMathematics has been used in biology as early as the 13th century, when Fibonacci used the famous Fibonacci series to describe a growing population of rabbits. In the 18th century, Daniel Bernoulli applied mathematics to describe the effect of smallpox on the human population. Thomas Malthus' 1789 essay on the growth of the human population was based on the concept of exponential growth. Pierre François Verhulst formulated the logistic growth model in 1836.[citation needed]\n\nFritz Müller described the evolutionary benefits of what is now called Müllerian mimicry in 1879, in an account notable for being the first use of a mathematical argument in evolutionary ecology to show how powerful the effect of natural selection would be, unless one includes Malthus's discussion of the effects of population growth that influenced Charles Darwin: Malthus argued that growth would be exponential (he uses the word \"geometric\") while resources (the environment's carrying capacity) could only grow arithmetically.[6]\n\nThe term \"theoretical biology\" was first used as a monograph title by Johannes Reinke in 1901, and soon after by Jakob von Uexküll in 1920. One founding text is considered to be On Growth and Form (1917) by D'Arcy Thompson,[7] and other early pioneers include Ronald Fisher, Hans Leo Przibram, Vito Volterra, Nicolas Rashevsky and Conrad Hal Waddington.[8]\n\nInterest in the field has grown rapidly from the 1960s onwards. Some reasons for this include:\n\nSeveral areas of specialized research in mathematical and theoretical biology[10][11][12][13][14] as well as external links to related projects in various universities are concisely presented in the following subsections, including also a large number of appropriate validating references from a list of several thousands of published authors contributing to this field. Many of the included examples are characterised by highly complex, nonlinear, and supercomplex mechanisms, as it is being increasingly recognised that the result of such interactions may only be understood through a combination of mathematical, logical, physical/chemical, molecular and computational models.\n\nAbstract relational biology (ARB) is concerned with the study of general, relational models of complex biological systems, usually abstracting out specific morphological, or anatomical, structures. Some of the simplest models in ARB are the Metabolic-Replication, or (M,R)--systems introduced by Robert Rosen in 1957–1958 as abstract, relational models of cellular and organismal organization.\n\nOther approaches include the notion of autopoiesis developed by Maturana and Varela, Kauffman's Work-Constraints cycles, and more recently the notion of closure of constraints.[15]\n\nAlgebraic biology (also known as symbolic systems biology) applies the algebraic methods of symbolic computation to the study of biological problems, especially in genomics, proteomics, analysis of molecular structures and study of genes.[16][17][18]\n\nAn elaboration of systems biology to understand the more complex life processes was developed since 1970 in connection with molecular set theory, relational biology and algebraic biology.\n\nA monograph on this topic summarizes an extensive amount of published research in this area up to 1986,[19][20][21] including subsections in the following areas: computer modeling in biology and medicine, arterial system models, neuron models, biochemical and oscillation networks, quantum automata, quantum computers in molecular biology and genetics,[22] cancer modelling,[23] neural nets, genetic networks, abstract categories in relational biology,[24] metabolic-replication systems, category theory[25] applications in biology and medicine,[26] automata theory, cellular automata,[27] tessellation models[28][29] and complete self-reproduction, chaotic systems in organisms, relational biology and organismic theories.[16][30]\n\nModeling cell and molecular biology\n\nThis area has received a boost due to the growing importance of molecular biology.[13]\n\nModelling physiological systems\n\nComputational neuroscience (also known as theoretical neuroscience or mathematical neuroscience) is the theoretical study of the nervous system.[43][44]\n\nEcology and evolutionary biology have traditionally been the dominant fields of mathematical biology.\n\nEvolutionary biology has been the subject of extensive mathematical theorizing. The traditional approach in this area, which includes complications from genetics, is population genetics. Most population geneticists consider the appearance of new alleles by mutation, the appearance of new genotypes by recombination, and changes in the frequencies of existing alleles and genotypes at a small number of gene loci. When infinitesimal effects at a large number of gene loci are considered, together with the assumption of linkage equilibrium or quasi-linkage equilibrium, one derives quantitative genetics. Ronald Fisher made fundamental advances in statistics, such as analysis of variance, via his work on quantitative genetics. Another important branch of population genetics that led to the extensive development of coalescent theory is phylogenetics. Phylogenetics is an area that deals with the reconstruction and analysis of phylogenetic (evolutionary) trees and networks based on inherited characteristics[45] Traditional population genetic models deal with alleles and genotypes, and are frequently stochastic.\n\nMany population genetics models assume that population sizes are constant. Variable population sizes, often in the absence of genetic variation, are treated by the field of population dynamics. Work in this area dates back to the 19th century, and even as far as 1798 when Thomas Malthus formulated the first principle of population dynamics, which later became known as the Malthusian growth model. The Lotka–Volterra predator-prey equations are another famous example. Population dynamics overlap with another active area of research in mathematical biology: mathematical epidemiology, the study of infectious disease affecting populations. Various models of the spread of infections have been proposed and analyzed, and provide important results that may be applied to health policy decisions.\n\nIn evolutionary game theory, developed first by John Maynard Smith and George R. Price, selection acts directly on inherited phenotypes, without genetic complications. This approach has been mathematically refined to produce the field of adaptive dynamics.\n\nThe earlier stages of mathematical biology were dominated by mathematical biophysics, described as the application of mathematics in biophysics, often involving specific physical/mathematical models of biosystems and their components or compartments.\n\nThe following is a list of mathematical descriptions and their assumptions.\n\nA fixed mapping between an initial state and a final state. Starting from an initial condition and moving forward in time, a deterministic process always generates the same trajectory, and no two trajectories cross in state space.\n\nA random mapping between an initial state and a final state, making the state of the system a random variable with a corresponding probability distribution.\n\nOne classic work in this area is Alan Turing's paper on morphogenesis entitled The Chemical Basis of Morphogenesis, published in 1952 in the Philosophical Transactions of the Royal Society.\n\nA model of a biological system is converted into a system of equations, although the word 'model' is often used synonymously with the system of corresponding equations. The solution of the equations, by either analytical or numerical means, describes how the biological system behaves either over time or at equilibrium. There are many different types of equations and the type of behavior that can occur is dependent on both the model and the equations used. The model often makes assumptions about the system. The equations may also make assumptions about the nature of what may occur.\n\nMolecular set theory (MST) is a mathematical formulation of the wide-sense chemical kinetics of biomolecular reactions in terms of sets of molecules and their chemical transformations represented by set-theoretical mappings between molecular sets. It was introduced by Anthony Bartholomay, and its applications were developed in mathematical biology and especially in mathematical medicine.[52]\nIn a more general sense, MST is the theory of molecular categories defined as categories of molecular sets and their chemical transformations represented as set-theoretical mappings of molecular sets. The theory has also contributed to biostatistics and the formulation of clinical biochemistry problems in mathematical formulations of pathological, biochemical changes of interest to Physiology, Clinical Biochemistry and Medicine.[52]\n\nTheoretical approaches to biological organization aim to understand the interdependence between the parts of organisms. They emphasize the circularities that these interdependences lead to. Theoretical biologists developed several concepts to formalize this idea.\n\nFor example, abstract relational biology (ARB)[53] is concerned with the study of general, relational models of complex biological systems, usually abstracting out specific morphological, or anatomical, structures. Some of the simplest models in ARB are the Metabolic-Replication, or (M,R)--systems introduced by Robert Rosen in 1957–1958 as abstract, relational models of cellular and organismal organization.[54]\n\nThe eukaryotic cell cycle is very complex and has been the subject of intense study, since its misregulation leads to cancers.\nIt is possibly a good example of a mathematical model as it deals with simple calculus but gives valid results.  Two research groups [55][56] have produced several models of the cell cycle simulating several organisms. They have recently produced a generic eukaryotic cell cycle model that can represent a particular eukaryote depending on the values of the parameters, demonstrating that the idiosyncrasies of the individual cell cycles are due to different protein concentrations and affinities, while the underlying mechanisms are conserved (Csikasz-Nagy et al., 2006).\n\nBy means of a system of ordinary differential equations these models show the change in time (dynamical system) of the protein inside a single typical cell; this type of model is called a deterministic process (whereas a model describing a statistical distribution of protein concentrations in a population of cells is called a stochastic process).\n\nTo obtain these equations an iterative series of steps must be done: first the several models and observations are combined to form a consensus diagram and the appropriate kinetic laws are chosen to write the differential equations, such as rate kinetics for stoichiometric reactions, Michaelis-Menten kinetics for enzyme substrate reactions and Goldbeter–Koshland kinetics for ultrasensitive transcription factors, afterwards the parameters of the equations (rate constants, enzyme efficiency coefficients and Michaelis constants) must be fitted to match observations; when they cannot be fitted the kinetic equation is revised and when that is not possible the wiring diagram is modified. The parameters are fitted and validated using observations of both wild type and mutants, such as protein half-life and cell size.\n\nTo fit the parameters, the differential equations must be studied. This can be done either by simulation or by analysis. In a simulation, given a starting vector (list of the values of the variables), the progression of the system is calculated by solving the equations at each time-frame in small increments.\n\n In analysis, the properties of the equations are used to investigate the behavior of the system depending on the values of the parameters and variables. A system of differential equations can be represented as a vector field, where each vector described the change (in concentration of two or more protein) determining where and how fast the trajectory (simulation) is heading. Vector fields can have several special points: a stable point, called a sink, that attracts in all directions (forcing the concentrations to be at a certain value), an unstable point, either a source or a saddle point, which repels (forcing the concentrations to change away from a certain value), and a limit cycle, a closed trajectory towards which several trajectories spiral towards (making the concentrations oscillate).\n\nA better representation, which handles the large number of variables and parameters, is a bifurcation diagram using bifurcation theory. The presence of these special steady-state points at certain values of a parameter (e.g. mass) is represented by a point and once the parameter passes a certain value, a qualitative change occurs, called a bifurcation, in which the nature of the space changes, with profound consequences for the protein concentrations: the cell cycle has phases (partially corresponding to G1 and G2) in which mass, via a stable point, controls cyclin levels, and phases (S and M phases) in which the concentrations change independently, but once the phase has changed at a bifurcation event (Cell cycle checkpoint), the system cannot go back to the previous levels since at the current mass the vector field is profoundly different and the mass cannot be reversed back through the bifurcation event, making a checkpoint irreversible. In particular the S and M checkpoints are regulated by means of special bifurcations called a Hopf bifurcation and an infinite period bifurcation.[citation needed]\n"
    },
    {
        "title": "Computational linguistics",
        "content": "Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.\n\nThe field overlapped with artificial intelligence since the efforts in the United States in the 1950s to use computers to automatically translate texts from foreign languages, particularly Russian scientific journals, into English.[1] Since rule-based approaches were able to make arithmetic (systematic) calculations much faster and more accurately than humans, it was expected that lexicon, morphology, syntax and semantics can be learned using explicit rules, as well. After the failure of rule-based approaches, David Hays[2] coined the term in order to distinguish the field from AI and co-founded both the Association for Computational Linguistics (ACL) and the International Committee on Computational Linguistics (ICCL) in the 1970s and 1980s. What started as an effort to translate between languages evolved into a much wider field of natural language processing.[3][4]\n\nIn order to be able to meticulously study the English language, an annotated text corpus was much  needed. The Penn Treebank[5] was one of the most used corpora. It consisted of IBM computer manuals, transcribed telephone conversations, and other texts, together containing over 4.5 million words of American English, annotated using both  part-of-speech tagging and syntactic bracketing.[6]\n\nJapanese sentence corpora were analyzed and a pattern of log-normality was found in relation to sentence length.[7]\n\nThe fact that during language acquisition, children are largely only exposed to positive evidence,[8] meaning that the only evidence for what is a correct form is provided, and no evidence for what is not correct,[9] was a limitation for the models at the time because the now available deep learning models were not available in late 1980s.[10]\n\nIt has been shown that languages can be learned with a combination of simple input presented incrementally as the child develops better memory and longer attention span,[11] which explained the long period of language acquisition in human infants and children.[11]\n\nRobots have been used to test linguistic theories.[12] Enabled to learn as children might, models were created based on an affordance model in which mappings between actions, perceptions, and effects were created and linked to spoken words. Crucially, these robots were able to acquire functioning word-to-meaning mappings without needing grammatical structure.\n\nUsing the Price equation and Pólya urn dynamics, researchers have created a system which not only predicts future linguistic evolution but also gives insight into the evolutionary history of modern-day languages.[13]\n\nChomsky's theories have influenced computational linguistics, particularly in understanding how infants learn complex grammatical structures, such as those described in Chomsky normal form.[14] Attempts have been made to determine how an infant learns a \"non-normal grammar\" as theorized by Chomsky normal form.[9] Research in this area combines structural approaches with computational models to analyze large linguistic corpora like the Penn Treebank, helping to uncover patterns in language acquisition.[15]\n"
    },
    {
        "title": "Mathematical economics",
        "content": "Empirical methods\n\nPrescriptive and policy\n\nMathematical economics is the application of mathematical methods to represent theories and analyze problems in economics.  Often, these applied methods are beyond simple geometry, and may include differential and integral calculus, difference and differential equations, matrix algebra, mathematical programming, or other computational methods.[1][2] Proponents of this approach claim that it allows the formulation of theoretical relationships with rigor, generality, and simplicity.[3]\n\nMathematics allows economists to form meaningful, testable propositions about  wide-ranging and complex subjects which could less easily  be expressed informally. Further, the language of mathematics allows economists to make specific, positive claims about controversial or contentious subjects that would be impossible without mathematics.[4] Much of economic theory is currently presented in terms of mathematical economic models, a set of stylized and simplified mathematical relationships asserted to clarify assumptions and implications.[5]\n\nBroad applications include:\n\nFormal economic modeling began in the 19th century with the use of differential calculus to represent and explain economic behavior, such as utility maximization, an early economic application of mathematical optimization. Economics became more mathematical as a discipline throughout the first half of the 20th century, but introduction of new and generalized techniques in the period around the Second World War, as in game theory, would greatly broaden the use of mathematical formulations in economics.[8][7]\n\nThis rapid systematizing of economics alarmed critics of the discipline as well as some noted economists.  John Maynard Keynes, Robert Heilbroner, Friedrich Hayek and others have criticized the broad use of mathematical models for human behavior, arguing that some human choices are irreducible to mathematics.\n\nThe use of mathematics in the service of social and economic analysis dates back to the 17th century.  Then, mainly in German universities, a style of instruction emerged which dealt specifically with detailed presentation of data as it related to public administration.  Gottfried Achenwall lectured in this fashion, coining the term statistics.  At the same time, a small group of professors in England established a method of \"reasoning by figures upon things relating to government\" and referred to this practice as Political Arithmetick.[9] Sir William Petty wrote at length on issues that would later concern economists, such as taxation, Velocity of money and national income, but while his analysis was numerical, he rejected abstract mathematical methodology.  Petty's use of detailed numerical data (along with John Graunt) would influence statisticians and economists for some time, even though Petty's works were largely ignored by English scholars.[10]\n\nThe mathematization of economics began in earnest in the 19th century.  Most of the economic analysis of the time was what would later be called classical economics.  Subjects were discussed and dispensed with through algebraic means, but calculus was not used.  More importantly, until Johann Heinrich von Thünen's The Isolated State in 1826, economists did not develop explicit and abstract models for behavior in order to apply the tools of mathematics.  Thünen's model of farmland use represents the first example of marginal analysis.[11]  Thünen's work was largely theoretical, but he also mined empirical data in order to attempt to support his generalizations.  In comparison to his contemporaries, Thünen built economic models and tools, rather than applying previous tools to new problems.[12]\n\nMeanwhile, a new cohort of scholars trained in the mathematical methods of the physical sciences  gravitated to economics, advocating and applying those methods to their subject,[13] and described today as moving from geometry to mechanics.[14]\nThese included W.S. Jevons who presented a paper on a \"general mathematical theory of political economy\" in 1862, providing an outline for use of the theory of marginal utility in political economy.[15]  In 1871, he published The Principles of Political Economy, declaring that  the subject as science \"must be mathematical simply because it deals with quantities\". Jevons expected that only collection of statistics for price and quantities would permit the subject as presented to become an exact science.[16]  Others preceded and followed in expanding mathematical representations of economic problems.\n[17]\n\nAugustin Cournot and Léon Walras built the tools of the discipline axiomatically around utility, arguing that individuals sought to maximize their utility across choices in a way that could be described mathematically.[18]  At the time, it was thought that utility was quantifiable, in units known as utils.[19]  Cournot, Walras and Francis Ysidro Edgeworth are considered the precursors to modern mathematical economics.[20]\n\nCournot, a professor of mathematics, developed a mathematical treatment in 1838 for duopoly—a market condition defined by competition between two sellers.[20]  This treatment of competition, first published in Researches into the Mathematical Principles of Wealth,[21] is referred to as Cournot duopoly. It is assumed that both sellers had equal access to the market and could produce their goods without cost.  Further, it assumed that both goods were homogeneous.  Each seller would vary her output based on the output of the other and the market price would be determined by the total quantity supplied.  The profit for each firm would be determined by multiplying their output by the per unit market price.  Differentiating the profit function with respect to quantity supplied for each firm left a system of linear equations, the simultaneous solution of which gave the equilibrium quantity, price and profits.[22] Cournot's contributions to the mathematization of economics would be neglected for decades, but eventually influenced many of the marginalists.[22][23]  Cournot's models of duopoly and oligopoly also represent one of the first formulations of non-cooperative games.  Today the solution can be given as a Nash equilibrium but Cournot's work preceded modern game theory by over 100 years.[24]\n\nWhile Cournot provided a solution for what would later be called partial equilibrium, Léon Walras attempted to formalize discussion of the economy as a whole through a theory of general competitive equilibrium.  The behavior of every economic actor would be considered on both the production and consumption side.  Walras originally presented four separate models of exchange, each recursively included in the next.  The solution of the resulting system of equations (both linear and non-linear) is the general equilibrium.[25]  At the time, no general solution could be expressed for a system of arbitrarily many equations, but Walras's attempts produced two famous results in economics.  The first is Walras' law and the second is the principle of tâtonnement.  Walras' method was considered highly mathematical for the time and Edgeworth commented at length about this fact in his review of Éléments d'économie politique pure (Elements of Pure Economics).[26]\n\nWalras' law was introduced as a theoretical answer to the problem of determining the solutions in general equilibrium.  His notation is different from modern notation but can be constructed using more modern summation notation.  Walras assumed that in equilibrium, all money would be spent on all goods: every good would be sold at the market price for that good and every buyer would expend their last dollar on a basket of goods.  Starting from this assumption, Walras could then show that if there were n markets and n-1 markets cleared (reached equilibrium conditions) that the nth market would clear as well.  This is easiest to visualize with two markets (considered in most texts as a market for goods and a market for money).  If one of two markets has reached an equilibrium state, no additional goods (or conversely, money) can enter or exit the second market, so it must be in a state of equilibrium as well.  Walras used this statement to move toward a proof of existence of solutions to general equilibrium but it is commonly used today to illustrate market clearing in money markets at the undergraduate level.[27]\n\nTâtonnement (roughly, French for groping toward) was meant to serve as the practical expression of Walrasian general equilibrium.  Walras abstracted the marketplace as an auction of goods where the auctioneer would call out prices and market participants would wait until they could each satisfy their personal reservation prices for the quantity desired (remembering here that this is an auction on all goods, so everyone has a reservation price for their desired basket of goods).[28]\n\nOnly when all buyers are satisfied with the given market price would transactions occur.  The market would \"clear\" at that price—no surplus or shortage would exist.  The word tâtonnement is used to describe the directions the market takes in groping toward equilibrium, settling high or low prices on different goods until a price is agreed upon for all goods.  While the process appears dynamic, Walras only presented a static model, as no transactions would occur until all markets were in equilibrium.  In practice, very few markets operate in this manner.[29]\n\nEdgeworth introduced mathematical elements to Economics explicitly in Mathematical Psychics: An Essay on the Application of Mathematics to the Moral Sciences, published in 1881.[30]  He adopted Jeremy Bentham's felicific calculus to economic behavior, allowing the outcome of each decision to be converted into a change in utility.[31]  Using this assumption, Edgeworth built a model of exchange on three assumptions: individuals are self-interested, individuals act to maximize utility, and individuals are \"free to recontract with another independently of...any third party\".[32]\n\nGiven two individuals, the set of solutions where both individuals can maximize utility is described by the contract curve on what is now known as an Edgeworth Box.  Technically, the construction of the two-person solution to Edgeworth's problem was not developed graphically until 1924 by Arthur Lyon Bowley.[34]  The contract curve of the Edgeworth box (or more generally on any set of solutions to Edgeworth's problem for more actors) is referred to as the core of an economy.[35]\n\nEdgeworth devoted considerable effort to insisting that mathematical proofs were appropriate for all schools of thought in economics.  While at the helm of The Economic Journal, he published several articles criticizing the mathematical rigor of rival researchers,  including Edwin Robert Anderson Seligman, a noted skeptic of mathematical economics.[36]  The articles focused on a back and forth over tax incidence and responses by producers.  Edgeworth noticed that a monopoly producing a good that had jointness of supply but not jointness of demand (such as first class and economy on an airplane, if the plane flies, both sets of seats fly with it) might actually lower the price seen by the consumer for one of the two commodities if a tax were applied.  Common sense and more traditional, numerical analysis seemed to indicate that this was preposterous.  Seligman insisted that the results Edgeworth achieved were a quirk of his mathematical formulation.  He suggested that the assumption of a continuous demand function and an infinitesimal change in the tax resulted in the paradoxical predictions.  Harold Hotelling later showed that Edgeworth was correct and that the same result (a \"diminution of price as a result of the tax\") could occur with a discontinuous demand function and large changes in the tax rate.[37]\n\nFrom the later-1930s, an array of new mathematical tools from differential calculus and differential equations, convex sets, and graph theory were deployed to advance  economic theory in a way similar to new mathematical methods earlier applied to physics.[8][38] The process was later described as moving from mechanics to axiomatics.[39]\n\nVilfredo Pareto analyzed microeconomics by treating decisions by economic actors as attempts to change a given allotment of goods to another, more preferred allotment.  Sets of allocations could then be treated as Pareto efficient (Pareto optimal is an equivalent term) when no exchanges could occur between actors that could make at least one individual better off without making any other individual worse off.[40]  Pareto's proof is commonly conflated with Walrassian equilibrium or informally ascribed to Adam Smith's Invisible hand hypothesis.[41] Rather, Pareto's statement was the first formal assertion of what would be known as the first fundamental theorem of welfare economics.[42]  These models lacked the inequalities of the next generation of mathematical economics.\n\nIn the landmark treatise Foundations of Economic Analysis (1947), Paul Samuelson identified a common paradigm and mathematical structure across multiple fields in the subject, building on previous work by Alfred Marshall. Foundations took mathematical concepts from  physics and applied them to economic problems. This broad view (for example, comparing Le Chatelier's principle to tâtonnement) drives the fundamental premise of mathematical economics: systems of economic actors may be modeled and their behavior described much like any other system. This extension followed on the work of the marginalists in the previous century and extended it significantly. Samuelson approached the problems of applying individual utility maximization over aggregate groups with comparative statics, which compares two different equilibrium states after an exogenous change in a variable. This and other methods in the book provided the foundation for mathematical economics in the 20th century.[7][43]\n\nRestricted models of general equilibrium were formulated  by John von Neumann in 1937.[44] Unlike earlier versions, the models of von Neumann had inequality constraints. For his model of an expanding economy, von Neumann proved the existence and uniqueness of an equilibrium using his generalization of Brouwer's fixed point theorem. Von Neumann's model of an expanding economy considered the matrix pencil  A - λ B  with nonnegative matrices A and B; von Neumann sought probability vectors p and q and a positive number λ that would solve the complementarity equation\n\nalong with two inequality systems expressing economic efficiency. In this model, the (transposed) probability vector p represents the prices of the goods while the probability vector q represents the \"intensity\" at which the production process would run.  The unique solution λ represents the rate of growth of the economy, which equals the interest rate. Proving the existence of a positive growth rate and proving that the growth rate equals the interest rate were remarkable achievements, even for von Neumann.[45][46][47] Von Neumann's results have been viewed as a special case of linear programming, where von Neumann's model uses only nonnegative matrices.[48] The study of von Neumann's model of an expanding economy continues to interest mathematical economists with interests in computational economics.[49][50][51]\n\nIn 1936, the Russian–born economist Wassily Leontief built his model of input-output analysis from the 'material balance' tables constructed by Soviet economists, which themselves followed earlier work by the physiocrats.  With his model, which described a system of production and demand processes, Leontief described how changes in demand in one economic sector would influence production in another.[52] In practice, Leontief  estimated the coefficients of his simple models, to address economically interesting questions.  In production economics, \"Leontief technologies\" produce outputs using constant proportions of inputs, regardless of the price of inputs, reducing the value of Leontief models for understanding economies but allowing their parameters to be estimated relatively easily. In contrast, the von Neumann model of an expanding economy allows for choice of techniques, but the coefficients must be estimated for each technology.[53][54]\n\nIn mathematics, mathematical optimization (or optimization or mathematical programming) refers to the selection of a best element from some set of available alternatives.[55] In the simplest case, an optimization problem  involves maximizing or minimizing a real function by selecting input values of the function and computing the corresponding values of the function. The solution process includes satisfying general necessary and sufficient conditions for optimality. For optimization problems, specialized notation may be used as to the function and its input(s). More generally, optimization includes finding the best available element of some function given a defined domain and may use a variety of different  computational optimization techniques.[56]\n\nEconomics is closely enough linked to optimization by agents in an economy that an influential definition relatedly describes economics qua science as the \"study of human behavior as a relationship between ends and scarce means\" with alternative uses.[57] Optimization problems run through modern economics, many with explicit economic or technical constraints. In microeconomics, the utility maximization problem and its dual problem, the expenditure minimization problem for a given level of utility, are economic optimization problems.[58] Theory posits that consumers  maximize their utility, subject to their budget constraints and that firms maximize their profits, subject to their production functions, input costs, and market demand.[59]\n\nEconomic equilibrium is studied in optimization theory as a key ingredient of economic theorems that in principle could be tested against empirical data.[7][60]  Newer developments have occurred in dynamic programming and modeling optimization with risk and uncertainty, including applications to portfolio theory, the economics of information, and search theory.[59]\n\nOptimality properties for an entire market system may be stated in mathematical terms, as in formulation of the two fundamental theorems of welfare economics[61] and in the Arrow–Debreu model of general equilibrium (also discussed below).[62] More concretely, many problems are amenable to analytical (formulaic) solution. Many others may be sufficiently complex to require numerical methods of solution, aided by software.[56] Still others are complex but tractable enough to allow computable methods of solution, in particular computable general equilibrium models for the entire economy.[63]\n\nLinear and nonlinear programming have profoundly affected microeconomics, which had earlier considered only equality constraints.[64] Many of the mathematical economists who received Nobel Prizes in Economics had conducted notable research using linear programming: Leonid Kantorovich, Leonid Hurwicz, Tjalling Koopmans, Kenneth J. Arrow, Robert Dorfman, Paul Samuelson and Robert Solow.[65] Both Kantorovich and Koopmans acknowledged that George B. Dantzig deserved to share their Nobel Prize for linear programming. Economists who conducted research in nonlinear programming also have won the  Nobel prize, notably Ragnar Frisch in addition to Kantorovich, Hurwicz, Koopmans, Arrow, and Samuelson.\n\nLinear programming was developed to aid the allocation of resources in firms and in industries during the 1930s in Russia and during the 1940s in the United States. During the Berlin airlift (1948), linear programming was used to plan the shipment of supplies to prevent Berlin from starving after the Soviet  blockade.[66][67]\n\nExtensions to nonlinear optimization with inequality constraints were achieved in 1951 by Albert W. Tucker and Harold Kuhn, who considered the nonlinear optimization problem:\n\nIn allowing inequality constraints, the Kuhn–Tucker approach generalized the classic method of Lagrange multipliers, which (until then) had  allowed only equality constraints.[68]  The Kuhn–Tucker approach inspired further research on Lagrangian duality, including the treatment of inequality constraints.[69][70] The duality theory of nonlinear programming is particularly satisfactory when applied to convex minimization problems, which enjoy the convex-analytic duality theory of Fenchel and Rockafellar; this convex duality is particularly strong for polyhedral convex functions, such as those arising in linear programming. Lagrangian duality and convex analysis are used daily in operations research, in the scheduling of power plants, the planning of production schedules for factories, and the routing of airlines (routes, flights, planes, crews).[70]\n\nEconomic dynamics allows for changes in economic variables over time, including in dynamic systems.  The problem of finding optimal functions for such changes is studied in variational calculus and in optimal control theory.  Before the Second World War, Frank Ramsey and Harold Hotelling used the calculus of variations to that end.\n\nFollowing Richard Bellman's work on dynamic programming and the 1962 English translation of L. Pontryagin et al.'s earlier work,[71] optimal control theory was used more extensively in economics in addressing dynamic problems, especially as to economic growth equilibrium and stability of economic systems,[72] of which a textbook example is optimal consumption and saving.[73] A crucial distinction is between deterministic and stochastic control models.[74]  Other applications of optimal control theory include those in finance, inventories, and production for example.[75]\n\nIt was in the course of proving of the existence of an optimal equilibrium in his 1937 model of economic growth that John von Neumann introduced functional analytic methods to include topology in economic theory, in particular,  fixed-point theory through his generalization of Brouwer's fixed-point theorem.[8][44][76] Following von Neumann's program, Kenneth Arrow and Gérard Debreu formulated abstract models of economic equilibria using convex sets  and fixed–point theory.  In introducing the Arrow–Debreu model in 1954, they proved the existence (but not the uniqueness) of an equilibrium and also proved that every Walras equilibrium is Pareto efficient; in general, equilibria need not be unique.[77] In their models, the (\"primal\") vector space represented quantities while the \"dual\" vector space represented prices.[78]\n\nIn Russia, the mathematician Leonid Kantorovich developed economic models in partially ordered vector spaces, that emphasized the duality between quantities and prices.[79] Kantorovich renamed prices as \"objectively determined valuations\"  which were abbreviated in Russian as \"o. o. o.\", alluding to the difficulty of discussing prices in the Soviet Union.[78][80][81]\n\nEven in finite dimensions, the concepts of functional analysis have illuminated economic theory, particularly in clarifying the role of prices as normal vectors to a hyperplane supporting a convex set, representing production or consumption possibilities. However, problems of describing optimization over time or under uncertainty require the use of infinite–dimensional function spaces, because agents are choosing among functions or stochastic processes.[78][82][83][84]\n\nJohn von Neumann's work on functional analysis and topology broke new ground in mathematics and economic theory.[44][85] It also left advanced mathematical economics with fewer applications of differential calculus. In particular, general equilibrium theorists used general topology, convex geometry, and optimization theory more than differential calculus, because the approach of differential calculus had failed to establish the existence of an equilibrium.\n\nHowever, the decline of differential calculus should not be exaggerated, because differential calculus has always been used in graduate training and in applications. Moreover, differential calculus has returned to the highest levels of mathematical economics, general equilibrium theory (GET), as practiced by the \"GET-set\" (the humorous designation due to Jacques H. Drèze). In the 1960s and 1970s, however, Gérard Debreu and Stephen Smale led a revival of the use of differential calculus in mathematical economics. In particular, they were able to prove the existence of a general equilibrium, where earlier writers had failed, because of their novel mathematics: Baire category from general topology and Sard's lemma from differential topology. Other economists associated with the use of differential analysis include Egbert Dierker, Andreu Mas-Colell, and Yves Balasko.[86][87] These advances have changed the traditional narrative of the history of mathematical economics, following von Neumann, which celebrated the abandonment of differential calculus.\n\nJohn von Neumann, working with Oskar Morgenstern on the theory of games, broke new mathematical ground in 1944 by extending functional analytic methods related to convex sets and topological fixed-point theory to economic analysis.[8][85] Their work thereby avoided the traditional differential calculus, for which the maximum–operator did not apply to non-differentiable functions. Continuing von Neumann's work in cooperative game theory, game theorists Lloyd S. Shapley, Martin Shubik, Hervé Moulin, Nimrod Megiddo, Bezalel Peleg influenced economic research in politics and economics. For example, research on the fair prices in cooperative games and fair values for voting games led to changed rules for voting in legislatures and for accounting for the costs in public–works projects. For example, cooperative game theory was used in designing the water distribution system of Southern Sweden and for setting rates for dedicated telephone lines in the US.\n\nEarlier neoclassical theory had bounded only the range of bargaining outcomes and in special cases, for  example bilateral monopoly or along the contract curve of the Edgeworth box.[88]  Von Neumann and Morgenstern's results were similarly weak. Following von Neumann's program, however, John Nash used fixed–point theory to prove conditions under which the bargaining problem and noncooperative games can generate a unique equilibrium solution.[89] Noncooperative game theory has been adopted as a fundamental aspect of  experimental economics,[90] behavioral economics,[91] information economics,[92] industrial organization,[93] and  political economy.[94] It has also given rise to the subject of mechanism design (sometimes called reverse game theory), which has private and public-policy applications as to ways of improving economic efficiency through incentives for information sharing.[95]\n\nIn 1994, Nash, John Harsanyi, and Reinhard Selten received the Nobel Memorial Prize in Economic Sciences their work on non–cooperative games. Harsanyi and Selten were awarded for their work on repeated games. Later work extended their results to computational methods of modeling.[96]\n\nAgent-based computational economics (ACE) as a named field is relatively recent, dating from about the 1990s as to published work. It studies economic processes, including whole economies, as dynamic systems of interacting agents over time. As such, it falls in the paradigm of complex adaptive systems.[97] In corresponding agent-based models, agents are not real people but \"computational objects modeled as interacting according to rules\" ... \"whose micro-level interactions create emergent patterns\"   in space and time.[98] The rules are formulated to predict behavior and social interactions based on incentives and information. The theoretical assumption of mathematical optimization by agents markets is replaced by the less restrictive postulate of agents with bounded rationality adapting to market forces.[99]\n\nACE models apply numerical methods of analysis to computer-based simulations of complex dynamic problems for which more conventional methods, such as theorem formulation, may not find ready use.[100] Starting from specified initial conditions, the computational economic system is modeled as evolving over time as its constituent agents repeatedly interact with each other. In these respects, ACE has been characterized as a bottom-up culture-dish approach to the study of the economy.[101] In contrast to other standard modeling methods, ACE events are driven solely by initial conditions, whether or not equilibria exist or are computationally tractable.   ACE modeling, however, includes agent adaptation, autonomy, and learning.[102] It has a similarity to, and overlap with, game theory as an agent-based method for modeling social interactions.[96] Other dimensions of the approach include such standard economic subjects as competition and collaboration,[103] market structure and industrial organization,[104] transaction costs,[105] welfare economics[106] and mechanism design,[95] information and uncertainty,[107] and macroeconomics.[108][109]\n\nThe method is said to benefit from continuing improvements in modeling techniques of computer science and increased computer capabilities.  Issues include those common to experimental economics in general[110] and by comparison[111] and to development of a common framework for empirical validation and resolving open questions in agent-based modeling.[112] The ultimate scientific objective of the method has been described as \"test[ing] theoretical findings against real-world data in ways that permit empirically supported theories to cumulate over time, with each researcher's work building appropriately on the work that has gone before\".[113]\n\nOver the course of the 20th century, articles in \"core journals\"[115] in economics have been almost exclusively written by economists in academia.  As a result, much of the material transmitted in those journals relates to economic theory, and \"economic theory itself has been continuously more abstract and mathematical.\"[116] A subjective assessment of mathematical techniques[117] employed in these core journals showed a decrease in articles that use neither geometric representations nor mathematical notation from 95% in 1892 to 5.3% in 1990.[118]  A 2007 survey of ten of the top economic journals finds that only 5.8% of the articles published in 2003 and 2004 both lacked statistical analysis of data and  lacked displayed mathematical expressions that were indexed with numbers at the margin of the page.[119]\n\nBetween the world wars, advances in mathematical statistics and a cadre of mathematically trained economists  led to econometrics, which was the name proposed for the discipline of advancing economics by using mathematics and statistics. Within economics, \"econometrics\" has often been used for statistical methods in economics, rather than mathematical economics. Statistical econometrics features the application of linear regression and time series analysis to economic data.\n\nRagnar Frisch coined the word \"econometrics\" and helped to found both the Econometric Society in 1930 and the journal Econometrica in 1933.[120][121]  A student of Frisch's, Trygve Haavelmo published The Probability Approach in Econometrics in 1944, where he asserted that precise statistical analysis could be used as a tool to validate mathematical theories about economic actors with data from complex sources.[122]  This linking of statistical analysis of systems to economic theory was also promulgated by the Cowles Commission (now the Cowles Foundation) throughout the 1930s and 1940s.[123]\n\nThe roots of modern econometrics can be traced to the American economist Henry L. Moore.  Moore studied agricultural productivity and attempted to fit changing values of productivity for plots of corn and other crops to a curve using different values of elasticity.  Moore made several errors in his work, some from his choice of models and some from limitations in his use of mathematics.  The accuracy of Moore's models also was limited by the poor data for national accounts in the United States at the time.  While his first models of production were static, in 1925 he published a dynamic \"moving equilibrium\" model designed to explain business cycles—this periodic variation from over-correction in supply and demand curves is now known as the cobweb model.  A more formal derivation of this model was made later by Nicholas Kaldor, who is largely credited for its exposition.[124]\n\nMuch of classical economics can be presented in simple geometric terms or elementary mathematical notation. Mathematical economics, however, conventionally makes use of calculus and matrix algebra in economic analysis in order to make powerful claims that would be more difficult without such mathematical tools. These tools are prerequisites for formal study, not only in mathematical economics but in contemporary economic theory in general. Economic problems often involve so many variables that mathematics is the only practical way of attacking and solving them.  Alfred Marshall argued that every economic problem which can be quantified, analytically expressed and solved, should be treated by means of mathematical work.[126]\n\nEconomics has become increasingly dependent upon mathematical methods and the mathematical tools it employs have become more sophisticated.  As a result, mathematics has become considerably more important to professionals in economics and finance. Graduate programs in both economics and finance require strong undergraduate preparation in mathematics for admission and, for this reason, attract an increasingly high number of mathematicians. Applied mathematicians apply mathematical principles to practical problems, such as economic analysis and other economics-related issues, and many economic problems are often defined as integrated into the scope of applied mathematics.[18]\n\nThis integration results from the formulation of economic problems as stylized models with clear assumptions and falsifiable predictions.  This modeling may be informal or prosaic, as it was in Adam Smith's The Wealth of Nations, or it may be formal, rigorous and mathematical.\n\nBroadly speaking, formal economic models may be classified as stochastic or deterministic and as discrete or continuous.  At a practical level, quantitative modeling is applied to many areas of economics and several methodologies have evolved more or less independently of each other.[127]\n\nThe great appeal of mathematical economics is that it brings a degree of rigor to economic thinking, particularly around charged political topics. For example, during the discussion of the efficacy of a corporate tax cut for increasing the wages of workers, a simple mathematical model proved beneficial to understanding the issues at hand.\n\n\nAs an intellectual exercise, the following problem was posed by Prof. Greg Mankiw of Harvard University:[128]\nAn open economy has the production function \n\n\n\ny\n=\nf\n(\nk\n)\n\n\n{\\textstyle y=f(k)}\n\n, where \n\n\n\ny\n\n\n{\\textstyle y}\n\n is output per worker and \n\n\n\nk\n\n\n{\\textstyle k}\n\n is capital per worker. The capital stock adjusts so that the after-tax marginal product of capital equals the exogenously given world interest rate \n\n\n\nr\n\n\n{\\textstyle r}\n\n...How much will the tax cut increase wages?\nTo answer this question, we follow John H. Cochrane of the Hoover Institution.[129] Suppose an open economy has the production function:\n\n\n\nY\n=\nF\n(\nK\n,\nL\n)\n=\nf\n(\nk\n)\nL\n,\n\nk\n=\nK\n\n/\n\nL\n\n\n{\\displaystyle Y=F(K,L)=f(k)L,\\quad k=K/L}\n\nWhere the variables in this equation are:\n\nThe standard choice for the production function is the Cobb-Douglas production function:\n\n\n\nY\n=\nA\n\nK\n\nα\n\n\n\nL\n\n1\n−\nα\n\n\n=\nA\n\nk\n\nα\n\n\nL\n,\n\nα\n∈\n[\n0\n,\n1\n]\n\n\n{\\displaystyle Y=AK^{\\alpha }L^{1-\\alpha }=Ak^{\\alpha }L,\\quad \\alpha \\in [0,1]}\n\nwhere \n\n\n\nA\n\n\n{\\textstyle A}\n\n is the factor of productivity - assumed to be a constant. A corporate tax cut in this model is equivalent to a tax on capital. With taxes, firms look to maximize:\n\n\n\nJ\n=\n\nmax\n\nK\n,\nL\n\n\n\n(\n1\n−\nτ\n)\n\n[\n\nF\n(\nK\n,\nL\n)\n−\nw\nL\n\n]\n\n−\nr\nK\n≡\n\nmax\n\nK\n,\nL\n\n\n\n(\n1\n−\nτ\n)\n\n[\n\nf\n(\nk\n)\n−\nw\n\n]\n\nL\n−\nr\nK\n\n\n{\\displaystyle J=\\max _{K,L}\\;(1-\\tau )\\left[F(K,L)-wL\\right]-rK\\equiv \\max _{K,L}\\;(1-\\tau )\\left[f(k)-w\\right]L-rK}\n\nwhere \n\n\n\nτ\n\n\n{\\textstyle \\tau }\n\n is the capital tax rate, \n\n\n\nw\n\n\n{\\textstyle w}\n\n is wages per worker, and \n\n\n\nr\n\n\n{\\textstyle r}\n\n is the exogenous interest rate. Then the first-order optimality conditions become:\n\n\n\n\n\n\n\n\n\n\n∂\nJ\n\n\n∂\nK\n\n\n\n\n\n\n=\n(\n1\n−\nτ\n)\n\nf\n′\n\n(\nk\n)\n−\nr\n\n\n\n\n\n\n\n∂\nJ\n\n\n∂\nL\n\n\n\n\n\n\n=\n(\n1\n−\nτ\n)\n\n[\n\nf\n(\nk\n)\n−\n\nf\n′\n\n(\nk\n)\nk\n−\nw\n\n]\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\frac {\\partial J}{\\partial K}}&=(1-\\tau )f'(k)-r\\\\{\\frac {\\partial J}{\\partial L}}&=(1-\\tau )\\left[f(k)-f'(k)k-w\\right]\\end{aligned}}}\n\nTherefore, the optimality conditions imply that:\n\n\n\nr\n=\n(\n1\n−\nτ\n)\n\nf\n′\n\n(\nk\n)\n,\n\nw\n=\nf\n(\nk\n)\n−\n\nf\n′\n\n(\nk\n)\nk\n\n\n{\\displaystyle r=(1-\\tau )f'(k),\\quad w=f(k)-f'(k)k}\n\nDefine total taxes \n\n\n\nX\n=\nτ\n[\nF\n(\nK\n,\nL\n)\n−\nw\nL\n]\n\n\n{\\textstyle X=\\tau [F(K,L)-wL]}\n\n. This implies that taxes per worker \n\n\n\nx\n\n\n{\\textstyle x}\n\n are:\n\n\n\nx\n=\nτ\n[\nf\n(\nk\n)\n−\nw\n]\n=\nτ\n\nf\n′\n\n(\nk\n)\nk\n\n\n{\\displaystyle x=\\tau [f(k)-w]=\\tau f'(k)k}\n\nThen the change in taxes per worker, given the tax rate, is:\n\n\n\n\n\n\nd\nx\n\n\nd\nτ\n\n\n\n=\n\n\n\n\n\nf\n′\n\n(\nk\n)\nk\n\n⏟\n\n\n\nStatic\n\n\n+\n\n\n\n\nτ\n\n[\n\n\nf\n′\n\n(\nk\n)\n+\n\nf\n″\n\n(\nk\n)\nk\n\n]\n\n\n\n\nd\nk\n\n\nd\nτ\n\n\n\n\n⏟\n\n\n\nDynamic\n\n\n\n\n{\\displaystyle {dx \\over {d\\tau }}=\\underbrace {f'(k)k} _{\\text{Static}}+\\underbrace {\\tau \\left[f'(k)+f''(k)k\\right]{dk \\over {d\\tau }}} _{\\text{Dynamic}}}\n\nTo find the change in wages, we differentiate the second optimality condition for the per worker wages to obtain:\n\n\n\n\n\n\nd\nw\n\n\nd\nτ\n\n\n\n=\n\n[\n\n\nf\n′\n\n(\nk\n)\n−\n\nf\n′\n\n(\nk\n)\n−\n\nf\n″\n\n(\nk\n)\nk\n\n]\n\n\n\n\nd\nk\n\n\nd\nτ\n\n\n\n=\n−\n\nf\n″\n\n(\nk\n)\nk\n\n\n\nd\nk\n\n\nd\nτ\n\n\n\n\n\n{\\displaystyle {\\frac {dw}{d\\tau }}=\\left[f'(k)-f'(k)-f''(k)k\\right]{\\frac {dk}{d\\tau }}=-f''(k)k{\\frac {dk}{d\\tau }}}\n\nAssuming that the interest rate is fixed at \n\n\n\nr\n\n\n{\\textstyle r}\n\n, so that \n\n\n\nd\nr\n\n/\n\nd\nτ\n=\n0\n\n\n{\\textstyle dr/d\\tau =0}\n\n, we may differentiate the first optimality condition for the interest rate to find:\n\n\n\n\n\n\nd\nk\n\n\nd\nτ\n\n\n\n=\n\n\n\n\nf\n′\n\n(\nk\n)\n\n\n(\n1\n−\nτ\n)\n\nf\n″\n\n(\nk\n)\n\n\n\n\n\n{\\displaystyle {dk \\over {d\\tau }}={f'(k) \\over {(1-\\tau )f''(k)}}}\n\nFor the moment, let's focus only on the static effect of a capital tax cut, so that \n\n\n\nd\nx\n\n/\n\nd\nτ\n=\n\nf\n′\n\n(\nk\n)\nk\n\n\n{\\textstyle dx/d\\tau =f'(k)k}\n\n. If we substitute this equation into equation for wage changes with respect to the tax rate, then we find that:\n\n\n\n\n\n\nd\nw\n\n\nd\nτ\n\n\n\n=\n−\n\n\n\n\nf\n′\n\n(\nk\n)\nk\n\n\n1\n−\nτ\n\n\n\n=\n−\n\n\n1\n\n1\n−\nτ\n\n\n\n\n\n\nd\nx\n\n\nd\nτ\n\n\n\n\n\n{\\displaystyle {dw \\over {d\\tau }}=-{\\frac {f'(k)k}{1-\\tau }}=-{1 \\over {1-\\tau }}{\\frac {dx}{d\\tau }}}\n\nTherefore, the static effect of a capital tax cut on wages is:\n\n\n\n\n\n\nd\nw\n\n\nd\nx\n\n\n\n=\n−\n\n\n1\n\n1\n−\nτ\n\n\n\n\n\n{\\displaystyle {dw \\over {dx}}=-{1 \\over {1-\\tau }}}\n\nBased on the model, it seems possible that we may achieve a rise in the wage of a worker greater than the amount of the tax cut. But that only considers the static effect, and we know that the dynamic effect must be accounted for. In the dynamic model, we may rewrite the equation for changes in taxes per worker with respect to the tax rate as:\n\n\n\n\n\n\n\n\n\n\nd\nx\n\n\nd\nτ\n\n\n\n\n\n\n=\n\nf\n′\n\n(\nk\n)\nk\n+\nτ\n\n[\n\n\nf\n′\n\n(\nk\n)\n+\n\nf\n″\n\n(\nk\n)\nk\n\n]\n\n\n\n\nd\nk\n\n\nd\nτ\n\n\n\n\n\n\n\n\n\n=\n\nf\n′\n\n(\nk\n)\nk\n+\n\n\nτ\n\n1\n−\nτ\n\n\n\n\n\n\n[\n\nf\n′\n\n(\nk\n)\n\n]\n\n2\n\n\n+\n\nf\n′\n\n(\nk\n)\n\nf\n″\n\n(\nk\n)\nk\n\n\n\nf\n″\n\n(\nk\n)\n\n\n\n\n\n\n\n\n\n=\n\n\nτ\n\n1\n−\nτ\n\n\n\n\n\n\n\nf\n′\n\n(\nk\n\n)\n\n2\n\n\n\n\n\nf\n″\n\n(\nk\n)\n\n\n\n+\n\n\n1\n\n1\n−\nτ\n\n\n\n\nf\n′\n\n(\nk\n)\nk\n\n\n\n\n\n\n=\n\n\n\n\nf\n′\n\n(\nk\n)\n\n\n1\n−\nτ\n\n\n\n\n[\n\nτ\n\n\n\n\nf\n′\n\n(\nk\n)\n\n\n\nf\n″\n\n(\nk\n)\n\n\n\n+\nk\n\n]\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{dx \\over {d\\tau }}&=f'(k)k+\\tau \\left[f'(k)+f''(k)k\\right]{dk \\over {d\\tau }}\\\\&=f'(k)k+{\\tau  \\over {1-\\tau }}{[f'(k)]^{2}+f'(k)f''(k)k \\over {f''(k)}}\\\\&={\\tau  \\over {1-\\tau }}{f'(k)^{2} \\over {f''(k)}}+{1 \\over {1-\\tau }}f'(k)k\\\\&={f'(k) \\over {1-\\tau }}\\left[\\tau {f'(k) \\over {f''(k)}}+k\\right]\\end{aligned}}}\n\nRecalling that \n\n\n\nd\nw\n\n/\n\nd\nτ\n=\n−\n\nf\n′\n\n(\nk\n)\nk\n\n/\n\n(\n1\n−\nτ\n)\n\n\n{\\textstyle dw/d\\tau =-f'(k)k/(1-\\tau )}\n\n, we have that:\n\n\n\n\n\n\nd\nw\n\n\nd\nx\n\n\n\n=\n−\n\n\n\n\n\n\nf\n′\n\n(\nk\n)\nk\n\n\n1\n−\nτ\n\n\n\n\n\n\n\n\nf\n′\n\n(\nk\n)\n\n\n1\n−\nτ\n\n\n\n\n[\n\nτ\n\n\n\n\nf\n′\n\n(\nk\n)\n\n\n\nf\n″\n\n(\nk\n)\n\n\n\n+\nk\n\n]\n\n\n\n\n=\n−\n\n\n1\n\nτ\n\n\n\n\nf\n′\n\n(\nk\n)\n\n\nk\n\nf\n″\n\n(\nk\n)\n\n\n\n+\n1\n\n\n\n\n\n{\\displaystyle {\\frac {dw}{dx}}=-{{\\frac {f'(k)k}{1-\\tau }} \\over {{\\frac {f'(k)}{1-\\tau }}\\left[\\tau {\\frac {f'(k)}{f''(k)}}+k\\right]}}=-{\\frac {1}{\\tau {\\frac {f'(k)}{kf''(k)}}+1}}}\n\nUsing the Cobb-Douglas production function, we have that:\n\n\n\n\n\n\n\nf\n′\n\n(\nk\n)\n\n\nk\n\nf\n″\n\n(\nk\n)\n\n\n\n=\n−\n\n\n1\n\n1\n−\nα\n\n\n\n\n\n{\\displaystyle {f'(k) \\over {kf''(k)}}=-{1 \\over {1-\\alpha }}}\n\nTherefore, the dynamic effect of a capital tax cut on wages is:\n\n\n\n\n\n\nd\nw\n\n\nd\nx\n\n\n\n=\n−\n\n\n\n1\n−\nα\n\n\n1\n−\nτ\n−\nα\n\n\n\n\n\n{\\displaystyle {dw \\over {dx}}=-{1-\\alpha  \\over {1-\\tau -\\alpha }}}\n\nIf we take \n\n\n\nα\n=\nτ\n=\n1\n\n/\n\n3\n\n\n{\\textstyle \\alpha =\\tau =1/3}\n\n, then the dynamic effect of lowering capital taxes on wages will be even larger than the static effect. Moreover, if there are positive externalities to capital accumulation, the effect of the tax cut on wages would be larger than in the model we just derived. It is important to note that the result is a combination of:\n\nThis result showing that, under certain assumptions, a corporate tax cut can boost the wages of workers by more than the lost revenue does not imply that the magnitude is correct.  Rather, it suggests a basis for policy analysis that is not grounded in handwaving.  If the assumptions are reasonable, then the model is an acceptable approximation of reality; if they are not, then better models should be developed.\n\nNow let's assume that instead of the Cobb-Douglas production function we have a more general constant elasticity of substitution (CES) production function:\n\n\n\nf\n(\nk\n)\n=\nA\n\n\n[\n\nα\n\nk\n\nρ\n\n\n+\n(\n1\n−\nα\n)\n\n]\n\n\n1\n\n/\n\nρ\n\n\n\n\n{\\displaystyle f(k)=A\\left[\\alpha k^{\\rho }+(1-\\alpha )\\right]^{1/\\rho }}\n\nwhere \n\n\n\nρ\n=\n1\n−\n\nσ\n\n−\n1\n\n\n\n\n{\\textstyle \\rho =1-\\sigma ^{-1}}\n\n; \n\n\n\nσ\n\n\n{\\textstyle \\sigma }\n\n is the elasticity of substitution between capital and labor. The relevant quantity we want to calculate is \n\n\n\n\nf\n′\n\n\n/\n\nk\n\nf\n″\n\n\n\n{\\textstyle f'/kf''}\n\n, which may be derived as:\n\n\n\n\n\n\nf\n′\n\n\nk\n\nf\n″\n\n\n\n\n=\n−\n\n\n1\n\n1\n−\nρ\n−\n\n\n\nα\n(\n1\n−\nρ\n)\n\n\nα\n+\n(\n1\n−\nα\n)\n\nk\n\n−\nρ\n\n\n\n\n\n\n\n\n\n\n{\\displaystyle {f' \\over {kf''}}=-{1 \\over {1-\\rho -{\\alpha (1-\\rho ) \\over {\\alpha +(1-\\alpha )k^{-\\rho }}}}}}\n\nTherefore, we may use this to find that:\n\n\n\n\n\n\n\n1\n+\nτ\n\n\n\nf\n′\n\n\nk\n\nf\n″\n\n\n\n\n\n\n\n=\n1\n−\n\n\n\nτ\n[\nα\n+\n(\n1\n−\nα\n)\n\nk\n\n−\nρ\n\n\n]\n\n\n(\n1\n−\nρ\n)\n[\nα\n+\n(\n1\n−\nα\n)\n\nk\n\n−\nρ\n\n\n]\n−\nα\n(\n1\n−\nρ\n)\n\n\n\n\n\n\n\n\n\n=\n\n\n\n(\n1\n−\nρ\n−\nτ\n)\n[\nα\n+\n(\n1\n−\nα\n)\n\nk\n\n−\nρ\n\n\n]\n−\nα\n(\n1\n−\nρ\n)\n\n\n(\n1\n−\nρ\n)\n[\nα\n+\n(\n1\n−\nα\n)\n\nk\n\n−\nρ\n\n\n]\n−\nα\n(\n1\n−\nρ\n)\n\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}1+\\tau {f' \\over {kf''}}&=1-{\\tau [\\alpha +(1-\\alpha )k^{-\\rho }] \\over {(1-\\rho )[\\alpha +(1-\\alpha )k^{-\\rho }]-\\alpha (1-\\rho )}}\\\\[6pt]&={(1-\\rho -\\tau )[\\alpha +(1-\\alpha )k^{-\\rho }]-\\alpha (1-\\rho ) \\over {(1-\\rho )[\\alpha +(1-\\alpha )k^{-\\rho }]-\\alpha (1-\\rho )}}\\end{aligned}}}\n\nTherefore, under a general CES model, the dynamic effect of a capital tax cut on wages is:\n\n\n\n\n\n\nd\nw\n\n\nd\nx\n\n\n\n=\n−\n\n\n\n(\n1\n−\nρ\n)\n[\nα\n+\n(\n1\n−\nα\n)\n\nk\n\n−\nρ\n\n\n]\n−\nα\n(\n1\n−\nρ\n)\n\n\n(\n1\n−\nρ\n−\nτ\n)\n[\nα\n+\n(\n1\n−\nα\n)\n\nk\n\n−\nρ\n\n\n]\n−\nα\n(\n1\n−\nρ\n)\n\n\n\n\n\n{\\displaystyle {dw \\over {dx}}=-{(1-\\rho )[\\alpha +(1-\\alpha )k^{-\\rho }]-\\alpha (1-\\rho ) \\over {(1-\\rho -\\tau )[\\alpha +(1-\\alpha )k^{-\\rho }]-\\alpha (1-\\rho )}}}\n\nWe recover the Cobb-Douglas solution when \n\n\n\nρ\n=\n0\n\n\n{\\textstyle \\rho =0}\n\n.  When \n\n\n\nρ\n=\n1\n\n\n{\\textstyle \\rho =1}\n\n, which is the case when perfect substitutes exist, we find that \n\n\n\nd\nw\n\n/\n\nd\nx\n=\n0\n\n\n{\\textstyle dw/dx=0}\n\n - there is no effect of changes in capital taxes on wages.  And when \n\n\n\nρ\n=\n−\n∞\n\n\n{\\textstyle \\rho =-\\infty }\n\n, which is the case when perfect complements exist, we find that \n\n\n\nd\nw\n\n/\n\nd\nx\n=\n−\n1\n\n\n{\\textstyle dw/dx=-1}\n\n - a cut in capital taxes increases wages by exactly one dollar.\n\nThe Austrian school — while making many of the same normative economic arguments as mainstream economists from marginalist traditions, such as the Chicago school — differs methodologically from mainstream neoclassical schools of economics, in particular in their sharp critiques of the mathematization of economics.[130] Friedrich Hayek contended that the use of formal techniques projects a scientific exactness that does not appropriately account for informational limitations faced by real economic agents. [131]\n\nIn an interview in 1999, the economic historian  Robert Heilbroner stated:[132]\n\nI guess the scientific approach began to penetrate and soon dominate the profession in the past twenty to thirty years. This came about in part because of the \"invention\" of mathematical analysis of various kinds and, indeed, considerable improvements in it. This is the age in which we have not only more data but more sophisticated use of data. So there is a strong feeling that this is a data-laden science and a data-laden undertaking, which, by virtue of the sheer numerics, the sheer equations, and the sheer look of a journal page, bears a certain resemblance to science . . . That one central activity looks scientific. I understand that. I think that is genuine. It approaches being a universal law. But resembling a science is different from being a science.\nHeilbroner stated that \"some/much of economics is not naturally quantitative and therefore does not lend itself to mathematical exposition.\"[133]\n\nPhilosopher Karl Popper discussed the scientific standing of economics in the 1940s and 1950s. He argued that mathematical economics suffered from being tautological. In other words, insofar as economics became a mathematical theory, mathematical economics ceased to rely on empirical refutation  but rather relied on mathematical proofs and disproof.[134] According to Popper,  falsifiable assumptions can be tested by experiment and observation while unfalsifiable assumptions can be explored mathematically for their consequences and for their consistency with other assumptions.[135]\n\nSharing Popper's concerns about assumptions in economics generally, and not just mathematical economics, Milton Friedman declared that \"all assumptions are unrealistic\".  Friedman proposed judging economic models by their predictive performance rather than by the match between their assumptions and reality.[136]\n\nConsidering mathematical economics, J.M. Keynes wrote in The General Theory:[137]\n\nIt is a great fault of symbolic pseudo-mathematical methods of formalising a system of economic analysis ... that they expressly assume strict independence between the factors involved and lose their cogency and authority if this hypothesis is disallowed; whereas, in ordinary discourse, where we are not blindly manipulating and know all the time what we are doing and what the words mean, we can keep ‘at the back of our heads’ the necessary reserves and qualifications and the adjustments which we shall have to make later on, in a way in which we cannot keep complicated partial differentials ‘at the back’ of several pages of algebra which assume they all vanish. Too large a proportion of recent ‘mathematical’ economics are merely concoctions, as imprecise as the initial assumptions they rest on, which allow the author to lose sight of the complexities and interdependencies of the real world in a maze of pretentious and unhelpful symbols.\nIn response to these criticisms, Paul Samuelson argued that mathematics is a language, repeating a thesis of Josiah Willard Gibbs. In economics, the language of mathematics is sometimes necessary for representing substantive problems. Moreover, mathematical economics has led to conceptual advances in economics.[138]  In particular, Samuelson gave the example of microeconomics, writing that \"few people are ingenious enough to grasp [its] more complex parts... without resorting to the language of mathematics, while most ordinary individuals can do so fairly easily with the aid of mathematics.\"[139]\n\nSome economists state that mathematical economics deserves support just like other forms of mathematics, particularly its neighbors in mathematical optimization and mathematical statistics and increasingly in theoretical computer science. Mathematical economics and other mathematical sciences have a history in which theoretical advances have regularly contributed to the reform of the more applied branches of economics. In particular, following the program of John von Neumann, game theory now provides the foundations for describing much of applied economics, from  statistical decision theory (as \"games against nature\") and econometrics to general equilibrium theory and industrial organization. In the last decade, with the rise of the internet, mathematical economists and optimization experts and computer scientists have worked on problems of pricing for on-line services --- their contributions using mathematics from cooperative game theory, nondifferentiable optimization, and combinatorial games.\n\nRobert M. Solow concluded that mathematical economics was the core \"infrastructure\" of contemporary economics:\n\nEconomics is no longer a fit conversation piece for ladies and gentlemen. It has become a technical subject. Like any technical subject it attracts some people who are more interested in the technique than the subject. That is too bad, but it may be inevitable. In any case, do not kid yourself: the technical core of economics is indispensable infrastructure for the political economy. That is why, if you consult [a reference in contemporary economics]  looking for enlightenment about the world today, you will be led to technical economics, or history, or nothing at all.[140]\n\nProminent mathematical economists include the following.\n\n\n\n\n\n\n"
    },
    {
        "title": "Philosophy of mathematics",
        "content": "\n\nPhilosophy of mathematics is the branch of philosophy that deals with the nature of mathematics and its relationship to other areas of philosophy, particularly epistemology and metaphysics. Central questions posed include whether or not mathematical objects are purely abstract entities or are in some way concrete, and in what the relationship such objects have with physical reality consists.[1]\n\nMajor themes that are dealt with in philosophy of mathematics include:\n\nThe connection between mathematics and material reality has led to philosophical debates since at least the time of Pythagoras. The ancient philosopher Plato argued that abstractions that reflect material reality have themselves a reality that exists outside space and time. As a result, the philosophical view that mathematical objects somehow exist on their own in abstraction is often referred to as Platonism. Independently of their possible philosophical opinions, modern mathematicians may be generally considered as Platonists, since they think of and talk of their objects of study as real objects.[2]\n\nArmand Borel summarized this view of mathematics reality as follows, and provided quotations of G. H. Hardy, Charles Hermite, Henri Poincaré and Albert Einstein that support his views.[3]\n\n Something becomes objective (as opposed to \"subjective\") as soon as we are convinced that it exists in the minds of others in the same form as it does in ours and that we can think about it and discuss it together.[4] Because the language of mathematics is so precise, it is ideally suited to defining concepts for which such a consensus exists. In my opinion, that is sufficient to provide us with a feeling of an objective existence, of a reality of mathematics ...\nMathematical reasoning requires rigor. This means that the definitions must be absolutely unambiguous and the proofs must be reducible to a succession of applications of syllogisms or inference rules,[a] without any use of empirical evidence and intuition.[b][6]\n\nThe rules of rigorous reasoning have been established by the ancient Greek philosophers under the name of logic. Logic is not specific to mathematics, but, in mathematics, the standard of rigor is much higher than elsewhere.\n\nFor many centuries, logic, although used for mathematical proofs, belonged to philosophy and was not specifically studied by mathematicians.[7] Circa the end of the 19th century, several paradoxes made questionable the logical foundation of mathematics, and consequently the validity of the whole mathematics. This has been called the foundational crisis of mathematics. Some of these paradoxes consist of results that seem to contradict the common intuition, such as the possibility to construct valid non-Euclidean geometries in which the parallel postulate is wrong, the Weierstrass function that is continuous but nowhere  differentiable, and the study by Georg Cantor of infinite sets, which led to consider several sizes of infinity (infinite cardinals). Even more striking, Russell's paradox shows that the phrase \"the set of all sets\" is self contradictory.\n\nSeveral methods have been proposed to solve the problem by changing of logical framework, such as  constructive mathematics and intuitionistic logic. Roughly speaking, the first one consists of requiring that every existence theorem must provide an explicit example, and the second one excludes from mathematical reasoning the law of excluded middle and double negation elimination.\n\nThe problems of foundation of mathematics has been eventually resolved with the rise of mathematical logic as a new area of mathematics. In this framework, a mathematical or logical theory consists of a formal language that defines the well-formed of assertions, a set of basic assertions called axioms and a set of inference rules that allow producing new assertions from one or several known assertions. A theorem of such a theory is either an axiom or an assertion that can be obtained from previously known theorems by the application of an inference rule. The Zermelo–Fraenkel set theory with the axiom of choice, generally called ZFC, is such a theory in which all mathematics have been restated; it is used implicitely in all mathematics texts that do not specify explicitly on which foundations they are based. Moreover, the other proposed foundations can be modeled and studied inside ZFC.\n\nIt results that \"rigor\" is no more a relevant concept in mathematics, as a proof is either correct or erroneous, and a \"rigorous proof\" is simply a pleonasm. Where a special concept of rigor comes into play is in the socialized aspects of a proof. In particular, proofs are rarely written in full details, and some steps of a proof are generally considered as trivial, easy, or straightforward, and therefore left to the reader. As most proof errors occur in these skipped steps, a new proof requires to be verified by other specialists of the subject, and can be considered as reliable only after having been accepted by the community of the specialists, which may need several years.[8]\n\nAlso, the concept of \"rigor\" may remain useful for teaching to beginners what is a mathematical proof.[9]\n\nMathematics is used in most sciences for modeling phenomena, which then allows predictions to be made from experimental laws.[10] The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model.[11] Inaccurate predictions, rather than being caused by invalid mathematical concepts, imply the need to change the mathematical model used.[12] For example, the perihelion precession of Mercury could only be explained after the emergence of Einstein's general relativity, which replaced Newton's law of gravitation as a better mathematical model.[13]\n\nThere is still a philosophical debate whether mathematics is a science. However, in practice, mathematicians are typically grouped with scientists, and mathematics shares much in common with the physical sciences. Like them, it is falsifiable, which means in mathematics that, if a result or a theory is wrong, this can be proved by providing a counterexample. Similarly as in science, theories and results (theorems) are often obtained from experimentation.[14] In mathematics, the experimentation may consist of computation on selected examples or of the study of figures or other representations of mathematical objects (often mind representations without physical support). For example, when asked how he came about his theorems, Gauss once replied \"durch planmässiges Tattonieren\" (through systematic experimentation).[15] However, some authors emphasize that mathematics differs from the modern notion of science by not relying on empirical evidence.[16][17][18][19]\n\nThe unreasonable effectiveness of mathematics is a phenomenon that was named and first made explicit by physicist Eugene Wigner.[20] It is the fact that many mathematical theories (even the \"purest\") have applications outside their initial object. These applications may be completely outside their initial area of mathematics, and may concern physical phenomena that were completely unknown when the mathematical theory was introduced.[21] Examples of unexpected applications of mathematical theories can be found in many areas of mathematics.\n\nA notable example is the prime factorization of natural numbers that was discovered more than 2,000 years before its common use for secure internet communications through the RSA cryptosystem.[22] A second historical example is the theory of ellipses. They were studied by the ancient Greek mathematicians as conic sections (that is, intersections of cones with planes). It is almost 2,000 years later that Johannes Kepler discovered that the trajectories of the planets are ellipses.[23]\n\nIn the 19th century, the internal development of geometry (pure mathematics) led to definition and study of non-Euclidean geometries, spaces of dimension higher than three and manifolds. At this time, these concepts seemed totally disconnected from the physical reality, but at the beginning of the 20th century, Albert Einstein developed the theory of relativity that uses fundamentally these concepts. In particular, spacetime of special relativity is a non-Euclidean space of dimension four, and spacetime of general relativity is a (curved) manifold of dimension four.[24][25]\n\nA striking aspect of the interaction between mathematics and physics is when mathematics drives research in physics. This is illustrated by the discoveries of the positron and the baryon \n\n\n\n\nΩ\n\n−\n\n\n.\n\n\n{\\displaystyle \\Omega ^{-}.}\n\n In both cases, the equations of the theories had unexplained solutions, which led to conjecture of the existence of an unknown particle, and the search for these particles. In both cases, these particles were discovered a few years later by specific experiments.[26][27][28]\n\nThe origin of mathematics is of arguments and disagreements. Whether the birth of mathematics was by chance or induced by necessity during the development of similar subjects, such as physics, remains an area of contention.[29][30]\n\nMany thinkers have contributed their ideas concerning the nature of mathematics. Today, some[who?] philosophers of mathematics aim to give accounts of this form of inquiry and its products as they stand, while others emphasize a role for themselves that goes beyond simple interpretation to critical analysis. There are traditions of mathematical philosophy in both Western philosophy and Eastern philosophy. Western philosophies of mathematics go as far back as Pythagoras, who described the theory \"everything is mathematics\" (mathematicism), Plato, who paraphrased Pythagoras, and studied the ontological status of mathematical objects, and Aristotle, who studied logic and issues related to infinity (actual versus potential).\n\nGreek philosophy on mathematics was strongly influenced by their study of geometry. For example, at one time, the Greeks held the opinion that 1 (one) was not a number, but rather a unit of arbitrary length. A number was defined as a multitude. Therefore, 3, for example, represented a certain multitude of units, and was thus \"truly\" a number. At another point, a similar argument was made that 2 was not a number but a fundamental notion of a pair. These views come from the heavily geometric straight-edge-and-compass viewpoint of the Greeks: just as lines drawn in a geometric problem are measured in proportion to the first arbitrarily drawn line, so too are the numbers on a number line measured in proportion to the arbitrary first \"number\" or \"one\".[citation needed]\n\nThese earlier Greek ideas of numbers were later upended by the discovery of the irrationality of the square root of two. Hippasus, a disciple of Pythagoras, showed that the diagonal of a unit square was incommensurable with its (unit-length) edge: in other words he proved there was no existing (rational) number that accurately depicts the proportion of the diagonal of the unit square to its edge. This caused a significant re-evaluation of Greek philosophy of mathematics. According to legend, fellow Pythagoreans were so traumatized by this discovery that they murdered Hippasus to stop him from spreading his heretical idea.[31] Simon Stevin was one of the first in Europe to challenge Greek ideas in the 16th century. Beginning with Leibniz, the focus shifted strongly to the relationship between mathematics and logic. This perspective dominated the philosophy of mathematics through the time of Frege and of Russell, but was brought into question by developments in the late 19th and early 20th centuries.\n\nA perennial issue in the philosophy of mathematics concerns the relationship between logic and mathematics at their joint foundations. While 20th-century philosophers continued to ask the questions mentioned at  the outset of this article, the philosophy of mathematics in the 20th century was characterized by a predominant interest in formal logic, set theory (both naive set theory and axiomatic set theory), and foundational issues.\n\nIt is a profound puzzle that on the one hand mathematical truths seem to have a compelling inevitability, but on the other hand the source of their \"truthfulness\" remains elusive. Investigations into this issue are known as the foundations of mathematics program.\n\nAt the start of the 20th century, philosophers of mathematics were already beginning to divide into various schools of thought about all these questions, broadly distinguished by their pictures of mathematical epistemology and ontology. Three schools, formalism, intuitionism, and logicism, emerged at this time, partly in response to the increasingly widespread worry that mathematics as it stood, and analysis in particular, did not live up to the standards of certainty and rigor that had been taken for granted. Each school addressed the issues that came to the fore at that time, either attempting to resolve them or claiming that mathematics is not entitled to its status as our most trusted knowledge.\n\nSurprising and counter-intuitive developments in formal logic and set theory early in the 20th century led to new questions concerning what was traditionally called the foundations of mathematics. As the century unfolded, the initial focus of concern expanded to an open exploration of the fundamental axioms of mathematics, the axiomatic approach having been taken for granted since the time of Euclid around 300 BCE as the natural basis for mathematics. Notions of axiom, proposition and proof, as well as the notion of a proposition being true of a mathematical object (see Assignment), were formalized, allowing them to be treated mathematically. The Zermelo–Fraenkel axioms for set theory were formulated which provided a conceptual framework in which much mathematical discourse would be interpreted. In mathematics, as in physics, new and unexpected ideas had arisen and significant changes were coming. With Gödel numbering, propositions could be interpreted as referring to themselves or other propositions, enabling inquiry into the consistency of mathematical theories. This reflective critique in which the theory under review \"becomes itself the object of a mathematical study\" led Hilbert to call such study metamathematics or proof theory.[32]\n\nAt the middle of the century, a new mathematical theory was created by Samuel Eilenberg and Saunders Mac Lane, known as category theory, and it became a new contender for the natural language of mathematical thinking.[33] As the 20th century progressed, however, philosophical opinions diverged as to just how well-founded were the questions about foundations that were raised at the century's beginning. Hilary Putnam summed up one common view of the situation in the last third of the century by saying:\n\nWhen philosophy discovers something wrong with science, sometimes science has to be changed—Russell's paradox comes to mind, as does Berkeley's attack on the actual infinitesimal—but more often it is philosophy that has to be changed. I do not think that the difficulties that philosophy finds with classical mathematics today are genuine difficulties; and I think that the philosophical interpretations of mathematics that we are being offered on every hand are wrong, and that \"philosophical interpretation\" is just what mathematics doesn't need.[34]: 169–170 \n\nPhilosophy of mathematics today proceeds along several different lines of inquiry, by philosophers of mathematics, logicians, and mathematicians, and there are many schools of thought on the subject. The schools are addressed separately in the next section, and their assumptions explained.\n\nContemporary schools of thought in the philosophy of mathematics include: artistic, Platonism, mathematicism, logicism, formalism, conventionalism, intuitionism, constructivism, finitism, structuralism, embodied mind theories (Aristotelian realism, psychologism, empiricism), fictionalism, social constructivism, and non-traditional schools.\n\nThe view that claims that mathematics is the aesthetic combination of assumptions, and then also claims that mathematics is an art. A famous mathematician who claims that is the British G. H. Hardy.[35] For Hardy, in his book, A Mathematician's Apology, the definition of mathematics was more like the aesthetic combination of concepts.[36]\n\nMax Tegmark's mathematical universe hypothesis (or mathematicism) goes further than Platonism in asserting that not only do all mathematical objects exist, but nothing else does. Tegmark's sole postulate is: All structures that exist mathematically also exist physically. That is, in the sense that \"in those [worlds] complex enough to contain self-aware substructures [they] will subjectively perceive themselves as existing in a physically 'real' world\".[37][38]\n\nLogicism is the thesis that mathematics is reducible to logic, and hence nothing but a part of logic.[39]: 41  Logicists hold that mathematics can be known a priori, but suggest that our knowledge of mathematics is just part of our knowledge of logic in general, and is thus analytic, not requiring any special faculty of mathematical intuition. In this view, logic is the proper foundation of mathematics, and all mathematical statements are necessary logical truths.\n\nRudolf Carnap (1931) presents the logicist thesis in two parts:[39]\n\nGottlob Frege was the founder of logicism. In his seminal Die Grundgesetze der Arithmetik (Basic Laws of Arithmetic) he built up arithmetic from a system of logic with a general principle of comprehension, which he called \"Basic Law V\" (for concepts F and G, the extension of F equals the extension of G if and only if for all objects a, Fa equals Ga), a principle that he took to be acceptable as part of logic.\n\nFrege's construction was flawed. Bertrand Russell discovered that Basic Law V is inconsistent (this is Russell's paradox). Frege abandoned his logicist program soon after this, but it was continued by Russell and Whitehead. They attributed the paradox to \"vicious circularity\" and built up what they called ramified type theory to deal with it. In this system, they were eventually able to build up much of modern mathematics but in an altered, and excessively complex form (for example, there were different natural numbers in each type, and there were infinitely many types). They also had to make several compromises in order to develop much of mathematics, such as the \"axiom of reducibility\". Even Russell said that this axiom did not really belong to logic.\n\nModern logicists (like Bob Hale, Crispin Wright, and perhaps others) have returned to a program closer to Frege's. They have abandoned Basic Law V in favor of abstraction principles such as Hume's principle (the number of objects falling under the concept F equals the number of objects falling under the concept G if and only if the extension of F and the extension of G can be put into one-to-one correspondence). Frege required Basic Law V to be able to give an explicit definition of the numbers, but all the properties of numbers can be derived from Hume's principle. This would not have been enough for Frege because (to paraphrase him) it does not exclude the possibility that the number 3 is in fact Julius Caesar. In addition, many of the weakened principles that they have had to adopt to replace Basic Law V no longer seem so obviously analytic, and thus purely logical.\n\nFormalism holds that mathematical statements may be thought of as statements about the consequences of certain string manipulation rules. For example, in the \"game\" of Euclidean geometry (which is seen as consisting of some strings called \"axioms\", and some \"rules of inference\" to generate new strings from given ones), one can prove that the Pythagorean theorem holds (that is, one can generate the string corresponding to the Pythagorean theorem). According to formalism, mathematical truths are not about numbers and sets and triangles and the like—in fact, they are not \"about\" anything at all.\n\nAnother version of formalism is known as deductivism.[40] In deductivism, the Pythagorean theorem is not an absolute truth, but a relative one, if it follows deductively from the appropriate axioms. The same is held to be true for all other mathematical statements.\n\nFormalism need not mean that mathematics is nothing more than a meaningless symbolic game. It is usually hoped that there exists some interpretation in which the rules of the game hold. (Compare this position to structuralism.) But it does allow the working mathematician to continue in his or her work and leave such problems to the philosopher or scientist. Many formalists would say that in practice, the axiom systems to be studied will be suggested by the demands of science or other areas of mathematics.\n\nA major early proponent of formalism was David Hilbert, whose program was intended to be a complete and consistent axiomatization of all of mathematics.[41] Hilbert aimed to show the consistency of mathematical systems from the assumption that the \"finitary arithmetic\" (a subsystem of the usual arithmetic of the positive integers, chosen to be philosophically uncontroversial) was consistent. Hilbert's goals of creating a system of mathematics that is both complete and consistent were seriously undermined by the second of Gödel's incompleteness theorems, which states that sufficiently expressive consistent axiom systems can never prove their own consistency. Since any such axiom system would contain the finitary arithmetic as a subsystem, Gödel's theorem implied that it would be impossible to prove the system's consistency relative to that (since it would then prove its own consistency, which Gödel had shown was impossible). Thus, in order to show that any axiomatic system of mathematics is in fact consistent, one needs to first assume the consistency of a system of mathematics that is in a sense stronger than the system to be proven consistent.\n\nHilbert was initially a deductivist, but, as may be clear from above, he considered certain metamathematical methods to yield intrinsically meaningful results and was a realist with respect to the finitary arithmetic. Later, he held the opinion that there was no other meaningful mathematics whatsoever, regardless of interpretation.\n\nOther formalists, such as Rudolf Carnap, Alfred Tarski, and Haskell Curry, considered mathematics to be the investigation of formal axiom systems. Mathematical logicians study formal systems but are just as often realists as they are formalists.\n\nFormalists are relatively tolerant and inviting to new approaches to logic, non-standard number systems, new set theories, etc. The more games we study, the better. However, in all three of these examples, motivation is drawn from existing mathematical or philosophical concerns. The \"games\" are usually not arbitrary.\n\nThe main critique of formalism is that the actual mathematical ideas that occupy mathematicians are far removed from the string manipulation games mentioned above. Formalism is thus silent on the question of which axiom systems ought to be studied, as none is more meaningful than another from a formalistic point of view.\n\nRecently, some[who?] formalist mathematicians have proposed that all of our formal mathematical knowledge should be systematically encoded in computer-readable formats, so as to facilitate automated proof checking of mathematical proofs and the use of interactive theorem proving in the development of mathematical theories and computer software. Because of their close connection with computer science, this idea is also advocated by mathematical intuitionists and constructivists in the \"computability\" tradition—see QED project for a general overview.\n\nThe French mathematician Henri Poincaré was among the first to articulate a conventionalist view. Poincaré's use of non-Euclidean geometries in his work on differential equations convinced him that Euclidean geometry should not be regarded as a priori truth. He held that axioms in geometry should be chosen for the results they produce, not for their apparent coherence with human intuitions about the physical world.\n\nIn mathematics, intuitionism is a program of methodological reform whose motto is that \"there are no non-experienced mathematical truths\" (L. E. J. Brouwer). From this springboard, intuitionists seek to reconstruct what they consider to be the corrigible portion of mathematics in accordance with Kantian concepts of being, becoming, intuition, and knowledge. Brouwer, the founder of the movement, held that mathematical objects arise from the a priori forms of the volitions that inform the perception of empirical objects.[42]\n\nA major force behind intuitionism was L. E. J. Brouwer, who rejected the usefulness of formalized logic of any sort for mathematics. His student Arend Heyting postulated an intuitionistic logic, different from the classical Aristotelian logic; this logic does not contain the law of the excluded middle and therefore frowns upon proofs by contradiction. The axiom of choice is also rejected in most intuitionistic set theories, though in some versions it is accepted.\n\nIn intuitionism, the term \"explicit construction\" is not cleanly defined, and that has led to criticisms. Attempts have been made to use the concepts of Turing machine or computable function to fill this gap, leading to the claim that only questions regarding the behavior of finite algorithms are meaningful and should be investigated in mathematics. This has led to the study of the computable numbers, first introduced by Alan Turing. Not surprisingly, then, this approach to mathematics is sometimes associated with theoretical computer science.\n\nLike intuitionism, constructivism involves the regulative principle that only mathematical entities which can be explicitly constructed in a certain sense should be admitted to mathematical discourse. In this view, mathematics is an exercise of the human intuition, not a game played with meaningless symbols. Instead, it is about entities that we can create directly through mental activity. In addition, some adherents of these schools reject non-constructive proofs, such as using proof by contradiction when showing the existence of an object or when trying to establish the truth of some proposition. Important work was done by Errett Bishop, who managed to prove versions of the most important theorems in real analysis as constructive analysis in his 1967 Foundations of Constructive Analysis.[43]\n\nFinitism is an extreme form of constructivism, according to which a mathematical object does not exist unless it can be constructed from natural numbers in a finite number of steps. In her book Philosophy of Set Theory, Mary Tiles characterized those who allow countably infinite objects as classical finitists, and those who deny even countably infinite objects as strict finitists.\n\nThe most famous proponent of finitism was Leopold Kronecker,[44] who said:\n\nGod created the natural numbers, all else is the work of man.\nUltrafinitism is an even more extreme version of finitism, which rejects not only infinities but finite quantities that cannot feasibly be constructed with available resources. Another variant of finitism is Euclidean arithmetic, a system developed by John Penn Mayberry in his book The Foundations of Mathematics in the Theory of Sets.[45] Mayberry's system is Aristotelian in general inspiration and, despite his strong rejection of any role for operationalism or feasibility in the foundations of mathematics, comes to somewhat similar conclusions, such as, for instance, that super-exponentiation is not a legitimate finitary function.\n\nStructuralism is a position holding that mathematical theories describe structures, and that mathematical objects are exhaustively defined by their places in such structures, consequently having no intrinsic properties. For instance, it would maintain that all that needs to be known about the number 1 is that it is the first whole number after 0. Likewise all the other whole numbers are defined by their places in a structure, the number line. Other examples of mathematical objects might include lines and planes in geometry, or elements and operations in abstract algebra.\n\nStructuralism is an epistemologically realistic view in that it holds that mathematical statements have an objective truth value. However, its central claim only relates to what kind of entity a mathematical object is, not to what kind of existence mathematical objects or structures have (not, in other words, to their ontology). The kind of existence mathematical objects have would clearly be dependent on that of the structures in which they are embedded; different sub-varieties of structuralism make different ontological claims in this regard.[46]\n\nThe ante rem structuralism (\"before the thing\") has a similar ontology to Platonism.  Structures are held to have a real but abstract and immaterial existence. As such, it faces the standard epistemological problem of explaining the interaction between such abstract structures and flesh-and-blood mathematicians (see Benacerraf's identification problem).\n\nThe in re structuralism (\"in the thing\") is the equivalent of Aristotelian realism. Structures are held to exist inasmuch as some concrete system exemplifies them. This incurs the usual issues that some perfectly legitimate structures might accidentally happen not to exist, and that a finite physical world might not be \"big\" enough to accommodate some otherwise legitimate structures.\n\nThe post rem structuralism (\"after the thing\") is anti-realist about structures in a way that parallels nominalism. Like nominalism, the post rem approach denies the existence of abstract mathematical objects with properties other than their place in a relational structure.  According to this view mathematical systems exist, and have structural features in common. If something is true of a structure, it will be true of all systems exemplifying the structure. However, it is merely instrumental to talk of structures being \"held in common\" between systems: they in fact have no independent existence.\n\nEmbodied mind theories hold that mathematical thought is a natural outgrowth of the human cognitive apparatus which finds itself in our physical universe. For example, the abstract concept of number springs from the experience of counting discrete objects (requiring the human senses such as sight for detecting the objects, touch; and signalling from the brain). It is held that mathematics is not universal and does not exist in any real sense, other than in human brains. Humans construct, but do not discover, mathematics.\n\nThe cognitive processes of pattern-finding and distinguishing objects are also subject to neuroscience; if mathematics is considered to be relevant to a natural world (such as from realism or a degree of it, as opposed to pure solipsism).\n\nIts actual relevance to reality, while accepted to be a trustworthy approximation (it is also suggested the evolution of perceptions, the body, and the senses may have been necessary for survival) is not necessarily accurate to a full realism (and is still subject to flaws such as illusion, assumptions (consequently; the foundations and axioms in which mathematics have been formed by humans), generalisations, deception, and hallucinations). As such, this may also raise questions for the modern scientific method for its compatibility with general mathematics; as while relatively reliable, it is still limited by what can be measured by empiricism which may not be as reliable as previously assumed (see also: 'counterintuitive' concepts in such as quantum nonlocality, and action at a distance).\n\nAnother issue is that one numeral system may not necessarily be applicable to problem solving. Subjects such as complex numbers or imaginary numbers require specific changes to more commonly used axioms of mathematics; otherwise they cannot be adequately understood.\n\nAlternatively, computer programmers may use hexadecimal for its 'human-friendly' representation of binary-coded values, rather than decimal (convenient for counting because humans have ten fingers). The axioms or logical rules behind mathematics also vary through time (such as the adaption and invention of zero).\n\nAs perceptions from the human brain are subject to illusions, assumptions, deceptions, (induced) hallucinations, cognitive errors or assumptions in a general context, it can be questioned whether they are accurate or strictly indicative of truth (see also: philosophy of being), and the nature of empiricism itself in relation to the universe and whether it is independent to the senses and the universe.\n\nThe human mind has no special claim on reality or approaches to it built out of math. If such constructs as Euler's identity are true then they are true as a map of the human mind and cognition.\n\nEmbodied mind theorists thus explain the effectiveness of mathematics—mathematics was constructed by the brain in order to be effective in this universe.\n\nThe most accessible, famous, and infamous treatment of this perspective is Where Mathematics Comes From, by George Lakoff and Rafael E. Núñez. In addition, mathematician Keith Devlin has investigated similar concepts with his book The Math Instinct, as has neuroscientist Stanislas Dehaene with his book The Number Sense. For more on the philosophical ideas that inspired this perspective, see cognitive science of mathematics.\n\nAristotelian realism holds that mathematics studies properties such as symmetry, continuity and order that can be literally realized in the physical world (or in any other world there might be). It contrasts with Platonism in holding that the objects of mathematics, such as numbers, do not exist in an \"abstract\" world but can be physically realized. For example, the number 4 is realized in the relation between a heap of parrots and the universal \"being a parrot\" that divides the heap into so many parrots.[47][48] Aristotelian realism is defended by James Franklin and the Sydney School in the philosophy of mathematics and is close to the view of Penelope Maddy that when an egg carton is opened, a set of three eggs is perceived (that is, a mathematical entity realized in the physical world).[49] A problem for Aristotelian realism is what account to give of higher infinities, which may not be realizable in the physical world.\n\nThe Euclidean arithmetic developed by John Penn Mayberry in his book The Foundations of Mathematics in the Theory of Sets[45] also falls into the Aristotelian realist tradition. Mayberry, following Euclid, considers numbers to be simply \"definite multitudes of units\" realized in nature—such as \"the members of the London Symphony Orchestra\" or \"the trees in Birnam wood\". Whether or not there are definite multitudes of units for which Euclid's Common Notion 5 (the whole is greater than the part) fails and which would consequently be reckoned as infinite is for Mayberry essentially a question about Nature and does not entail any transcendental suppositions.\n\nPsychologism in the philosophy of mathematics is the position that mathematical concepts and/or truths are grounded in, derived from or explained by psychological facts (or laws).\n\nJohn Stuart Mill seems to have been an advocate of a type of logical psychologism, as were many 19th-century German logicians such as Sigwart and Erdmann as well as a number of psychologists, past and present: for example, Gustave Le Bon. Psychologism was famously criticized by Frege in his The Foundations of Arithmetic, and many of his works and essays, including his review of Husserl's Philosophy of Arithmetic. Edmund Husserl, in the first volume of his Logical Investigations, called \"The Prolegomena of Pure Logic\", criticized psychologism thoroughly and sought to distance himself from it. The \"Prolegomena\" is considered a more concise, fair, and thorough refutation of psychologism than the criticisms made by Frege, and also it is considered today by many as being a memorable refutation for its decisive blow to psychologism. Psychologism was also criticized by Charles Sanders Peirce and Maurice Merleau-Ponty.\n\nMathematical empiricism is a form of realism that denies that mathematics can be known a priori at all. It says that we discover mathematical facts by empirical research, just like facts in any of the other sciences. It is not one of the classical three positions advocated in the early 20th century, but primarily arose in the middle of the century. However, an important early proponent of a view like this was John Stuart Mill. Mill's view was widely criticized, because, according to critics, such as A.J. Ayer,[50] it makes statements like \"2 + 2 = 4\" come out as uncertain, contingent truths, which we can only learn by observing instances of two pairs coming together and forming a quartet.\n\nKarl Popper was another philosopher to point out empirical aspects of mathematics, observing that \"most mathematical theories are, like those of physics and biology, hypothetico-deductive: pure mathematics therefore turns out to be much closer to the natural sciences whose hypotheses are conjectures, than it seemed even recently.\"[51] Popper also noted he would \"admit a system as empirical or scientific only if it is capable of being tested by experience.\"[52]\n\nContemporary mathematical empiricism, formulated by W. V. O. Quine and Hilary Putnam, is primarily supported by the indispensability argument: mathematics is indispensable to all empirical sciences, and if we want to believe in the reality of the phenomena described by the sciences, we ought also believe in the reality of those entities required for this description. That is, since physics needs to talk about electrons to say why light bulbs behave as they do, then electrons must exist. Since physics needs to talk about numbers in offering any of its explanations, then numbers must exist. In keeping with Quine and Putnam's overall philosophies, this is a naturalistic argument. It argues for the existence of mathematical entities as the best explanation for experience, thus stripping mathematics of being distinct from the other sciences.\n\nPutnam strongly rejected the term \"Platonist\" as implying an over-specific ontology that was not necessary to mathematical practice in any real sense. He advocated a form of \"pure realism\" that rejected mystical notions of truth and accepted much quasi-empiricism in mathematics. This grew from the increasingly popular assertion in the late 20th century that no one foundation of mathematics could be ever proven to exist. It is also sometimes called \"postmodernism in mathematics\" although that term is considered overloaded by some and insulting by others. Quasi-empiricism argues that in doing their research, mathematicians test hypotheses as well as prove theorems. A mathematical argument can transmit falsity from the conclusion to the premises just as well as it can transmit truth from the premises to the conclusion. Putnam has argued that any theory of mathematical realism would include quasi-empirical methods. He proposed that an alien species doing mathematics might well rely on quasi-empirical methods primarily, being willing often to forgo rigorous and axiomatic proofs, and still be doing mathematics—at perhaps a somewhat greater risk of failure of their calculations. He gave a detailed argument for this in New Directions.[53] Quasi-empiricism was also developed by Imre Lakatos.\n\nThe most important criticism of empirical views of mathematics is approximately the same as that raised against Mill. If mathematics is just as empirical as the other sciences, then this suggests that its results are just as fallible as theirs, and just as contingent. In Mill's case the empirical justification comes directly, while in Quine's case it comes indirectly, through the coherence of our scientific theory as a whole, i.e. consilience after E.O. Wilson. Quine suggests that mathematics seems completely certain because the role it plays in our web of belief is extraordinarily central, and that it would be extremely difficult for us to revise it, though not impossible.\n\nFor a philosophy of mathematics that attempts to overcome some of the shortcomings of Quine and Gödel's approaches by taking aspects of each see Penelope Maddy's Realism in Mathematics. Another example of a realist theory is the embodied mind theory.\n\nFor experimental evidence suggesting that human infants can do elementary arithmetic, see Brian Butterworth.\n\nMathematical fictionalism was brought to fame in 1980 when Hartry Field published Science Without Numbers,[54] which rejected and in fact reversed Quine's indispensability argument. Where Quine suggested that mathematics was indispensable for our best scientific theories, and therefore should be accepted as a body of truths talking about independently existing entities, Field suggested that mathematics was dispensable, and therefore should be considered as a body of falsehoods not talking about anything real. He did this by giving a complete axiomatization of Newtonian mechanics with no reference to numbers or functions at all. He started with the \"betweenness\" of Hilbert's axioms to characterize space without coordinatizing it, and then added extra relations between points to do the work formerly done by vector fields. Hilbert's geometry is mathematical, because it talks about abstract points, but in Field's theory, these points are the concrete points of physical space, so no special mathematical objects at all are needed.\n\nHaving shown how to do science without using numbers, Field proceeded to rehabilitate mathematics as a kind of useful fiction. He showed that mathematical physics is a conservative extension of his non-mathematical physics (that is, every physical fact provable in mathematical physics is already provable from Field's system), so that mathematics is a reliable process whose physical applications are all true, even though its own statements are false. Thus, when doing mathematics, we can see ourselves as telling a sort of story, talking as if numbers existed. For Field, a statement like \"2 + 2 = 4\" is just as fictitious as \"Sherlock Holmes lived at 221B Baker Street\"—but both are true according to the relevant fictions.\n\nAnother fictionalist, Mary Leng, expresses the perspective succinctly by dismissing any seeming connection between mathematics and the physical world as \"a happy coincidence\". This rejection separates fictionalism from other forms of anti-realism, which see mathematics itself as artificial but still bounded or fitted to reality in some way.[55]\n\nBy this account, there are no metaphysical or epistemological problems special to mathematics. The only worries left are the general worries about non-mathematical physics, and about fiction in general. Field's approach has been very influential, but is widely rejected. This is in part because of the requirement of strong fragments of second-order logic to carry out his reduction, and because the statement of conservativity seems to require quantification over abstract models or deductions.[citation needed]\n\nSocial constructivism sees mathematics primarily as a social construct, as a product of culture, subject to correction and change. Like the other sciences, mathematics is viewed as an empirical endeavor whose results are constantly evaluated and may be discarded. However, while on an empiricist view the evaluation is some sort of comparison with \"reality\", social constructivists emphasize that the direction of mathematical research is dictated by the fashions of the social group performing it or by the needs of the society financing it. However, although such external forces may change the direction of some mathematical research, there are strong internal constraints—the mathematical traditions, methods, problems, meanings and values into which mathematicians are enculturated—that work to conserve the historically defined discipline.\n\nThis runs counter to the traditional beliefs of working mathematicians, that mathematics is somehow pure or objective. But social constructivists argue that mathematics is in fact grounded by much uncertainty: as mathematical practice evolves, the status of previous mathematics is cast into doubt, and is corrected to the degree it is required or desired by the current mathematical community. This can be seen in the development of analysis from reexamination of the calculus of Leibniz and Newton. They argue further that finished mathematics is often accorded too much status, and folk mathematics not enough, due to an overemphasis on axiomatic proof and peer review as practices.\n\nThe social nature of mathematics is highlighted in its subcultures. Major discoveries can be made in one branch of mathematics and be relevant to another, yet the relationship goes undiscovered for lack of social contact between mathematicians. Social constructivists argue each speciality forms its own epistemic community and often has great difficulty communicating, or motivating the investigation of unifying conjectures that might relate different areas of mathematics. Social constructivists see the process of \"doing mathematics\" as actually creating the meaning, while social realists see a deficiency either of human capacity to abstractify, or of human's cognitive bias, or of mathematicians' collective intelligence as preventing the comprehension of a real universe of mathematical objects. Social constructivists sometimes reject the search for foundations of mathematics as bound to fail, as pointless or even meaningless.\n\nContributions to this school have been made by Imre Lakatos and Thomas Tymoczko, although it is not clear that either would endorse the title.[clarification needed] More recently Paul Ernest has explicitly formulated a social constructivist philosophy of mathematics.[56] Some consider the work of Paul Erdős as a whole to have advanced this view (although he personally rejected it) because of his uniquely broad collaborations, which prompted others to see and study \"mathematics as a social activity\", e.g., via the Erdős number. Reuben Hersh has also promoted the social view of mathematics, calling it a \"humanistic\" approach,[57] similar to but not quite the same as that associated with Alvin White;[58] one of Hersh's co-authors, Philip J. Davis, has expressed sympathy for the social view as well.\n\nRather than focus on narrow debates about the true nature of mathematical truth, or even on practices unique to mathematicians such as the proof, a growing movement from the 1960s to the 1990s began to question the idea of seeking foundations or finding any one right answer to why mathematics works. The starting point for this was Eugene Wigner's famous 1960 paper \"The Unreasonable Effectiveness of Mathematics in the Natural Sciences\", in which he argued that the happy coincidence of mathematics and physics being so well matched seemed to be unreasonable and hard to explain.\n\nRealist and constructivist theories are normally taken to be contraries. However, Karl Popper[59] argued that a number statement such as \"2 apples + 2 apples = 4 apples\" can be taken in two senses. In one sense it is irrefutable and logically true. In the second sense it is factually true and falsifiable. Another way of putting this is to say that a single number statement can express two propositions: one of which can be explained on constructivist lines; the other on realist lines.[60]\n\nInnovations in the philosophy of language during the 20th century renewed interest in whether mathematics is, as is often said,[citation needed] the language of science. Although some[who?] mathematicians and philosophers would accept the statement \"mathematics is a language\" (most consider that the language of mathematics is a part of mathematics to which mathematics cannot be reduced),[citation needed] linguists[who?] believe that the implications of such a statement must be considered. For example, the tools of linguistics are not generally applied to the symbol systems of mathematics, that is, mathematics is studied in a markedly different way from other languages. If mathematics is a language, it is a different type of language from natural languages. Indeed, because of the need for clarity and specificity, the language of mathematics is far more constrained than natural languages studied by linguists. However, the methods developed by Frege and Tarski for the study of mathematical language have been extended greatly by Tarski's student Richard Montague and other linguists working in formal semantics to show that the distinction between mathematical language and natural language may not be as great as it seems.\n\nMohan Ganesalingam has analysed mathematical language using tools from formal linguistics.[61] Ganesalingam notes that some features of natural language are not necessary when analysing mathematical language (such as tense), but many of the same analytical tools can be used (such as context-free grammars). One important difference is that mathematical objects have clearly defined types, which can be explicitly defined in a text: \"Effectively, we are allowed to introduce a word in one part of a sentence, and declare its part of speech in another; and this operation has no analogue in natural language.\"[61]: 251 \n\nThis argument, associated with Willard Quine and Hilary Putnam, is considered by Stephen Yablo to be one of the most challenging arguments in favor of the acceptance of the existence of abstract mathematical entities, such as numbers and sets.[62] The form of the argument is as follows.\n\nThe justification for the first premise is the most controversial. Both Putnam and Quine invoke naturalism to justify the exclusion of all non-scientific entities, and hence to defend the \"only\" part of \"all and only\". The assertion that \"all\" entities postulated in scientific theories, including numbers, should be accepted as real is justified by confirmation holism. Since theories are not confirmed in a piecemeal fashion, but as a whole, there is no justification for excluding any of the entities referred to in well-confirmed theories. This puts the nominalist who wishes to exclude the existence of sets and non-Euclidean geometry, but to include the existence of quarks and other undetectable entities of physics, for example, in a difficult position.[63]\n\nThe anti-realist \"epistemic argument\" against Platonism has been made by Paul Benacerraf and Hartry Field. Platonism posits that mathematical objects are abstract entities. By general agreement, abstract entities cannot interact causally with concrete, physical entities (\"the truth-values of our mathematical assertions depend on facts involving Platonic entities that reside in a realm outside of space-time\"[64]). Whilst our knowledge of concrete, physical objects is based on our ability to perceive them, and therefore to causally interact with them, there is no parallel account of how mathematicians come to have knowledge of abstract objects.[65][66][67] Another way of making the point is that if the Platonic world were to disappear, it would make no difference to the ability of mathematicians to generate proofs, etc., which is already fully accountable in terms of physical processes in their brains.\n\nField developed his views into fictionalism. Benacerraf also developed the philosophy of mathematical structuralism, according to which there are no mathematical objects. Nonetheless, some versions of structuralism are compatible with some versions of realism.\n\nThe argument hinges on the idea that a satisfactory naturalistic account of thought processes in terms of brain processes can be given for mathematical reasoning along with everything else. One line of defense is to maintain that this is false, so that mathematical reasoning uses some special intuition that involves contact with the Platonic realm. A modern form of this argument is given by Sir Roger Penrose.[68]\n\nAnother line of defense is to maintain that abstract objects are relevant to mathematical reasoning in a way that is non-causal, and not analogous to perception. This argument is developed by Jerrold Katz in his 2000 book Realistic Rationalism.\n\nA more radical defense is denial of physical reality, i.e. the mathematical universe hypothesis. In that case, a mathematician's knowledge of mathematics is one mathematical object making contact with another.\n\nMany practicing mathematicians have been drawn to their subject because of a sense of beauty they perceive in it. One sometimes hears the sentiment that mathematicians would like to leave philosophy to the philosophers and get back to mathematics—where, presumably, the beauty lies.\n\nIn his work on the divine proportion, H.E. Huntley relates the feeling of reading and understanding someone else's proof of a theorem of mathematics to that of a viewer of a masterpiece of art—the reader of a proof has a similar sense of exhilaration at understanding as the original author of the proof, much as, he argues, the viewer of a masterpiece has a sense of exhilaration similar to the original painter or sculptor. Indeed, one can study mathematical and scientific writings as literature.\n\nPhilip J. Davis and Reuben Hersh have commented that the sense of mathematical beauty is universal amongst practicing mathematicians. By way of example, they provide two proofs of the irrationality of √2. The first is the traditional proof by contradiction, ascribed to Euclid; the second is a more direct proof involving the fundamental theorem of arithmetic that, they argue, gets to the heart of the issue. Davis and Hersh argue that mathematicians find the second proof more aesthetically appealing because it gets closer to the nature of the problem.\n\nPaul Erdős was well known for his notion of a hypothetical \"Book\" containing the most elegant or beautiful mathematical proofs. There is not universal agreement that a result has one \"most elegant\" proof; Gregory Chaitin has argued against this idea.\n\nPhilosophers have sometimes criticized mathematicians' sense of beauty or elegance as being, at best, vaguely stated. By the same token, however, philosophers of mathematics have sought to characterize what makes one proof more desirable than another when both are logically sound.\n\nAnother aspect of aesthetics concerning mathematics is mathematicians' views towards the possible uses of mathematics for purposes deemed unethical or inappropriate. The best-known exposition of this view occurs in G. H. Hardy's book A Mathematician's Apology, in which Hardy argues that pure mathematics is superior in beauty to applied mathematics precisely because it cannot be used for war and similar ends.\n"
    },
    {
        "title": "Mathematics education",
        "content": "In contemporary education, mathematics education—known in Europe as the didactics or pedagogy of mathematics—is the practice of teaching, learning, and carrying out scholarly research into the transfer of mathematical knowledge.\n\nAlthough research into mathematics education is primarily concerned with the tools, methods, and approaches that facilitate practice or the study of practice, it also covers an extensive field of study encompassing a variety of different concepts, theories and methods. National and international organisations regularly hold conferences and publish literature in order to improve mathematics education.\n\nElementary mathematics were a core part of education in many ancient civilisations, including ancient Egypt, ancient Babylonia, ancient Greece, ancient Rome, and Vedic India.[citation needed] In most cases, formal education was only available to male children with sufficiently high status, wealth, or caste.[citation needed] The oldest known mathematics textbook is the Rhind papyrus, dated from circa 1650 BCE.[1]\n\nHistorians of Mesopotamia have confirmed that use of the Pythagorean rule dates back to the Old Babylonian Empire (20th–16th centuries BC) and that it was being taught in scribal schools over one thousand years before the birth of Pythagoras.[2][3][4][5][6]\n\nIn Plato's division of the liberal arts into the trivium and the quadrivium, the quadrivium included the mathematical fields of arithmetic and geometry. This structure was continued in the structure of classical education that was developed in medieval Europe. The teaching of geometry was almost universally based on Euclid's Elements. Apprentices to trades such as masons, merchants, and moneylenders could expect to learn such practical mathematics as was relevant to their profession.\n\nIn the Middle Ages, the academic status of mathematics declined, because it was strongly associated with trade and commerce, and considered somewhat un-Christian.[7] Although it continued to be taught in European universities, it was seen as subservient to the study of natural, metaphysical, and moral philosophy. The first modern arithmetic curriculum (starting with addition, then subtraction, multiplication, and division) arose at reckoning schools in Italy in the 1300s.[8] Spreading along trade routes, these methods were designed to be used in commerce. They contrasted with Platonic math taught at universities, which was more philosophical and concerned numbers as concepts rather than calculating methods.[8] They also contrasted with mathematical methods learned by artisan apprentices, which were specific to the tasks and tools at hand. For example, the division of a board into thirds can be accomplished with a piece of string, instead of measuring the length and using the arithmetic operation of division.[7]\n\nThe first mathematics textbooks to be written in English and French were published by Robert Recorde, beginning with The Grounde of Artes in 1543. However, there are many different writings on mathematics and mathematics methodology that date back to 1800 BCE. These were mostly located in Mesopotamia, where the Sumerians were practicing multiplication and division. There are also artifacts demonstrating their methodology for solving equations like the quadratic equation. After the Sumerians, some of the most famous ancient works on mathematics came from Egypt in the form of the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus. The more famous Rhind Papyrus has been dated back to approximately 1650 BCE, but it is thought to be a copy of an even older scroll. This papyrus was essentially an early textbook for Egyptian students.\n\nThe social status of mathematical study was improving by the seventeenth century, with the University of Aberdeen creating a Mathematics Chair in 1613, followed by the Chair in Geometry being set up in University of Oxford in 1619 and the Lucasian Chair of Mathematics being established by the University of Cambridge in 1662.\n\nIn the 18th and 19th centuries, the Industrial Revolution led to an enormous increase in urban populations. Basic numeracy skills, such as the ability to tell the time, count money, and carry out simple arithmetic, became essential in this new urban lifestyle. Within the new public education systems, mathematics became a central part of the curriculum from an early age.\n\nBy the twentieth century, mathematics was part of the core curriculum in all developed countries.\n\nDuring the twentieth century, mathematics education was established as an independent field of research. Main events in this development include the following:\n\nMidway through the twentieth century, the cultural impact of the \"electronic age\" (McLuhan) was also taken up by educational theory and the teaching of mathematics. While previous approach focused on \"working with specialized 'problems' in arithmetic\", the emerging structural approach to knowledge had \"small children meditating about number theory and 'sets'.\"[10] Since the 1980s, there have been a number of efforts to reform the traditional curriculum, which focuses on continuous mathematics and relegates even some basic discrete concepts to advanced study, to better balance coverage of the continuous and discrete sides of the subject:[11]\n\nSimilar efforts are also underway to shift more focus to mathematical modeling as well as its relationship to discrete math.[12]\n\nAt different times and in different cultures and countries, mathematics education has attempted to achieve a variety of different objectives. These objectives have included:\n\nThe method or methods used in any particular context are largely determined by the objectives that the relevant educational system is trying to achieve. Methods of teaching mathematics include the following:\n\nDifferent levels of mathematics are taught at different ages and in somewhat different sequences in different countries. Sometimes a class may be taught at an earlier age than typical as a special or honors class.\n\nElementary mathematics in most countries is taught similarly, though there are differences. Most countries tend to cover fewer topics in greater depth than in the United States.[26] During the primary school years, children learn about whole numbers and arithmetic, including addition, subtraction, multiplication, and division.[27] Comparisons and measurement are taught, in both numeric and pictorial form, as well as fractions and proportionality, patterns, and various topics related to geometry.[28]\n\nAt high school level in most of the US, algebra, geometry, and analysis (pre-calculus and calculus) are taught as separate courses in different years. \nOn the other hand, in most other countries (and in a few US states), mathematics is taught as an integrated subject, with topics from all branches of mathematics studied every year; \nstudents thus undertake a pre-defined course - entailing several topics - rather than choosing courses à la carte as in the United States. \nEven in these cases, however, several \"mathematics\" options may be offered, selected based on the student's intended studies post high school.\n(In South Africa, for example, the options are Mathematics, Mathematical Literacy and Technical Mathematics.)\nThus, a science-oriented curriculum typically overlaps the first year of university mathematics, and includes differential calculus and trigonometry at age 16–17 and integral calculus, complex numbers, analytic geometry, exponential and logarithmic functions, and infinite series in their final year of secondary school; Probability and statistics are similarly often taught.\n\nAt college and university level, science and engineering students will be required to take multivariable calculus, differential equations, and linear algebra; at several US colleges, the minor or AS in mathematics substantively comprises these courses. Mathematics majors study additional other areas within pure mathematics—and often in applied mathematics—with the requirement of specified advanced courses in analysis and modern algebra. Other topics in pure mathematics include differential geometry, set theory, and topology. Applied mathematics may be taken as a major subject in its own right, such as partial differential equations, optimization, and numerical analysis. Specific topics are taught within other courses: for example, civil engineers may be required to study fluid mechanics,[29] and \"math for computer science\" might include graph theory, permutation, probability, and formal mathematical proofs.[30] Pure and applied math degrees often include modules in probability theory or mathematical statistics, as well as stochastic processes. (Theoretical) physics is mathematics-intensive, often overlapping substantively with the pure or applied math degree. Business mathematics is usually limited to introductory calculus and (sometimes) matrix calculations; economics programs additionally cover optimization, often differential equations and linear algebra, and sometimes analysis. Business and social science students also typically take statistics and probability courses.\n\nThroughout most of history, standards for mathematics education were set locally, by individual schools or teachers, depending on the levels of achievement that were relevant to, realistic for, and considered socially appropriate for their pupils.\n\nIn modern times, there has been a move towards regional or national standards, usually under the umbrella of a wider standard school curriculum. In England, for example, standards for mathematics education are set as part of the National Curriculum for England,[31] while Scotland maintains its own educational system. Many other countries have centralized ministries which set national standards or curricula, and sometimes even textbooks.\n\nMa (2000) summarized the research of others who found, based on nationwide data, that students with higher scores on standardized mathematics tests had taken more mathematics courses in high school. This led some states to require three years of mathematics instead of two. But because this requirement was often met by taking another lower-level mathematics course, the additional courses had a “diluted” effect in raising achievement levels.[32]\n\nIn North America, the National Council of Teachers of Mathematics (NCTM) published the Principles and Standards for School Mathematics in 2000 for the United States and Canada, which boosted the trend towards reform mathematics. In 2006, the NCTM released Curriculum Focal Points, which recommend the most important mathematical topics for each grade level through grade 8. However, these standards were guidelines to implement as American states and Canadian provinces chose. In 2010, the National Governors Association Center for Best Practices and the Council of Chief State School Officers published the Common Core State Standards for US states, which were subsequently adopted by most states. Adoption of the Common Core State Standards in mathematics is at the discretion of each state, and is not mandated by the federal government.[33] \"States routinely review their academic standards and may choose to change or add onto the standards to best meet the needs of their students.\"[34] The NCTM has state affiliates that have different education standards at the state level. For example, Missouri has the Missouri Council of Teachers of Mathematics (MCTM) which has its pillars and standards of education listed on its website. The MCTM also offers membership opportunities to teachers and future teachers so that they can stay up to date on the changes in math educational standards.[35]\n\nThe Programme for International Student Assessment (PISA), created by the Organisation for the Economic Co-operation and Development (OECD), is a global program studying the reading, science, and mathematics abilities of 15-year-old students.[36] The first assessment was conducted in the year 2000 with 43 countries participating.[37] PISA has repeated this assessment every three years to provide comparable data, helping to guide global education to better prepare youth for future economies. There have been many ramifications following the results of triennial PISA assessments due to implicit and explicit responses of stakeholders, which have led to education reform and policy change.[37][38][23]\n\nAccording to Hiebert and Grouws, \"Robust, useful theories of classroom teaching do not yet exist.\"[39] However, there are useful theories on how children learn mathematics, and much research has been conducted in recent decades to explore how these theories can be applied to teaching. The following results are examples of some of the current findings in the field of mathematics education.\n\nAs with other educational research (and the social sciences in general), mathematics education research depends on both quantitative and qualitative studies. Quantitative research includes studies that use inferential statistics to answer specific questions, such as whether a certain teaching method gives significantly better results than the status quo. The best quantitative studies involve randomized trials where students or classes are randomly assigned different methods to test their effects. They depend on large samples to obtain statistically significant results.\n\nQualitative research, such as case studies, action research, discourse analysis, and clinical interviews, depend on small but focused samples in an attempt to understand student learning and to look at how and why a given method gives the results it does. Such studies cannot conclusively establish that one method is better than another, as randomized trials can, but unless it is understood why treatment X is better than treatment Y, application of results of quantitative studies will often lead to \"lethal mutations\"[39] of the finding in actual classrooms. Exploratory qualitative research is also useful for suggesting new hypotheses, which can eventually be tested by randomized experiments. Both qualitative and quantitative studies, therefore, are considered essential in education—just as in the other social sciences.[47] Many studies are “mixed”, simultaneously combining aspects of both quantitative and qualitative research, as appropriate.\n\nThere has been some controversy over the relative strengths of different types of research. Because of an opinion that randomized trials provide clear, objective evidence on “what works”, policymakers often consider only those studies. Some scholars have pushed for more random experiments in which teaching methods are randomly assigned to classes.[48][49] In other disciplines concerned with human subjects—like biomedicine, psychology, and policy evaluation—controlled, randomized experiments remain the preferred method of evaluating treatments.[50][51] Educational statisticians and some mathematics educators have been working to increase the use of randomized experiments to evaluate teaching methods.[49] On the other hand, many scholars in educational schools have argued against increasing the number of randomized experiments, often because of philosophical objections, such as the ethical difficulty of randomly assigning students to various treatments when the effects of such treatments are not yet known to be effective,[52] or the difficulty of assuring rigid control of the independent variable in fluid, real school settings.[53]\n\nIn the United States, the National Mathematics Advisory Panel (NMAP) published a report in 2008 based on studies, some of which used randomized assignment of treatments to experimental units, such as classrooms or students. The NMAP report's preference for randomized experiments received criticism from some scholars.[54] In 2010, the What Works Clearinghouse (essentially the research arm for the Department of Education) responded to ongoing controversy by extending its research base to include non-experimental studies, including regression discontinuity designs and single-case studies.[55]\n"
    },
    {
        "title": "Theory",
        "content": "\nA theory is a rational type of abstract thinking about a phenomenon, or the results of such thinking. The process of contemplative and rational thinking is often associated with such processes as observational study or research. Theories may be scientific, belong to a non-scientific discipline, or no discipline at all. Depending on the context, a theory's assertions might, for example, include generalized explanations of how nature works. The word has its roots in ancient Greek, but in modern use it has taken on several related meanings.\n\nIn modern science, the term \"theory\" refers to scientific theories, a well-confirmed type of explanation of nature, made in a way consistent with the scientific method, and fulfilling the criteria required by modern science. Such theories are described in such a way that scientific tests should be able to provide empirical support for it, or empirical contradiction (\"falsify\") of it. Scientific theories are the most reliable, rigorous, and comprehensive form of scientific knowledge,[1] in contrast to more common uses of the word \"theory\" that imply that something is unproven or speculative (which in formal terms is better characterized by the word hypothesis).[2] Scientific theories are distinguished from hypotheses, which are individual empirically testable conjectures, and from scientific laws, which are descriptive accounts of the way nature behaves under certain conditions.\n\nTheories guide the enterprise of finding facts rather than of reaching goals, and are neutral concerning alternatives among values.[3]: 131  A theory can be a body of knowledge, which may or may not be associated with particular explanatory models. To theorize is to develop this body of knowledge.[4]: 46 \n\nThe word theory or \"in theory\" is sometimes used outside of science to refer to something which the speaker did not experience or test before.[5] In science, this same concept is referred to as a hypothesis, and the word \"hypothetically\" is used both inside and outside of science. In its usage outside of science, the word \"theory\" is very often contrasted to \"practice\" (from Greek praxis, πρᾶξις) a Greek term for doing, which is opposed to theory.[6] A \"classical example\" of the distinction between \"theoretical\" and \"practical\" uses the discipline of medicine: medical theory involves trying to understand the causes and nature of health and sickness, while the practical side of medicine is trying to make people healthy. These two things are related but can be independent, because it is possible to research health and sickness without curing specific patients, and it is possible to cure a patient without knowing how the cure worked.[a]\n\nThe English word theory derives from a technical term in philosophy in Ancient Greek. As an everyday word, theoria, θεωρία, meant \"looking at, viewing, beholding\", but in more technical contexts it came to refer to contemplative or speculative understandings of natural things, such as those of natural philosophers, as opposed to more practical ways of knowing things, like that of skilled orators or artisans.[b] English-speakers have used the word theory since at least the late 16th century.[7] Modern uses of the word theory derive from the original definition, but have taken on new shades of meaning, still based on the idea of a theory as a thoughtful and rational explanation of the general nature of things.\n\nAlthough it has more mundane meanings in Greek, the word θεωρία apparently developed special uses early in the recorded history of the Greek language. In the book From Religion to Philosophy, Francis Cornford suggests that the Orphics used the word theoria to mean \"passionate sympathetic contemplation\".[8] Pythagoras changed the word to mean \"the passionless contemplation of rational, unchanging truth\" of mathematical knowledge, because he considered this intellectual pursuit the way to reach the highest plane of existence.[9] Pythagoras emphasized subduing emotions and bodily desires to help the intellect function at the higher plane of theory. Thus, it was Pythagoras who gave the word theory the specific meaning that led to the classical and modern concept of a distinction between theory (as uninvolved, neutral thinking) and practice.[10]\n\nAristotle's terminology, as already mentioned, contrasts theory with praxis or practice, and this contrast exists till today. For Aristotle, both practice and theory involve thinking, but the aims are different. Theoretical contemplation considers things humans do not move or change, such as nature, so it has no human aim apart from itself and the knowledge it helps create. On the other hand, praxis involves thinking, but always with an aim to desired actions, whereby humans cause change or movement themselves for their own ends. Any human movement that involves no conscious choice and thinking could not be an example of praxis or doing.[c]\n\nTheories are analytical tools for understanding, explaining, and making predictions about a given subject matter. There are theories in many and varied fields of study, including the arts and sciences. A formal theory is syntactic in nature and is only meaningful when given a semantic component by applying it to some content (e.g., facts and relationships of the actual historical world as it is unfolding). Theories in various fields of study are often expressed in natural language, but can be constructed in such a way that their general form is identical to a theory as it is expressed in the formal language of mathematical logic. Theories may be expressed mathematically, symbolically, or in common language, but are generally expected to follow principles of rational thought or logic.\n\nTheory is constructed of a set of sentences that are thought to be true statements about the subject under consideration. However, the truth of any one of these statements is always relative to the whole theory. Therefore, the same statement may be true with respect to one theory, and not true with respect to another. This is, in ordinary language, where statements such as \"He is a terrible person\" cannot be judged as true or false without reference to some interpretation of who \"He\" is and for that matter what a \"terrible person\" is under the theory.[11]\n\nSometimes two theories have exactly the same explanatory power because they make the same predictions. A pair of such theories is called indistinguishable or observationally equivalent, and the choice between them reduces to convenience or philosophical preference.[citation needed]\n\nThe form of theories is studied formally in mathematical logic, especially in model theory. When theories are studied in mathematics, they are usually expressed in some formal language and their statements are closed under application of certain procedures called rules of inference. A special case of this, an axiomatic theory, consists of axioms (or axiom schemata) and rules of inference. A theorem is a statement that can be derived from those axioms by application of these rules of inference. Theories used in applications are abstractions of observed phenomena and the resulting theorems provide solutions to real-world problems. Obvious examples include arithmetic (abstracting concepts of number), geometry (concepts of space), and probability (concepts of randomness and likelihood).\n\nGödel's incompleteness theorem shows that no consistent, recursively enumerable theory (that is, one whose theorems form a recursively enumerable set) in which the concept of natural numbers can be expressed, can include all true statements about them. As a result, some domains of knowledge cannot be formalized, accurately and completely, as mathematical theories. (Here, formalizing accurately and completely means that all true propositions—and only true propositions—are derivable within the mathematical system.) This limitation, however, in no way precludes the construction of mathematical theories that formalize large bodies of scientific knowledge.\n\nA theory is underdetermined (also called indeterminacy of data to theory) if a rival, inconsistent theory is at least as consistent with the evidence. Underdetermination is an epistemological issue about the relation of evidence to conclusions.[citation needed]\n\nA theory that lacks supporting evidence is generally, more properly, referred to as a hypothesis.[12]\n\nIf a new theory better explains and predicts a phenomenon than an old theory (i.e., it has more explanatory power), we are justified in believing that the newer theory describes reality more correctly. This is called an intertheoretic reduction because the terms of the old theory can be reduced to the terms of the new one. For instance, our historical understanding about sound, light and heat have been reduced to wave compressions and rarefactions, electromagnetic waves, and molecular kinetic energy, respectively. These terms, which are identified with each other, are called intertheoretic identities. When an old and new theory are parallel in this way, we can conclude that the new one describes the same reality, only more completely.\n\nWhen a new theory uses new terms that do not reduce to terms of an older theory, but rather replace them because they misrepresent reality, it is called an intertheoretic elimination. For instance, the obsolete scientific theory that put forward an understanding of heat transfer in terms of the movement of caloric fluid was eliminated when a theory of heat as energy replaced it. Also, the theory that phlogiston is a substance released from burning and rusting material was eliminated with the new understanding of the reactivity of oxygen.\n\nTheories are distinct from theorems. A theorem is derived deductively from axioms (basic assumptions) according to a formal system of rules, sometimes as an end in itself and sometimes as a first step toward being tested or applied in a concrete situation; theorems are said to be true in the sense that the conclusions of a theorem are logical consequences of the axioms. Theories are abstract and conceptual, and are supported or challenged by observations in the world. They are 'rigorously tentative', meaning that they are proposed as true and expected to satisfy careful examination to account for the possibility of faulty inference or incorrect observation. Sometimes theories are incorrect, meaning that an explicit set of observations contradicts some fundamental objection or application of the theory, but more often theories are corrected to conform to new observations, by restricting the class of phenomena the theory applies to or changing the assertions made. An example of the former is the restriction of classical mechanics to phenomena involving macroscopic length scales and particle speeds much lower than the speed of light.\n\nTheory is often distinguished from practice or praxis. The question of whether theoretical models of work are relevant to work itself is of interest to scholars of professions such as medicine, engineering, law, and management.[13]: 802 \n\nThe gap between theory and practice has been framed as a knowledge transfer where there is a task of translating research knowledge to be application in practice, and ensuring that practitioners are made aware of it. Academics have been criticized for not attempting to transfer the knowledge they produce to practitioners.[13]: 804 [14] Another framing supposes that theory and knowledge seek to understand different problems and model the world in different words (using different ontologies and epistemologies). Another framing says that research does not produce theory that is relevant to practice.[13]: 803 \n\nIn the context of management, Van de Van and Johnson propose a form of engaged scholarship where scholars examine problems that occur in practice, in an interdisciplinary fashion, producing results that create both new practical results as well as new theoretical models, but targeting theoretical results shared in an academic fashion.[13]: 815  They use a metaphor of \"arbitrage\" of ideas between disciplines, distinguishing it from collaboration.[13]: 803 \n\nIn science, the term \"theory\" refers to \"a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment.\"[15][16] Theories must also meet further requirements, such as the ability to make falsifiable predictions with consistent accuracy across a broad area of scientific inquiry, and production of strong evidence in favor of the theory from multiple independent sources (consilience).\n\nThe strength of a scientific theory is related to the diversity of phenomena it can explain, which is measured by its ability to make falsifiable predictions with respect to those phenomena. Theories are improved (or replaced by better theories) as more evidence is gathered, so that accuracy in prediction improves over time; this increased accuracy corresponds to an increase in scientific knowledge. Scientists use theories as a foundation to gain further scientific knowledge, as well as to accomplish goals such as inventing technology or curing diseases.\n\n\nThe United States National Academy of Sciences defines scientific theories as follows:\nThe formal scientific definition of \"theory\" is quite different from the everyday meaning of the word. It refers to a comprehensive explanation of some aspect of nature that is supported by a vast body of evidence. Many scientific theories are so well established that no new evidence is likely to alter them substantially. For example, no new evidence will demonstrate that the Earth does not orbit around the sun (heliocentric theory), or that living things are not made of cells (cell theory), that matter is not composed of atoms, or that the surface of the Earth is not divided into solid plates that have moved over geological timescales (the theory of plate tectonics) ... One of the most useful properties of scientific theories is that they can be used to make predictions about natural events or phenomena that have not yet been observed.[17]\nFrom the American Association for the Advancement of Science:\n\nA scientific theory is a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment. Such fact-supported theories are not \"guesses\" but reliable accounts of the real world. The theory of biological evolution is more than \"just a theory.\" It is as factual an explanation of the universe as the atomic theory of matter or the germ theory of disease. Our understanding of gravity is still a work in progress. But the phenomenon of gravity, like evolution, is an accepted fact.[16]\nThe term theory is not appropriate for describing scientific models or untested, but intricate hypotheses.\n\nThe logical positivists thought of scientific theories as deductive theories—that a theory's content is based on some formal system of logic and on basic axioms. In a deductive theory, any sentence which is a logical consequence of one or more of the axioms is also a sentence of that theory.[11] This is called the received view of theories.\n\nIn the semantic view of theories, which has largely replaced the received view,[18][19] theories are viewed as scientific models. A model is an abstract and informative representation of reality (a \"model of reality\"), similar to the way that a map is a graphical model that represents the territory of a city or country. In this approach, theories are a specific category of models that fulfill the necessary criteria. (See Theories as models for further discussion.)\n\nIn physics the term theory is generally used for a mathematical framework—derived from a small set of basic postulates (usually symmetries, like equality of locations in space or in time, or identity of electrons, etc.)—which is capable of producing experimental predictions for a given category of physical systems. One good example is classical electromagnetism, which encompasses results derived from gauge symmetry (sometimes called gauge invariance) in a form of a few equations called Maxwell's equations. The specific mathematical aspects of classical electromagnetic theory are termed \"laws of electromagnetism\", reflecting the level of consistent and reproducible evidence that supports them. Within electromagnetic theory generally, there are numerous hypotheses about how electromagnetism applies to specific situations. Many of these hypotheses are already considered adequately tested, with new ones always in the making and perhaps untested.\n\nCertain tests may be infeasible or technically difficult. As a result, theories may make predictions that have not been confirmed or proven incorrect. These predictions may be described informally as \"theoretical\". They can be tested later, and if they are incorrect, this may lead to revision, invalidation, or rejection of the theory.\n[20]\n\nIn mathematics, the term theory is used differently than its use in science ─ necessarily so, since mathematics contains no explanations of natural phenomena per se, even though it may help provide insight into natural systems or be inspired by them. In the general sense, a mathematical theory is a branch of mathematics devoted to some specific topics or methods, such as set theory, number theory, group theory, probability theory, game theory, control theory, perturbation theory, etc., such as might be appropriate for a single textbook.\n\nIn mathematical logic, a theory has a related but different sense: it is the collection of the theorems that can be deduced from a given set of axioms, given a given set of inference rules.\n\nA theory can be either descriptive as in science, or prescriptive (normative) as in philosophy.[21] The latter are those whose subject matter consists not of empirical data, but rather of ideas. At least some of the elementary theorems of a philosophical theory are statements whose truth cannot necessarily be scientifically tested through empirical observation.\n\nA field of study is sometimes named a \"theory\" because its basis is some initial set of assumptions describing the field's approach to the subject. These assumptions are the elementary theorems of the particular theory, and can be thought of as the axioms of that field. Some commonly known examples include set theory and number theory; however literary theory, critical theory, and music theory are also of the same form.\n\nOne form of philosophical theory is a metatheory or meta-theory. A metatheory is a theory whose subject matter is some other theory or set of theories. In other words, it is a theory about theories. Statements made in the metatheory about the theory are called metatheorems.\n\nA political theory is an ethical theory about the law and government. Often the term \"political theory\" refers to a general view, or specific ethic, political belief or attitude, thought about politics.\n\nIn social science, jurisprudence is the philosophical theory of law. Contemporary philosophy of law addresses problems internal to law and legal systems, and problems of law as a particular social institution.\n\nMost of the following are scientific theories. Some are not, but rather encompass a body of knowledge or art, such as Music theory and Visual Arts Theories.\n"
    },
    {
        "title": "Theorem",
        "content": "In mathematics and formal logic, a theorem is a statement that has been proven, or can be proven.[a][2][3] The proof of a theorem is a logical argument that uses the inference rules of a deductive system to establish that the theorem is a logical consequence of the axioms and previously proved theorems.\n\nIn mainstream mathematics, the axioms and the inference rules are commonly left implicit, and, in this case, they are almost always those of Zermelo–Fraenkel set theory with the axiom of choice (ZFC), or of a less powerful theory, such as Peano arithmetic.[b] Generally, an assertion that is explicitly called a theorem is a proved result that is not an immediate consequence of other known theorems. Moreover, many authors qualify as theorems only the most important results, and use the terms lemma, proposition and corollary for less important theorems.\n\nIn mathematical logic, the concepts of theorems and proofs have been formalized in order to allow mathematical reasoning about them. In this context, statements become well-formed formulas of some formal language. A theory consists of some basis statements called axioms, and some deducing rules (sometimes included in the axioms). The theorems of the theory are the statements that can be derived from the axioms by using the deducing rules.[c] This formalization led to proof theory, which allows proving general theorems about theorems and proofs. In particular, Gödel's incompleteness theorems show that every consistent theory containing the natural numbers has true statements on natural numbers that are not theorems of the theory (that is they cannot be proved inside the theory).\n\nAs the axioms are often abstractions of properties of the physical world, theorems may be considered as expressing some truth, but in contrast to the notion of a scientific law, which is experimental, the justification of the truth of a theorem is purely deductive.[6][d]\nA conjecture is a tentative proposition that may evolve to become a theorem if proven true.\n\nUntil the end of the 19th century and the foundational crisis of mathematics, all mathematical theories were built from a few basic properties that were considered as self-evident; for example, the facts that every natural number has a successor, and that there is exactly one  line that passes through two given distinct points. These basic properties that were considered as absolutely evident were called postulates or axioms; for example Euclid's postulates. All theorems were proved by using implicitly or explicitly these basic properties, and, because of the evidence of these basic properties, a proved theorem was considered as a definitive truth, unless there was an error in the proof. For example, the sum of the interior angles of a triangle equals 180°, and this was considered as an undoubtable fact.\n\nOne aspect of the foundational crisis of mathematics was the discovery of non-Euclidean geometries that do not lead to any contradiction, although, in such geometries, the sum of the angles of a triangle is different from 180°. So, the property \"the sum of the angles of a triangle equals 180°\" is either true or false, depending whether Euclid's fifth postulate is assumed or denied. Similarly, the use of \"evident\" basic properties of sets leads to the contradiction of Russell's paradox. This has been resolved by elaborating the rules that are allowed for manipulating sets.\n\nThis crisis has been resolved by revisiting the foundations of mathematics to make them more rigorous. In these new foundations, a theorem is a well-formed formula of a mathematical theory that can be proved from the axioms and inference rules of the theory. So, the above theorem on the sum of the angles of a triangle becomes: Under the axioms and inference rules of Euclidean geometry, the sum of the interior angles of a triangle equals 180°. Similarly, Russell's paradox disappears because, in an axiomatized set theory, the set of all sets cannot be expressed with a well-formed formula. More precisely, if the set of all sets can be expressed with a well-formed formula, this implies that the theory is inconsistent, and every well-formed assertion, as well as its negation, is a theorem.\n\nIn this context, the validity of a theorem depends only on the correctness of its proof. It is independent from the truth, or even the significance of the axioms. This does not mean that the significance of the axioms is uninteresting, but only that the validity of a theorem is independent from the significance of the axioms. This independence may be useful by allowing the use of results of some area of mathematics in apparently unrelated areas.\n\nAn important consequence of this way of thinking about mathematics is that it allows defining mathematical theories and theorems as mathematical objects, and to prove theorems about them. Examples are Gödel's incompleteness theorems. In particular, there are well-formed assertions than can be proved to not be a theorem of the ambient theory, although they can be proved in a wider theory. An example is Goodstein's theorem, which can be stated in Peano arithmetic, but is proved to be not provable in Peano arithmetic. However, it is provable in some more general theories, such as Zermelo–Fraenkel set theory.\n\nMany mathematical theorems are conditional statements, whose proofs deduce conclusions from conditions known as hypotheses or premises. In light of the interpretation of proof as justification of truth, the conclusion is often viewed as a necessary consequence of the hypotheses. Namely, that the conclusion is true in case the hypotheses are true—without any further assumptions. However, the conditional could also be interpreted differently in certain deductive systems, depending on the meanings assigned to the derivation rules and the conditional symbol (e.g., non-classical logic).\n\nAlthough theorems can be written in a completely symbolic form (e.g., as propositions in propositional calculus), they are often expressed informally in a natural language such as English for better readability. The same is true of proofs, which are often expressed as logically organized and clearly worded informal arguments, intended to convince readers of the truth of the statement of the theorem beyond any doubt, and from which a formal symbolic proof can in principle be constructed.\n\nIn addition to the better readability, informal arguments are typically easier to check than purely symbolic ones—indeed, many mathematicians would express a preference for a proof that not only demonstrates the validity of a theorem, but also explains in some way why it is obviously true. In some cases, one might even be able to substantiate a theorem by using a picture as its proof.\n\nBecause theorems lie at the core of mathematics, they are also central to its aesthetics. Theorems are often described as being \"trivial\", or \"difficult\", or \"deep\", or even \"beautiful\". These subjective judgments vary not only from person to person, but also with time and culture: for example, as a proof is obtained, simplified or better understood, a theorem that was once difficult may become trivial.[7] On the other hand, a deep theorem may be stated simply, but its proof may involve surprising and subtle connections between disparate areas of mathematics. Fermat's Last Theorem is a particularly well-known example of such a theorem.[8]\n\nLogically, many theorems are of the form of an indicative conditional: If A, then B. Such a theorem does not assert B — only that B is a necessary consequence of A. In this case, A is called the hypothesis of the theorem (\"hypothesis\" here means something very different from a conjecture), and B the conclusion of the theorem. The two together (without the proof) are called the proposition or statement of the theorem (e.g. \"If A, then B\" is the proposition). Alternatively, A and B can be also termed the antecedent and the consequent, respectively.[9] The theorem \"If n is an even natural number, then n/2 is a natural number\" is a typical example in which the hypothesis is \"n is an even natural number\", and the conclusion is \"n/2 is also a natural number\".\n\nIn order for a theorem to be proved, it must be in principle expressible as a precise, formal statement. However, theorems are usually expressed in natural language rather than in a completely symbolic form—with the presumption that a formal statement can be derived from the informal one.\n\nIt is common in mathematics to choose a number of hypotheses within a given language and declare that the theory consists of all statements provable from these hypotheses. These hypotheses form the foundational basis of the theory and are called axioms or postulates. The field of mathematics known as proof theory studies formal languages, axioms and the structure of proofs.\n\nSome theorems are \"trivial\", in the sense that they follow from definitions, axioms, and other theorems in obvious ways and do not contain any surprising insights. Some, on the other hand, may be called \"deep\", because their proofs may be long and difficult, involve areas of mathematics superficially distinct from the statement of the theorem itself, or show surprising connections between disparate areas of mathematics.[10] A theorem might be simple to state and yet be deep. An excellent example is Fermat's Last Theorem,[8] and there are many other examples of simple yet deep theorems in number theory and combinatorics, among other areas.\n\nOther theorems have a known proof that cannot easily be written down. The most prominent examples are the four color theorem and the Kepler conjecture. Both of these theorems are only known to be true by reducing them to a computational search that is then verified by a computer program. Initially, many mathematicians did not accept this form of proof, but it has become more widely accepted. The mathematician Doron Zeilberger has even gone so far as to claim that these are possibly the only nontrivial results that mathematicians have ever proved.[11] Many mathematical theorems can be reduced to more straightforward computation, including polynomial identities, trigonometric identities[e] and hypergeometric identities.[12][page needed]\n\nTheorems in mathematics and theories in science are fundamentally different in their epistemology. A scientific theory cannot be proved; its key attribute is that it is falsifiable, that is, it makes predictions about the natural world that are testable by experiments. Any disagreement between prediction and experiment demonstrates the incorrectness of the scientific theory, or at least limits its accuracy or domain of validity. Mathematical theorems, on the other hand, are purely abstract formal statements: the proof of a theorem cannot involve experiments or other empirical evidence in the same way such evidence is used to support scientific theories.[6]\n\nNonetheless, there is some degree of empiricism and data collection involved in the discovery of mathematical theorems. By establishing a pattern, sometimes with the use of a powerful computer, mathematicians may have an idea of what to prove, and in some cases even a plan for how to set about doing the proof. It is also possible to find a single counter-example and so establish the impossibility of a proof for the proposition as-stated, and possibly suggest restricted forms of the original proposition that might have feasible proofs.\n\nFor example, both the Collatz conjecture and the Riemann hypothesis are well-known unsolved problems; they have been extensively studied through empirical checks, but remain unproven. The Collatz conjecture has been verified for start values up to about 2.88 × 1018. The Riemann hypothesis has been verified to hold for the first 10 trillion non-trivial zeroes of the zeta function. Although most mathematicians can tolerate supposing that the conjecture and the hypothesis are true, neither of these propositions is considered proved.\n\nSuch evidence does not constitute proof. For example, the Mertens conjecture is a statement about natural numbers that is now known to be false, but no explicit counterexample (i.e., a natural number n for which the Mertens function M(n) equals or exceeds the square root of n) is known: all numbers less than 1014 have the Mertens property, and the smallest number that does not have this property is only known to be less than the exponential of 1.59 × 1040, which is approximately 10 to the power 4.3 × 1039. Since the number of particles in the universe is generally considered less than 10 to the power 100 (a googol), there is no hope to find an explicit counterexample by exhaustive search.\n\nThe word \"theory\" also exists in mathematics, to denote a body of mathematical axioms, definitions and theorems, as in, for example, group theory (see mathematical theory). There are also \"theorems\" in science, particularly physics, and in engineering, but they often have statements and proofs in which physical assumptions and intuition play an important role; the physical axioms on which such \"theorems\" are based are themselves falsifiable.\n\nA number of different terms for mathematical statements exist; these terms indicate the role statements play in a particular subject. The distinction between different terms is sometimes rather arbitrary, and the usage of some terms has evolved over time.\n\nOther terms may also be used for historical or customary reasons, for example:\n\nA few well-known theorems have even more idiosyncratic names, for example, the division algorithm, Euler's formula, and the Banach–Tarski paradox.\n\nA theorem and its proof are typically laid out as follows:\n\nThe end of the proof may be signaled by the letters Q.E.D. (quod erat demonstrandum) or by one of the tombstone marks, such as \"□\" or \"∎\", meaning \"end of proof\", introduced by Paul Halmos following their use in magazines to mark the end of an article.[15]\n\nThe exact style depends on the author or publication. Many publications provide instructions or macros for typesetting in the house style.\n\nIt is common for a theorem to be preceded by definitions describing the exact meaning of the terms used in the theorem. It is also common for a theorem to be preceded by a number of propositions or lemmas which are then used in the proof. However, lemmas are sometimes embedded in the proof of a theorem, either with nested proofs, or with their proofs presented after the proof of the theorem.\n\nCorollaries to a theorem are either presented between the theorem and the proof, or directly after the proof. Sometimes, corollaries have proofs of their own that explain why they follow from the theorem.\n\nIt has been estimated that over a quarter of a million theorems are proved every year.[16]\n\nThe well-known aphorism, \"A mathematician is a device for turning coffee into theorems\", is probably due to Alfréd Rényi, although it is often attributed to Rényi's colleague Paul Erdős (and Rényi may have been thinking of Erdős), who was famous for the many theorems he produced, the number of his collaborations, and his coffee drinking.[17]\n\nThe classification of finite simple groups is regarded by some to be the longest proof of a theorem. It comprises tens of thousands of pages in 500 journal articles by some 100 authors. These papers are together believed to give a complete proof, and several ongoing projects hope to shorten and simplify this proof.[18] Another theorem of this type is the four color theorem whose computer generated proof is too long for a human to read. It is among the longest known proofs of a theorem whose statement can be easily understood by a layman.[citation needed]\n\nIn mathematical logic, a formal theory is a set of sentences within a formal language. A sentence is a well-formed formula with no free variables. A sentence that is a member of a theory is one of its theorems, and the theory is the set of its theorems. Usually a theory is understood to be closed under the relation of logical consequence. Some accounts define a theory to be closed under the semantic consequence relation (\n\n\n\n⊨\n\n\n{\\displaystyle \\models }\n\n), while others define it to be closed under the syntactic consequence, or derivability relation (\n\n\n\n⊢\n\n\n{\\displaystyle \\vdash }\n\n).[19][20][21][22][23][24][25][26][27][28]\n\nFor a theory to be closed under a derivability relation, it must be associated with a deductive system that specifies how the theorems are derived. The deductive system may be stated explicitly, or it may be clear from the context. The closure of the empty set under the relation of logical consequence yields the set that contains just those sentences that are the theorems of the deductive system.\n\nIn the broad sense in which the term is used within logic, a theorem does not have to be true, since the theory that contains it may be unsound relative to a given semantics, or relative to the standard interpretation of the underlying language. A theory that is inconsistent has all sentences as theorems.\n\nThe definition of theorems as sentences of a formal language is useful within proof theory, which is a branch of mathematics that studies the structure of formal proofs and the structure of provable formulas. It is also important in model theory, which is concerned with the relationship between formal theories and structures that are able to provide a semantics for them through interpretation.\n\nAlthough theorems may be uninterpreted sentences, in practice mathematicians are more interested in the meanings of the sentences, i.e. in the propositions they express. What makes formal theorems useful and interesting is that they may be interpreted as true propositions and their derivations may be interpreted as a proof of their truth. A theorem whose interpretation is a true statement about a formal system (as opposed to within a formal system) is called a metatheorem.\n\nSome important theorems in mathematical logic are:\n\nThe concept of a formal theorem is fundamentally syntactic, in contrast to the notion of a true proposition, which introduces semantics. Different deductive systems can yield other interpretations, depending on the presumptions of the derivation rules (i.e. belief, justification or other modalities). The soundness of a formal system depends on whether or not all of its theorems are also validities. A validity is a formula that is true under any possible interpretation (for example, in classical propositional logic, validities are tautologies). A formal system is considered semantically complete when all of its theorems are also tautologies.\n\n\n"
    },
    {
        "title": "Mathematical proof",
        "content": "\n\nA mathematical proof is a deductive argument for a mathematical statement, showing that the stated assumptions logically guarantee the conclusion. The argument may use other previously established statements, such as theorems; but every proof can, in principle, be constructed using only certain basic or original assumptions known as axioms,[2][3][4] along with the accepted rules of inference. Proofs are examples of exhaustive deductive reasoning which establish logical certainty, to be distinguished from empirical arguments or non-exhaustive inductive reasoning which establish \"reasonable expectation\". Presenting many cases in which the statement holds is not enough for a proof, which must demonstrate that the statement is true in all possible cases. A proposition that has not been proved but is believed to be true is known as a conjecture, or a hypothesis if frequently used as an assumption for further mathematical work.\n\nProofs employ logic expressed in mathematical symbols, along with natural language which usually admits some ambiguity. In most mathematical literature, proofs are written in terms of rigorous informal logic. Purely formal proofs, written fully in symbolic language without the involvement of natural language, are considered in proof theory. The distinction between formal and informal proofs has led to much examination of current and historical mathematical practice, quasi-empiricism in mathematics, and so-called folk mathematics, oral traditions in the mainstream mathematical community or in other cultures. The philosophy of mathematics is concerned with the role of language and logic in proofs, and mathematics as a language.\n\nThe word proof derives from the Latin probare 'to test'; related words include English probe, probation, and probability, as well as Spanish probar 'to taste' (sometimes 'to touch' or 'to test'),[5] Italian provare 'to try', and German probieren 'to try'. The legal term probity means authority or credibility, the power of testimony to prove facts when given by persons of reputation or status.[6]\n\nPlausibility arguments using heuristic devices such as pictures and analogies preceded strict mathematical proof.[7] It is likely that the idea of demonstrating a conclusion first arose in connection with geometry, which originated in practical problems of land measurement.[8] The development of mathematical proof is primarily the product of ancient Greek mathematics, and one of its greatest achievements.[9] Thales (624–546 BCE) and Hippocrates of Chios (c. 470–410 BCE) gave some of the first known proofs of theorems in geometry. Eudoxus (408–355 BCE) and Theaetetus (417–369 BCE) formulated theorems but did not prove them. Aristotle (384–322 BCE) said definitions should describe the concept being defined in terms of other concepts already known.\n\nMathematical proof was revolutionized by Euclid (300 BCE), who introduced the axiomatic method still in use today. It starts with undefined terms and axioms, propositions concerning the undefined terms which are assumed to be self-evidently true (from Greek axios 'something worthy'). From this basis, the method proves theorems using deductive logic. Euclid's Elements was read by anyone who was considered educated in the West until the middle of the 20th century.[10] In addition to theorems of geometry, such as the Pythagorean theorem, the Elements also covers number theory, including a proof that the square root of two is irrational and a proof that there are infinitely many prime numbers.\n\nFurther advances also took place in medieval Islamic mathematics. In the 10th century, the Iraqi mathematician Al-Hashimi worked with numbers as such, called \"lines\" but not necessarily considered as measurements of geometric objects, to prove algebraic propositions concerning multiplication, division, etc., including the existence of irrational numbers.[11] An inductive proof for arithmetic progressions was introduced in the Al-Fakhri (1000) by Al-Karaji, who used it to prove the binomial theorem and properties of Pascal's triangle.\n\nModern proof theory treats proofs as inductively defined data structures, not requiring an assumption that axioms are \"true\" in any sense. This allows parallel mathematical theories as formal models of a given intuitive concept, based on alternate sets of axioms, for example axiomatic set theory and non-Euclidean geometry.\n\nAs practiced, a proof is expressed in natural language and is a rigorous argument intended to convince the audience of the truth of a statement. The standard of rigor is not absolute and has varied throughout history. A proof can be presented differently depending on the intended audience. To gain acceptance, a proof has to meet communal standards of rigor; an argument considered vague or incomplete may be rejected.\n\nThe concept of proof is formalized in the field of mathematical logic.[12] A formal proof is written in a formal language instead of natural language. A formal proof is a sequence of formulas in a formal language, starting with an assumption, and with each subsequent formula a logical consequence of the preceding ones. This definition makes the concept of proof amenable to study. Indeed, the field of proof theory studies formal proofs and their properties, the most famous and surprising being that almost all axiomatic systems can generate certain undecidable statements not provable within the system.\n\nThe definition of a formal proof is intended to capture the concept of proofs as written in the practice of mathematics. The soundness of this definition amounts to the belief that a published proof can, in principle, be converted into a formal proof. However, outside the field of automated proof assistants, this is rarely done in practice. A classic question in philosophy asks whether mathematical proofs are analytic or synthetic. Kant, who introduced the analytic–synthetic distinction, believed mathematical proofs are synthetic, whereas Quine argued in his 1951 \"Two Dogmas of Empiricism\" that such a distinction is untenable.[13]\n\nProofs may be admired for their mathematical beauty. The mathematician Paul Erdős was known for describing proofs which he found to be particularly elegant as coming from \"The Book\", a hypothetical tome containing the most beautiful method(s) of proving each theorem. The book Proofs from THE BOOK, published in 2003, is devoted to presenting 32 proofs its editors find particularly pleasing.\n\nIn direct proof, the conclusion is established by logically combining the axioms, definitions, and earlier theorems.[14] For example, direct proof can be used to prove that the sum of two even integers is always even:\n\nThis proof uses the definition of even integers, the integer properties of closure under addition and multiplication, and the distributive property.\n\nDespite its name, mathematical induction is a method of deduction, not a form of inductive reasoning. In proof by mathematical induction, a single \"base case\" is proved, and an \"induction rule\" is proved that establishes that any arbitrary case implies the next case. Since in principle the induction rule can be applied repeatedly (starting from the proved base case), it follows that all (usually infinitely many) cases are provable.[15] This avoids having to prove each case individually. A variant of mathematical induction is proof by infinite descent, which can be used, for example, to prove the irrationality of the square root of two.\n\nA common application of proof by mathematical induction is to prove that a property known to hold for one number holds for all natural numbers:[16]\nLet N = {1, 2, 3, 4, ...} be the set of natural numbers, and let P(n) be a mathematical statement involving the natural number n belonging to N such that\n\nFor example, we can prove by induction that all positive integers of the form 2n − 1 are odd.  Let P(n) represent \"2n − 1 is odd\":\n\nThe shorter phrase \"proof by induction\" is often used instead of \"proof by mathematical induction\".[17]\n\nProof by contraposition infers the statement \"if p then q\" by establishing the logically equivalent contrapositive statement: \"if not q then not p\".\n\nFor example, contraposition can be used to establish that, given an integer \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, if \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x^{2}}\n\n is even, then \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is even:\n\nIn proof by contradiction, also known by the Latin phrase reductio ad absurdum (by reduction to the absurd), it is shown that if some statement is assumed true, a logical contradiction occurs, hence the statement must be false.  A famous example involves the proof that \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n is an irrational number:\n\nTo paraphrase: if one could write \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n as a fraction, this fraction could never be written in lowest terms, since 2 could always be factored from numerator and denominator.\n\nProof by construction, or proof by example, is the construction of a concrete example with a property to show that something having that property exists. Joseph Liouville, for instance, proved the existence of transcendental numbers by constructing an explicit example.  It can also be used to construct a counterexample to disprove a proposition that all elements have a certain property.\n\nIn proof by exhaustion, the conclusion is established by dividing it into a finite number of cases and proving each one separately. The number of cases sometimes can become very large. For example, the first proof of the four color theorem was a proof by exhaustion with 1,936 cases. This proof was controversial because the majority of the cases were checked by a computer program, not by hand.[18]\n\nA closed chain inference shows that a collection of statements are pairwise equivalent.\n\nIn order to prove that the statements \n\n\n\n\nφ\n\n1\n\n\n,\n…\n,\n\nφ\n\nn\n\n\n\n\n{\\displaystyle \\varphi _{1},\\ldots ,\\varphi _{n}}\n\n are each pairwise equivalent, proofs are given for the implications \n\n\n\n\nφ\n\n1\n\n\n⇒\n\nφ\n\n2\n\n\n\n\n{\\displaystyle \\varphi _{1}\\Rightarrow \\varphi _{2}}\n\n, \n\n\n\n\nφ\n\n2\n\n\n⇒\n\nφ\n\n3\n\n\n\n\n{\\displaystyle \\varphi _{2}\\Rightarrow \\varphi _{3}}\n\n, \n\n\n\n…\n\n\n{\\displaystyle \\dots }\n\n, \n\n\n\n\nφ\n\nn\n−\n1\n\n\n⇒\n\nφ\n\nn\n\n\n\n\n{\\displaystyle \\varphi _{n-1}\\Rightarrow \\varphi _{n}}\n\n and \n\n\n\n\nφ\n\nn\n\n\n⇒\n\nφ\n\n1\n\n\n\n\n{\\displaystyle \\varphi _{n}\\Rightarrow \\varphi _{1}}\n\n.[19][20]\n\nThe pairwise equivalence of the statements then results from the transitivity of the material conditional.\n\nA probabilistic proof is one in which an example is shown to exist, with certainty, by using methods of probability theory. Probabilistic proof, like proof by construction, is one of many ways to prove existence theorems.\n\nIn the probabilistic method, one seeks an object having a given property, starting with a large set of candidates. One assigns a certain probability for each candidate to be chosen, and then proves that there is a non-zero probability that a chosen candidate will have the desired property. This does not specify which candidates have the property, but the probability could not be positive without at least one.\n\nA probabilistic proof is not to be confused with an argument that a theorem is 'probably' true, a 'plausibility argument'. The work toward the Collatz conjecture shows how far plausibility is from genuine proof, as does the disproof of the Mertens conjecture. While most mathematicians do not think that probabilistic evidence for the properties of a given object counts as a genuine mathematical proof, a few mathematicians and philosophers have argued that at least some types of probabilistic evidence (such as Rabin's probabilistic algorithm for testing primality) are as good as genuine mathematical proofs.[21][22]\n\nA combinatorial proof establishes the equivalence of different expressions by showing that they count the same object in different ways. Often a bijection between two sets is used to show that the expressions for their two sizes are equal. Alternatively, a double counting argument provides two different expressions for the size of a single set, again showing that the two expressions are equal.\n\nA nonconstructive proof establishes that a mathematical object with a certain property exists—without explaining how such an object can be found. Often, this takes the form of a proof by contradiction in which the nonexistence of the object is proved to be impossible. In contrast, a constructive proof establishes that a particular object exists by providing a method of finding it. The following famous example of a nonconstructive proof shows that there exist two irrational numbers a and b such that \n\n\n\n\na\n\nb\n\n\n\n\n{\\displaystyle a^{b}}\n\n is a rational number. This proof uses that \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n is irrational (an easy proof is known since Euclid), but not that \n\n\n\n\n\n\n2\n\n\n\n\n2\n\n\n\n\n\n{\\displaystyle {\\sqrt {2}}^{\\sqrt {2}}}\n\n is irrational (this is true, but the proof is not elementary).\n\nThe expression \"statistical proof\" may be used technically or colloquially in areas of pure mathematics, such as involving cryptography, chaotic series, and probabilistic number theory or analytic number theory.[23][24][25] It is less commonly used to refer to a mathematical proof in the branch of mathematics known as mathematical statistics.  See also the \"Statistical proof using data\" section below.\n\nUntil the twentieth century it was assumed that any proof could, in principle, be checked by a competent mathematician to confirm its validity.[7] However, computers are now used both to prove theorems and to carry out calculations that are too long for any human or team of humans to check; the first proof of the four color theorem is an example of a computer-assisted proof. Some mathematicians are concerned that the possibility of an error in a computer program or a run-time error in its calculations calls the validity of such computer-assisted proofs into question. In practice, the chances of an error invalidating a computer-assisted proof can be reduced by incorporating redundancy and self-checks into calculations, and by developing multiple independent approaches and programs. Errors can never be completely ruled out in case of verification of a proof by humans either, especially if the proof contains natural language and requires deep mathematical insight to uncover the potential hidden assumptions and fallacies involved.\n\nA statement that is neither provable nor disprovable from a set of axioms is called undecidable (from those axioms). One example is the parallel postulate, which is neither provable nor refutable from the remaining axioms of Euclidean geometry.\n\nMathematicians have shown there are many statements that are neither provable nor disprovable in Zermelo–Fraenkel set theory with the axiom of choice (ZFC), the standard system of set theory in mathematics (assuming that ZFC is consistent); see List of statements undecidable in ZFC.\n\nGödel's (first) incompleteness theorem shows that many axiom systems of mathematical interest will have undecidable statements.\n\nWhile early mathematicians such as Eudoxus of Cnidus did not use proofs, from Euclid to the foundational mathematics developments of the late 19th and 20th centuries, proofs were an essential part of mathematics.[26] With the increase in computing power in the 1960s, significant work began to be done investigating mathematical objects beyond the proof-theorem framework,[27] in experimental mathematics. Early pioneers of these methods intended the work ultimately to be resolved into a classical proof-theorem framework, e.g. the early development of fractal geometry,[28] which was ultimately so resolved.\n\nAlthough not a formal proof, a visual demonstration of a mathematical theorem is sometimes called a \"proof without words\". The left-hand picture below is an example of a historic visual proof of the Pythagorean theorem in the case of the (3,4,5) triangle.\n\nSome illusory visual proofs, such as the missing square puzzle, can be constructed in a way which appear to prove a supposed mathematical fact but only do so by neglecting tiny errors (for example, supposedly straight lines which actually bend slightly) which are unnoticeable until the entire picture is closely examined, with lengths and angles precisely measured or calculated.\n\nAn elementary proof is a proof which only uses basic techniques. More specifically, the term is used in number theory to refer to proofs that make no use of complex analysis. For some time it was thought that certain theorems, like the prime number theorem, could only be proved using \"higher\" mathematics. However, over time, many of these results have been reproved using only elementary techniques.\n\nA particular way of organising a proof using two parallel columns is often used as a mathematical exercise in elementary geometry classes in the United States.[29] The proof is written as a series of lines in two columns. In each line, the left-hand column contains a proposition, while the right-hand column contains a brief explanation of how the corresponding proposition in the left-hand column is either an axiom, a hypothesis, or can be logically derived from previous propositions. The left-hand column is typically headed \"Statements\" and the right-hand column is typically headed \"Reasons\".[30]\n\nThe expression \"mathematical proof\" is used by lay people to refer to using mathematical methods or arguing with mathematical objects, such as numbers, to demonstrate something about everyday life, or when data used in an argument is numerical. It is sometimes also used to mean a \"statistical proof\" (below), especially when used to argue from data.\n\n\"Statistical proof\" from data refers to the application of statistics, data analysis, or Bayesian analysis to infer propositions regarding the probability of data. While using mathematical proof to establish theorems in statistics, it is usually not a mathematical proof in that the assumptions from which probability statements are derived require empirical evidence from outside mathematics to verify. In physics, in addition to statistical methods, \"statistical proof\" can refer to the specialized mathematical methods of physics applied to analyze data in a particle physics experiment or observational study in physical cosmology. \"Statistical proof\" may also refer to raw data or a convincing diagram involving data, such as scatter plots, when the data or diagram is adequately convincing without further analysis.\n\nProofs using inductive logic, while considered mathematical in nature, seek to establish propositions with a degree of certainty, which acts in a similar manner to probability, and may be less than full certainty. Inductive logic should not be confused with mathematical induction.\n\nBayesian analysis uses Bayes' theorem to update a person's assessment of likelihoods of hypotheses when new evidence or information is acquired.\n\nPsychologism views mathematical proofs as psychological or mental objects.  Mathematician philosophers, such as Leibniz, Frege, and Carnap have variously criticized this view and attempted to develop a semantics for what they considered to be the language of thought, whereby standards of mathematical proof might be applied to empirical science.\n\nPhilosopher-mathematicians such as Spinoza have attempted to formulate philosophical arguments in an axiomatic manner, whereby mathematical proof standards could be applied to argumentation in general philosophy. Other mathematician-philosophers have tried to use standards of mathematical proof and reason, without empiricism, to arrive at statements outside of mathematics, but having the certainty of propositions deduced in a mathematical proof, such as Descartes' cogito argument.\n\nSometimes, the abbreviation \"Q.E.D.\" is written to indicate the end of a proof. This abbreviation stands for \"quod erat demonstrandum\", which is Latin for \"that which was to be demonstrated\". A more common alternative is to use a square or a rectangle, such as □ or ∎, known as a \"tombstone\" or \"halmos\" after its eponym Paul Halmos. Often, \"which was to be shown\" is verbally stated when writing \"QED\", \"□\", or \"∎\" during an oral presentation. Unicode explicitly provides the \"end of proof\" character, U+220E (∎) (220E(hex) = 8718(dec)).\n"
    },
    {
        "title": "Science",
        "content": "\n\n\n\nScience is a systematic discipline that builds and organises knowledge in the form of testable hypotheses and predictions about the universe.[1][2] Modern science is typically divided into two or three major branches:[3] the natural sciences (e.g., physics, chemistry, and biology), which study the physical world; and the behavioural sciences (e.g., economics, psychology, and sociology), which study individuals and societies.[4][5] The formal sciences (e.g., logic, mathematics, and theoretical computer science), which study formal systems governed by axioms and rules,[6][7] are sometimes described as being sciences as well; however, they are often regarded as a separate field because they rely on deductive reasoning instead of the scientific method or empirical evidence as their main methodology.[8][9] Applied sciences are disciplines that use scientific knowledge for practical purposes, such as engineering and medicine.[10][11][12]\n\nThe history of science spans the majority of the historical record, with the earliest identifiable predecessors to modern science dating to the Bronze Age in Egypt and Mesopotamia (c. 3000–1200 BCE). Their contributions to mathematics, astronomy, and medicine entered and shaped the Greek natural philosophy of classical antiquity, whereby formal attempts were made to provide explanations of events in the physical world based on natural causes, while further advancements, including the introduction of the Hindu–Arabic numeral system, were made during the Golden Age of India.[13]: 12 [14][15][16] Scientific research deteriorated in these regions after the fall of the Western Roman Empire during the Early Middle Ages (400–1000 CE), but in the Medieval renaissances (Carolingian Renaissance, Ottonian Renaissance and the Renaissance of the 12th century) scholarship flourished again. Some Greek manuscripts lost in Western Europe were preserved and expanded upon in the Middle East during the Islamic Golden Age,[17] along with the later efforts of Byzantine Greek scholars who brought Greek manuscripts from the dying Byzantine Empire to Western Europe at the start of the Renaissance.\n\nThe recovery and assimilation of Greek works and Islamic inquiries into Western Europe from the 10th to 13th centuries revived natural philosophy,[18][19][20] which was later transformed by the Scientific Revolution that began in the 16th century[21] as new ideas and discoveries departed from previous Greek conceptions and traditions.[22][23] The scientific method soon played a greater role in knowledge creation and it was not until the 19th century that many of the institutional and professional features of science began to take shape,[24][25] along with the changing of \"natural philosophy\" to \"natural science\".[26]\n\nNew knowledge in science is advanced by research from scientists who are motivated by curiosity about the world and a desire to solve problems.[27][28] Contemporary scientific research is highly collaborative and is usually done by teams in academic and research institutions,[29] government agencies,[30] and companies.[31] The practical impact of their work has led to the emergence of science policies that seek to influence the scientific enterprise by prioritising the ethical and moral development of commercial products, armaments, health care, public infrastructure, and environmental protection.\n\nThe word science has been used in Middle English since the 14th century in the sense of \"the state of knowing\". The word was borrowed from the Anglo-Norman language as the suffix -cience, which was borrowed from the Latin word scientia, meaning \"knowledge, awareness, understanding\". It is a noun derivative of the Latin sciens meaning \"knowing\", and undisputedly derived from the Latin sciō, the present participle scīre, meaning \"to know\".[32]\n\nThere are many hypotheses for science's ultimate word origin. According to Michiel de Vaan, Dutch linguist and Indo-Europeanist, sciō may have its origin in the Proto-Italic language as *skije- or *skijo- meaning \"to know\", which may originate from Proto-Indo-European language as *skh1-ie, *skh1-io, meaning \"to incise\". The Lexikon der indogermanischen Verben proposed sciō is a back-formation of nescīre, meaning \"to not know, be unfamiliar with\", which may derive from Proto-Indo-European *sekH- in Latin secāre, or *skh2-, from *sḱʰeh2(i)- meaning \"to cut\".[33]\n\nIn the past, science was a synonym for \"knowledge\" or \"study\", in keeping with its Latin origin. A person who conducted scientific research was called a \"natural philosopher\" or \"man of science\".[34] In 1834, William Whewell introduced the term scientist in a review of Mary Somerville's book On the Connexion of the Physical Sciences,[35] crediting it to \"some ingenious gentleman\" (possibly himself).[36]\n\nScience has no single origin. Rather, scientific thinking emerged gradually over the course of tens of thousands of years,[37][38] taking different forms around the world, and few details are known about the very earliest developments. Women likely played a central role in prehistoric science,[39] as did religious rituals.[40] Some scholars use the term \"protoscience\" to label activities in the past that resemble modern science in some but not all features;[41][42][43] however, this label has also been criticised as denigrating,[44] or too suggestive of presentism, thinking about those activities only in relation to modern categories.[45]\n\nDirect evidence for scientific processes becomes clearer with the advent of writing systems in the Bronze Age civilisations of Ancient Egypt and Mesopotamia (c. 3000–1200 BCE), creating the earliest written records in the history of science.[13]: 12–15 [14] Although the words and concepts of \"science\" and \"nature\" were not part of the conceptual landscape at the time, the ancient Egyptians and Mesopotamians made contributions that would later find a place in Greek and medieval science: mathematics, astronomy, and medicine.[46][13]: 12  From the 3rd millennium BCE, the ancient Egyptians developed a non-positional decimal numbering system,[47] solved practical problems using geometry,[48] and developed a calendar.[49] Their healing therapies involved drug treatments and the supernatural, such as prayers, incantations, and rituals.[13]: 9 \n\nThe ancient Mesopotamians used knowledge about the properties of various natural chemicals for manufacturing pottery, faience, glass, soap, metals, lime plaster, and waterproofing.[50] They studied animal physiology, anatomy, behaviour, and astrology for divinatory purposes.[51] The Mesopotamians had an intense interest in medicine and the earliest medical prescriptions appeared in Sumerian during the Third Dynasty of Ur.[50][52] They seem to have studied scientific subjects which had practical or religious applications and had little interest in satisfying curiosity.[50]\n\nIn classical antiquity, there is no real ancient analogue of a modern scientist. Instead, well-educated, usually upper-class, and almost universally male individuals performed various investigations into nature whenever they could afford the time.[53] Before the invention or discovery of the concept of phusis or nature by the pre-Socratic philosophers, the same words tend to be used to describe the natural \"way\" in which a plant grows,[54] and the \"way\" in which, for example, one tribe worships a particular god. For this reason, it is claimed that these men were the first philosophers in the strict sense and the first to clearly distinguish \"nature\" and \"convention\".[55]\n\nThe early Greek philosophers of the Milesian school, which was founded by Thales of Miletus and later continued by his successors Anaximander and Anaximenes, were the first to attempt to explain natural phenomena without relying on the supernatural.[56] The Pythagoreans developed a complex number philosophy[57]: 467–468  and contributed significantly to the development of mathematical science.[57]: 465  The theory of atoms was developed by the Greek philosopher Leucippus and his student Democritus.[58][59] Later, Epicurus would develop a full natural cosmology based on atomism, and would adopt a \"canon\" (ruler, standard) which established physical criteria or standards of scientific truth.[60] The Greek doctor Hippocrates established the tradition of systematic medical science[61][62] and is known as \"The Father of Medicine\".[63]\n\nA turning point in the history of early philosophical science was Socrates' example of applying philosophy to the study of human matters, including human nature, the nature of political communities, and human knowledge itself. The Socratic method as documented by Plato's dialogues is a dialectic method of hypothesis elimination: better hypotheses are found by steadily identifying and eliminating those that lead to contradictions. The Socratic method searches for general commonly-held truths that shape beliefs and scrutinises them for consistency.[64] Socrates criticised the older type of study of physics as too purely speculative and lacking in self-criticism.[65]\n\nIn the 4th century BCE, Aristotle created a systematic programme of teleological philosophy.[66] In the 3rd century BCE, Greek astronomer Aristarchus of Samos was the first to propose a heliocentric model of the universe, with the Sun at the centre and all the planets orbiting it.[67] Aristarchus's model was widely rejected because it was believed to violate the laws of physics,[67] while Ptolemy's Almagest, which contains a geocentric description of the Solar System, was accepted through the early Renaissance instead.[68][69] The inventor and mathematician Archimedes of Syracuse made major contributions to the beginnings of calculus.[70] Pliny the Elder was a Roman writer and polymath, who wrote the seminal encyclopaedia Natural History.[71][72][73]\n\nPositional notation for representing numbers likely emerged between the 3rd and 5th centuries CE along Indian trade routes. This numeral system made efficient arithmetic operations more accessible and would eventually become standard for mathematics worldwide.[74]\n\nDue to the collapse of the Western Roman Empire, the 5th century saw an intellectual decline, with knowledge of classical Greek conceptions of the world deteriorating in Western Europe.[13]: 194  Latin encyclopaedists of the period such as Isidore of Seville preserved the majority of general ancient knowledge.[75] In contrast, because the Byzantine Empire resisted attacks from invaders, they were able to preserve and improve prior learning.[13]: 159  John Philoponus, a Byzantine scholar in the 6th century, started to question Aristotle's teaching of physics, introducing the theory of impetus.[13]: 307, 311, 363, 402  His criticism served as an inspiration to medieval scholars and Galileo Galilei, who extensively cited his works ten centuries later.[13]: 307–308 [76]\n\nDuring late antiquity and the Early Middle Ages, natural phenomena were mainly examined via the Aristotelian approach. The approach includes Aristotle's four causes: material, formal, moving, and final cause.[77] Many Greek classical texts were preserved by the Byzantine Empire and Arabic translations were done by groups such as the Nestorians and the Monophysites. Under the Abbasids, these Arabic translations were later improved and developed by Arabic scientists.[78] By the 6th and 7th centuries, the neighbouring Sasanian Empire established the medical Academy of Gondishapur, which was considered by Greek, Syriac, and Persian physicians as the most important medical hub of the ancient world.[79]\n\nIslamic study of Aristotelianism flourished in the House of Wisdom established in the Abbasid capital of Baghdad, Iraq[80] and the flourished[81] until the Mongol invasions in the 13th century. Ibn al-Haytham, better known as Alhazen, used controlled experiments in his optical study.[a][83][84] Avicenna's compilation of The Canon of Medicine, a medical encyclopaedia, is considered to be one of the most important publications in medicine and was used until the 18th century.[85]\n\nBy the 11th century, most of Europe had become Christian,[13]: 204  and in 1088, the University of Bologna emerged as the first university in Europe.[86] As such, demand for Latin translation of ancient and scientific texts grew,[13]: 204  a major contributor to the Renaissance of the 12th century. Renaissance scholasticism in western Europe flourished, with experiments done by observing, describing, and classifying subjects in nature.[87] In the 13th century, medical teachers and students at Bologna began opening human bodies, leading to the first anatomy textbook based on human dissection by Mondino de Luzzi.[88]\n\nNew developments in optics played a role in the inception of the Renaissance, both by challenging long-held metaphysical ideas on perception, as well as by contributing to the improvement and development of technology such as the camera obscura and the telescope. At the start of the Renaissance, Roger Bacon, Vitello, and John Peckham each built up a scholastic ontology upon a causal chain beginning with sensation, perception, and finally apperception of the individual and universal forms of Aristotle.[82]: Book I  A model of vision later known as perspectivism was exploited and studied by the artists of the Renaissance. This theory uses only three of Aristotle's four causes: formal, material, and final.[89]\n\nIn the 16th century, Nicolaus Copernicus formulated a heliocentric model of the Solar System, stating that the planets revolve around the Sun, instead of the geocentric model where the planets and the Sun revolve around the Earth. This was based on a theorem that the orbital periods of the planets are longer as their orbs are farther from the centre of motion, which he found not to agree with Ptolemy's model.[90]\n\nJohannes Kepler and others challenged the notion that the only function of the eye is perception, and shifted the main focus in optics from the eye to the propagation of light.[89][91] Kepler is best known, however, for improving Copernicus' heliocentric model through the discovery of Kepler's laws of planetary motion. Kepler did not reject Aristotelian metaphysics and described his work as a search for the Harmony of the Spheres.[92] Galileo had made significant contributions to astronomy, physics and engineering. However, he became persecuted after Pope Urban VIII sentenced him for writing about the heliocentric model.[93]\n\nThe printing press was widely used to publish scholarly arguments, including some that disagreed widely with contemporary ideas of nature.[94] Francis Bacon and René Descartes published philosophical arguments in favour of a new type of non-Aristotelian science. Bacon emphasised the importance of experiment over contemplation, questioned the Aristotelian concepts of formal and final cause, promoted the idea that science should study the laws of nature and the improvement of all human life.[95] Descartes emphasised individual thought and argued that mathematics rather than geometry should be used to study nature.[96]\n\nAt the start of the Age of Enlightenment, Isaac Newton formed the foundation of classical mechanics by his Philosophiæ Naturalis Principia Mathematica, greatly influencing future physicists.[97] Gottfried Wilhelm Leibniz incorporated terms from Aristotelian physics, now used in a new non-teleological way. This implied a shift in the view of objects: objects were now considered as having no innate goals. Leibniz assumed that different types of things all work according to the same general laws of nature, with no special formal or final causes.[98]\n\nDuring this time the declared purpose and value of science became producing wealth and inventions that would improve human lives, in the materialistic sense of having more food, clothing, and other things. In Bacon's words, \"the real and legitimate goal of sciences is the endowment of human life with new inventions and riches\", and he discouraged scientists from pursuing intangible philosophical or spiritual ideas, which he believed contributed little to human happiness beyond \"the fume of subtle, sublime or pleasing [speculation]\".[99]\n\nScience during the Enlightenment was dominated by scientific societies and academies,[100] which had largely replaced universities as centres of scientific research and development. Societies and academies were the backbones of the maturation of the scientific profession. Another important development was the popularisation of science among an increasingly literate population.[101] Enlightenment philosophers turned to a few of their scientific predecessors – Galileo, Kepler, Boyle, and Newton principally – as the guides to every physical and social field of the day.[102][103]\n\nThe 18th century saw significant advancements in the practice of medicine[104] and physics;[105] the development of biological taxonomy by Carl Linnaeus;[106] a new understanding of magnetism and electricity;[107] and the maturation of chemistry as a discipline.[108] Ideas on human nature, society, and economics evolved during the Enlightenment. Hume and other Scottish Enlightenment thinkers developed A Treatise of Human Nature, which was expressed historically in works by authors including James Burnett, Adam Ferguson, John Millar and William Robertson, all of whom merged a scientific study of how humans behaved in ancient and primitive cultures with a strong awareness of the determining forces of modernity.[109] Modern sociology largely originated from this movement.[110] In 1776, Adam Smith published The Wealth of Nations, which is often considered the first work on modern economics.[111]\n\nDuring the 19th century, many distinguishing characteristics of contemporary modern science began to take shape. These included the transformation of the life and physical sciences; the frequent use of precision instruments; the emergence of terms such as \"biologist\", \"physicist\", and \"scientist\"; an increased professionalisation of those studying nature; scientists gaining cultural authority over many dimensions of society; the industrialisation of numerous countries; the thriving of popular science writings; and the emergence of science journals.[112] During the late 19th century, psychology emerged as a separate discipline from philosophy when Wilhelm Wundt founded the first laboratory for psychological research in 1879.[113]\n\nDuring the mid-19th century Charles Darwin and Alfred Russel Wallace independently proposed the theory of evolution by natural selection in 1858, which explained how different plants and animals originated and evolved. Their theory was set out in detail in Darwin's book On the Origin of Species, published in 1859.[114] Separately, Gregor Mendel presented his paper, \"Experiments on Plant Hybridisation\" in 1865,[115] which outlined the principles of biological inheritance, serving as the basis for modern genetics.[116]\n\nEarly in the 19th century John Dalton suggested the modern atomic theory, based on Democritus's original idea of indivisible particles called atoms.[117] The laws of conservation of energy, conservation of momentum and conservation of mass suggested a highly stable universe where there could be little loss of resources. However, with the advent of the steam engine and the Industrial Revolution there was an increased understanding that not all forms of energy have the same energy qualities, the ease of conversion to useful work or to another form of energy.[118] This realisation led to the development of the laws of thermodynamics, in which the free energy of the universe is seen as constantly declining: the entropy of a closed universe increases over time.[b]\n\nThe electromagnetic theory was established in the 19th century by the works of Hans Christian Ørsted, André-Marie Ampère, Michael Faraday, James Clerk Maxwell, Oliver Heaviside, and Heinrich Hertz. The new theory raised questions that could not easily be answered using Newton's framework. The discovery of X-rays inspired the discovery of radioactivity by Henri Becquerel and Marie Curie in 1896,[121] Marie Curie then became the first person to win two Nobel Prizes.[122] In the next year came the discovery of the first subatomic particle, the electron.[123]\n\nIn the first half of the century the development of antibiotics and artificial fertilisers improved human living standards globally.[124][125] Harmful environmental issues such as ozone depletion, ocean acidification, eutrophication, and climate change came to the public's attention and caused the onset of environmental studies.[126]\n\nDuring this period scientific experimentation became increasingly larger in scale and funding.[127] The extensive technological innovation stimulated by World War I, World War II, and the Cold War led to competitions between global powers, such as the Space Race and nuclear arms race.[128][129] Substantial international collaborations were also made, despite armed conflicts.[130]\n\nIn the late 20th century active recruitment of women and elimination of sex discrimination greatly increased the number of women scientists, but large gender disparities remained in some fields.[131] The discovery of the cosmic microwave background in 1964[132] led to a rejection of the steady-state model of the universe in favour of the Big Bang theory of Georges Lemaître.[133]\n\nThe century saw fundamental changes within science disciplines. Evolution became a unified theory in the early 20th-century when the modern synthesis reconciled Darwinian evolution with classical genetics.[134] Albert Einstein's theory of relativity and the development of quantum mechanics complement classical mechanics to describe physics in extreme length, time and gravity.[135][136] Widespread use of integrated circuits in the last quarter of the 20th century combined with communications satellites led to a revolution in information technology and the rise of the global internet and mobile computing, including smartphones. The need for mass systematisation of long, intertwined causal chains and large amounts of data led to the rise of the fields of systems theory and computer-assisted scientific modelling.[137]\n\nThe Human Genome Project was completed in 2003 by identifying and mapping all of the genes of the human genome.[138] The first induced pluripotent human stem cells were made in 2006, allowing adult cells to be transformed into stem cells and turn into any cell type found in the body.[139] With the affirmation of the Higgs boson discovery in 2013, the last particle predicted by the Standard Model of particle physics was found.[140] In 2015, gravitational waves, predicted by general relativity a century before, were first observed.[141][142] In 2019, the international collaboration Event Horizon Telescope presented the first direct image of a black hole's accretion disc.[143]\n\nModern science is commonly divided into three major branches: natural science, social science, and formal science.[3] Each of these branches comprises various specialised yet overlapping scientific disciplines that often possess their own nomenclature and expertise.[144] Both natural and social sciences are empirical sciences,[145] as their knowledge is based on empirical observations and is capable of being tested for its validity by other researchers working under the same conditions.[146]\n\nNatural science is the study of the physical world. It can be divided into two main branches: life science and physical science. These two branches may be further divided into more specialised disciplines. For example, physical science can be subdivided into physics, chemistry, astronomy, and earth science. Modern natural science is the successor to the natural philosophy that began in Ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches that were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[147] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and other biotic beings.[148] Today, \"natural history\" suggests observational descriptions aimed at popular audiences.[149]\n\nSocial science is the study of human behaviour and the functioning of societies.[4][5] It has many disciplines that include, but are not limited to anthropology, economics, history, human geography, political science, psychology, and sociology.[4] In the social sciences, there are many competing theoretical perspectives, many of which are extended through competing research programmes such as the functionalists, conflict theorists, and interactionists in sociology.[4] Due to the limitations of conducting controlled experiments involving large groups of individuals or complex situations, social scientists may adopt other research methods such as the historical method, case studies, and cross-cultural studies. Moreover, if quantitative information is available, social scientists may rely on statistical approaches to better understand social relationships and processes.[4]\n\nFormal science is an area of study that generates knowledge using formal systems.[150][6][7] A formal system is an abstract structure used for inferring theorems from axioms according to a set of rules.[151] It includes mathematics,[152][153] systems theory, and theoretical computer science. The formal sciences share similarities with the other two branches by relying on objective, careful, and systematic study of an area of knowledge. They are, however, different from the empirical sciences as they rely exclusively on deductive reasoning, without the need for empirical evidence, to verify their abstract concepts.[8][154][146] The formal sciences are therefore a priori disciplines and because of this, there is disagreement on whether they constitute a science.[155][156] Nevertheless, the formal sciences play an important role in the empirical sciences. Calculus, for example, was initially invented to understand motion in physics.[157] Natural and social sciences that rely heavily on mathematical applications include mathematical physics,[158] chemistry,[159] biology,[160] finance,[161] and economics.[162]\n\nApplied science is the use of the scientific method and knowledge to attain practical goals and includes a broad range of disciplines such as engineering and medicine.[163][12] Engineering is the use of scientific principles to invent, design and build machines, structures and technologies.[164] Science may contribute to the development of new technologies.[165] Medicine is the practice of caring for patients by maintaining and restoring health through the prevention, diagnosis, and treatment of injury or disease.[166][167] The applied sciences are often contrasted with the basic sciences, which are focused on advancing scientific theories and laws that explain and predict events in the natural world.[168][169]\n\nComputational science applies computing power to simulate real-world situations, enabling a better understanding of scientific problems than formal mathematics alone can achieve. The use of machine learning and artificial intelligence is becoming a central feature of computational contributions to science, for example in agent-based computational economics, random forests, topic modeling and various forms of prediction. However, machines alone rarely advance knowledge as they require human guidance and capacity to reason; and they can introduce bias against certain social groups or sometimes underperform against humans.[170][171]\n\nInterdisciplinary science involves the combination of two or more disciplines into one,[172] such as bioinformatics, a combination of biology and computer science[173] or cognitive sciences. The concept has existed since the ancient Greek period and it became popular again in the 20th century.[174]\n\nScientific research can be labelled as either basic or applied research. Basic research is the search for knowledge and applied research is the search for solutions to practical problems using this knowledge. Most understanding comes from basic research, though sometimes applied research targets specific practical problems. This leads to technological advances that were not previously imaginable.[175]\n\nScientific research involves using the scientific method, which seeks to objectively explain the events of nature in a reproducible way.[176] Scientists usually take for granted a set of basic assumptions that are needed to justify the scientific method: there is an objective reality shared by all rational observers; this objective reality is governed by natural laws; these laws were discovered by means of systematic observation and experimentation.[2] Mathematics is essential in the formation of hypotheses, theories, and laws, because it is used extensively in quantitative modelling, observing, and collecting measurements.[177] Statistics is used to summarise and analyse data, which allows scientists to assess the reliability of experimental results.[178]\n\nIn the scientific method an explanatory thought experiment or hypothesis is put forward as an explanation using parsimony principles and is expected to seek consilience – fitting with other accepted facts related to an observation or scientific question.[179] This tentative explanation is used to make falsifiable predictions, which are typically posted before being tested by experimentation. Disproof of a prediction is evidence of progress.[176]: 4–5 [180] Experimentation is especially important in science to help establish causal relationships to avoid the correlation fallacy, though in some sciences such as astronomy or geology, a predicted observation might be more appropriate.[181]\n\nWhen a hypothesis proves unsatisfactory it is modified or discarded. If the hypothesis survives testing, it may become adopted into the framework of a scientific theory, a validly reasoned, self-consistent model or framework for describing the behaviour of certain natural events. A theory typically describes the behaviour of much broader sets of observations than a hypothesis; commonly, a large number of hypotheses can be logically bound together by a single theory. Thus, a theory is a hypothesis explaining various other hypotheses. In that vein, theories are formulated according to most of the same scientific principles as hypotheses. Scientists may generate a model, an attempt to describe or depict an observation in terms of a logical, physical or mathematical representation, and to generate new hypotheses that can be tested by experimentation.[182]\n\nWhile performing experiments to test hypotheses, scientists may have a preference for one outcome over another.[183][184] Eliminating the bias can be achieved through transparency, careful experimental design, and a thorough peer review process of the experimental results and conclusions.[185][186] After the results of an experiment are announced or published, it is normal practice for independent researchers to double-check how the research was performed, and to follow up by performing similar experiments to determine how dependable the results might be.[187] Taken in its entirety, the scientific method allows for highly creative problem solving while minimising the effects of subjective and confirmation bias.[188] Intersubjective verifiability, the ability to reach a consensus and reproduce results, is fundamental to the creation of all scientific knowledge.[189]\n\nScientific research is published in a range of literature.[190] Scientific journals communicate and document the results of research carried out in universities and various other research institutions, serving as an archival record of science. The first scientific journals, Journal des sçavans followed by Philosophical Transactions, began publication in 1665. Since that time the total number of active periodicals has steadily increased. In 1981, one estimate for the number of scientific and technical journals in publication was 11,500.[191]\n\nMost scientific journals cover a single scientific field and publish the research within that field; the research is normally expressed in the form of a scientific paper. Science has become so pervasive in modern societies that it is considered necessary to communicate the achievements, news, and ambitions of scientists to a wider population.[192]\n\nThe replication crisis is an ongoing methodological crisis that affects parts of the social and life sciences. In subsequent investigations, the results of many scientific studies have been proven to be unrepeatable.[193] The crisis has long-standing roots; the phrase was coined in the early 2010s[194] as part of a growing awareness of the problem. The replication crisis represents an important body of research in metascience, which aims to improve the quality of all scientific research while reducing waste.[195]\n\nAn area of study or speculation that masquerades as science in an attempt to claim legitimacy that it would not otherwise be able to achieve is sometimes referred to as pseudoscience, fringe science, or junk science.[196][197] Physicist Richard Feynman coined the term \"cargo cult science\" for cases in which researchers believe, and at a glance, look like they are doing science but lack the honesty to allow their results to be rigorously evaluated.[198] Various types of commercial advertising, ranging from hype to fraud, may fall into these categories. Science has been described as \"the most important tool\" for separating valid claims from invalid ones.[199]\n\nThere can also be an element of political or ideological bias on all sides of scientific debates. Sometimes, research may be characterised as \"bad science\", research that may be well-intended but is incorrect, obsolete, incomplete, or over-simplified expositions of scientific ideas. The term \"scientific misconduct\" refers to situations such as where researchers have intentionally misrepresented their published data or have purposely given credit for a discovery to the wrong person.[200]\n\n\nThere are different schools of thought in the philosophy of science. The most popular position is empiricism, which holds that knowledge is created by a process involving observation; scientific theories generalise observations.[201] Empiricism generally encompasses inductivism, a position that explains how general theories can be made from the finite amount of empirical evidence available. Many versions of empiricism exist, with the predominant ones being Bayesianism and the hypothetico-deductive method.[202][201]\n\nEmpiricism has stood in contrast to rationalism, the position originally associated with Descartes, which holds that knowledge is created by the human intellect, not by observation.[203] Critical rationalism is a contrasting 20th-century approach to science, first defined by Austrian-British philosopher Karl Popper. Popper rejected the way that empiricism describes the connection between theory and observation. He claimed that theories are not generated by observation, but that observation is made in the light of theories, and that the only way theory A can be affected by observation is after theory A were to conflict with observation, but theory B were to survive the observation.[204]\nPopper proposed replacing verifiability with falsifiability as the landmark of scientific theories, replacing induction with falsification as the empirical method.[204] Popper further claimed that there is actually only one universal method, not specific to science: the negative method of criticism, trial and error,[205] covering all products of the human mind, including science, mathematics, philosophy, and art.[206]\n\nAnother approach, instrumentalism, emphasises the utility of theories as instruments for explaining and predicting phenomena. It views scientific theories as black boxes, with only their input (initial conditions) and output (predictions) being relevant. Consequences, theoretical entities, and logical structure are claimed to be things that should be ignored.[207] Close to instrumentalism is constructive empiricism, according to which the main criterion for the success of a scientific theory is whether what it says about observable entities is true.[208]\n\nThomas Kuhn argued that the process of observation and evaluation takes place within a paradigm, a logically consistent \"portrait\" of the world that is consistent with observations made from its framing. He characterised normal science as the process of observation and \"puzzle solving\", which takes place within a paradigm, whereas revolutionary science occurs when one paradigm overtakes another in a paradigm shift.[209] Each paradigm has its own distinct questions, aims, and interpretations. The choice between paradigms involves setting two or more \"portraits\" against the world and deciding which likeness is most promising. A paradigm shift occurs when a significant number of observational anomalies arise in the old paradigm and a new paradigm makes sense of them. That is, the choice of a new paradigm is based on observations, even though those observations are made against the background of the old paradigm. For Kuhn, acceptance or rejection of a paradigm is a social process as much as a logical process. Kuhn's position, however, is not one of relativism.[210]\n\nAnother approach often cited in debates of scientific scepticism against controversial movements like \"creation science\" is methodological naturalism. Naturalists maintain that a difference should be made between natural and supernatural, and science should be restricted to natural explanations.[211] Methodological naturalism maintains that science requires strict adherence to empirical study and independent verification.[212]\n\nThe scientific community is a network of interacting scientists who conduct scientific research. The community consists of smaller groups working in scientific fields. By having peer review, through discussion and debate within journals and conferences, scientists maintain the quality of research methodology and objectivity when interpreting results.[213]\n\nScientists are individuals who conduct scientific research to advance knowledge in an area of interest.[214][215] In modern times, many professional scientists are trained in an academic setting and, upon completion, attain an academic degree, with the highest degree being a doctorate (e.g. a Doctor of Philosophy, or PhD).[216] Many scientists pursue careers in various sectors of the economy such as academia, industry, government, and nonprofit organisations.[217][218][219]\n\nScientists exhibit a strong curiosity about reality and a desire to apply scientific knowledge for the benefit of health, nations, the environment, or industries. Other motivations include recognition by their peers and prestige. In modern times, many scientists have advanced degrees in an area of science and pursue careers in various sectors of the economy, such as academia, industry, government, and nonprofit environments.[220][221][222]\n\nScience has historically been a male-dominated field, with notable exceptions. Women in science faced considerable discrimination in science, much as they did in other areas of male-dominated societies. For example, women were frequently passed over for job opportunities and denied credit for their work.[223] The achievements of women in science have been attributed to the defiance of their traditional role as labourers within the domestic sphere.[224]\n\nLearned societies for the communication and promotion of scientific thought and experimentation have existed since the Renaissance.[225] Many scientists belong to a learned society that promotes their respective scientific discipline, profession, or group of related disciplines.[226] Membership may either be open to all, require possession of scientific credentials, or conferred by election.[227] Most scientific societies are nonprofit organisations,[228] and many are professional associations. Their activities typically include holding regular conferences for the presentation and discussion of new research results and publishing or sponsoring academic journals in their discipline. Some societies act as professional bodies, regulating the activities of their members in the public interest, or the collective interest of the membership.\n\nThe professionalisation of science, begun in the 19th century, was partly enabled by the creation of national distinguished academies of sciences such as the Italian Accademia dei Lincei in 1603,[229] the British Royal Society in 1660,[230] the French Academy of Sciences in 1666,[231] the American National Academy of Sciences in 1863,[232] the German Kaiser Wilhelm Society in 1911,[233] and the Chinese Academy of Sciences in 1949.[234] International scientific organisations, such as the International Science Council, are devoted to international cooperation for science advancement.[235]\n\nScience awards are usually given to individuals or organisations that have made significant contributions to a discipline. They are often given by prestigious institutions; thus, it is considered a great honour for a scientist receiving them. Since the early Renaissance, scientists have often been awarded medals, money, and titles. The Nobel Prize, a widely regarded prestigious award, is awarded annually to those who have achieved scientific advances in the fields of medicine, physics, and chemistry.[236]\n\nScientific research is often funded through a competitive process in which potential research projects are evaluated and only the most promising receive funding. Such processes, which are run by government, corporations, or foundations, allocate scarce funds. Total research funding in most developed countries is between 1.5% and 3% of GDP.[237] In the OECD, around two-thirds of research and development in scientific and technical fields is carried out by industry, and 20% and 10%, respectively, by universities and government. The government funding proportion in certain fields is higher, and it dominates research in social science and the humanities. In less developed nations, the government provides the bulk of the funds for their basic scientific research.[238]\n\nMany governments have dedicated agencies to support scientific research, such as the National Science Foundation in the United States,[239] the National Scientific and Technical Research Council in Argentina,[240] Commonwealth Scientific and Industrial Research Organisation in Australia,[241] National Centre for Scientific Research in France,[242] the Max Planck Society in Germany,[243] and National Research Council in Spain.[244] In commercial research and development, all but the most research-orientated corporations focus more heavily on near-term commercialisation possibilities than research driven by curiosity.[245]\n\nScience policy is concerned with policies that affect the conduct of the scientific enterprise, including research funding, often in pursuance of other national policy goals such as technological innovation to promote commercial product development, weapons development, health care, and environmental monitoring. Science policy sometimes refers to the act of applying scientific knowledge and consensus to the development of public policies. In accordance with public policy being concerned about the well-being of its citizens, science policy's goal is to consider how science and technology can best serve the public.[246] Public policy can directly affect the funding of capital equipment and intellectual infrastructure for industrial research by providing tax incentives to those organisations that fund research.[192]\n\nScience education for the general public is embedded in the school curriculum, and is supplemented by online pedagogical content (for example, YouTube and Khan Academy), museums, and science magazines and blogs. Major organisations of scientists such as the American Association for the Advancement of Science (AAAS) consider the sciences to be a part of the liberal arts traditions of learning, along with philosophy and history.[247] Scientific literacy is chiefly concerned with an understanding of the scientific method, units and methods of measurement, empiricism, a basic understanding of statistics (correlations, qualitative versus quantitative observations, aggregate statistics), and a basic understanding of core scientific fields such as physics, chemistry, biology, ecology, geology, and computation. As a student advances into higher stages of formal education, the curriculum becomes more in depth. Traditional subjects usually included in the curriculum are natural and formal sciences, although recent movements include social and applied science as well.[248]\n\nThe mass media face pressures that can prevent them from accurately depicting competing scientific claims in terms of their credibility within the scientific community as a whole. Determining how much weight to give different sides in a scientific debate may require considerable expertise regarding the matter.[249] Few journalists have real scientific knowledge, and even beat reporters who are knowledgeable about certain scientific issues may be ignorant about other scientific issues that they are suddenly asked to cover.[250][251]\n\nScience magazines such as New Scientist, Science & Vie, and Scientific American cater to the needs of a much wider readership and provide a non-technical summary of popular areas of research, including notable discoveries and advances in certain fields of research.[252] The science fiction genre, primarily speculative fiction, can transmit the ideas and methods of science to the general public.[253] Recent efforts to intensify or develop links between science and non-scientific disciplines, such as literature or poetry, include the Creative Writing Science resource developed through the Royal Literary Fund.[254]\n\nWhile the scientific method is broadly accepted in the scientific community, some fractions of society reject certain scientific positions or are sceptical about science. Examples are the common notion that COVID-19 is not a major health threat to the US (held by 39% of Americans in August 2021)[255] or the belief that climate change is not a major threat to the US (also held by 40% of Americans, in late 2019 and early 2020).[256] Psychologists have pointed to four factors driving rejection of scientific results:[257]\n\nAnti-science attitudes often seem to be caused by fear of rejection in social groups. For instance, climate change is perceived as a threat by only 22% of Americans on the right side of the political spectrum, but by 85% on the left.[259] That is, if someone on the left would not consider climate change as a threat, this person may face contempt and be rejected in that social group. In fact, people may rather deny a scientifically accepted fact than lose or jeopardise their social status.[260]\n\nAttitudes towards science are often determined by political opinions and goals. Government, business and advocacy groups have been known to use legal and economic pressure to influence scientific researchers. Many factors can act as facets of the politicisation of science such as anti-intellectualism, perceived threats to religious beliefs, and fear for business interests.[262] Politicisation of science is usually accomplished when scientific information is presented in a way that emphasises the uncertainty associated with the scientific evidence.[263] Tactics such as shifting conversation, failing to acknowledge facts, and capitalising on doubt of scientific consensus have been used to gain more attention for views that have been undermined by scientific evidence.[264] Examples of issues that have involved the politicisation of science include the global warming controversy, health effects of pesticides, and health effects of tobacco.[264][265]\n"
    },
    {
        "title": "Mathematical object",
        "content": "A mathematical object is an abstract concept arising in mathematics.[1] Typically, a mathematical object can be a value that can be assigned to a symbol, and therefore can be involved in formulas. Commonly encountered mathematical objects include numbers, expressions, shapes, functions, and sets. Mathematical objects can be very complex; for example, theorems, proofs, and even formal theories are considered as mathematical objects in proof theory.\n\nIn Philosophy of mathematics, the concept of \"mathematical objects\" touches on topics of existence, identity, and the nature of reality.[2] In metaphysics, objects are often considered entities that possess properties and can stand in various relations to one another.[3] Philosophers debate whether mathematical objects have an independent existence outside of human thought (realism), or if their existence is dependent on mental constructs or language (idealism and nominalism). Objects can range from the concrete: such as physical objects usually studied in applied mathematics, to the abstract, studied in pure mathematics. What constitutes an \"object\" is foundational to many areas of philosophy, from ontology (the study of being) to epistemology (the study of knowledge). In mathematics, objects are often seen as entities that exist independently of the physical world, raising questions about their ontological status.[4][5] There are varying schools of thought which offer different perspectives on the matter, and many famous mathematicians and philosophers each have differing opinions on which is more correct.[6]\n\nQuine-Putnam indispensability is an argument for the existence of mathematical objects based on their unreasonable effectiveness in the natural sciences. Every branch of science relies largely on large and often vastly different areas of mathematics. From physics' use of Hilbert spaces in quantum mechanics and differential geometry in general relativity to biology's use of chaos theory and combinatorics (see mathematical biology), not only does mathematics help with predictions, it allows these areas to have an elegant language to express these ideas. Moreover, it is hard to imagine how areas like quantum mechanics and general relativity could have developed without their assistance from mathematics, and therefore, one could argue that mathematics is indispensable to these theories. It is because of this unreasonable effectiveness and indispensability of mathematics that philosophers Willard Quine and Hilary Putnam argue that we should believe the mathematical objects for which these theories depend actually exist, that is, we ought to have an ontological commitment to them. The argument is described by the following syllogism:[7]\n(Premise 1)   We ought to have ontological commitment to all and only the entities that are indispensable to our best scientific theories.\n\n(Premise 2)   Mathematical entities are indispensable to our best scientific theories.\n\n\n(Conclusion) We ought to have ontological commitment to mathematical entities\nThis argument resonates with a philosophy in applied mathematics called Naturalism[8] (or sometimes Predicativism)[9] which states that the only authoritative standards on existence are those of science.\n\nPlatonism asserts that mathematical objects are seen as real, abstract entities that exist independently of human thought, often in some Platonic realm. Just as physical objects like electrons and planets exist, so do numbers and sets. And just as statements about electrons and planets are true or false as these objects contain perfectly objective properties, so are statements about numbers and sets. Mathematicians discover these objects rather than invent them.[10][11] (See also: Mathematical Platonism)\n\nSome some notable platonists include:\n\nNominalism denies the independent existence of mathematical objects. Instead, it suggests that they are merely convenient fictions or shorthand for describing relationships and structures within our language and theories. Under this view, mathematical objects do not have an existence beyond the symbols and concepts we use.[13][14]\n\nSome notable nominalists include:\n\nLogicism asserts that all mathematical truths can be reduced to logical truths, and all objects forming the subject matter of those branches of mathematics are logical objects. In other words, mathematics is fundamentally a branch of logic, and all mathematical concepts, theorems, and truths can be derived from purely logical principles and definitions. Logicism faced challenges, particularly with the Russillian axioms, the Multiplicative axiom (now called the Axiom of Choice) and his Axiom of Infinity, and later with the discovery of Gödel's incompleteness theorems, which showed that any sufficiently powerful formal system (like those used to express arithmetic) cannot be both complete and consistent. This meant that not all mathematical truths could be derived purely from a logical system, undermining the logicist program.[16]\n\nSome notable logicists include:\n\nMathematical formalism treats objects as symbols within a formal system. The focus is on the manipulation of these symbols according to specified rules, rather than on the objects themselves. One common understanding of formalism takes mathematics as not a body of propositions representing an abstract piece of reality but much more akin to a game, bringing with it no more ontological commitment of objects or properties than playing ludo or chess. In this view, mathematics is about the consistency of formal systems rather than the discovery of pre-existing objects. Some philosophers consider logicism to be a type of formalism.[19]\n\nSome notable formalists include:\n\nMathematical constructivism asserts that it is necessary to find (or \"construct\") a specific example of a mathematical object in order to prove that an example exists. Contrastingly, in classical mathematics, one can prove the existence of a mathematical object without \"finding\" that object explicitly, by assuming its non-existence and then deriving a contradiction from that assumption. Such a proof by contradiction might be called non-constructive, and a constructivist might reject it. The constructive viewpoint involves a verificational interpretation of the existential quantifier, which is at odds with its classical interpretation.[23] There are many forms of constructivism.[24] These include Brouwer's program of intutionism, the finitism of Hilbert and Bernays, the constructive recursive mathematics of mathematicians Shanin and Markov, and Bishop's program of constructive analysis.[25] Constructivism also includes the study of constructive set theories such as Constructive Zermelo–Fraenkel and the study of philosophy.\n\nSome notable constructivists include:\n\nStructuralism suggests that mathematical objects are defined by their place within a structure or system. The nature of a number, for example, is not tied to any particular thing, but to its role within the system of arithmetic. In a sense, the thesis is that mathematical objects (if there are such objects) simply have no intrinsic nature.[26][27]\n\nSome notable structuralists include:\n\nFrege famously distinguished between functions and objects.[30] According to his view, a function is a kind of ‘incomplete’ entity that maps arguments to values, and is denoted by an incomplete expression, whereas an object is a ‘complete’ entity and can be denoted by a singular term. Frege reduced properties and relations to functions and so these entities are not included among the objects. Some authors make use of Frege's notion of ‘object’ when discussing abstract objects.[31] But though Frege's sense of ‘object’ is important, it is not the only way to use the term. Other philosophers include properties and relations among the abstract objects. And when the background context for discussing objects is type theory, properties and relations of higher type (e.g., properties of properties, and properties of relations) may be all be considered ‘objects’. This latter use of ‘object’ is interchangeable with ‘entity.’ It is this more broad interpretation that mathematicians mean when they use the term 'object'.[32]\n\nCitations\n\nFurther reading\n"
    },
    {
        "title": "Abstraction (mathematics)",
        "content": "Abstraction in mathematics is the process of extracting the underlying structures, patterns or properties of a mathematical concept, removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena.[1][2][3] In other words, to be abstract is to remove context and application.[4] Two of the most highly abstract areas of modern mathematics are category theory and model theory.\n\nMany areas of mathematics began with the study of real world problems, before the underlying rules and concepts were identified and defined as abstract structures. For example, geometry has its origins in the calculation of distances and areas in the real world, and algebra started with methods of solving problems in arithmetic.\n\nAbstraction is an ongoing process in mathematics and the historical development of many mathematical topics exhibits a progression from the concrete to the abstract. For example, the first steps in the abstraction of geometry were historically made by the ancient Greeks, with Euclid's Elements being the earliest extant documentation of the axioms of plane geometry—though Proclus tells of an earlier axiomatisation by Hippocrates of Chios.[5] In the 17th century, Descartes introduced Cartesian co-ordinates which allowed the development of analytic geometry. Further steps in abstraction were taken by Lobachevsky, Bolyai, Riemann and Gauss, who generalised the concepts of geometry to develop non-Euclidean geometries. Later in the 19th century, mathematicians generalised geometry even further, developing such areas as geometry in n dimensions, projective geometry, affine geometry and finite geometry. Finally Felix Klein's \"Erlangen program\" identified the underlying theme of all of these geometries, defining each of them as the study of properties invariant under a given group of symmetries. This level of abstraction revealed connections between geometry and abstract algebra.[6]\n\nIn mathematics, abstraction can be advantageous in the following ways:\n\nOn the other hand, abstraction can also be disadvantageous in that highly abstract concepts can be difficult to learn.[7] A degree of mathematical maturity and experience may be needed for conceptual assimilation of abstractions. \n\nBertrand Russell, in The Scientific Outlook (1931), writes that \"Ordinary language is totally unsuited for expressing what physics really asserts, since the words of everyday life are not sufficiently abstract. Only mathematics and mathematical logic can say as little as the physicist means to say.\"[8]\n"
    },
    {
        "title": "Axiom",
        "content": "\n\nAn axiom, postulate, or assumption is a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments. The word comes from the Ancient Greek word ἀξίωμα (axíōma), meaning 'that which is thought worthy or fit' or 'that which commends itself as evident'.[1][2]\n\nThe precise definition varies across fields of study. In classic philosophy, an axiom is a statement that is so evident or well-established, that it is accepted without controversy or question.[3] In modern logic, an axiom is a premise or starting point for reasoning.[4]\n\nIn mathematics, an axiom may be a \"logical axiom\" or a \"non-logical axiom\". Logical axioms are taken to be true within the system of logic they define and are often shown in symbolic form (e.g., (A and B) implies A), while non-logical axioms are substantive assertions about the elements of the domain of a specific mathematical theory, for example a + 0 = a in integer arithmetic.\n\nNon-logical axioms may also be called \"postulates\", \"assumptions\" or \"proper axioms\".[5] In most cases, a non-logical axiom is simply a formal logical expression used in deduction to build a mathematical theory, and might or might not be self-evident in nature (e.g., the parallel postulate in Euclidean geometry). To axiomatize a system of knowledge is to show that its claims can be derived from a small, well-understood set of sentences (the axioms), and there are typically many ways to axiomatize a given mathematical domain.\n\nAny axiom is a statement that serves as a starting point from which other statements are logically derived. Whether it is meaningful (and, if so, what it means) for an axiom to be \"true\" is a subject of debate in the philosophy of mathematics.[6]\n\nThe word axiom comes from the Greek word ἀξίωμα (axíōma), a verbal noun from the verb ἀξιόειν (axioein), meaning \"to deem worthy\", but also \"to require\", which in turn comes from ἄξιος (áxios), meaning \"being in balance\", and hence \"having (the same) value (as)\", \"worthy\", \"proper\". Among the ancient Greek philosophers and mathematicians, axioms were taken to be immediately evident propositions, foundational and common to many fields of investigation, and self-evidently true without any further argument or proof.[7]\n\nThe root meaning of the word postulate is to \"demand\"; for instance, Euclid demands that one agree that some things can be done (e.g., any two points can be joined by a straight line).[8]\n\nAncient geometers maintained some distinction between axioms and postulates. While commenting on Euclid's books, Proclus remarks that \"Geminus held that this [4th] Postulate should not be classed as a postulate but as an axiom, since it does not, like the first three Postulates, assert the possibility of some construction but expresses an essential property.\"[9] Boethius translated 'postulate' as petitio and called the axioms  notiones communes but in later manuscripts this usage was not always strictly kept.[citation needed]\n\nThe logico-deductive method whereby conclusions (new knowledge) follow from premises (old knowledge) through the application of sound arguments (syllogisms, rules of inference) was developed by the ancient Greeks, and has become the core principle of modern mathematics. Tautologies excluded, nothing can be deduced if nothing is assumed. Axioms and postulates are thus the basic assumptions underlying a given body of deductive knowledge. They are accepted without demonstration. All other assertions (theorems, in the case of mathematics) must be proven with the aid of these basic assumptions. However, the interpretation of mathematical knowledge has changed from ancient times to the modern, and consequently the terms axiom and postulate hold a slightly different meaning for the present day mathematician, than they did for Aristotle and Euclid.[7]\n\nThe ancient Greeks considered geometry as just one of several sciences, and held the theorems of geometry on par with scientific facts. As such, they developed and used the logico-deductive method as a means of avoiding error, and for structuring and communicating knowledge. Aristotle's posterior analytics is a definitive exposition of the classical view.[10]\n\nAn \"axiom\", in classical terminology, referred to a self-evident assumption common to many branches of science. A good example would be the assertion that:\n\nWhen an equal amount is taken from equals, an equal amount results.\nAt the foundation of the various sciences lay certain additional hypotheses that were accepted without proof. Such a hypothesis was termed a postulate. While the axioms were common to many sciences, the postulates of each particular science were different. Their validity had to be established by means of real-world experience. Aristotle warns that the content of a science cannot be successfully communicated if the learner is in doubt about the truth of the postulates.[11]\n\nThe classical approach is well-illustrated[a] by Euclid's Elements, where a list of postulates is given (common-sensical geometric facts drawn from our experience), followed by a list of \"common notions\" (very basic, self-evident assertions).\n\nA lesson learned by mathematics in the last 150 years is that it is useful to strip the meaning away from the mathematical assertions (axioms, postulates, propositions, theorems) and definitions. One must concede the need for primitive notions, or undefined terms or concepts, in any study. Such abstraction or formalization makes mathematical knowledge more general, capable of multiple different meanings, and therefore useful in multiple contexts. Alessandro Padoa, Mario Pieri, and Giuseppe Peano were pioneers in this movement.\n\nStructuralist mathematics goes further, and develops theories and axioms (e.g. field theory, group theory, topology, vector spaces) without any particular application in mind. The distinction between an \"axiom\" and a \"postulate\" disappears. The postulates of Euclid are profitably motivated by saying that they lead to a great wealth of geometric facts. The truth of these complicated facts rests on the acceptance of the basic hypotheses. However, by throwing out Euclid's fifth postulate, one can get theories that have meaning in wider contexts (e.g., hyperbolic geometry). As such, one must simply be prepared to use labels such as \"line\" and \"parallel\" with greater flexibility. The development of hyperbolic geometry taught mathematicians that it is useful to regard postulates as purely formal statements, and not as facts based on experience.\n\nWhen mathematicians employ the field axioms, the intentions are even more abstract. The propositions of field theory do not concern any one particular application; the mathematician now works in complete abstraction. There are many examples of fields; field theory gives correct knowledge about them all.\n\nIt is not correct to say that the axioms of field theory are \"propositions that are regarded as true without proof.\" Rather, the field axioms are a set of constraints. If any given system of addition and multiplication satisfies these constraints, then one is in a position to instantly know a great deal of extra information about this system.\n\nModern mathematics formalizes its foundations to such an extent that mathematical theories can be regarded as mathematical objects, and mathematics itself can be regarded as a branch of logic. Frege, Russell, Poincaré, Hilbert, and Gödel are some of the key figures in this development.\n\nAnother lesson learned in modern mathematics is to examine purported proofs carefully for hidden assumptions.\n\nIn the modern understanding, a set of axioms is any collection of formally stated assertions from which other formally stated assertions follow – by the application of certain well-defined rules. In this view, logic becomes just another formal system. A set of axioms should be consistent; it should be impossible to derive a contradiction from the axioms. A set of axioms should also be non-redundant; an assertion that can be deduced from other axioms need not be regarded as an axiom.\n\nIt was the early hope of modern logicians that various branches of mathematics, perhaps all of mathematics, could be derived from a consistent collection of basic axioms. An early success of the formalist program was Hilbert's formalization[b] of Euclidean geometry,[12] and the related demonstration of the consistency of those axioms.\n\nIn a wider context, there was an attempt to base all of mathematics on Cantor's set theory. Here, the emergence of Russell's paradox and similar antinomies of naïve set theory raised the possibility that any such system could turn out to be inconsistent.\n\nThe formalist project suffered a setback a century ago, when Gödel showed that it is possible, for any sufficiently large set of axioms (Peano's axioms, for example) to construct a statement whose truth is independent of that set of axioms. As a corollary, Gödel proved that the consistency of a theory like Peano arithmetic is an unprovable assertion within the scope of that theory.[13]\n\nIt is reasonable to believe in the consistency of Peano arithmetic because it is satisfied by the system of natural numbers, an infinite but intuitively accessible formal system. However, at present, there is no known way of demonstrating the consistency of the modern Zermelo–Fraenkel axioms for set theory. Furthermore, using techniques of forcing (Cohen) one can show that the continuum hypothesis (Cantor) is independent of the Zermelo–Fraenkel axioms.[14] Thus, even this very general set of axioms cannot be regarded as the definitive foundation for mathematics.\n\nExperimental sciences - as opposed to mathematics and logic - also have general founding assertions from which a deductive reasoning can be built so as to express propositions that predict properties - either still general or much more specialized to a specific experimental context. For instance, Newton's laws in classical mechanics, Maxwell's equations in classical electromagnetism, Einstein's equation in general relativity, Mendel's laws of genetics, Darwin's Natural selection law, etc. These founding assertions are usually called principles or postulates so as to distinguish from mathematical axioms.\n\nAs a matter of facts, the role of axioms in mathematics and postulates in experimental sciences is different. In mathematics one neither \"proves\" nor \"disproves\" an axiom. A set of mathematical axioms gives a set of rules that fix a conceptual realm, in which the theorems logically follow. In contrast, in experimental sciences, a set of postulates shall allow deducing results that match or do not match experimental results. If postulates do not allow deducing experimental predictions, they do not set a scientific conceptual framework and have to be completed or made more accurate. If the postulates allow deducing predictions of experimental results, the comparison with experiments allows falsifying (falsified) the theory that the postulates install. A theory is considered valid as long as it has not been falsified.\n\nNow, the transition between the mathematical axioms and  scientific postulates is always slightly blurred, especially in physics. This is due to the heavy use of mathematical tools to support the physical theories. For instance, the introduction of Newton's laws rarely establishes as a prerequisite neither Euclidean geometry or differential calculus that they imply. It became more apparent when Albert Einstein first introduced special relativity where the invariant quantity is no more the Euclidean length \n\n\n\nl\n\n\n{\\displaystyle l}\n\n (defined as \n\n\n\n\nl\n\n2\n\n\n=\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n+\n\nz\n\n2\n\n\n\n\n{\\displaystyle l^{2}=x^{2}+y^{2}+z^{2}}\n\n) > but the Minkowski spacetime interval \n\n\n\ns\n\n\n{\\displaystyle s}\n\n (defined as \n\n\n\n\ns\n\n2\n\n\n=\n\nc\n\n2\n\n\n\nt\n\n2\n\n\n−\n\nx\n\n2\n\n\n−\n\ny\n\n2\n\n\n−\n\nz\n\n2\n\n\n\n\n{\\displaystyle s^{2}=c^{2}t^{2}-x^{2}-y^{2}-z^{2}}\n\n), and then general relativity where flat Minkowskian geometry is replaced with pseudo-Riemannian geometry on curved manifolds.\n\nIn quantum physics, two sets of postulates have coexisted for some time, which provide a very nice example of falsification. The 'Copenhagen school' (Niels Bohr, Werner Heisenberg, Max Born) developed an operational approach with a complete mathematical formalism that involves the description of quantum system by vectors ('states') in a separable Hilbert space, and physical quantities as linear operators that act in this Hilbert space. This approach is fully falsifiable and has so far produced the most accurate predictions in physics. But it has the unsatisfactory aspect of not allowing answers to questions one would naturally ask. For this reason, another 'hidden variables' approach was developed for some time by Albert Einstein, Erwin Schrödinger, David Bohm. It was created so as to try to give deterministic explanation to phenomena such as entanglement. This approach assumed that the Copenhagen school description was not complete, and postulated that some yet unknown variable was to be added to the theory so as to allow answering some of the questions it does not answer (the founding elements of which were discussed as the EPR paradox in 1935). Taking this idea seriously, John Bell derived in 1964 a prediction that would lead to different experimental results (Bell's inequalities) in the Copenhagen and the Hidden variable case. The experiment was conducted first by Alain Aspect in the early 1980s, and the result excluded the simple hidden variable approach (sophisticated hidden variables could still exist but their properties would still be more disturbing than the problems they try to solve). This does not mean that the conceptual framework of quantum physics can be considered as complete now, since some open questions still exist (the limit between the quantum and classical realms, what happens during a quantum measurement, what happens in a completely closed quantum system such as the universe itself, etc.).\n\nIn the field of mathematical logic, a clear distinction is made between two notions of axioms: logical and non-logical (somewhat similar to the ancient distinction between \"axioms\" and \"postulates\" respectively).\n\nThese are certain formulas in a formal language that are universally valid, that is, formulas that are satisfied by every assignment of values.  Usually one takes as logical axioms at least some minimal set of tautologies that is sufficient for proving all tautologies in the language; in the case of predicate logic more logical axioms than that are required, in order to prove logical truths that are not tautologies in the strict sense.\n\nIn propositional logic, it is common to take as logical axioms all formulae of the following forms, where \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n, \n\n\n\nχ\n\n\n{\\displaystyle \\chi }\n\n, and \n\n\n\nψ\n\n\n{\\displaystyle \\psi }\n\n can be any formulae of the language and where the included primitive connectives are only \"\n\n\n\n¬\n\n\n{\\displaystyle \\neg }\n\n\" for negation of the immediately following proposition and \"\n\n\n\n→\n\n\n{\\displaystyle \\to }\n\n\" for implication from antecedent to consequent propositions:\n\nEach of these patterns is an axiom schema, a rule for generating an infinite number of axioms.  For example, if \n\n\n\nA\n\n\n{\\displaystyle A}\n\n, \n\n\n\nB\n\n\n{\\displaystyle B}\n\n, and \n\n\n\nC\n\n\n{\\displaystyle C}\n\n are propositional variables, then \n\n\n\nA\n→\n(\nB\n→\nA\n)\n\n\n{\\displaystyle A\\to (B\\to A)}\n\n and \n\n\n\n(\nA\n→\n¬\nB\n)\n→\n(\nC\n→\n(\nA\n→\n¬\nB\n)\n)\n\n\n{\\displaystyle (A\\to \\lnot B)\\to (C\\to (A\\to \\lnot B))}\n\n are both instances of axiom schema 1, and hence are axioms.  It can be shown that with only these three axiom schemata and modus ponens, one can prove all tautologies of the propositional calculus.  It can also be shown that no pair of these schemata is sufficient for proving all tautologies with modus ponens.\n\nOther axiom schemata involving the same or different sets of primitive connectives can be alternatively constructed.[15]\n\nThese axiom schemata are also used in the predicate calculus, but additional logical axioms are needed to include a quantifier in the calculus.[16]\n\nAxiom of Equality.Let \n\n\n\n\n\nL\n\n\n\n\n{\\displaystyle {\\mathfrak {L}}}\n\n be a first-order language. For each variable \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, the below formula is universally valid.\n\n\n\n\n\nx\n=\nx\n\n\n{\\displaystyle x=x}\n\n\n\nThis means that, for any variable symbol \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, the formula \n\n\n\nx\n=\nx\n\n\n{\\displaystyle x=x}\n\n can be regarded as an axiom. Additionally, in this example, for this not to fall into vagueness and a never-ending series of \"primitive notions\", either a precise notion of what we mean by \n\n\n\nx\n=\nx\n\n\n{\\displaystyle x=x}\n\n (or, for that matter, \"to be equal\") has to be well established first, or a purely formal and syntactical usage of the symbol \n\n\n\n=\n\n\n{\\displaystyle =}\n\n  has to be enforced, only regarding it as a string and only a string of symbols, and mathematical logic does indeed do that.\n\nAnother, more interesting example axiom scheme, is that which provides us with what is known as Universal Instantiation:\n\nAxiom scheme for Universal Instantiation.Given a formula \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n in a first-order language \n\n\n\n\n\nL\n\n\n\n\n{\\displaystyle {\\mathfrak {L}}}\n\n, a variable \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and a term \n\n\n\nt\n\n\n{\\displaystyle t}\n\n that is substitutable for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n in \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n, the below formula is universally valid.\n\n\n\n\n\n∀\nx\n\nϕ\n→\n\nϕ\n\nt\n\n\nx\n\n\n\n\n{\\displaystyle \\forall x\\,\\phi \\to \\phi _{t}^{x}}\n\n\n\nWhere the symbol \n\n\n\n\nϕ\n\nt\n\n\nx\n\n\n\n\n{\\displaystyle \\phi _{t}^{x}}\n\n stands for the formula \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n with the term \n\n\n\nt\n\n\n{\\displaystyle t}\n\n substituted for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. (See Substitution of variables.) In informal terms, this example allows us to state that, if we know that a certain property \n\n\n\nP\n\n\n{\\displaystyle P}\n\n holds for every \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and that \n\n\n\nt\n\n\n{\\displaystyle t}\n\n stands for a particular object in our structure, then we should be able to claim \n\n\n\nP\n(\nt\n)\n\n\n{\\displaystyle P(t)}\n\n. Again, we are claiming that the formula \n\n\n\n∀\nx\nϕ\n→\n\nϕ\n\nt\n\n\nx\n\n\n\n\n{\\displaystyle \\forall x\\phi \\to \\phi _{t}^{x}}\n\n is valid, that is, we must be able to give a \"proof\" of this fact, or more properly speaking, a metaproof.  These examples are metatheorems of our theory of mathematical logic since we are dealing with the very concept of proof itself. Aside from this, we can also have Existential Generalization:\n\nAxiom scheme for Existential Generalization. Given a formula \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n in a first-order language \n\n\n\n\n\nL\n\n\n\n\n{\\displaystyle {\\mathfrak {L}}}\n\n, a variable \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and a term \n\n\n\nt\n\n\n{\\displaystyle t}\n\n that is substitutable for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n in \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n, the below formula is universally valid.\n\n\n\n\n\n\nϕ\n\nt\n\n\nx\n\n\n→\n∃\nx\n\nϕ\n\n\n{\\displaystyle \\phi _{t}^{x}\\to \\exists x\\,\\phi }\n\n\n\nNon-logical axioms are formulas that play the role of theory-specific assumptions. Reasoning about two different structures, for example, the natural numbers and the integers, may involve the same logical axioms; the non-logical axioms aim to capture what is special about a particular structure (or set of structures, such as groups). Thus non-logical axioms, unlike logical axioms, are not tautologies. Another name for a non-logical axiom is postulate.[5]\n\nAlmost every modern mathematical theory starts from a given set of non-logical axioms, and it was thought that, in principle, every theory could be axiomatized in this way and formalized down to the bare language of logical formulas.[citation needed][further explanation needed]\n\nNon-logical axioms are often simply referred to as axioms in mathematical discourse. This does not mean that it is claimed that they are true in some absolute sense. For instance, in some groups, the group operation is commutative, and this can be asserted with the introduction of an additional axiom, but without this axiom, we can do quite well developing (the more general) group theory, and we can even take its negation as an axiom for the study of non-commutative groups.\n\nThis section gives examples of mathematical theories that are developed entirely from a set of non-logical axioms (axioms, henceforth). A rigorous treatment of any of these topics begins with a specification of these axioms.\n\nBasic theories, such as arithmetic, real analysis and complex analysis are often introduced non-axiomatically, but implicitly or explicitly there is generally an assumption that the axioms being used are the axioms of Zermelo–Fraenkel set theory with choice, abbreviated ZFC, or some very similar system of axiomatic set theory like Von Neumann–Bernays–Gödel set theory, a conservative extension of ZFC. Sometimes slightly stronger theories such as Morse–Kelley set theory or set theory with a strongly inaccessible cardinal allowing the use of a Grothendieck universe is used, but in fact, most mathematicians can actually prove all they need in systems weaker than ZFC, such as second-order arithmetic.[citation needed]\n\nThe study of topology in mathematics extends all over through point set topology, algebraic topology, differential topology, and all the related paraphernalia, such as homology theory, homotopy theory. The development of abstract algebra brought with itself group theory, rings, fields, and Galois theory.\n\nThis list could be expanded to include most fields of mathematics, including measure theory, ergodic theory, probability, representation theory, and differential geometry.\n\nThe Peano axioms are the most widely used axiomatization of first-order arithmetic. They are a set of axioms strong enough to prove many important facts about number theory and they allowed Gödel to establish his famous second incompleteness theorem.[17]\n\nWe have a language \n\n\n\n\n\n\nL\n\n\n\nN\nT\n\n\n=\n{\n0\n,\nS\n}\n\n\n{\\displaystyle {\\mathfrak {L}}_{NT}=\\{0,S\\}}\n\n where \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n is a constant symbol and \n\n\n\nS\n\n\n{\\displaystyle S}\n\n is a unary function and the following axioms:\n\nThe standard structure is \n\n\n\n\n\nN\n\n\n=\n⟨\n\nN\n\n,\n0\n,\nS\n⟩\n\n\n{\\displaystyle {\\mathfrak {N}}=\\langle \\mathbb {N} ,0,S\\rangle }\n\n where \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n is the set of natural numbers, \n\n\n\nS\n\n\n{\\displaystyle S}\n\n is the successor function and \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n is naturally interpreted as the number 0.\n\nProbably the oldest, and most famous, list of axioms are the 4 + 1 Euclid's postulates of plane geometry. The axioms are referred to as \"4 + 1\" because for nearly two millennia the fifth (parallel) postulate (\"through a point outside a line there is exactly one parallel\") was suspected of being derivable from the first four. Ultimately, the fifth postulate was found to be independent of the first four. One can assume that exactly one parallel through a point outside a line exists, or that infinitely many exist. This choice gives us two alternative forms of geometry in which the interior angles of a triangle add up to exactly 180 degrees or less, respectively, and are known as Euclidean and hyperbolic geometries. If one also removes the second postulate (\"a line can be extended indefinitely\") then elliptic geometry arises, where there is no parallel through a point outside a line, and in which the interior angles of a triangle add up to more than 180 degrees.\n\nThe objectives of the study are within the domain of real numbers. The real numbers are uniquely picked out (up to isomorphism) by the properties of a Dedekind complete ordered field, meaning that any nonempty set of real numbers with an upper bound has a least upper bound. However, expressing these properties as axioms requires the use of second-order logic. The Löwenheim–Skolem theorems tell us that if we restrict ourselves to first-order logic, any axiom system for the reals admits other models, including both models that are smaller than the reals and models that are larger. Some of the latter are studied in non-standard analysis.\n\nA deductive system consists of a set \n\n\n\nΛ\n\n\n{\\displaystyle \\Lambda }\n\n of logical axioms, a set \n\n\n\nΣ\n\n\n{\\displaystyle \\Sigma }\n\n of non-logical axioms, and a set \n\n\n\n{\n(\nΓ\n,\nϕ\n)\n}\n\n\n{\\displaystyle \\{(\\Gamma ,\\phi )\\}}\n\n of rules of inference.  A desirable property of a deductive system is that it be complete.  A system is said to be complete if, for all formulas \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n,\n\n\n\n\n\n\nif \n\nΣ\n⊨\nϕ\n\n then \n\nΣ\n⊢\nϕ\n\n\n{\\displaystyle {\\text{if }}\\Sigma \\models \\phi {\\text{ then }}\\Sigma \\vdash \\phi }\n\n\n\nthat is, for any statement that is a logical consequence of \n\n\n\nΣ\n\n\n{\\displaystyle \\Sigma }\n\n there actually exists a deduction of the statement from \n\n\n\nΣ\n\n\n{\\displaystyle \\Sigma }\n\n.  This is sometimes expressed as \"everything that is true is provable\", but it must be understood that \"true\" here means \"made true by the set of axioms\", and not, for example, \"true in the intended interpretation\". Gödel's completeness theorem establishes the completeness of a certain commonly used type of deductive system.\n\nNote that \"completeness\" has a different meaning here than it does in the context of Gödel's first incompleteness theorem, which states that no recursive, consistent set of non-logical axioms \n\n\n\nΣ\n\n\n{\\displaystyle \\Sigma }\n\n of the Theory of Arithmetic is complete, in the sense that there will always exist an arithmetic statement \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n such that neither \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n nor \n\n\n\n¬\nϕ\n\n\n{\\displaystyle \\lnot \\phi }\n\n can be proved from the given set of axioms.\n\nThere is thus, on the one hand, the notion of completeness of a deductive system and on the other hand that of completeness of a set of non-logical axioms.  The completeness theorem and the incompleteness theorem, despite their names, do not contradict one another.\n\nEarly mathematicians regarded axiomatic geometry as a model of physical space, implying, there could ultimately only be one such model. The idea that alternative mathematical systems might exist was very troubling to mathematicians of the 19th century and the developers of systems such as Boolean algebra made elaborate efforts to derive them from traditional arithmetic. Galois showed just before his untimely death that these efforts were largely wasted. Ultimately, the abstract parallels between algebraic systems were seen to be more important than the details, and modern algebra was born.  In the modern view, axioms may be any set of formulas, as long as they are not known to be inconsistent.\n"
    },
    {
        "title": "Reason",
        "content": "Reason is the capacity of consciously applying logic by drawing valid conclusions from new or existing information, with the aim of seeking the truth.[1] It is associated with such characteristically human activities as philosophy, religion, science, language, mathematics, and art, and is normally considered to be a distinguishing ability possessed by humans.[2][3] Reason is sometimes referred to as rationality.[4]\n\nReasoning involves using more-or-less rational processes of thinking and cognition to extrapolate from one's existing knowledge to generate new knowledge, and involves the use of one's intellect. The field of logic studies the ways in which humans can use formal reasoning to produce logically valid arguments and true conclusions.[5] Reasoning may be subdivided into forms of logical reasoning, such as deductive reasoning, inductive reasoning, and abductive reasoning.\n\nAristotle drew a distinction between logical discursive reasoning (reason proper), and intuitive reasoning,[6]: VI.7  in which the reasoning process through intuition—however valid—may tend toward the personal and the subjectively opaque. In some social and political settings logical and intuitive modes of reasoning may clash, while in other contexts intuition and formal reason are seen as complementary rather than adversarial. For example, in mathematics, intuition is often necessary for the creative processes involved with arriving at a formal proof, arguably the most difficult of formal reasoning tasks.\n\nReasoning, like habit or intuition, is one of the ways by which thinking moves from one idea to a related idea. For example, reasoning is the means by which rational individuals understand the significance of sensory information from their environments, or conceptualize abstract dichotomies such as cause and effect, truth and falsehood, or good and evil. Reasoning, as a part of executive decision making, is also closely identified with the ability to self-consciously change, in terms of goals, beliefs, attitudes, traditions, and institutions, and therefore with the capacity for freedom and self-determination.[7]\n\nPsychologists and cognitive scientists have attempted to study and explain how people reason, e.g. which cognitive and neural processes are engaged, and how cultural factors affect the inferences that people draw. The field of automated reasoning studies how reasoning may or may not be modeled computationally. Animal psychology considers the question of whether animals other than humans can reason.\n\nIn the English language and other modern European languages, \"reason\", and related words, represent words which have always been used to translate Latin and classical Greek terms in their philosophical sense.\n\nThe earliest major philosophers to publish in English, such as Francis Bacon, Thomas Hobbes, and John Locke also routinely wrote in Latin and French, and compared their terms to Greek, treating the words \"logos\", \"ratio\", \"raison\" and \"reason\" as interchangeable. The meaning of the word \"reason\" in senses such as \"human reason\" also overlaps to a large extent with \"rationality\" and the adjective of \"reason\" in philosophical contexts is normally \"rational\", rather than \"reasoned\" or \"reasonable\".[11] Some philosophers, Hobbes for example, also used the word ratiocination as a synonym for \"reasoning\".\n\nIn contrast to the use of \"reason\" as an abstract noun, a reason is a consideration that either explains or justifies events, phenomena, or behavior.[10] Reasons justify decisions, reasons support explanations of natural phenomena, and reasons can be given to explain the actions (conduct) of individuals.\n\nThe words are connected in this way: using reason, or reasoning, means providing good reasons. For example, when evaluating a moral decision, \"morality is, at the very least, the effort to guide one's conduct by reason—that is, doing what there are the best reasons for doing—while giving equal [and impartial] weight to the interests of all those affected by what one does.\"[12]\n\nThe proposal that reason gives humanity a special position in nature has been argued[citation needed] to be a defining characteristic of western philosophy and later western science, starting with classical Greece. Philosophy can be described as a way of life based upon reason, while reason has been among the major subjects of philosophical discussion since ancient times. Reason is often said to be reflexive, or \"self-correcting\", and the critique of reason has been a persistent theme in philosophy.[13]\n\nFor many classical philosophers, nature was understood teleologically, meaning that every type of thing had a definitive purpose that fit within a natural order that was itself understood to have aims. Perhaps starting with Pythagoras or Heraclitus, the cosmos was even said to have reason.[14] Reason, by this account, is not just a characteristic that people happen to have. Reason was considered of higher stature than other characteristics of human nature, because it is something people share with nature itself, linking an apparently immortal part of the human mind with the divine order of the cosmos. Within the human mind or soul (psyche), reason was described by Plato as being the natural monarch which should rule over the other parts, such as spiritedness (thumos) and the passions. Aristotle, Plato's student, defined human beings as rational animals, emphasizing reason as a characteristic of human nature. He described the highest human happiness or well being (eudaimonia) as a life which is lived consistently, excellently, and completely in accordance with reason.[6]: I \n\nThe conclusions to be drawn from the discussions of Aristotle and Plato on this matter are amongst the most debated in the history of philosophy.[15] But teleological accounts such as Aristotle's were highly influential for those who attempt to explain reason in a way that is consistent with monotheism and the immortality and divinity of the human soul. For example, in the neoplatonist account of Plotinus, the cosmos has one soul, which is the seat of all reason, and the souls of all people are part of this soul. Reason is for Plotinus both the provider of form to material things, and the light which brings people's souls back into line with their source.[16]\n\nThe classical view of reason, like many important Neoplatonic and Stoic ideas, was readily adopted by the early Church[17] as the Church Fathers saw Greek Philosophy as an indispensable instrument given to mankind so that we may understand revelation.[18][verification needed] For example, the greatest among the early Church Fathers and Doctors of the Church such as Augustine of Hippo, Basil of Caesarea, and Gregory of Nyssa were as much Neoplatonic philosophers as they were Christian theologians, and they adopted the Neoplatonic view of human reason and its implications for our relationship to creation, to ourselves, and to God.\n\nThe Neoplatonic conception of the rational aspect of the human soul was widely adopted by medieval Islamic philosophers and continues to hold significance in Iranian philosophy.[15] As European intellectual life reemerged from the Dark Ages, the Christian Patristic tradition and the influence of esteemed Islamic scholars like Averroes and Avicenna contributed to the development of the Scholastic view of reason, which laid the foundation for our modern understanding of this concept.[19]\n\nAmong the Scholastics who relied on the classical concept of reason for the development of their doctrines, none were more influential than Saint Thomas Aquinas, who put this concept at the heart of his Natural Law. In this doctrine, Thomas concludes that because humans have reason and because reason is a spark of the divine, every single human life is invaluable, all humans are equal, and every human is born with an intrinsic and permanent set of basic rights.[20] On this foundation, the idea of human rights would later be constructed by Spanish theologians at the School of Salamanca.\n\nOther Scholastics, such as Roger Bacon and Albertus Magnus, following the example of Islamic scholars such as Alhazen, emphasised reason an intrinsic human ability to decode the created order and the structures that underlie our experienced physical reality. This interpretation of reason was instrumental to the development of the scientific method in the early Universities of the high Middle Ages.[21]\n\nThe early modern era was marked by a number of significant changes in the understanding of reason, starting in Europe. One of the most important of these changes involved a change in the metaphysical understanding of human beings. Scientists and philosophers began to question the teleological understanding of the world.[22] Nature was no longer assumed to be human-like, with its own aims or reason, and human nature was no longer assumed to work according to anything other than the same \"laws of nature\" which affect inanimate things. This new understanding eventually displaced the previous world view that derived from a spiritual understanding of the universe.\n\nAccordingly, in the 17th century, René Descartes explicitly rejected the traditional notion of humans as \"rational animals\", suggesting instead that they are nothing more than \"thinking things\" along the lines of other \"things\" in nature. Any grounds of knowledge outside that understanding was, therefore, subject to doubt.\n\nIn his search for a foundation of all possible knowledge, Descartes decided to throw into doubt all knowledge—except that of the mind itself in the process of thinking:\n\nAt this time I admit nothing that is not necessarily true. I am therefore precisely nothing but a thinking thing; that is a mind, or intellect, or understanding, or reason—words of whose meanings I was previously ignorant.[23]\nThis eventually became known as epistemological or \"subject-centred\" reason, because it is based on the knowing subject, who perceives the rest of the world and itself as a set of objects to be studied, and successfully mastered, by applying the knowledge accumulated through such study. Breaking with tradition and with many thinkers after him, Descartes explicitly did not divide the incorporeal soul into parts, such as reason and intellect, describing them instead as one indivisible incorporeal entity.\n\nA contemporary of Descartes, Thomas Hobbes described reason as a broader version of \"addition and subtraction\" which is not limited to numbers.[24] This understanding of reason is sometimes termed \"calculative\" reason. Similar to Descartes, Hobbes asserted that \"No discourse whatsoever, can end in absolute knowledge of fact, past, or to come\" but that \"sense and memory\" is absolute knowledge.[25]\n\nIn the late 17th century through the 18th century, John Locke and David Hume developed Descartes's line of thought still further. Hume took it in an especially skeptical direction, proposing that there could be no possibility of deducing relationships of cause and effect, and therefore no knowledge is based on reasoning alone, even if it seems otherwise.[26]\n\nHume famously remarked that, \"We speak not strictly and philosophically when we talk of the combat of passion and of reason. Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them.\"[27] Hume also took his definition of reason to unorthodox extremes by arguing, unlike his predecessors, that human reason is not qualitatively different from either simply conceiving individual ideas, or from judgments associating two ideas,[28] and that \"reason is nothing but a wonderful and unintelligible instinct in our souls, which carries us along a certain train of ideas, and endows them with particular qualities, according to their particular situations and relations.\"[29] It followed from this that animals have reason, only much less complex than human reason.\n\nIn the 18th century, Immanuel Kant attempted to show that Hume was wrong by demonstrating that a \"transcendental\" self, or \"I\", was a necessary condition of all experience. Therefore, suggested Kant, on the basis of such a self, it is in fact possible to reason both about the conditions and limits of human knowledge. And so long as these limits are respected, reason can be the vehicle of morality, justice, aesthetics, theories of knowledge (epistemology), and understanding.[citation needed][30]\n\nIn the formulation of Kant, who wrote some of the most influential modern treatises on the subject, the great achievement of reason (German: Vernunft) is that it is able to exercise a kind of universal law-making. Kant was able therefore to reformulate the basis of moral-practical, theoretical, and aesthetic reasoning on \"universal\" laws.\n\nHere, practical reasoning is the self-legislating or self-governing formulation of universal norms, and theoretical reasoning is the way humans posit universal laws of nature.[31]\n\nUnder practical reason, the moral autonomy or freedom of people depends on their ability, by the proper exercise of that reason, to behave according to laws that are given to them. This contrasted with earlier forms of morality, which depended on religious understanding and interpretation, or on nature, for their substance.[32]\n\nAccording to Kant, in a free society each individual must be able to pursue their goals however they see fit, as long as their actions conform to principles given by reason. He formulated such a principle, called the \"categorical imperative\", which would justify an action only if it could be universalized:\n\nAct only according to that maxim whereby you can, at the same time, will that it should become a universal law.[33]\nIn contrast to Hume, Kant insisted that reason itself (German Vernunft) could be used to find solutions to metaphysical problems, especially the discovery of the foundations of morality. Kant claimed that these solutions could be found with his \"transcendental logic\", which unlike normal logic is not just an instrument that can be used indifferently, as it was for Aristotle, but a theoretical science in its own right and the basis of all the others.[34]\n\nAccording to Jürgen Habermas, the \"substantive unity\" of reason has dissolved in modern times, such that it can no longer answer the question \"How should I live?\" Instead, the unity of reason has to be strictly formal, or \"procedural\". He thus described reason as a group of three autonomous spheres (on the model of Kant's three critiques):\n\nFor Habermas, these three spheres are the domain of experts, and therefore need to be mediated with the \"lifeworld\" by philosophers. In drawing such a picture of reason, Habermas hoped to demonstrate that the substantive unity of reason, which in pre-modern societies had been able to answer questions about the good life, could be made up for by the unity of reason's formalizable procedures.[35]\n\nHamann, Herder, Kant, Hegel, Kierkegaard, Nietzsche, Heidegger, Foucault, Rorty, and many other philosophers have contributed to a debate about what reason means, or ought to mean. Some, like Kierkegaard, Nietzsche, and Rorty, are skeptical about subject-centred, universal, or instrumental reason, and even skeptical toward reason as a whole. Others, including Hegel, believe that it has obscured the importance of intersubjectivity, or \"spirit\" in human life, and they attempt to reconstruct a model of what reason should be.\n\nSome thinkers, e.g. Foucault, believe there are other forms of reason, neglected but essential to modern life, and to our understanding of what it means to live a life according to reason.[13] Others suggest that there is not just one reason or rationality, but multiple possible systems of reason or rationality which may conflict (in which case there is no super-rational system one can appeal to in order to resolve the conflict).[36]\n\nIn the last several decades, a number of proposals have been made to \"re-orient\" this critique of reason, or to recognize the \"other voices\" or \"new departments\" of reason:\n\nFor example, in opposition to subject-centred reason, Habermas has proposed a model of communicative reason that sees it as an essentially cooperative activity, based on the fact of linguistic intersubjectivity.[37]\n\nNikolas Kompridis proposed a widely encompassing view of reason as \"that ensemble of practices that contributes to the opening and preserving of openness\" in human affairs, and a focus on reason's possibilities for social change.[38]\n\nThe philosopher Charles Taylor, influenced by the 20th century German philosopher Martin Heidegger, proposed that reason ought to include the faculty of disclosure, which is tied to the way we make sense of things in everyday life, as a new \"department\" of reason.[39]\n\nIn the essay \"What is Enlightenment?\", Michel Foucault proposed a critique based on Kant's distinction between \"private\" and \"public\" uses of reason:[40]\n\nThe terms logic or logical are sometimes used as if they were identical with reason or rational, or sometimes logic is seen as the most pure or the defining form of reason: \"Logic is about reasoning—about going from premises to a conclusion. ... When you do logic, you try to clarify reasoning and separate good from bad reasoning.\"[41] In modern economics, rational choice is assumed to equate to logically consistent choice.[42]\n\nHowever, reason and logic can be thought of as distinct—although logic is one important aspect of reason. Author Douglas Hofstadter, in Gödel, Escher, Bach, characterizes the distinction in this way: Logic is done inside a system while reason is done outside the system by such methods as skipping steps, working backward, drawing diagrams, looking at examples, or seeing what happens if you change the rules of the system.[43] Psychologists Mark H. Bickard and Robert L. Campbell argue that \"rationality cannot be simply assimilated to logicality\"; they note that \"human knowledge of logic and logical systems has developed\" over time through reasoning, and logical systems \"can't construct new logical systems more powerful than themselves\", so reasoning and rationality must involve more than a system of logic.[44][45] Psychologist David Moshman, citing Bickhard and Campbell, argues for a \"metacognitive conception of rationality\" in which a person's development of reason \"involves increasing consciousness and control of logical and other inferences\".[45][46]\n\nReason is a type of thought, and logic involves the attempt to describe a system of formal rules or norms of appropriate reasoning.[45] The oldest surviving writing to explicitly consider the rules by which reason operates are the works of the Greek philosopher Aristotle, especially Prior Analytics and Posterior Analytics.[47][non-primary source needed] Although the Ancient Greeks had no separate word for logic as distinct from language and reason, Aristotle's newly coined word \"syllogism\" (syllogismos) identified logic clearly for the first time as a distinct field of study.[48] When Aristotle referred to \"the logical\" (hē logikē), he was referring more broadly to rational thought.[49]\n\nAs pointed out by philosophers such as Hobbes, Locke, and Hume, some animals are also clearly capable of a type of \"associative thinking\", even to the extent of associating causes and effects. A dog once kicked, can learn how to recognize the warning signs and avoid being kicked in the future, but this does not mean the dog has reason in any strict sense of the word. It also does not mean that humans acting on the basis of experience or habit are using their reason.[29]\n\nHuman reason requires more than being able to associate two ideas—even if those two ideas might be described by a reasoning human as a cause and an effect—perceptions of smoke, for example, and memories of fire. For reason to be involved, the association of smoke and the fire would have to be thought through in a way that can be explained, for example as cause and effect. In the explanation of Locke, for example, reason requires the mental use of a third idea in order to make this comparison by use of syllogism.[50]\n\nMore generally, according to Charles Sanders Peirce, reason in the strict sense requires the ability to create and manipulate a system of symbols, as well as indices and icons, the symbols having only a nominal, though habitual, connection to either (for example) smoke or fire.[51] One example of such a system of symbols and signs is language.\n\nThe connection of reason to symbolic thinking has been expressed in different ways by philosophers. Thomas Hobbes described the creation of \"Markes, or Notes of remembrance\" as speech.[52] He used the word speech as an English version of the Greek word logos so that speech did not need to be communicated.[53] When communicated, such speech becomes language, and the marks or notes or remembrance are called \"Signes\" by Hobbes. Going further back, although Aristotle is a source of the idea that only humans have reason (logos), he does mention that animals with imagination, for whom sense perceptions can persist, come closest to having something like reasoning and nous, and even uses the word \"logos\" in one place to describe the distinctions which animals can perceive in such cases.[54]\n\nReason and imagination rely on similar mental processes.[55] Imagination is not only found in humans. Aristotle asserted that phantasia (imagination: that which can hold images or phantasmata) and phronein (a type of thinking that can judge and understand in some sense) also exist in some animals.[56] According to him, both are related to the primary perceptive ability of animals, which gathers the perceptions of different senses and defines the order of the things that are perceived without distinguishing universals, and without deliberation or logos. But this is not yet reason, because human imagination is different.\n\nTerrence Deacon and Merlin Donald, writing about the origin of language, connect reason not only to language, but also mimesis.[57] They describe the ability to create language as part of an internal modeling of reality, and specific to humankind. Other results are consciousness, and imagination or fantasy. In contrast, modern proponents of a genetic predisposition to language itself include Noam Chomsky and Steven Pinker.[clarification needed]\n\nIf reason is symbolic thinking, and peculiarly human, then this implies that humans have a special ability to maintain a clear consciousness of the distinctness of \"icons\" or images and the real things they represent. Merlin Donald writes:[58]: 172 \n\nA dog might perceive the \"meaning\" of a fight that was realistically play-acted by humans, but it could not reconstruct the message or distinguish the representation from its referent (a real fight).... Trained apes are able to make this distinction; young children make this distinction early—hence, their effortless distinction between play-acting an event and the event itself\nIn classical descriptions, an equivalent description of this mental faculty is eikasia, in the philosophy of Plato.[59]: Ch.5  This is the ability to perceive whether a perception is an image of something else, related somehow but not the same, and therefore allows humans to perceive that a dream or memory or a reflection in a mirror is not reality as such. What Klein refers to as dianoetic eikasia is the eikasia concerned specifically with thinking and mental images, such as those mental symbols, icons, signes, and marks discussed above as definitive of reason. Explaining reason from this direction: human thinking is special in that we often understand visible things as if they were themselves images of our intelligible \"objects of thought\" as \"foundations\" (hypothēses in Ancient Greek). This thinking (dianoia) is \"...an activity which consists in making the vast and diffuse jungle of the visible world depend on a plurality of more 'precise' noēta\".[59]: 122 \n\nBoth Merlin Donald and the Socratic authors such as Plato and Aristotle emphasize the importance of mimēsis, often translated as imitation or representation. Donald writes:[58]: 169 \n\nImitation is found especially in monkeys and apes [...but...] Mimesis is fundamentally different from imitation and mimicry in that it involves the invention of intentional representations.... Mimesis is not absolutely tied to external communication.\nMimēsis is a concept, now popular again in academic discussion, that was particularly prevalent in Plato's works. In Aristotle, it is discussed mainly in the Poetics. In Michael Davis's account of the theory of man in that work:[60]\n\nIt is the distinctive feature of human action, that whenever we choose what we do, we imagine an action for ourselves as though we were inspecting it from the outside. Intentions are nothing more than imagined actions, internalizings of the external. All action is therefore imitation of action; it is poetic...[61]\nDonald, like Plato (and Aristotle, especially in On Memory and Recollection), emphasizes the peculiarity in humans of voluntary initiation of a search through one's mental world. The ancient Greek anamnēsis, normally translated as \"recollection\" was opposed to mneme or \"memory\". Memory, shared with some animals,[62] requires a consciousness not only of what happened in the past, but also that something happened in the past, which is in other words a kind of eikasia[59]: 109  \"...but nothing except man is able to recollect.\"[63] Recollection is a deliberate effort to search for and recapture something once known. Klein writes that, \"To become aware of our having forgotten something means to begin recollecting.\"[59]: 112  Donald calls the same thing autocueing, which he explains as follows:[58]: 173 [64] \"Mimetic acts are reproducible on the basis of internal, self-generated cues. This permits voluntary recall of mimetic representations, without the aid of external cues—probably the earliest form of representational thinking.\"\n\nIn a celebrated paper, the fantasy author and philologist J.R.R. Tolkien wrote in his essay \"On Fairy Stories\" that the terms \"fantasy\" and \"enchantment\" are connected to not only \"the satisfaction of certain primordial human desires\" but also \"the origin of language and of the mind\".[This quote needs a citation]\n\nA subdivision of philosophy and a variety of reasoning is logic. The traditional main division made in philosophy is between deductive reasoning and inductive reasoning. Formal logic has been described as the science of deduction.[65] The study of inductive reasoning is generally carried out within the field known as informal logic or critical thinking.\n\nDeduction is a form of reasoning in which a conclusion follows necessarily from the stated premises. A deduction is also the name for the conclusion reached by a deductive reasoning process. A classic example of deductive reasoning is evident in syllogisms like the following:\n\nThe reasoning in this argument is deductively valid because there is no way in which both premises could be true and the conclusion be false.\n\nInduction is a form of inference that produces properties or relations about unobserved objects or types based on previous observations or experiences, or that formulates general statements or laws based on limited observations of recurring phenomenal patterns.\n\nInductive reasoning contrasts with deductive reasoning in that, even in the strongest cases of inductive reasoning, the truth of the premises does not guarantee the truth of the conclusion. Instead, the conclusion of an inductive argument follows with some degree of probability. For this reason also, the conclusion of an inductive argument contains more information than is already contained in the premises. Thus, this method of reasoning is ampliative.\n\nA classic example of inductive reasoning comes from the empiricist David Hume:\n\nAnalogical reasoning is a form of inductive reasoning from a particular to a particular. It is often used in case-based reasoning, especially legal reasoning.[66] An example follows:\n\nAnalogical reasoning is a weaker form of inductive reasoning from a single example, because inductive reasoning typically uses a large number of examples to reason from the particular to the general.[67] Analogical reasoning often leads to wrong conclusions. For example:\n\nAbductive reasoning, or argument to the best explanation, is a form of reasoning that does not fit in either the deductive or inductive categories, since it starts with incomplete set of observations and proceeds with likely possible explanations. The conclusion in an abductive argument does not follow with certainty from its premises and concerns something unobserved. What distinguishes abduction from the other forms of reasoning is an attempt to favour one conclusion above others, by subjective judgement or by attempting to falsify alternative explanations or by demonstrating the likelihood of the favoured conclusion, given a set of more or less disputable assumptions. For example, when a patient displays certain symptoms, there might be various possible causes, but one of these is preferred above others as being more probable.\n\nFlawed reasoning in arguments is known as fallacious reasoning. Bad reasoning within arguments can result from either a formal fallacy or an informal fallacy.\n\nFormal fallacies occur when there is a problem with the form, or structure, of the argument. The word \"formal\" refers to this link to the form of the argument. An argument that contains a formal fallacy will always be invalid.\n\nAn informal fallacy is an error in reasoning that occurs due to a problem with the content, rather than the form or structure, of the argument.\n\nIn law relating to the actions of an employer or a public body, a decision or action which falls outside the range of actions or decision available when acting in good faith can be described as \"unreasonable\". Use of the term is considered in the English law cases of Short v Poole Corporation (1926), Associated Provincial Picture Houses Ltd v Wednesbury Corporation (1947) and Braganza v BP Shipping Limited (2015).[68]\n\nPhilosophy is often characterized as a pursuit of rational understanding, entailing a more rigorous and dedicated application of human reasoning than commonly employed. Philosophers have long debated two fundamental questions regarding reason, essentially examining reasoning itself as a human endeavor, or philosophizing about philosophizing. The first question delves into whether we can place our trust in reason's ability to attain knowledge and truth more effectively than alternative methods. The second question explores whether a life guided by reason, a life that aims to be guided by reason, can be expected to lead to greater happiness compared to other approaches to life.\n\nSince classical antiquity a question has remained constant in philosophical debate (sometimes seen as a conflict between Platonism and Aristotelianism) concerning the role of reason in confirming truth. People use logic, deduction, and induction to reach conclusions they think are true. Conclusions reached in this way are considered, according to Aristotle, more certain than sense perceptions on their own.[69] On the other hand, if such reasoned conclusions are only built originally upon a foundation of sense perceptions, then our most logical conclusions can never be said to be certain because they are built upon the very same fallible perceptions they seek to better.[70]\n\nThis leads to the question of what types of first principles, or starting points of reasoning, are available for someone seeking to come to true conclusions. In Greek, \"first principles\" are archai, \"starting points\",[71] and the faculty used to perceive them is sometimes referred to in Aristotle[72] and Plato[73] as nous which was close in meaning to awareness or consciousness.[74]\n\nEmpiricism (sometimes associated with Aristotle[75] but more correctly associated with British philosophers such as John Locke and David Hume, as well as their ancient equivalents such as Democritus) asserts that sensory impressions are the only available starting points for reasoning and attempting to attain truth. This approach always leads to the controversial conclusion that absolute knowledge is not attainable. Idealism, (associated with Plato and his school), claims that there is a \"higher\" reality, within which certain people can directly discover truth without needing to rely only upon the senses, and that this higher reality is therefore the primary source of truth.\n\nPhilosophers such as Plato, Aristotle, Al-Farabi, Avicenna, Averroes, Maimonides, Aquinas, and Hegel are sometimes said[by whom?] to have argued that reason must be fixed and discoverable—perhaps by dialectic, analysis, or study. In the vision of these thinkers, reason is divine or at least has divine attributes. Such an approach allowed religious philosophers such as Thomas Aquinas and Étienne Gilson to try to show that reason and revelation are compatible. According to Hegel, \"...the only thought which Philosophy brings with it to the contemplation of History, is the simple conception of reason; that reason is the Sovereign of the World; that the history of the world, therefore, presents us with a rational process.\"[76]\n\nSince the 17th century rationalists, reason has often been taken to be a subjective faculty, or rather the unaided ability (pure reason) to form concepts. For Descartes, Spinoza, and Leibniz, this was associated with mathematics. Kant attempted to show that pure reason could form concepts (time and space) that are the conditions of experience. Kant made his argument in opposition to Hume, who denied that reason had any role to play in experience.\n\nAfter Plato and Aristotle, western literature often treated reason as being the faculty that trained the passions and appetites.[citation needed] Stoic philosophy, by contrast, claimed most emotions were merely false judgements.[77][78] According to the Stoics the only good is virtue, and the only evil is vice, therefore emotions that judged things other than vice to be bad (such as fear or distress), or things other than virtue to be good (such as greed) were simply false judgements and should be discarded (though positive emotions based on true judgements, such as kindness, were acceptable).[77][78][79] After the critiques of reason in the early Enlightenment the appetites were rarely discussed or were conflated with the passions.[citation needed] Some Enlightenment camps took after the Stoics to say reason should oppose passion rather than order it, while others like the Romantics believed that passion displaces reason, as in the maxim \"follow your heart\".[citation needed]\n\nReason has been seen as cold, an \"enemy of mystery and ambiguity\",[80] a slave, or judge, of the passions, notably in the work of David Hume. More recently, Freud wrote, “It seems as though the activity of the other agencies of the mind is able only to modify the pleasure principle but not to nullify it; and it remains a question of the greatest theoretical importance, and one that has not yet been answered, when and how it is ever possible for the pleasure principle to be overcome.” [81]\n\nReasoning that claims the object of a desire is demanded by logic alone is called rationalization.[citation needed]\n\nRousseau first proposed, in his second Discourse, that reason and political life is not natural and is possibly harmful to mankind.[82] He asked what really can be said about what is natural to mankind. What, other than reason and civil society, \"best suits his constitution\"? Rousseau saw \"two principles prior to reason\" in human nature. First we hold an intense interest in our own well-being. Secondly we object to the suffering or death of any sentient being, especially one like ourselves.[83] These two passions lead us to desire more than we could achieve. We become dependent upon each other, and on relationships of authority and obedience. This effectively puts the human race into slavery. Rousseau says that he almost dares to assert that nature does not destine men to be healthy. According to Richard Velkley, \"Rousseau outlines certain programs of rational self-correction, most notably the political legislation of the Contrat Social and the moral education in Émile. All the same, Rousseau understands such corrections to be only ameliorations of an essentially unsatisfactory condition, that of socially and intellectually corrupted humanity.\"[This quote needs a citation]\n\nThis quandary presented by Rousseau led to Kant's new way of justifying reason as freedom to create good and evil. These therefore are not to be blamed on nature or God. In various ways, German Idealism after Kant, and major later figures such Nietzsche, Bergson, Husserl, Scheler, and Heidegger, remain preoccupied with problems coming from the metaphysical demands or urges of reason.[84] Rousseau and these later writers also exerted a large influence on art and politics. Many writers (such as Nikos Kazantzakis) extol passion and disparage reason. In politics modern nationalism comes from Rousseau's argument that rationalist cosmopolitanism brings man ever further from his natural state.[85]\n\nIn Descartes' Error, Antonio Damasio presents the \"Somatic Marker Hypothesis\" which states that emotions guide behavior and decision-making. Damasio argues that these somatic markers (known collectively as \"gut feelings\") are \"intuitive signals\" that direct our decision making processes in a certain way that cannot be solved with rationality alone. Damasio further argues that rationality requires emotional input in order to function.\n\nThere are many religious traditions, some of which are explicitly fideist and others of which claim varying degrees of rationalism. Secular critics sometimes accuse all religious adherents of irrationality; they claim such adherents are guilty of ignoring, suppressing, or forbidding some kinds of reasoning concerning some subjects (such as religious dogmas, moral taboos, etc.).[86] Though theologies and religions such as classical monotheism typically do not admit to being irrational, there is often a perceived conflict or tension between faith and tradition on the one hand, and reason on the other, as potentially competing sources of wisdom, law, and truth.[74][87]\n\nReligious adherents sometimes respond by arguing that faith and reason can be reconciled, or have different non-overlapping domains, or that critics engage in a similar kind of irrationalism:\n\nSome commentators have claimed that Western civilization can be almost defined by its serious testing of the limits of tension between \"unaided\" reason and faith in \"revealed\" truths—figuratively summarized as Athens and Jerusalem, respectively.[93] Leo Strauss spoke of a \"Greater West\" that included all areas under the influence of the tension between Greek rationalism and Abrahamic revelation, including the Muslim lands. He was particularly influenced by the Muslim philosopher Al-Farabi. To consider to what extent Eastern philosophy might have partaken of these important tensions, Strauss thought it best to consider whether dharma or tao may be equivalent to Nature (physis in Greek). According to Strauss the beginning of philosophy involved the \"discovery or invention of nature\" and the \"pre-philosophical equivalent of nature\" was supplied by \"such notions as 'custom' or 'ways'\", which appear to be really universal in all times and places. The philosophical concept of nature or natures as a way of understanding archai (first principles of knowledge) brought about a peculiar tension between reasoning on the one hand, and tradition or faith on the other.[74]\n\nScientific research into reasoning is carried out within the fields of psychology and cognitive science. Psychologists attempt to determine whether or not people are capable of rational thought in a number of different circumstances.\n\nAssessing how well someone engages in reasoning is the project of determining the extent to which the person is rational or acts rationally. It is a key research question in the psychology of reasoning and cognitive science of reasoning. Rationality is often divided into its respective theoretical and practical counterparts.\n\nExperimental cognitive psychologists carry out research on reasoning behaviour. Such research may focus, for example, on how people perform on tests of reasoning such as intelligence or IQ tests, or on how well people's reasoning matches ideals set by logic (see, for example, the Wason test).[94] Experiments examine how people make inferences from conditionals like if A then B and how they make inferences about alternatives like A or else B.[95] They test whether people can make valid deductions about spatial and temporal relations like A is to the left of B or A happens after B, and about quantified assertions like all the A are B.[96] Experiments investigate how people make inferences about factual situations, hypothetical possibilities, probabilities, and counterfactual situations.[97]\n\nDevelopmental psychologists investigate the development of reasoning from birth to adulthood. Piaget's theory of cognitive development was the first complete theory of reasoning development. Subsequently, several alternative theories were proposed, including the neo-Piagetian theories of cognitive development.[98]\n\nThe biological functioning of the brain is studied by neurophysiologists, cognitive neuroscientists, and neuropsychologists. This includes research into the structure and function of normally functioning brains, and of damaged or otherwise unusual brains. In addition to carrying out research into reasoning, some psychologists—for example clinical psychologists and psychotherapists—work to alter people's reasoning habits when those habits are unhelpful.\n\nIn artificial intelligence and computer science, scientists study and use automated reasoning for diverse applications including automated theorem proving the formal semantics of programming languages, and formal specification in software engineering.\n\nMeta-reasoning is reasoning about reasoning. In computer science, a system performs meta-reasoning when it is reasoning about its own operation.[99] This requires a programming language capable of reflection, the ability to observe and modify its own structure and behaviour.\n\nA species could benefit greatly from better abilities to reason about, predict, and understand the world. French social and cognitive scientists Dan Sperber and Hugo Mercier argue that, aside from these benefits, there could have been other forces driving the evolution of reason. They point out that reasoning is very difficult for humans to do effectively, and that it is hard for individuals to doubt their own beliefs (confirmation bias). Reasoning is most effective when it is done as a collective—as demonstrated by the success of projects like science. They suggest that there are not just individual, but group selection pressures at play. Any group that managed to find ways of reasoning effectively would reap benefits for all its members, increasing their fitness. This could also help explain why humans, according to Sperber, are not optimized to reason effectively alone. Sperber's & Mercier's argumentative theory of reasoning claims that reason may have more to do with winning arguments than with the search for the truth.[100]\n\nAristotle famously described reason (with language) as a part of human nature, because of which it is best for humans to live \"politically\" meaning in communities of about the size and type of a small city state (polis in Greek). For example:\n\nIt is clear, then, that a human being is more of a political politikon = of the polis] animal [zōion] than is any bee or than any of those animals that live in herds. For nature, as we say, makes nothing in vain, and humans are the only animals who possess reasoned speech [logos]. Voice, of course, serves to indicate what is painful and pleasant; that is why it is also found in other animals, because their nature has reached the point where they can perceive what is painful and pleasant and express these to each other. But speech [logos] serves to make plain what is advantageous and harmful and so also what is just and unjust. For it is a peculiarity of humans, in contrast to the other animals, to have perception of good and bad, just and unjust, and the like; and the community in these things makes a household or city [polis].... By nature, then, the drive for such a community exists in everyone, but the first to set one up is responsible for things of very great goodness. For as humans are the best of all animals when perfected, so they are the worst when divorced from law and right. The reason is that injustice is most difficult to deal with when furnished with weapons, and the weapons a human being has are meant by nature to go along with prudence and virtue, but it is only too possible to turn them to contrary uses. Consequently, if a human being lacks virtue, he is the most unholy and savage thing, and when it comes to sex and food, the worst. But justice is something political [to do with the polis], for right is the arrangement of the political community, and right is discrimination of what is just.[101]: I.2, 1253a \nIf human nature is fixed in this way, we can define what type of community is always best for people. This argument has remained a central argument in all political, ethical, and moral thinking since then, and has become especially controversial since firstly Rousseau's Second Discourse, and secondly, the Theory of Evolution. Already in Aristotle there was an awareness that the polis had not always existed and had to be invented or developed by humans themselves. The household came first, and the first villages and cities were just extensions of that, with the first cities being run as if they were still families with Kings acting like fathers.[101]: I.2, 1252b15 \n\nFriendship seems to prevail in man and woman according to nature [kata phusin]; for people are by nature [tēi phusei] pairing more than political [politikon], in as much as the household [oikos] is prior and more necessary than the polis and making children is more common [koinoteron] with the animals. In the other animals, community [koinōnia] goes no further than this, but people live together [sumoikousin] not only for the sake of making children, but also for the things for life; for from the start the functions [erga] are divided, and are different for man and woman. Thus they supply each other, putting their own into the common [eis to koinon]. It is for these reasons that both utility and pleasure seem to be found in this kind of friendship.[6]: VIII.12 \nRousseau in his Second Discourse finally took the shocking step of claiming that this traditional account has things in reverse: with reason, language, and rationally organized communities all having developed over a long period of time merely as a result of the fact that some habits of cooperation were found to solve certain types of problems, and that once such cooperation became more important, it forced people to develop increasingly complex cooperation—often only to defend themselves from each other.\n\nIn other words, according to Rousseau, reason, language, and rational community did not arise because of any conscious decision or plan by humans or gods, nor because of any pre-existing human nature. As a result, he claimed, living together in rationally organized communities like modern humans is a development with many negative aspects compared to the original state of man as an ape. If anything is specifically human in this theory, it is the flexibility and adaptability of humans. This view of the animal origins of distinctive human characteristics later received support from Charles Darwin's Theory of Evolution.\n\nThe two competing theories concerning the origins of reason are relevant to political and ethical thought because, according to the Aristotelian theory, a best way of living together exists independently of historical circumstances. According to Rousseau, we should even doubt that reason, language, and politics are a good thing, as opposed to being simply the best option given the particular course of events that led to today. Rousseau's theory, that human nature is malleable rather than fixed, is often taken to imply (for example by Karl Marx) a wider range of possible ways of living together than traditionally known.\n\nHowever, while Rousseau's initial impact encouraged bloody revolutions against traditional politics, including both the French Revolution and the Russian Revolution, his own conclusions about the best forms of community seem to have been remarkably classical, in favor of city-states such as Geneva, and rural living.\n"
    },
    {
        "title": "Rule of inference",
        "content": "In philosophy of logic and logic, specifically in deductive reasoning, a rule of inference, inference rule or transformation rule is a logical form consisting of a function which takes premises, analyzes their syntax, and returns a conclusion (or conclusions). \n\nFor example, the rule of inference called modus ponens takes two premises, one in the form \"If p then q\" and another in the form \"p\", and returns the conclusion \"q\". The rule is valid with respect to the semantics of classical logic (as well as the semantics of many other non-classical logics), in the sense that if the premises are true (under an interpretation), then so is the conclusion.\n\nTypically, a rule of inference preserves truth, a semantic property. In many-valued logic, it preserves a general designation. But a rule of inference's action is purely syntactic, and does not need to preserve any semantic property: any function from sets of formulae to formulae counts as a rule of inference. Usually only rules that are recursive are important; i.e. rules such that there is an effective procedure for determining whether any given formula is the conclusion of a given set of formulae according to the rule. An example of a rule that is not effective in this sense is the infinitary ω-rule.[1]\n\nPopular rules of inference in propositional logic include modus ponens, modus tollens, and contraposition. First-order predicate logic uses rules of inference to deal with logical quantifiers.\n\nIn formal logic (and many related areas), rules of inference are usually given in the following standard form:\n\n  Premise#1\n  Premise#2\n        ...\n  Premise#n   \n  Conclusion\n\nThis expression states that whenever in the course of some logical derivation the given premises have been obtained, the specified conclusion can be taken for granted as well. The exact formal language that is used to describe both premises and conclusions depends on the actual context of the derivations. In a simple case, one may use logical formulae, such as in:\n\nThis is the modus ponens rule of propositional logic. Rules of inference are often formulated as schemata employing metavariables.[2] In the rule (schema) above, the metavariables A and B can be instantiated to any element of the universe (or sometimes, by convention, a restricted subset such as propositions) to form an infinite set of inference rules.\n\nA proof system is formed from a set of rules chained together to form proofs, also called derivations. Any derivation has only one final conclusion, which is the statement proved or derived. If premises are left unsatisfied in the derivation, then the derivation is a proof of a hypothetical statement: \"if the premises hold, then the conclusion holds.\"\n\nIn a Hilbert system, the premises and conclusion of the inference rules are simply formulae of some language, usually employing metavariables. For graphical compactness of the presentation and to emphasize the distinction between axioms and rules of inference, this section uses the sequent notation (\n\n\n\n⊢\n\n\n{\\displaystyle \\vdash }\n\n) instead of a vertical presentation of rules.\nIn this notation, \n\n\n\n\n\n\n\n\n\n\nPremise \n\n1\n\n\n\n\n\nPremise \n\n2\n\n\n\n\n\nConclusion\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{array}{c}{\\text{Premise }}1\\\\{\\text{Premise }}2\\\\\\hline {\\text{Conclusion}}\\end{array}}}\n\n\n\nis written as \n\n\n\n(\n\nPremise \n\n1\n)\n,\n(\n\nPremise \n\n2\n)\n⊢\n(\n\nConclusion\n\n)\n\n\n{\\displaystyle ({\\text{Premise }}1),({\\text{Premise }}2)\\vdash ({\\text{Conclusion}})}\n\n.\n\nThe formal language for classical propositional logic can be expressed using just negation (¬), implication (→) and propositional symbols. A well-known axiomatization, comprising three axiom schemata and one inference rule (modus ponens), is:\n\nIt may seem redundant to have two notions of inference in this case, ⊢ and →. In classical propositional logic, they indeed coincide; the deduction theorem states that A ⊢ B if and only if ⊢ A → B. There is however a distinction worth emphasizing even in this case: the first notation describes a deduction, that is an activity of passing from sentences to sentences, whereas A → B is simply a formula made with a logical connective, implication in this case. Without an inference rule (like modus ponens in this case), there is no deduction or inference. This point is illustrated in Lewis Carroll's dialogue called \"What the Tortoise Said to Achilles\",[3] as well as later attempts by Bertrand Russell and Peter Winch to resolve the paradox introduced in the dialogue.\n\nFor some non-classical logics, the deduction theorem does not hold. For example, the three-valued logic of Łukasiewicz can be axiomatized as:[4]\n\nThis sequence differs from classical logic by the change in axiom 2 and the addition of axiom 4. The classical deduction theorem does not hold for this logic, however a modified form does hold, namely A ⊢ B if and only if ⊢ A → (A → B).[5]\n\nIn a set of rules, an inference rule could be redundant in the sense that it is admissible or derivable. A derivable rule is one whose conclusion can be derived from its premises using the other rules. An admissible rule is one whose conclusion holds whenever the premises hold. All derivable rules are admissible. To appreciate the difference, consider the following set of rules for defining the natural numbers (the judgment \n\n\n\nn\n\n\n\n\nn\na\nt\n\n\n\n\n{\\displaystyle n\\,\\,{\\mathsf {nat}}}\n\n asserts the fact that \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is a natural number):\n\nThe first rule states that 0 is a natural number, and the second states that s(n) is a natural number if n is. In this proof system, the following rule, demonstrating that the second successor of a natural number is also a natural number, is derivable:\n\nIts derivation is the composition of two uses of the successor rule above. The following rule for asserting the existence of a predecessor for any nonzero number is merely admissible:\n\nThis is a true fact of natural numbers, as can be proven by induction. (To prove that this rule is admissible, assume a derivation of the premise and induct on it to produce a derivation of \n\n\n\nn\n\n\n\n\nn\na\nt\n\n\n\n\n{\\displaystyle n\\,\\,{\\mathsf {nat}}}\n\n.) However, it is not derivable, because it depends on the structure of the derivation of the premise. Because of this, derivability is stable under additions to the proof system, whereas admissibility is not. To see the difference, suppose the following nonsense rule were added to the proof system:\n\nIn this new system, the double-successor rule is still derivable. However, the rule for finding the predecessor is no longer admissible, because there is no way to derive \n\n\n\n\n−\n3\n\n\n\n\n\nn\na\nt\n\n\n\n\n{\\displaystyle \\mathbf {-3} \\,\\,{\\mathsf {nat}}}\n\n. The brittleness of admissibility comes from the way it is proved: since the proof can induct on the structure of the derivations of the premises, extensions to the system add new cases to this proof, which may no longer hold.\n\nAdmissible rules can be thought of as theorems of a proof system. For instance, in a sequent calculus where cut elimination holds, the cut rule is admissible.\n"
    },
    {
        "title": "Natural science",
        "content": "\nNatural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation.[1] Mechanisms such as peer review and reproducibility of findings are used to try to ensure the validity of scientific advances.\n\nNatural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements that can be explained as clear statements of the \"laws of nature\".[2]\n\nModern natural science succeeded more classical approaches to natural philosophy. Galileo, Kepler, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[3] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on.[4] Today, \"natural history\" suggests observational descriptions aimed at popular audiences.[5]\n\nPhilosophers of science have suggested several criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and reproducibility of findings, are amongst the most respected criteria in today's global scientific community.\n\nIn natural science, impossibility assertions come to be widely accepted as overwhelmingly probable rather than considered proven to the point of being unchallengeable. The basis for this strong acceptance is a combination of extensive evidence of something not occurring, combined with an underlying theory, very successful in making predictions, whose assumptions lead logically to the conclusion that something is impossible. While an impossibility assertion in natural science can never be proved, it could be refuted by the observation of a single counterexample. Such a counterexample would require that the assumptions underlying the theory that implied the impossibility be re-examined.\n\nThis field encompasses a diverse set of disciplines that examine phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n\nThe biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n\nSome key developments in biology were the discovery of genetics, evolution through natural selection, the germ theory of disease, and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n\nModern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology look at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n\nEarth science (also known as geoscience) is an all-embracing term for the sciences related to the planet Earth, including geology, geography, geophysics, geochemistry, climatology, glaciology, hydrology, meteorology, and oceanography.\n\nAlthough mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly paleontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research, and to environmental assessment and remediation.\n\nAlthough sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques, and practices and also the fact of it having a wide range of sub-disciplines under its wing, atmospheric science is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the space. The timescale of the study also varies from day to century. Sometimes, the field also includes the study of climatic patterns on planets other than Earth.[6]\n\nThe serious study of oceans began in the early- to mid-20th century. As a field of natural science, it is relatively young, but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences, or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices.\n\nPlanetary science or planetology, is the scientific study of planets, which include terrestrial planets like the Earth, and other types of planets, such as gas giants and ice giants. Planetary science also concerns other celestial bodies, such as dwarf planets moons, asteroids, and comets. This largely includes the Solar System, but recently has started to expand to exoplanets, particularly terrestrial exoplanets. It explores various objects, spanning from micrometeoroids to gas giants, to establish their composition, movements, genesis, interrelation, and past. Planetary science is an interdisciplinary domain, having originated from astronomy and Earth science, and currently encompassing a multitude of areas, such as planetary geology, cosmochemistry, atmospheric science, physics, oceanography, hydrology, theoretical planetology, glaciology, and exoplanetology. Related fields encompass space physics, which delves into the impact of the Sun on the bodies in the Solar System, and astrobiology.\n\nPlanetary science comprises interconnected observational and theoretical branches. Observational research entails a combination of space exploration, primarily through robotic spacecraft missions utilizing remote sensing, and comparative experimental work conducted in Earth-based laboratories. The theoretical aspect involves extensive mathematical modelling and computer simulation.\n\nTypically, planetary scientists are situated within astronomy and physics or Earth sciences departments in universities or research centers. However, there are also dedicated planetary science institutes worldwide. Generally, individuals pursuing a career in planetary science undergo graduate-level studies in one of the Earth sciences, astronomy, astrophysics, geophysics, or physics. They then focus their research within the discipline of planetary science. Major conferences are held annually, and numerous peer reviewed journals cater to the diverse research interests in planetary science. Some planetary scientists are employed by private research centers and frequently engage in collaborative research initiatives.\n\nConstituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations, and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n\nMost chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n\nEarly experiments in chemistry had their roots in the system of alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gases, and Antoine Lavoisier, who developed the theory of the conservation of mass.\n\nThe discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n\nPhysics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. Physics is generally regarded as foundational because all other natural sciences use and obey the field's principles and laws. Physics relies heavily on mathematics as the logical framework for formulating and quantifying principles.\n\nThe study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n\nThe field of physics is vast and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein, and Lev Landau, who worked in multiple areas.\n\nAstronomy is a natural science that studies celestial objects and phenomena. Objects of interest include planets, moons, stars, nebulae, galaxies, and comets. Astronomy is the study of everything in the universe beyond Earth's atmosphere, including objects we can see with our naked eyes. It is one of the oldest sciences.\n\nAstronomers of early civilizations performed methodical observations of the night sky, and astronomical artifacts have been found from much earlier periods. There are two types of astronomy: observational astronomy and theoretical astronomy. Observational astronomy is focused on acquiring and analyzing data, mainly using basic principles of physics. In contrast, Theoretical astronomy is oriented towards developing computer or analytical models to describe astronomical objects and phenomena.\n\nThis discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, geology, and motion of celestial objects, as well as the formation and development of the universe.\n\nAstronomy includes examining, studying, and modeling stars, planets, and comets. Most of the information used by astronomers is gathered by remote observation. However, some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium). There is considerable overlap with physics and in some areas of earth science. There are also interdisciplinary fields such as astrophysics, planetary sciences, and cosmology, along with allied disciplines such as space physics and astrochemistry.\n\nWhile the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n\nThe mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation. However, it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n\nThe distinctions between the natural science disciplines are not always sharp, and they share many cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, physical chemistry, geochemistry and astrochemistry.\n\nA particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields, such as economics, law, and social sciences.\n\nA comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is vast and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n\nThere is also a subset of cross-disciplinary fields with strong currents that run counter to specialization by the nature of the problems they address. Put another way: In some fields of integrative application, specialists in more than one field are a key part of most scientific discourse. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n\nMaterials science is a relatively new, interdisciplinary field that deals with the study of matter and its properties and the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics, and engineering applications of materials, including metals, ceramics, artificial polymers, and many others. The field's core deals with relating the structure of materials with their properties.\n\nMaterials science is at the forefront of research in science and engineering. It is an essential part of forensic engineering (the investigation of materials, products, structures, or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available, and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n\nThe basis of materials science involves studying the structure of materials and relating them to their properties. Understanding this structure-property correlation, material scientists can then go on to study the relative performance of a material in a particular application. The major determinants of the structure of a material and, thus, of its properties are its constituent chemical elements and how it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure and thus its properties.\n\nSome scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival.[7] People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation.[7] These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science.[8] While the writings show an interest in astronomy, mathematics, and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was, in all cases, religious or mythological, not scientific.[9]\n\nA tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments.[10] They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth.[11] The five phases – fire, earth, metal, wood, and water – described a cycle of transformations in nature. The water turned into wood, which turned into the fire when it burned. The ashes left by fire were earth.[12] Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang, and understood the relationship between the pulse, the heart, and the flow of blood in the body centuries before it became accepted in the West.[13]\n\nLittle evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts.[13] They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed.[13] Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm.[13] A healthy life resulted from a balance among these humors.[13] In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind, and space.[13] Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.[13]\n\nPre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC. However, an element of magic and mythology remained.[14] Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods.[14] Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature.[15] In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles.[16] Pythagoras applied Greek innovations in mathematics to astronomy and suggested that the earth was spherical.[16]\n\nLater Socratic and Platonic thought focused on ethics, morals, and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists.[17] Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy.[18] In his History of Animals, he described the inner workings of 110 species, including the stingray, catfish and bee.[19] He investigated chick embryos by breaking open eggs and observing them at various stages of development.[20] Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science.[21] He also presented philosophies about physics, nature, and astronomy using inductive reasoning in his works Physics and Meteorology.[22]\n\nWhile Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science.[23] Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth.[24] Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism.[25] Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.[26]\n\nAristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate.[27]\n\nIn the Byzantine Empire, John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian, was the first to question Aristotle's physics teaching. Unlike Aristotle, who based his physics on verbal argument, Philoponus instead relied on observation and argued for observation rather than resorting to a verbal argument.[28] He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.[29][30]\n\nA revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy.[31] The words alcohol, algebra and zenith all have Arabic roots.[32]\n\nAristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin.[33] The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy.[34] European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England.[35] Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic.[36] This approach, however, was seen by some detractors as heresy.[36] By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars.[37] Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy.[37] These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church.[38] A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of ex-communication.\"[38]\n\nIn the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called On the Sciences into Latin, calling the study of the mechanics of nature Scientia naturalis, or natural science.[39] Gundissalinus also proposed his classification of the natural sciences in his 1150 work On the Division of Philosophy.[39] This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe.[39] Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics.[40] Following Al-Farabi, he separated the sciences into eight parts, including: physics, cosmology, meteorology, minerals science, and plant and animal science.[40]\n\nLater, philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote On the Order of the Sciences in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting, and theater, while defining natural science as the science that deals with bodies in motion.[41] Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth, and water, and in all inanimate things made from them.\"[42] These sciences also covered plants, animals and celestial bodies.[42] Later in the 13th century, a Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on a matter not only for their existence but also for their definition.\"[43] There was broad agreement among scholars in medieval times that natural science was about bodies in motion. However, there was division about including fields such as medicine, music, and perspective.[44] Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist, and where in the atmosphere rain is formed.[45]\n\nIn the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult.[46] Natural philosophy appeared in various forms, from treatises to encyclopedias to commentaries on Aristotle.[47] The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it.[48] Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries.[49] The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective.[50] Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works.[51] \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.[52]\n\nBy the 16th and 17th centuries, natural philosophy evolved beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated.[53] The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West.[53] Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false.[54] Several 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon, made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy superficial.[55]\n\nThe titles of Galileo's work Two New Sciences and Johannes Kepler's New Astronomy underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world.[57] Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature.[58] To achieve this, he wrote that \"human life [must] be endowed with discoveries and powers.\"[59] He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\"[57] Bacon proposed that scientific inquiry be supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition, and forms at the time.[59] Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock.[60] Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation.[61] Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution.[62] Newton in 1687 published his The Mathematical Principles of Natural Philosophy, or Principia Mathematica, which set the groundwork for physical laws that remained current until the 19th century.[63]\n\nSome modern scholars, including Andrew Cunningham, Perry Williams, and Floris Cohen, argue that natural philosophy is not properly called science and that genuine scientific inquiry began only with the scientific revolution.[64] According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy is one defining characteristic of the Scientific Revolution.\"[64] Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th, and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics, and astronomy began to be applied to questions raised by natural philosophy.[64] Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and, in doing so, joined natural philosophy and mathematics for the first time, producing an early work of modern physics.[65]\n\nThe scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry.[66] One of its principal advances was the use of the scientific method to investigate nature. Data was collected, and repeatable measurements were made in experiments.[67] Scientists then formed hypotheses to explain the results of these experiments.[68] The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy.[68] The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.[66]\n\nNewton, an English mathematician and physicist, was a seminal figure in the scientific revolution.[69] Drawing on advances made in astronomy by Copernicus, Brahe, and Kepler, Newton derived the universal law of gravitation and laws of motion.[70] These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently, according to separate physical rules.[71] Newton, for example, showed that the tides were caused by the gravitational pull of the moon.[72] Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena.[73] While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.[73]\n\nIn the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles.[74] Faraday proposed that forces in nature operated in \"fields\" that filled space.[75] The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene.[75] James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics.[74] Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon each other and were a medium for transmitting charged waves.[74]\n\nSignificant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air.[75] Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation.[75] He also constructed a table of 33 elements and invented modern chemical nomenclature.[75] Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus, in the 1750s, introduced scientific names for all his species.[76]\n\nBy the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of natural science. The term scientist was coined by William Whewell in an 1834 review of Mary Somerville's On the Connexion of the Sciences.[77] But the word did not enter general use until nearly the end of the same century.[citation needed]\n\nAccording to a famous 1923 textbook, Thermodynamics and the Free Energy of Chemical Substances, by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall,[78] the natural sciences contain three great branches:\n\nAside from the logical and mathematical sciences, there are three great branches of natural science which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.[79]\nToday, natural sciences are more commonly divided into life sciences, such as botany and zoology, and physical sciences, which include physics, chemistry, astronomy, and Earth sciences.\n"
    },
    {
        "title": "Engineering",
        "content": "\n\nEngineering is the practice of using natural science, mathematics, and the engineering design process[1] to solve technical problems, increase efficiency and productivity, and improve systems. Modern engineering comprises many subfields which include designing and improving infrastructure, machinery, vehicles, electronics, materials, and energy systems.[2]\n\nThe discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.\n\nThe term engineering is derived from the Latin ingenium, meaning \"cleverness\".[3]\n\nThe American Engineers' Council for Professional Development (ECPD, the predecessor of ABET)[4] has defined \"engineering\" as:\n\nThe creative application of scientific principles to design or develop structures, machines, apparatus, or manufacturing processes, or works utilizing them singly or in combination; or to construct or operate the same with full cognizance of their design; or to forecast their behavior under specific operating conditions; all as respects an intended function, economics of operation and safety to life and property.[5][6]\nEngineering has existed since ancient times, when humans devised inventions such as the wedge, lever, wheel and pulley, etc.\n\nThe term engineering is derived from the word engineer, which itself dates back to the 14th century when an engine'er (literally, one who builds or operates a siege engine) referred to \"a constructor of military engines\".[7] In this context, now obsolete, an \"engine\" referred to a military machine, i.e., a mechanical contraption used in war (for example, a catapult). Notable examples of the obsolete usage which have survived to the present day are military engineering corps, e.g., the U.S. Army Corps of Engineers.\n\nThe word \"engine\" itself is of even older origin, ultimately deriving from the Latin ingenium (c. 1250), meaning \"innate quality, especially mental power, hence a clever invention.\"[8]\n\nLater, as the design of civilian structures, such as bridges and buildings, matured as a technical discipline, the term civil engineering[6] entered the lexicon as a way to distinguish between those specializing in the construction of such non-military projects and those involved in the discipline of military engineering.\n\nThe pyramids in ancient Egypt, ziggurats of Mesopotamia, the Acropolis and Parthenon in Greece, the Roman aqueducts, Via Appia and Colosseum, Teotihuacán, and the Brihadeeswarar Temple of Thanjavur, among many others, stand as a testament to the ingenuity and skill of ancient civil and military engineers. Other monuments, no longer standing, such as the Hanging Gardens of Babylon and the Pharos of Alexandria, were important engineering achievements of their time and were considered among the Seven Wonders of the Ancient World.\n\nThe six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times.[9] The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC.[10] The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale,[11] and to move large objects in ancient Egyptian technology.[12] The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia c. 3000 BC,[11] and then in ancient Egyptian technology c. 2000 BC.[13] The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC,[14] and ancient Egypt during the Twelfth Dynasty (1991–1802 BC).[15] The screw, the last of the simple machines to be invented,[16] first appeared in Mesopotamia during the Neo-Assyrian period (911–609) BC.[14] The Egyptian pyramids were built using three of the six simple machines, the inclined plane, the wedge, and the lever, to create structures like the Great Pyramid of Giza.[17]\n\nThe earliest civil engineer known by name is Imhotep.[6] As one of the officials of the Pharaoh, Djosèr, he probably designed and supervised the construction of the Pyramid of Djoser (the Step Pyramid) at Saqqara in Egypt around 2630–2611 BC.[18] The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC.[19]\n\nKush developed the Sakia during the 4th century BC, which relied on animal power instead of human energy.[20] Hafirs were developed as a type of reservoir in Kush to store and contain water as well as boost irrigation.[21] Sappers were employed to build causeways during military campaigns.[22] Kushite ancestors built speos during the Bronze Age between 3700 and 3250 BC.[23] Bloomeries and blast furnaces were also created during the 7th centuries BC in Kush.[24][25][26][27]\n\nAncient Greece developed machines in both civilian and military domains. The Antikythera mechanism, an early known mechanical analog computer,[28][29] and the mechanical inventions of Archimedes, are examples of Greek mechanical engineering. Some of Archimedes' inventions, as well as the Antikythera mechanism, required sophisticated knowledge of differential gearing or epicyclic gearing, two key principles in machine theory that helped design the gear trains of the Industrial Revolution, and are widely used in fields such as robotics and automotive engineering.[30]\n\nAncient Chinese, Greek, Roman and Hunnic armies employed military machines and inventions such as artillery which was developed by the Greeks around the 4th century BC,[31] the trireme, the ballista and the catapult. In the Middle Ages, the trebuchet was developed.\n\nThe earliest practical wind-powered machines, the windmill and wind pump, first appeared in the Muslim world during the Islamic Golden Age, in what are now Iran, Afghanistan, and Pakistan, by the 9th century AD.[32][33][34][35] The earliest practical steam-powered machine was a steam jack driven by a steam turbine, described in 1551 by Taqi al-Din Muhammad ibn Ma'ruf in Ottoman Egypt.[36][37]\n\nThe cotton gin was invented in India by the 6th century AD,[38] and the spinning wheel was invented in the Islamic world by the early 11th century,[39] both of which were fundamental to the growth of the cotton industry. The spinning wheel was also a precursor to the spinning jenny, which was a key development during the early Industrial Revolution in the 18th century.[40]\n\nThe earliest programmable machines were developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated flute player invented by the Banu Musa brothers, described in their Book of Ingenious Devices, in the 9th century.[41][42] In 1206, Al-Jazari invented programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns.[43]\n\nBefore the development of modern engineering, mathematics was used by artisans and craftsmen, such as millwrights, clockmakers, instrument makers and surveyors. Aside from these professions, universities were not believed to have had much practical significance to technology.[44]: 32 \n\nA standard reference for the state of mechanical arts during the Renaissance is given in the mining engineering treatise De re metallica (1556), which also contains sections on geology, mining, and chemistry. De re metallica was the standard chemistry reference for the next 180 years.[44]\n\nThe science of classical mechanics, sometimes called Newtonian mechanics, formed the scientific basis of much of modern engineering.[44] With the rise of engineering as a profession in the 18th century, the term became more narrowly applied to fields in which mathematics and science were applied to these ends. Similarly, in addition to military and civil engineering, the fields then known as the mechanic arts became incorporated into engineering.\n\nCanal building was an important engineering work during the early phases of the Industrial Revolution.[45]\n\nJohn Smeaton was the first self-proclaimed civil engineer and is often regarded as the \"father\" of civil engineering. He was an English civil engineer responsible for the design of bridges, canals, harbors, and lighthouses. He was also a capable mechanical engineer and an eminent physicist. Using a model water wheel, Smeaton conducted experiments for seven years, determining ways to increase efficiency.[46]: 127    Smeaton introduced iron axles and gears to water wheels.[44]: 69  Smeaton also made mechanical improvements to the Newcomen steam engine. Smeaton designed the third Eddystone Lighthouse (1755–59) where he pioneered the use of 'hydraulic lime' (a form of mortar which will set under water) and developed a technique involving dovetailed blocks of granite in the building of the lighthouse. He is important in the history, rediscovery of, and development of modern cement, because he identified the compositional requirements needed to obtain \"hydraulicity\" in lime; work which led ultimately to the invention of Portland cement.\n\nApplied science led to the development of the steam engine. The sequence of events began with the invention of the barometer and the measurement of atmospheric pressure by Evangelista Torricelli in 1643, demonstration of the force of atmospheric pressure by Otto von Guericke using the Magdeburg hemispheres in 1656, laboratory experiments by Denis Papin, who built experimental model steam engines and demonstrated the use of a piston, which he published in 1707. Edward Somerset, 2nd Marquess of Worcester published a book of 100 inventions containing a method for raising waters similar to a coffee percolator. Samuel Morland, a mathematician and inventor who worked on pumps, left notes at the Vauxhall Ordinance Office on a steam pump design that Thomas Savery read. In 1698 Savery built a steam pump called \"The Miner's Friend\". It employed both vacuum and pressure.[47] Iron merchant Thomas Newcomen, who built the first commercial piston steam engine in 1712, was not known to have any scientific training.[46]: 32 \n\nThe application of steam-powered cast iron blowing cylinders for providing pressurized air for blast furnaces lead to a large increase in iron production in the late 18th century. The higher furnace temperatures made possible with steam-powered blast allowed for the use of more lime in blast furnaces, which enabled the transition from charcoal to coke.[48] These innovations lowered the cost of iron, making horse railways and iron bridges practical. The puddling process, patented by Henry Cort in 1784 produced large scale quantities of wrought iron. Hot blast, patented by James Beaumont Neilson in 1828, greatly lowered the amount of fuel needed to smelt iron. With the development of the high pressure steam engine, the power to weight ratio of steam engines made practical steamboats and locomotives possible.[49] New steel making processes, such as the Bessemer process and the open hearth furnace, ushered in an area of heavy engineering in the late 19th century.\n\nOne of the most famous engineers of the mid-19th century was Isambard Kingdom Brunel, who built railroads, dockyards and steamships.\n\nThe Industrial Revolution created a demand for machinery with metal parts, which led to the development of several machine tools. Boring cast iron cylinders with precision was not possible until John Wilkinson invented his boring machine, which is considered the first machine tool.[50] Other machine tools included the screw cutting lathe, milling machine, turret lathe and the metal planer. Precision machining techniques were developed in the first half of the 19th century. These included the use of gigs to guide the machining tool over the work and fixtures to hold the work in the proper position. Machine tools and machining techniques capable of producing interchangeable parts lead to large scale factory production by the late 19th century.[51]\n\nThe United States Census of 1850 listed the occupation of \"engineer\" for the first time with a count of 2,000.[52] There were fewer than 50 engineering graduates in the U.S. before 1865. In 1870 there were a dozen U.S. mechanical engineering graduates, with that number increasing to 43 per year in 1875. In 1890, there were 6,000 engineers in civil, mining, mechanical and electrical.[49]\n\nThere was no chair of applied mechanism and applied mechanics at Cambridge until 1875, and no chair of engineering at Oxford until 1907. Germany established technical universities earlier.[53]\n\nThe foundations of electrical engineering in the 1800s included the experiments of Alessandro Volta, Michael Faraday, Georg Ohm and others and the invention of the electric telegraph in 1816 and the electric motor in 1872. The theoretical work of James Maxwell (see: Maxwell's equations) and Heinrich Hertz in the late 19th century gave rise to the field of electronics. The later inventions of the vacuum tube and the transistor further accelerated the development of electronics to such an extent that electrical and electronics engineers currently outnumber their colleagues of any other engineering specialty.[6]\nChemical engineering developed in the late nineteenth century.[6] Industrial scale manufacturing demanded new materials and new processes and by 1880 the need for large scale production of chemicals was such that a new industry was created, dedicated to the development and large scale manufacturing of chemicals in new industrial plants.[6] The role of the chemical engineer was the design of these chemical plants and processes.[6]\n\nAeronautical engineering deals with aircraft design process design while aerospace engineering is a more modern term that expands the reach of the discipline by including spacecraft design. Its origins can be traced back to the aviation pioneers around the start of the 20th century although the work of Sir George Cayley has recently been dated as being from the last decade of the 18th century. Early knowledge of aeronautical engineering was largely empirical with some concepts and skills imported from other branches of engineering.[54]\n\nThe first PhD in engineering (technically, applied science and engineering) awarded in the United States went to Josiah Willard Gibbs at Yale University in 1863; it was also the second PhD awarded in science in the U.S.[55]\n\nOnly a decade after the successful flights by the Wright brothers, there was extensive development of aeronautical engineering through development of military aircraft that were used in World War I. Meanwhile, research to provide fundamental background science continued by combining theoretical physics with experiments.\n\nEngineering is a broad discipline that is often broken down into several sub-disciplines. Although an engineer will usually be trained in a specific discipline, he or she may become multi-disciplined through experience. Engineering is often characterized as having four main branches:[56][57][58] chemical engineering, civil engineering, electrical engineering, and mechanical engineering.\n\nChemical engineering is the application of physics, chemistry, biology, and engineering principles in order to carry out chemical processes on a commercial scale, such as the manufacture of commodity chemicals, specialty chemicals, petroleum refining, microfabrication, fermentation, and biomolecule production.\n\nCivil engineering is the design and construction of public and private works, such as infrastructure (airports, roads, railways, water supply, and treatment etc.), bridges, tunnels, dams, and buildings.[59][60] Civil engineering is traditionally broken into a number of sub-disciplines, including structural engineering, environmental engineering, and surveying. It is traditionally considered to be separate from military engineering.[61]\n\nElectrical engineering is the design, study, and manufacture of various electrical and electronic systems, such as broadcast engineering, electrical circuits, generators, motors, electromagnetic/electromechanical devices, electronic devices, electronic circuits, optical fibers, optoelectronic devices, computer systems, telecommunications, instrumentation, control systems, and electronics.\n\nMechanical engineering is the design and manufacture of physical or mechanical systems, such as power and energy systems, aerospace/aircraft products, weapon systems, transportation products, engines, compressors, powertrains, kinematic chains, vacuum technology, vibration isolation equipment, manufacturing, robotics, turbines, audio equipments, and mechatronics.\n\nBioengineering is the engineering of biological systems for a useful purpose. Examples of bioengineering research include bacteria engineered to produce chemicals, new medical imaging technology, portable and rapid disease diagnostic devices, prosthetics, biopharmaceuticals, and tissue-engineered organs.\n\nInterdisciplinary engineering draws from more than one of the principle branches of the practice. Historically, naval engineering and mining engineering were major branches. Other engineering fields are manufacturing engineering, acoustical engineering, corrosion engineering, instrumentation and control, aerospace, automotive, computer, electronic, information engineering, petroleum, environmental, systems, audio, software, architectural, agricultural, biosystems, biomedical,[62] geological, textile, industrial, materials,[63] and nuclear engineering.[64] These and other branches of engineering are represented in the 36 licensed member institutions of the UK Engineering Council.\n\nNew specialties sometimes combine with the traditional fields and form new branches – for example, Earth systems engineering and management involves a wide range of subject areas including engineering studies, environmental science, engineering ethics and philosophy of engineering.\n\nAerospace engineering covers the design, development, manufacture and operational behaviour of aircraft, satellites and rockets.\n\nMarine engineering covers the design, development, manufacture and operational behaviour of watercraft and stationary structures like oil platforms and ports.\n\nComputer engineering (CE) is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering.\n\nGeological engineering is associated with anything constructed on or within the Earth. This discipline applies geological sciences and engineering principles to direct or support the work of other disciplines such as civil engineering, environmental engineering, and mining engineering. Geological engineers are involved with impact studies for facilities and operations that affect surface and subsurface environments, such as rock excavations (e.g. tunnels), building foundation consolidation, slope and fill stabilization, landslide risk assessment, groundwater monitoring, groundwater remediation, mining excavations, and natural resource exploration.\n\nOne who practices engineering is called an engineer, and those licensed to do so may have more formal designations such as Professional Engineer, Chartered Engineer, Incorporated Engineer, Ingenieur, European Engineer, or Designated Engineering Representative.\n\nIn the engineering design process, engineers apply mathematics and sciences such as physics to find novel solutions to problems or to improve existing solutions. Engineers need proficient knowledge of relevant sciences for their design projects. As a result, many engineers continue to learn new material throughout their careers.\n\nIf multiple solutions exist, engineers weigh each design choice based on their merit and choose the solution that best matches the requirements. The task of the engineer is to identify, understand, and interpret the constraints on a design in order to yield a successful result. It is generally insufficient to build a technically successful product, rather, it must also meet further requirements.\n\nConstraints may include available resources, physical, imaginative or technical limitations, flexibility for future modifications and additions, and other factors, such as requirements for cost, safety, marketability, productivity, and serviceability. By understanding the constraints, engineers derive specifications for the limits within which a viable object or system may be produced and operated.\n\nEngineers use their knowledge of science, mathematics, logic, economics, and appropriate experience or tacit knowledge to find suitable solutions to a particular problem. Creating an appropriate mathematical model of a problem often allows them to analyze it (sometimes definitively), and to test potential solutions.[65]\n\nMore than one solution to a design problem usually exists so the different design choices have to be evaluated on their merits before the one judged most suitable is chosen. Genrich Altshuller, after gathering statistics on a large number of patents, suggested that compromises are at the heart of \"low-level\" engineering designs, while at a higher level the best design is one which eliminates the core contradiction causing the problem.[66]\n\nEngineers typically attempt to predict how well their designs will perform to their specifications prior to full-scale production. They use, among other things: prototypes, scale models, simulations, destructive tests, nondestructive tests, and stress tests. Testing ensures that products will perform as expected but only in so far as the testing has been representative of use in service. For products, such as aircraft, that are used differently by different users failures and unexpected shortcomings (and necessary design changes) can be expected throughout the operational life of the product.[67]\n\nEngineers take on the responsibility of producing designs that will perform as well as expected and, except those employed in specific areas of the arms industry, will not harm people. Engineers typically include a factor of safety in their designs to reduce the risk of unexpected failure.\n\nThe study of failed products is known as forensic engineering. It attempts to identify the cause of failure to allow a redesign of the product and so prevent a re-occurrence. Careful analysis is needed to establish the cause of failure of a product. The consequences of a failure may vary in severity from the minor cost of a machine breakdown to large loss of life in the case of accidents involving aircraft and large stationary structures like buildings and dams.[68]\n\nAs with all modern scientific and technological endeavors, computers and software play an increasingly important role. As well as the typical business application software there are a number of computer aided applications (computer-aided technologies) specifically for engineering. Computers can be used to generate models of fundamental physical processes, which can be solved using numerical methods.\n\nOne of the most widely used design tools in the profession is computer-aided design (CAD) software. It enables engineers to create 3D models, 2D drawings, and schematics of their designs. CAD together with digital mockup (DMU) and CAE software such as finite element method analysis or analytic element method allows engineers to create models of designs that can be analyzed without having to make expensive and time-consuming physical prototypes.\n\nThese allow products and components to be checked for flaws; assess fit and assembly; study ergonomics; and to analyze static and dynamic characteristics of systems such as stresses, temperatures, electromagnetic emissions, electrical currents and voltages, digital logic levels, fluid flows, and kinematics. Access and distribution of all this information is generally organized with the use of product data management software.[69]\n\nThere are also many tools to support specific engineering tasks such as computer-aided manufacturing (CAM) software to generate CNC machining instructions; manufacturing process management software for production engineering; EDA for printed circuit board (PCB) and circuit schematics for electronic engineers; MRO applications for maintenance management; and Architecture, engineering and construction (AEC) software for civil engineering.\n\nIn recent years the use of computer software to aid the development of goods has collectively come to be known as product lifecycle management (PLM).[70]\n\nThe engineering profession engages in a range of activities, from collaboration at the societal level, and smaller individual projects. Almost all engineering projects are obligated to a funding source: a company, a set of investors, or a government. The types of engineering that are less constrained by such a funding source, are pro bono, and open-design engineering.\n\nEngineering has interconnections with society, culture and human behavior. Most products and constructions used by modern society, are influenced by engineering. Engineering activities have an impact on the environment, society, economies, and public safety.\n\nEngineering projects can be controversial. Examples from different engineering disciplines include: the development of nuclear weapons, the Three Gorges Dam, the design and use of sport utility vehicles and the extraction of oil. In response, some engineering companies have enacted serious corporate and social responsibility policies.\n\nThe attainment of many of the Millennium Development Goals requires the achievement of sufficient engineering capacity to develop infrastructure and sustainable technological development.[71]\n\nOverseas development and relief NGOs make considerable use of engineers, to apply solutions in disaster and development scenarios. Some charitable organizations use engineering directly for development:\n\nEngineering companies in more developed economies face challenges with regard to the number of engineers being trained, compared with those retiring. This problem is prominent in the UK where engineering has a poor image and low status.[73] There are negative economic and political issues that this can cause, as well as ethical issues.[74] It is agreed the engineering profession faces an \"image crisis\".[75] The UK holds the most engineering companies compared to other European countries, together with the United States.[citation needed]\n\nMany engineering societies have established codes of practice and codes of ethics to guide members and inform the public at large. The National Society of Professional Engineers code of ethics states:\n\n Engineering is an important and learned profession. As members of this profession, engineers are expected to exhibit the highest standards of honesty and integrity. Engineering has a direct and vital impact on the quality of life for all people. Accordingly, the services provided by engineers require honesty, impartiality, fairness, and equity, and must be dedicated to the protection of the public health, safety, and welfare. Engineers must perform under a standard of professional behavior that requires adherence to the highest principles of ethical conduct.[76]\nIn Canada, engineers wear the Iron Ring as a symbol and reminder of the obligations and ethics associated with their profession.[77]\n\nScientists study the world as it is; engineers create the world that has never been.\nThere exists an overlap between the sciences and engineering practice; in engineering, one applies science. Both areas of endeavor rely on accurate observation of materials and phenomena. Both use mathematics and classification criteria to analyze and communicate observations.[citation needed]\n\nScientists may also have to complete engineering tasks, such as designing experimental apparatus or building prototypes. Conversely, in the process of developing technology, engineers sometimes find themselves exploring new phenomena, thus becoming, for the moment, scientists or more precisely \"engineering scientists\".[81]\n\nIn the book What Engineers Know and How They Know It,[82] Walter Vincenti asserts that engineering research has a character different from that of scientific research. First, it often deals with areas in which the basic physics or chemistry are well understood, but the problems themselves are too complex to solve in an exact manner.\n\nThere is a \"real and important\" difference between engineering and physics as similar to any science field has to do with technology.[83][84] Physics is an exploratory science that seeks knowledge of principles while engineering uses knowledge for practical applications of principles. The former equates an understanding into a mathematical principle while the latter measures variables involved and creates technology.[85][86][87] For technology, physics is an auxiliary and in a way technology is considered as applied physics.[88] Though physics and engineering are interrelated, it does not mean that a physicist is trained to do an engineer's job. A physicist would typically require additional and relevant training.[89] Physicists and engineers engage in different lines of work.[90] But PhD physicists who specialize in sectors of engineering physics and applied physics are titled as Technology officer, R&D Engineers and System Engineers.[91]\n\nAn example of this is the use of numerical approximations to the Navier–Stokes equations to describe aerodynamic flow over an aircraft, or the use of the finite element method to calculate the stresses in complex components. Second, engineering research employs many semi-empirical methods that are foreign to pure scientific research, one example being the method of parameter variation.[92]\n\nAs stated by Fung et al. in the revision to the classic engineering text Foundations of Solid Mechanics:\n\nEngineering is quite different from science. Scientists try to understand nature. Engineers try to make things that do not exist in nature. Engineers stress innovation and invention. To embody an invention the engineer must put his idea in concrete terms, and design something that people can use. That something can be a complex system, device, a gadget, a material, a method, a computing program, an innovative experiment, a new solution to a problem, or an improvement on what already exists. Since a design has to be realistic and functional, it must have its geometry, dimensions, and characteristics data defined. In the past engineers working on new designs found that they did not have all the required information to make design decisions. Most often, they were limited by insufficient scientific knowledge. Thus they studied mathematics, physics, chemistry, biology and mechanics. Often they had to add to the sciences relevant to their profession. Thus engineering sciences were born.[93]\nAlthough engineering solutions make use of scientific principles, engineers must also take into account safety, efficiency, economy, reliability, and constructability or ease of fabrication as well as the environment, ethical and legal considerations such as patent infringement or liability in the case of failure of the solution.[94]\n\nThe study of the human body, albeit from different directions and for different purposes, is an important common link between medicine and some engineering disciplines. Medicine aims to sustain, repair, enhance and even replace functions of the human body, if necessary, through the use of technology.\n\nModern medicine can replace several of the body's functions through the use of artificial organs and can significantly alter the function of the human body through artificial devices such as, for example, brain implants and pacemakers.[95][96] The fields of bionics and medical bionics are dedicated to the study of synthetic implants pertaining to natural systems.\n\nConversely, some engineering disciplines view the human body as a biological machine worth studying and are dedicated to emulating many of its functions by replacing biology with technology. This has led to fields such as artificial intelligence, neural networks, fuzzy logic, and robotics. There are also substantial interdisciplinary interactions between engineering and medicine.[97][98]\n\nBoth fields provide solutions to real world problems. This often requires moving forward before phenomena are completely understood in a more rigorous scientific sense and therefore experimentation and empirical knowledge is an integral part of both.\n\nMedicine, in part, studies the function of the human body. The human body, as a biological machine, has many functions that can be modeled using engineering methods.[99]\n\nThe heart for example functions much like a pump,[100] the skeleton is like a linked structure with levers,[101] the brain produces electrical signals etc.[102] These similarities as well as the increasing importance and application of engineering principles in medicine, led to the development of the field of biomedical engineering that uses concepts developed in both disciplines.\n\nNewly emerging branches of science, such as systems biology, are adapting analytical tools traditionally used for engineering, such as systems modeling and computational analysis, to the description of biological systems.[99]\n\nThere are connections between engineering and art, for example, architecture, landscape architecture and industrial design (even to the extent that these disciplines may sometimes be included in a university's Faculty of Engineering).[104][105][106]\n\nThe Art Institute of Chicago, for instance, held an exhibition about the art of NASA's aerospace design.[107] Robert Maillart's bridge design is perceived by some to have been deliberately artistic.[108] At the University of South Florida, an engineering professor, through a grant with the National Science Foundation, has developed a course that connects art and engineering.[104][109]\n\nAmong famous historical figures, Leonardo da Vinci is a well-known Renaissance artist and engineer, and a prime example of the nexus between art and engineering.[103][110]\n\nBusiness engineering deals with the relationship between professional engineering, IT systems, business administration and change management. Engineering management or \"Management engineering\" is a specialized field of management concerned with engineering practice or the engineering industry sector. The demand for management-focused engineers (or from the opposite perspective, managers with an understanding of engineering), has resulted in the development of specialized engineering management degrees that develop the knowledge and skills needed for these roles. During an engineering management course, students will develop industrial engineering skills, knowledge, and expertise, alongside knowledge of business administration, management techniques, and strategic thinking. Engineers specializing in change management must have in-depth knowledge of the application of industrial and organizational psychology principles and methods. Professional engineers often train as certified management consultants in the very specialized field of management consulting applied to engineering practice or the engineering sector. This work often deals with large scale complex business transformation or business process management initiatives in aerospace and defence, automotive, oil and gas, machinery, pharmaceutical, food and beverage, electrical and electronics, power distribution and generation, utilities and transportation systems. This combination of technical engineering practice, management consulting practice, industry sector knowledge, and change management expertise enables professional engineers who are also qualified as management consultants to lead major business transformation initiatives. These initiatives are typically sponsored by C-level executives.\n\nIn political science, the term engineering has been borrowed for the study of the subjects of social engineering and political engineering, which deal with forming political and social structures using engineering methodology coupled with political science principles. Marketing engineering and financial engineering have similarly borrowed the term.\n"
    },
    {
        "title": "Medicine",
        "content": "This is an accepted version of this page\n\n\n\n\nMedicine is the science[1] and practice[2] of caring for patients, managing the diagnosis, prognosis, prevention, treatment, palliation of their injury or disease, and promoting their health. Medicine encompasses a variety of health care practices evolved to maintain and restore health by the prevention and treatment of illness. Contemporary medicine applies biomedical sciences, biomedical research, genetics, and medical technology to diagnose, treat, and prevent injury and disease, typically through pharmaceuticals or surgery, but also through therapies as diverse as psychotherapy, external splints and traction, medical devices, biologics, and ionizing radiation, amongst others.[3]\n\nMedicine has been practiced since prehistoric times, and for most of this time it was an art (an area of creativity and skill), frequently having connections to the religious and philosophical beliefs of local culture. For example, a medicine man would apply herbs and say prayers for healing, or an ancient philosopher and physician would apply bloodletting according to the theories of humorism. In recent centuries, since the advent of modern science, most medicine has become a combination of art and science (both basic and applied, under the umbrella of medical science). For example, while stitching technique for sutures is an art learned through practice, knowledge of what happens at the cellular and molecular level in the tissues being stitched arises through science.\n\nPrescientific forms of medicine, now known as traditional medicine or folk medicine, remain commonly used in the absence of scientific medicine and are thus called alternative medicine. Alternative treatments outside of scientific medicine with ethical, safety and efficacy concerns are termed quackery.\n\nMedicine (UK: /ˈmɛdsɪn/ ⓘ, US: /ˈmɛdɪsɪn/ ⓘ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease.[4][5] The word \"medicine\" is derived from Latin medicus, meaning \"a physician\".[a][7] The word \"physic\" itself, from which \"physician\" derives, was the old word for what is now called a medicine, and also the field of medicine.[8]\n\nMedical availability and clinical practice vary across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners.[9]\n\nIn the developed world, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.[10]\n\nIn modern clinical practice, physicians and physician assistants personally assess patients to diagnose, prognose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins with an interaction with an examination of the patient's medical history and medical record, followed by a medical interview[11] and a physical examination. Basic diagnostic medical devices (e.g., stethoscope, tongue depressor) are typically used. After examining for signs and interviewing for symptoms, the doctor may order medical tests (e.g., blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions.[12] Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks, depending on the complexity of the issue.\n\nThe components of the medical interview[11] and encounter are:\n\nThe physical examination is the examination of the patient for medical signs of disease that are objective and observable, in contrast to symptoms that are volunteered by the patient and are not necessarily objectively observable.[13] The healthcare provider uses sight, hearing, touch, and sometimes smell (e.g., in infection, uremia, diabetic ketoacidosis). Four actions are the basis of physical examination: inspection, palpation (feel), percussion (tap to determine resonance characteristics), and auscultation (listen), generally in that order, although auscultation occurs prior to percussion and palpation for abdominal assessments.[14]\n\nThe clinical examination involves the study of:[15]\n\nIt is to likely focus on areas of interest highlighted in the medical history and may not include everything listed above.\n\nThe treatment plan may include ordering additional medical laboratory tests and medical imaging studies, starting therapy, referral to a specialist, or watchful observation. A follow-up may be advised. Depending upon the health insurance plan and the managed care system, various forms of \"utilization review\", such as prior authorization of tests, may place barriers on accessing expensive services.[16]\n\nThe medical decision-making (MDM) process includes the analysis and synthesis of all the above data to come up with a list of possible diagnoses (the differential diagnoses), along with an idea of what needs to be done to obtain a definitive diagnosis that would explain the patient's problem.\n\nOn subsequent visits, the process may be repeated in an abbreviated manner to obtain any new history, symptoms, physical findings, lab or imaging results, or specialist consultations.\n\nContemporary medicine is, in general, conducted within health care systems. Legal, credentialing, and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have a significant impact on the way medical care is provided.\n\nFrom ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals, and the Catholic Church today remains the largest non-government provider of medical services in the world.[17] Advanced industrial countries (with the exception of the United States)[18][19] and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system or compulsory private or cooperative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices, state-owned hospitals and clinics, or charities, most commonly a combination of all three.\n\nMost tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those who can afford to pay for it, have self-insured it (either directly or as part of an employment contract), or may be covered by care financed directly by the government or tribe.\n\nTransparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice of patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for its lack of openness,[20] new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other.\n\nThe health professionals who provide care in medicine comprise multiple professions, such as medics, nurses, physiotherapists, and psychologists. These professions will have their own ethical standards, professional education, and bodies. The medical profession has been conceptualized from a sociological perspective.[21]\n\nProvision of medical care is classified into primary, secondary, and tertiary care categories.[22]\n\nPrimary care medical services are provided by physicians, physician assistants, nurse practitioners, or other health professionals who have first contact with a patient seeking medical treatment or care.[23] These occur in physician offices, clinics, nursing homes, schools, home visits, and other places close to patients. About 90% of medical visits can be treated by the primary care provider. These include treatment of acute and chronic illnesses, preventive care and health education for all ages and both sexes.\n\nSecondary care medical services are provided by medical specialists in their offices or clinics or at local community hospitals for a patient referred by a primary care provider who first diagnosed or treated the patient.[24] Referrals are made for those patients who required the expertise or procedures performed by specialists. These include both ambulatory care and inpatient services, emergency departments, intensive care medicine, surgery services, physical therapy, labor and delivery, endoscopy units, diagnostic laboratory and medical imaging services, hospice centers, etc. Some primary care providers may also take care of hospitalized patients and deliver babies in a secondary care setting.\n\nTertiary care medical services are provided by specialist hospitals or regional centers equipped with diagnostic and treatment facilities not generally available at local hospitals. These include trauma centers, burn treatment centers, advanced neonatology unit services, organ transplants, high-risk pregnancy, radiation oncology, etc.\n\nModern medical care also depends on information – still delivered in many health care settings on paper records, but increasingly nowadays by electronic means.\n\nIn low-income countries, modern healthcare is often too expensive for the average person. International healthcare policy researchers have advocated that \"user fees\" be removed in these areas to ensure access, although even after removal, significant costs and barriers remain.[25]\n\nSeparation of prescribing and dispensing is a practice in medicine and pharmacy in which the physician who provides a medical prescription is independent from the pharmacist who provides the prescription drug. In the Western world there are centuries of tradition for separating pharmacists from physicians. In Asian countries, it is traditional for physicians to also provide drugs.[26]\n\nWorking together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, medical physicists, surgeons, surgeon's assistant, surgical technologist.\n\nThe scope and sciences underpinning human medicine overlap many other fields. A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments.\n\nPhysicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in.\n\nThe main branches of medicine are:\n\nIn the broadest meaning of \"medicine\", there are many different specialties. In the UK, most specialities have their own body or college, which has its own entrance examination. These are collectively known as the Royal Colleges, although not all currently use the term \"Royal\". The development of a speciality is often driven by new technology (such as the development of effective anaesthetics) or ways of working (such as emergency departments); the new specialty leads to the formation of a unifying body of doctors and the prestige of administering their own examination.\n\nWithin medical circles, specialities usually fit into one of two broad categories: \"Medicine\" and \"Surgery\". \"Medicine\" refers to the practice of non-operative medicine, and most of its subspecialties require preliminary training in Internal Medicine. In the UK, this was traditionally evidenced by passing the examination for the Membership of the Royal College of Physicians (MRCP) or the equivalent college in Scotland or Ireland. \"Surgery\" refers to the practice of operative medicine, and most subspecialties in this area require preliminary training in General Surgery, which in the UK leads to membership of the Royal College of Surgeons of England (MRCS). At present, some specialties of medicine do not fit easily into either of these categories, such as radiology, pathology, or anesthesia. Most of these have branched from one or other of the two camps above; for example anaesthesia developed first as a faculty of the Royal College of Surgeons (for which MRCS/FRCS would have been required) before becoming the Royal College of Anaesthetists and membership of the college is attained by sitting for the examination of the Fellowship of the Royal College of Anesthetists (FRCA).\n\nSurgery is an ancient medical specialty that uses operative manual and instrumental techniques on a patient to investigate or treat a pathological condition such as disease or injury, to help improve bodily function or appearance or to repair unwanted ruptured areas (for example, a perforated ear drum). Surgeons must also manage pre-operative, post-operative, and potential surgical candidates on the hospital wards. In some centers, anesthesiology is part of the division of surgery (for historical and logistical reasons), although it is not a surgical discipline. Other medical specialties may employ surgical procedures, such as ophthalmology and dermatology, but are not considered surgical sub-specialties per se.\n\nSurgical training in the U.S. requires a minimum of five years of residency after medical school. Sub-specialties of surgery often require seven or more years. In addition, fellowships can last an additional one to three years. Because post-residency fellowships can be competitive, many trainees devote two additional years to research. Thus in some cases surgical training will not finish until more than a decade after medical school. Furthermore, surgical training can be very difficult and time-consuming.\n\nSurgical subspecialties include those a physician may specialize in after undergoing general surgery residency training as well as several surgical fields with separate residency training. Surgical subspecialties that one may pursue following general surgery residency training: [27]\n\nOther surgical specialties within medicine with their own individual residency training:\n\nInternal medicine is the medical specialty dealing with the prevention, diagnosis, and treatment of adult diseases.[28] According to some sources, an emphasis on internal structures is implied.[29] In North America, specialists in internal medicine are commonly called \"internists\". Elsewhere, especially in Commonwealth nations, such specialists are often called physicians.[30] These terms, internist or physician (in the narrow sense, common outside North America), generally exclude practitioners of gynecology and obstetrics, pathology, psychiatry, and especially surgery and its subspecialities.\n\nBecause their patients are often seriously ill or require complex investigations, internists do much of their work in hospitals. Formerly, many internists were not subspecialized; such general physicians would see any complex nonsurgical problem; this style of practice has become much less common. In modern urban practice, most internists are subspecialists: that is, they generally limit their medical practice to problems of one organ system or to one particular area of medical knowledge. For example, gastroenterologists and nephrologists specialize respectively in diseases of the gut and the kidneys.[31]\n\nIn the Commonwealth of Nations and some other countries, specialist pediatricians and geriatricians are also described as specialist physicians (or internists) who have subspecialized by age of patient rather than by organ system. Elsewhere, especially in North America, general pediatrics is often a form of primary care.\n\nThere are many subspecialities (or subdisciplines) of internal medicine:\n\nTraining in internal medicine (as opposed to surgical training), varies considerably across the world: see the articles on medical education for more details. In North America, it requires at least three years of residency training after medical school, which can then be followed by a one- to three-year fellowship in the subspecialties listed above. In general, resident work hours in medicine are less than those in surgery, averaging about 60 hours per week in the US. This difference does not apply in the UK where all doctors are now required by law to work less than 48 hours per week on average.\n\nThe following are some major medical specialties that do not directly fit into any of the above-mentioned groups:\n\nSome interdisciplinary sub-specialties of medicine include:\n\nMedical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university.\n\nSince knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs. A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/ Archived 4 October 2018 at the Wayback Machine.[33]\n\nIn most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in \"evidence based\", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health.\n\nIn the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification. The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC.\n\nDoctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions.\n\nMedical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:\n\nValues such as these do not give answers as to how to handle a particular situation, but provide a useful framework for understanding conflicts. When moral values are in conflict, the result may be an ethical dilemma or crisis. Sometimes, no good solution to a dilemma in medical ethics exists, and occasionally, the values of the medical community (i.e., the hospital and its staff) conflict with the values of the individual patient, family, or larger non-medical community. Conflicts can also arise between health care providers, or among family members. For example, some argue that the principles of autonomy and beneficence clash when patients refuse blood transfusions, considering them life-saving; and truth-telling was not emphasized to a large extent before the HIV era.\n\nPrehistoric medicine incorporated plants (herbalism), animal parts, and minerals. In many cases these materials were used ritually as magical substances by priests, shamans, or medicine men. Well-known spiritual systems include animism (the notion of inanimate objects having spirits), spiritualism (an appeal to gods or communion with ancestor spirits); shamanism (the vesting of an individual with mystic powers); and divination (magically obtaining the truth). The field of medical anthropology examines the ways in which culture and society are organized around or impacted by issues of health, health care and related issues.\n\nThe earliest known medical texts in the world were found in the ancient Syrian city of Ebla and date back to 2500 BCE.[34][35][36] Other early records on medicine have been discovered from ancient Egyptian medicine, Babylonian Medicine, Ayurvedic medicine (in the Indian subcontinent), classical Chinese medicine (Alternative medicine) predecessor to the modern traditional Chinese medicine), and ancient Greek medicine and Roman medicine.\n\nIn Egypt, Imhotep (3rd millennium BCE) is the first physician in history known by name. The oldest Egyptian medical text is the Kahun Gynaecological Papyrus from around 2000 BCE, which describes gynaecological diseases. The Edwin Smith Papyrus dating back to 1600 BCE is an early work on surgery, while the Ebers Papyrus dating back to 1500 BCE is akin to a textbook on medicine.[37]\n\nIn China, archaeological evidence of medicine in Chinese dates back to the Bronze Age Shang dynasty, based on seeds for herbalism and tools presumed to have been used for surgery.[38] The Huangdi Neijing, the progenitor of Chinese medicine, is a medical text written beginning in the 2nd century BCE and compiled in the 3rd century.[39]\n\nIn India, the surgeon Sushruta described numerous surgical operations, including the earliest forms of plastic surgery.[40][unreliable source?][citation needed]Earliest records of dedicated hospitals come from Mihintale in Sri Lanka where evidence of dedicated medicinal treatment facilities for patients are found.[41][42]\n\nIn Greece, the ancient Greek physician Hippocrates, the \"father of modern medicine\",[43][44] laid the foundation for a rational approach to medicine. Hippocrates introduced the Hippocratic Oath for physicians, which is still relevant and in use today, and was the first to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, \"exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence\".[45][46] The Greek physician Galen was also one of the greatest surgeons of the ancient world and performed many audacious operations, including brain and eye surgeries. After the fall of the Western Roman Empire and the onset of the Early Middle Ages, the Greek tradition of medicine went into decline in Western Europe, although it continued uninterrupted in the Eastern Roman (Byzantine) Empire.\n\nMost of our knowledge of ancient Hebrew medicine during the 1st millennium BC comes from the Torah, i.e. the Five Books of Moses, which contain various health related laws and rituals. The Hebrew contribution to the development of modern medicine started in the Byzantine Era, with the physician Asaph the Jew.[47]\n\nThe concept of hospital as institution to offer medical care and possibility of a cure for the patients due to the ideals of Christian charity, rather than just merely a place to die, appeared in the Byzantine Empire.[48]\n\nAlthough the concept of uroscopy was known to Galen, he did not see the importance of using it to localize the disease. It was under the Byzantines with physicians such of Theophilus Protospatharius that they realized the potential in uroscopy to determine disease in a time when no microscope or stethoscope existed. That practice eventually spread to the rest of Europe.[49]\n\nAfter 750 CE, the Muslim world had the works of Hippocrates, Galen and Sushruta translated into Arabic, and Islamic physicians engaged in some significant medical research. Notable Islamic medical pioneers include the Persian polymath, Avicenna, who, along with Imhotep and Hippocrates, has also been called the \"father of medicine\".[50] He wrote The Canon of Medicine which became a standard medical text at many medieval European universities,[51] considered one of the most famous books in the history of medicine.[52] Others include Abulcasis,[53] Avenzoar,[54] Ibn al-Nafis,[55] and Averroes.[56] Persian physician Rhazes[57] was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine.[58] Some volumes of Rhazes's work Al-Mansuri, namely \"On Surgery\" and \"A General Book on Therapy\", became part of the medical curriculum in European universities.[59] Additionally, he has been described as a doctor's doctor,[60] the father of pediatrics,[57][61] and a pioneer of ophthalmology. For example, he was the first to recognize the reaction of the eye's pupil to light.[61] The Persian Bimaristan hospitals were an early example of public hospitals.[62][63]\n\nIn Europe, Charlemagne decreed that a hospital should be attached to each cathedral and monastery and the historian Geoffrey Blainey likened the activities of the Catholic Church in health care during the Middle Ages to an early version of a welfare state: \"It conducted hospitals for the old and orphanages for the young; hospices for the sick of all ages; places for the lepers; and hostels or inns where pilgrims could buy a cheap bed and meal\". It supplied food to the population during famine and distributed food to the poor. This welfare system the church funded through collecting taxes on a large scale and possessing large farmlands and estates. The Benedictine order was noted for setting up hospitals and infirmaries in their monasteries, growing medical herbs and becoming the chief medical care givers of their districts, as at the great Abbey of Cluny. The Church also established a network of cathedral schools and universities where medicine was studied. The Schola Medica Salernitana in Salerno, looking to the learning of Greek and Arab physicians, grew to be the finest medical school in Medieval Europe.[64]\n\nHowever, the fourteenth and fifteenth century Black Death devastated both the Middle East and Europe, and it has even been argued that Western Europe was generally more effective in recovering from the pandemic than the Middle East.[b] In the early modern period, important early figures in medicine and anatomy emerged in Europe, including Gabriele Falloppio and William Harvey.\n\nThe major shift in medical thinking was the gradual rejection, especially during the Black Death in the 14th and 15th centuries, of what may be called the \"traditional authority\" approach to science and medicine. This was the notion that because some prominent person in the past said something must be so, then that was the way it was, and anything one observed to the contrary was an anomaly (which was paralleled by a similar shift in European society in general – see Copernicus's rejection of Ptolemy's theories on astronomy). Physicians like Vesalius improved upon or disproved some of the theories from the past. The main tomes used both by medicine students and expert physicians were Materia Medica and Pharmacopoeia.\n\nAndreas Vesalius was the author of De humani corporis fabrica, an important book on human anatomy.[66] Bacteria and microorganisms were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field microbiology.[67] Independently from Ibn al-Nafis, Michael Servetus rediscovered the pulmonary circulation, but this discovery did not reach the public because it was written down for the first time in the \"Manuscript of Paris\"[68] in 1546, and later published in the theological work for which he paid with his life in 1553. Later this was described by Renaldus Columbus and Andrea Cesalpino. Herman Boerhaave is sometimes referred to as a \"father of physiology\" due to his exemplary teaching in Leiden and textbook 'Institutiones medicae' (1708). Pierre Fauchard has been called \"the father of modern dentistry\".[69]\n\nVeterinary medicine was, for the first time, truly separated from human medicine in 1761, when the French veterinarian Claude Bourgelat founded the world's first veterinary school in Lyon, France. Before this, medical doctors treated both humans and other animals.\n\nModern scientific biomedical research (where results are testable and reproducible) began to replace early Western traditions based on herbalism, the Greek \"four humours\" and other such pre-modern notions. The modern era really began with Edward Jenner's discovery of the smallpox vaccine at the end of the 18th century (inspired by the method of variolation originated in ancient China),[70] Robert Koch's discoveries around 1880 of the transmission of disease by bacteria, and then the discovery of antibiotics around 1900.\n\nThe post-18th century modernity period brought more groundbreaking researchers from Europe. From Germany and Austria, doctors Rudolf Virchow, Wilhelm Conrad Röntgen, Karl Landsteiner and Otto Loewi made notable contributions. In the United Kingdom, Alexander Fleming, Joseph Lister, Francis Crick and Florence Nightingale are considered important. Spanish doctor Santiago Ramón y Cajal is considered the father of modern neuroscience.\n\nFrom New Zealand and Australia came Maurice Wilkins, Howard Florey, and Frank Macfarlane Burnet.\n\nOthers that did significant work include William Williams Keen, William Coley, James D. Watson (United States); Salvador Luria (Italy); Alexandre Yersin (Switzerland); Kitasato Shibasaburō (Japan); Jean-Martin Charcot, Claude Bernard, Paul Broca (France); Adolfo Lutz (Brazil); Nikolai Korotkov (Russia); Sir William Osler (Canada); and Harvey Cushing (United States).\n\nAs science and technology developed, medicine became more reliant upon medications. Throughout history and in Europe right until the late 18th century, not only plant products were used as medicine, but also animal (including human) body parts and fluids.[71] Pharmacology developed in part from herbalism and some drugs are still derived from plants (atropine, ephedrine, warfarin, aspirin, digoxin, vinca alkaloids,[72] taxol, hyoscine, etc.).[73] Vaccines were discovered by Edward Jenner and Louis Pasteur.\n\nThe first antibiotic was arsphenamine (Salvarsan) discovered by Paul Ehrlich in 1908 after he observed that bacteria took up toxic dyes that human cells did not. The first major class of antibiotics was the sulfa drugs, derived by German chemists originally from azo dyes.\n\nPharmacology has become increasingly sophisticated; modern biotechnology allows drugs targeted towards specific physiological processes to be developed, sometimes designed for compatibility with the body to reduce side-effects. Genomics and knowledge of human genetics and human evolution is having increasingly significant influence on medicine, as the causative genes of most monogenic genetic disorders have now been identified, and the development of techniques in molecular biology, evolution, and genetics are influencing medical technology, practice and decision-making.\n\nEvidence-based medicine is a contemporary movement to establish the most effective algorithms of practice (ways of doing things) through the use of systematic reviews and meta-analysis. The movement is facilitated by modern global information science, which allows as much of the available evidence as possible to be collected and analyzed according to standard protocols that are then disseminated to healthcare providers. The Cochrane Collaboration leads this movement. A 2001 review of 160 Cochrane systematic reviews revealed that, according to two readers, 21.3% of the reviews concluded insufficient evidence, 20% concluded evidence of no effect, and 22.5% concluded positive effect.[74]\n\nEvidence-based medicine, prevention of medical error (and other \"iatrogenesis\"), and avoidance of unnecessary health care are a priority in modern medical systems. These topics generate significant political and public policy attention, particularly in the United States where healthcare is regarded as excessively costly but population health metrics lag similar nations.[75]\n\nGlobally, many developing countries lack access to care and access to medicines.[76] As of 2015[update], most wealthy developed countries provide health care to all citizens, with a few exceptions such as the United States where lack of health insurance coverage may limit access.[77]\n\n\n"
    },
    {
        "title": "Finance",
        "content": "Finance refers to monetary resources and to the study and discipline of money, currency, assets and liabilities.[a] As a subject of study, it is related to but distinct from economics, which is the study of the production, distribution, and consumption of goods and services.[b] \nBased on the scope of financial activities in financial systems, the discipline can be divided into personal, corporate, and public finance.\n\nIn these financial systems, assets are bought, sold, or traded as financial instruments, such as currencies, loans, bonds, shares, stocks, options, futures, etc. Assets can also be banked, invested, and insured to maximize value and minimize loss. In practice, risks are always present in any financial action and entities.\n\nDue to its wide scope, a broad range of subfields exists within finance. Asset-, money-, risk- and investment management aim to maximize value and minimize volatility. Financial analysis assesses the viability, stability, and profitability of an action or entity. Some fields are multidisciplinary, such as mathematical finance, financial law, financial economics, financial engineering and financial technology. These fields are the foundation of business and accounting. In some cases, theories in finance can be tested using the scientific method, covered by experimental finance.\n\nThe early history of finance parallels the early history of money, which is prehistoric. Ancient and medieval civilizations incorporated basic functions of finance, such as banking, trading and accounting, into their economies. In the late 19th century, the global financial system was formed.\n\nIn the middle of the 20th century, finance emerged as a distinct academic discipline,[c] separate from economics.[1] The earliest doctoral programs in finance were established in the 1960s and 1970s.[2]\nToday, finance is also widely studied through career-focused undergraduate and master's level programs.[3][4]\n\nAs outlined, the financial system consists of the flows of capital that take place between individuals and households (personal finance), governments (public finance), and businesses (corporate finance). \n\"Finance\" thus studies the process of channeling money from savers and investors to entities that need it.[d]\nSavers and investors have money available which could earn interest or dividends if put to productive use. Individuals, companies and governments must obtain money from some external source, such as loans or credit, when they lack sufficient funds to run their operations.\n\nIn general, an entity whose income exceeds its expenditure can lend or invest the excess, intending to earn a fair return. Correspondingly, an entity where income is less than expenditure can raise capital usually in one of two ways: \n(i) by borrowing in the form of a loan (private individuals), or by selling government or corporate bonds; \n(ii) by a corporation selling equity, also called stock or shares (which may take various forms: preferred stock or common stock). \nThe owners of both bonds and stock may be institutional investors—financial institutions such as investment banks and pension funds—or private individuals, called private investors or retail investors. (See Financial market participants.)\n\nThe lending is often indirect, through a financial intermediary such as a bank, or via the purchase of notes or bonds (corporate bonds, government bonds, or mutual bonds) in the bond market. \nThe lender receives interest, the borrower pays a higher interest than the lender receives, and the financial intermediary earns the difference for arranging the loan.[6][7][8]\nA bank aggregates the activities of many borrowers and lenders. A bank accepts deposits from lenders, on which it pays interest. The bank then lends these deposits to borrowers. Banks allow borrowers and lenders, of different sizes, to coordinate their activity.\n\nInvesting typically entails the purchase of stock, either individual securities or via a mutual fund, for example. Stocks are usually sold by corporations to investors so as to raise required capital in the form of \"equity financing\", as distinct from the debt financing described above. The financial intermediaries here are the investment banks. The investment banks find the initial investors and facilitate the listing of the securities, typically shares and bonds. \nAdditionally, they facilitate the securities exchanges, which allow their trade thereafter, as well as the various service providers which manage the performance or risk of these investments. These latter include mutual funds, pension funds, wealth managers, and stock brokers, typically servicing retail investors (private individuals).\n\nInter-institutional trade and investment, and fund-management at this scale, is referred to as \"wholesale finance\". \nInstitutions here extend the products offered, with related trading, to include bespoke options, swaps, and structured products, as well as specialized financing; this \"financial engineering\" is inherently mathematical, and these institutions are then the major employers of \"quants\" (see below). \nIn these institutions, risk management, regulatory capital, and compliance play major roles.\n\nAs outlined, finance comprises, broadly, the three areas of personal finance, corporate finance, and public finance.\nThese, in turn, overlap and employ various activities and sub-disciplines—chiefly investments, risk management, and quantitative finance.\n\nPersonal finance refers to the practice of budgeting to ensure enough funds are available to meet basic needs, while ensuring there is only a reasonable level of risk to lose said capital. Personal finance may involve paying for education, financing durable goods such as real estate and cars, buying insurance, investing, and saving for retirement.[9]\nPersonal finance may also involve paying for a loan or other debt obligations. \nThe main areas of personal finance are considered to be income, spending, saving, investing, and protection. \nThe following steps, as outlined by the Financial Planning Standards Board,[10] suggest that an individual will understand a potentially secure personal finance plan after:\n\nCorporate finance deals with the actions that managers take to increase the value of the firm to the shareholders, the sources of funding and the capital structure of corporations, and the tools and analysis used to allocate financial resources. \nWhile corporate finance is in principle different from managerial finance, which studies the financial management of all firms rather than corporations alone, the concepts are applicable to the financial problems of all firms,[12] \nand this area is then often referred to as \"business finance\".\n\nTypically, \"corporate finance\" relates to the long term objective of maximizing the value of the entity's assets, its stock, and its return to shareholders, while also balancing risk and profitability. This entails[13] three primary areas: \n\nThe latter creates the link with investment banking and securities trading, as above, in that the capital raised will generically comprise debt, i.e. corporate bonds, and equity, often listed shares.\nRe risk management within corporates, see below.\n\nFinancial managers—i.e. as distinct from corporate financiers—focus more on the short term elements of profitability, cash flow, and \"working capital management\" (inventory, credit and debtors), ensuring that the firm can safely and profitably carry out its financial and operational objectives; i.e. that it: \n(1) can service both maturing short-term debt repayments, and scheduled long-term debt payments, \nand (2) has sufficient cash flow for ongoing and upcoming operational expenses. (See Financial management and Financial planning and analysis.)\n\nPublic finance describes finance as related to sovereign states, sub-national entities, and related public entities or agencies. It generally encompasses a long-term strategic perspective regarding investment decisions that affect public entities.[15] These long-term strategic periods typically encompass five or more years.[16] Public finance is primarily concerned with:[17]\n\nCentral banks, such as the Federal Reserve System banks in the United States and the Bank of England in the United Kingdom, are strong players in public finance. They act as lenders of last resort as well as strong influences on monetary and credit conditions in the economy.[18]\n\nDevelopment finance, which is related, concerns investment in economic development projects provided by a (quasi) governmental institution on a non-commercial basis; these projects would otherwise not be able to get financing.\nA public–private partnership is primarily used for infrastructure projects: a private sector corporate provides the financing up-front, and then draws profits from taxpayers or users.\nClimate finance, and the related Environmental finance, address the financial strategies, resources and instruments used in climate change mitigation.\n\nInvestment management[12] is the professional asset management of various securities—typically shares and bonds, but also other assets, such as real estate, commodities and alternative investments—in order to meet specified investment goals for the benefit of investors.\n\nAs above, investors may be institutions, such as insurance companies, pension funds, corporations, charities, educational establishments, or private investors, either directly via investment contracts or, more commonly, via collective investment schemes like mutual funds, exchange-traded funds, or real estate investment trusts.\n\nAt the heart of investment management[12] is asset allocation—diversifying the exposure among these asset classes, and among individual securities within each asset class—as appropriate to the client's investment policy, in turn, a function of risk profile, investment goals, and investment horizon (see Investor profile). Here:\n\nOverlaid is the portfolio manager's investment style—broadly, active vs passive, value vs growth, and small cap vs. large cap—and investment strategy.\n\nIn a well-diversified portfolio, achieved investment performance will, in general, largely be a function of the asset mix selected, while the individual securities are less impactful. The specific approach or philosophy will also be significant, depending on the extent to which it is complementary with the market cycle.\nRisk management here is discussed immediately below.\n\nA quantitative fund is managed using computer-based mathematical techniques (increasingly, machine learning) instead of human judgment. The actual trading is typically automated via sophisticated algorithms.\n\nRisk management, in general, is the study of how to control risks and balance the possibility of gains; it is the process of measuring risk and then developing and implementing strategies to manage that risk. \nFinancial risk management[20][21] is the practice of protecting corporate value against financial risks, often by \"hedging\" exposure to these using financial instruments. \nThe focus is particularly on credit and market risk, and in banks, through regulatory capital, includes operational risk.\n\nFinancial risk management is related to corporate finance[12] in two ways. \nFirstly, firm exposure to market risk is a direct result of previous capital investments and funding decisions; \nwhile credit risk arises from the business's credit policy and is often addressed through credit insurance and provisioning.\nSecondly, both disciplines share the goal of enhancing or at least preserving, the firm's economic value, and in this context[22] overlaps also enterprise risk management, typically the domain of strategic management.  \nHere, businesses devote much time and effort to forecasting, analytics and performance monitoring. (See ALM and treasury management.)\n\nFor banks and other wholesale institutions,[23] risk management focuses on managing, and as necessary hedging, the various positions held by the institution—both trading positions and long term exposures—and on calculating and monitoring the resultant economic capital, and regulatory capital under Basel III. \nThe calculations here are mathematically sophisticated, and within the domain of quantitative finance as below. Credit risk is inherent in the business of banking, but additionally, these institutions are exposed to counterparty credit risk. Banks typically employ Middle office \"Risk Groups\", whereas front office risk teams provide risk \"services\" (or \"solutions\") to customers.\n\nAdditional to diversification, the fundamental risk mitigant here, investment managers will apply various hedging techniques as appropriate,[12] these may relate to the portfolio as a whole or to individual stocks. Bond portfolios are often (instead) managed via cash flow matching or immunization, while for derivative portfolios and positions, traders use \"the Greeks\" to measure and then offset sensitivities. In parallel, managers — active and passive — will monitor tracking error, thereby minimizing and preempting any underperformance vs their \"benchmark\".\n\nQuantitative finance—also referred to as \"mathematical finance\"—includes those finance activities where a sophisticated mathematical model is required,[24] and thus overlaps several of the above.\n\nAs a specialized practice area, quantitative finance comprises primarily three sub-disciplines; the underlying theory and techniques are discussed in the next section:\n\nDCF valuation formula widely applied in business and finance, since articulated in 1938. Here, to get the value of the firm, its forecasted free cash flows are discounted to the present using the weighted average cost of capital for the discount factor.\nFor share valuation investors use the related dividend discount model.\n\nFinancial theory is studied and developed within the disciplines of management, (financial) economics, accountancy and applied mathematics.\nAbstractly,[12][25] finance is concerned with the investment and deployment of assets and liabilities over \"space and time\"; \ni.e., it is about performing valuation and asset allocation today, based on the risk and uncertainty of future outcomes while appropriately incorporating the time value of money. \nDetermining the present value of these future values, \"discounting\", must be at the risk-appropriate discount rate, in turn, a major focus of finance-theory.[26]\nAs financial theory has roots in many disciplines, including mathematics, statistics, economics, physics, and psychology, it can be considered a mix of an art and science,[27] and there are ongoing related efforts to organize a list of unsolved problems in finance.\n\nManagerial finance is the branch of finance that deals with the financial aspects of the management of a company, and the financial dimension of managerial decision-making more broadly.[citation needed]\nIt provides the theoretical underpin for the practice described above, concerning itself with the managerial application of the various finance techniques.\nAcademics working in this area are typically based in business school finance departments, in accounting, or in management science.\n\nThe tools addressed and developed relate in the main to managerial accounting and corporate finance: \nthe former allow management to better understand, and hence act on, financial information relating to profitability and performance; the latter, as above, are about optimizing the overall financial structure, including its impact on working capital.\nKey aspects of managerial finance thus include:\n\nThe discussion, however, extends to business strategy more broadly, emphasizing alignment with the company's overall strategic objectives; and similarly incorporates the managerial perspectives of planning, directing, and controlling.\n\nFinancial economics[30] is the branch of economics that studies the interrelation of financial variables, such as prices, interest rates and shares, as opposed to real economic variables, i.e. goods and services. \nIt thus centers on pricing, decision making, and risk management in the financial markets,[30][25] and produces many of the commonly employed financial models. (Financial econometrics is the branch of financial economics that uses econometric techniques to parameterize the relationships suggested.)\n\nThe discipline has two main areas of focus:[25] asset pricing and corporate finance; the first being the perspective of providers of capital, i.e. investors, and the second of users of capital; respectively:\n\nFinancial mathematics[32] is the field of applied mathematics concerned with financial markets; \nLouis Bachelier's doctoral thesis, defended in 1900, is considered to be the first scholarly work in this area. \nThe field is largely focused on the modeling of derivatives—with much emphasis on interest rate- and credit risk modeling—while other important areas include insurance mathematics and quantitative portfolio management.\nRelatedly, the techniques developed are applied to pricing and hedging a wide range of asset-backed, government, and corporate-securities.\n\nAs above, in terms of practice, the field is referred to as quantitative finance and / or mathematical finance, and comprises primarily the three areas discussed.\nThe main mathematical tools and techniques are, correspondingly:\n\nMathematically, these separate into two analytic branches:\nderivatives pricing uses risk-neutral probability (or arbitrage-pricing probability), denoted by \"Q\";\nwhile risk and portfolio management generally use physical (or actual or actuarial) probability, denoted by \"P\".\nThese are interrelated through the above \"Fundamental theorem of asset pricing\".\n\nThe subject has a close relationship with financial economics, which, as outlined, is concerned with much of the underlying theory that is involved in financial mathematics:  generally, financial mathematics will derive and extend the mathematical models suggested. \nComputational finance is the branch of (applied) computer science that deals with problems of practical interest in finance, and especially[32] emphasizes the numerical methods applied here.\n\nExperimental finance[35]\naims to establish different market settings and environments to experimentally observe and provide a lens through which science can analyze agents' behavior and the resulting characteristics of trading flows, information diffusion, and aggregation, price setting mechanisms, and returns processes. Researchers in experimental finance can study to what extent existing financial economics theory makes valid predictions and therefore prove them, as well as attempt to discover new principles on which such theory can be extended and be applied to future financial decisions. Research may proceed by conducting trading simulations or by establishing and studying the behavior of people in artificial, competitive, market-like settings.\n\nBehavioral finance studies how the psychology of investors or managers affects financial decisions and markets[36] \nand is relevant when making a decision that can impact either negatively or positively on one of their areas. With more in-depth research into behavioral finance, it is possible to bridge what actually happens in financial markets with analysis based on financial theory.[37] \nBehavioral finance has grown over the last few decades to become an integral aspect of finance.[38]\n\nBehavioral finance includes such topics as:\n\nA strand of behavioral finance has been dubbed quantitative behavioral finance, which uses mathematical and statistical methodology to understand behavioral biases in conjunction with valuation. \n\n\n\nQuantum finance involves applying quantum mechanical approaches to financial theory, providing novel methods and perspectives in the field.[39] Quantum finance is an interdisciplinary field, in which theories and methods developed by quantum physicists and economists are applied to solve financial problems. It represents a branch known as econophysics. Although quantum computational methods have been around for quite some time and use the basic principles of physics to better understand the ways to implement and manage cash flows, it is mathematics that is actually important in this new scenario[40] \nFinance theory is heavily based on financial instrument pricing such as stock option pricing. Many of the problems facing the finance community have no known analytical solution. As a result, numerical methods and computer simulations for solving these problems have proliferated. This research area is known as computational finance. Many computational finance problems have a high degree of computational complexity and are slow to converge to a solution on classical computers. In particular, when it comes to option pricing, there is additional complexity resulting from the need to respond to quickly changing markets. For example, in order to take advantage of inaccurately priced stock options, the computation must complete before the next change in the almost continuously changing stock market. As a result, the finance community is always looking for ways to overcome the resulting performance issues that arise when pricing options. This has led to research that applies alternative computing techniques to finance. Most commonly used quantum financial models are quantum continuous model, quantum binomial model, multi-step quantum binomial model etc.\n\nThe origin of finance can be traced to the beginning of state formation and trade during the Bronze Age. The earliest historical evidence of finance is dated to around 3000 BCE. Banking originated in West Asia, where temples and palaces were used as safe places for the storage of valuables. Initially, the only valuable that could be deposited was grain, but cattle and precious materials were eventually included. During the same period, the Sumerian city of Uruk in Mesopotamia supported trade by lending as well as the use of interest. In Sumerian, \"interest\" was mas, which translates to \"calf\". In Greece and Egypt, the words used for interest, tokos and ms respectively, meant \"to give birth\". In these cultures, interest indicated a valuable increase, and seemed to consider it from the lender's point of view.[41] The Code of Hammurabi (1792–1750 BCE) included laws governing banking operations. The Babylonians were accustomed to charging interest at the rate of 20 percent per year. By 1200 BCE, cowrie shells were used as a form of money in China.\n\nThe use of coins as a means of representing money began in the years between 700 and 500 BCE.[42] Herodotus mentions the use of crude coins in Lydia around 687 BCE and, by 640 BCE, the Lydians had started to use coin money more widely and opened permanent retail shops.[43] Shortly after, cities in Classical Greece, such as Aegina, Athens, and Corinth, started minting their own coins between 595 and 570 BCE. During the Roman Republic, interest was outlawed by the Lex Genucia reforms in 342 BCE, though the provision went largely unenforced. Under Julius Caesar, a ceiling on interest rates of 12% was set, and much later under Justinian it was lowered even further to between 4% and 8%.[44]\n\nThe first stock exchange was opened in Antwerp in 1531.[45] Since then, popular exchanges such as the London Stock Exchange (founded in 1773) and the New York Stock Exchange (founded in 1793) were created.[46][47]\n\n\n"
    },
    {
        "title": "Computer science",
        "content": "\n\nComputer science is the study of computation, information, and automation.[1][2][3] Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software).[4][5][6]\n\nAlgorithms and data structures are central to computer science.[7]\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\n\nThe fundamental concern of computer science is determining what can and cannot be automated.[2][8][3][9][10] The Turing Award is generally recognized as the highest distinction in computer science.[11][12]\n\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.[16]\n\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623.[17] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[18] Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[19] He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\".[20] \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\"[20] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer.[21] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published[22] the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics,[23] and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic.[24][25] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine,[26] on which commands could be typed and the results printed automatically.[27] In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[28] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".[29]\n\n\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors.[30] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world.[31] Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946.[32] Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[33][34] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962.[35] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\nAlthough first proposed in 1956,[36] the term \"computer science\" appears in a 1959 article in Communications of the ACM,[37]\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921.[38] Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[37]\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962.[39] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[40] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[41] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\n\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested (albeit facetiously) in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist.[42] Three months later in the same journal, comptologist was suggested, followed next year by hypologist.[43] The term computics has also been suggested.[44] In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh).[45] \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"[46]\n\nA folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\"[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\n\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.[33] Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[36]\n\nThe relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined.[47] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[48]\n\nThe academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\n\n\nDespite the word science in its name, there is debate over whether or not computer science is a discipline of science,[49] mathematics,[50] or engineering.[51] Allen Newell and Herbert A. Simon argued in 1975, \nComputer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available.[51]\n It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science.[51] Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering.[51] They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.[51]\n\nProponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs that can be deductively reasoned through mathematical formal methods.[51] Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.[51]\n\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[52] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[33] Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences,[53] identifiable in some branches of artificial intelligence).[54]\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.[55]\n\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.[56][57]\nCSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[58]—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.[56]\n\nTheoretical computer science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. It aims to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\n\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\"[3] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\n\nThe famous P = NP? problem, one of the Millennium Prize Problems,[59] is an open problem in the theory of computation.\n\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.[60]\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\n[61]\n\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\n\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\n\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems.[62] The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\n\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier – whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of the unsolved problems in theoretical computer science.\n\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE,[63] as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.[64]\n\nHuman–computer interaction (HCI) is the field of study and research concerned with the design and use of computer systems, mainly based on the analysis of the interaction between humans and computer interfaces. HCI has several subfields that focus on the relationship between emotions, social behavior and brain activity with computers.\n\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\n\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\n\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[65] Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks Jr., members of the Machine Organization department in IBM's main research center in 1959.\n\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other.[66] A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the parallel random access machine model.[67] When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.[68]\n\nThis branch of computer science aims to manage networks between computers worldwide.\n\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\n\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked.[69] Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\n\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\n\nThe philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[70]\n\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\n\nMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.[76]\n\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[77][78] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[79]\n"
    },
    {
        "title": "Social science",
        "content": "\n\n\n\nSocial science is one of the branches of science, devoted to the study of societies and the relationships among members within those societies. The term was formerly used to refer to the field of sociology, the original \"science of society\", established in the 18th century. In addition to sociology, it now encompasses a wide array of academic disciplines, including anthropology, archaeology, economics, geography, history, linguistics, management, communication studies, psychology, culturology and political science.[1]\n\nPositivist social scientists use methods resembling those used in the natural sciences as tools for understanding societies, and so define science in its stricter modern sense. Interpretivist or speculative social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense.[2] In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research).[3] The term social research has also acquired a degree of autonomy as practitioners from various disciplines share similar goals and methods.[4]\n\nThe history of the social sciences began in the Age of Enlightenment after 1651,[5] which saw a revolution within natural philosophy, changing the basic framework by which individuals understood what was scientific. Social sciences came forth from the moral philosophy of the time and were influenced by the Age of Revolutions, such as the Industrial Revolution and the French Revolution.[6] The social sciences developed from the sciences (experimental and applied), or the systematic knowledge-bases or prescriptive practices, relating to the social improvement of a group of interacting entities.[7][8]\n\nThe beginnings of the social sciences in the 18th century are reflected in the grand encyclopedia of Diderot, with articles from Jean-Jacques Rousseau and other pioneers. The growth of the social sciences is also reflected in other specialized encyclopedias. The term \"social science\" was coined in French by Mirabeau in 1767, before becoming a distinct conceptual field in the nineteenth century.[9] Social science was influenced by positivism,[6] focusing on knowledge based on actual positive sense experience and avoiding the negative; metaphysical speculation was avoided. Auguste Comte used the term science sociale to describe the field, taken from the ideas of Charles Fourier; Comte also referred to the field as social physics.[6][10]\n\nFollowing this period, five paths of development sprang forth in the social sciences, influenced by Comte in other fields.[6] One route that was taken was the rise of social research. Large statistical surveys were undertaken in various parts of the United States and Europe. Another route undertaken was initiated by Émile Durkheim, studying \"social facts\", and Vilfredo Pareto, opening metatheoretical ideas and individual theories. A third means developed, arising from the methodological dichotomy present, in which social phenomena were identified with and understood; this was championed by figures such as Max Weber.[11] The fourth route taken, based in economics, was developed and furthered economic knowledge as a hard science. The last path was the correlation of knowledge and social values; the antipositivism and verstehen sociology of Max Weber firmly demanded this distinction. In this route, theory (description) and prescription were non-overlapping formal discussions of a subject.[12][13]\n\nThe foundation of social sciences in the West implies conditioned relationships between progressive and traditional spheres of knowledge. In some contexts, such as the Italian one, sociology slowly affirms itself and experiences the difficulty of affirming a strategic knowledge beyond philosophy and theology.[14]\n\nAround the start of the 20th century, Enlightenment philosophy was challenged in various quarters. After the use of classical theories since the end of the scientific revolution, various fields substituted mathematics studies for experimental studies and examining equations to build a theoretical structure. The development of social science subfields became very quantitative in methodology. The interdisciplinary and cross-disciplinary nature of scientific inquiry into human behaviour, social and environmental factors affecting it, made many of the natural sciences interested in some aspects of social science methodology.[15] Examples of boundary blurring include emerging disciplines like social research of medicine, sociobiology, neuropsychology, bioeconomics and the history and sociology of science. Increasingly, quantitative research and qualitative methods are being integrated in the study of human action and its implications and consequences. In the first half of the 20th century, statistics became a free-standing discipline of applied mathematics.[16] Statistical methods were used confidently.\n\nIn the contemporary period, Karl Popper and Talcott Parsons influenced the furtherance of the social sciences.[6] Researchers continue to search for a unified consensus on what methodology might have the power and refinement to connect a proposed \"grand theory\" with the various midrange theories that, with considerable success, continue to provide usable frameworks for massive, growing data banks; for more, see consilience. The social sciences will for the foreseeable future be composed of different zones in the research of, and sometimes distinct in approach toward, the field.[6]\n\nThe term \"social science\" may refer either to the specific sciences of society established by thinkers such as Comte, Durkheim, Marx, and Weber, or more generally to all disciplines outside of \"noble science\" and arts. By the late 19th century, the academic social sciences were constituted of five fields: jurisprudence and amendment of the law, education, health, economy and trade, and art.[7]\n\nAround the start of the 21st century, the expanding domain of economics in the social sciences has been described as economic imperialism.[17]\n\nA distinction is usually drawn between the social sciences and the humanities. Classicist Allan Bloom writes in The Closing of the American Mind (1987):\n\nSocial science and humanities have a mutual contempt for one another, the former looking down on the latter as unscientific, the latter regarding the former as philistine. […] The difference comes down to the fact that social science really wants to be predictive, meaning that man is predictable, while the humanities say that he is not.[18]\nThe social science disciplines are branches of knowledge taught and researched at the college or university level. Social science disciplines are defined and recognized by the academic journals in which research is published, and the learned social science societies and academic departments or faculties to which their practitioners belong. Social science fields of study usually have several sub-disciplines or branches, and the distinguishing lines between these are often both arbitrary and ambiguous.[citation needed] The following are widely-considered to be social sciences:[6]\n\nAnthropology is the holistic \"science of man\", a science of the totality of human existence. The discipline deals with the integration of different aspects of the social sciences, humanities, and human biology. In the twentieth century, academic disciplines have often been institutionally divided into three broad domains. Firstly, the natural sciences seek to derive general laws through reproducible and verifiable experiments. Secondly, the humanities generally study local traditions, through their history, literature, music, and arts, with an emphasis on understanding particular individuals, events, or eras. Finally, the social sciences have generally attempted to develop scientific methods to understand social phenomena in a generalizable way, though usually with methods distinct from those of the natural sciences.[citation needed]\n\nThe anthropological social sciences often develop nuanced descriptions rather than the general laws derived in physics or chemistry, or they may explain individual cases through more general principles, as in many fields of psychology. Anthropology (like some fields of history) does not easily fit into one of these categories, and different branches of anthropology draw on one or more of these domains.[19] Within the United States, anthropology is divided into four sub-fields: archaeology, physical or biological anthropology, anthropological linguistics, and cultural anthropology. It is an area that is offered at most undergraduate institutions. The word anthropos (ἄνθρωπος) in Ancient Greek means \"human being\" or \"person\". Eric Wolf described sociocultural anthropology as \"the most scientific of the humanities, and the most humanistic of the sciences\".[citation needed]\n\nThe goal of anthropology is to provide a holistic account of humans and human nature. This means that, though anthropologists generally specialize in only one sub-field, they always keep in mind the biological, linguistic, historic and cultural aspects of any problem. Since anthropology arose as a science in Western societies that were complex and industrial, a major trend within anthropology has been a methodological drive to study peoples in societies with more simple social organization, sometimes called \"primitive\" in anthropological literature, but without any connotation of \"inferior\".[20] Today, anthropologists use terms such as \"less complex\" societies or refer to specific modes of subsistence or production, such as \"pastoralist\" or \"forager\" or \"horticulturalist\" to refer to humans living in non-industrial, non-Western cultures, such people or folk (ethnos) remaining of great interest within anthropology.[citation needed]\n\nThe quest for holism leads most anthropologists to study a people in detail, using biogenetic, archaeological, and linguistic data alongside direct observation of contemporary customs.[21] In the 1990s and 2000s, calls for clarification of what constitutes a culture, of how an observer knows where his or her own culture ends and another begins, and other crucial topics in writing anthropology were heard. It is possible to view all human cultures as part of one large, evolving global culture. These dynamic relationships, between what can be observed on the ground, as opposed to what can be observed by compiling many local observations remain fundamental in any kind of anthropology, whether cultural, biological, linguistic or archaeological.[22]\n\nCommunication studies deals with processes of human communication, commonly defined as the sharing of symbols to create meaning. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examine how messages are interpreted through the political, cultural, economic, and social dimensions of their contexts. Communication is institutionalized under many different names at different universities, including communication, communication studies, speech communication, rhetorical studies, communication science, media studies, communication arts, mass communication, media ecology, and communication and media science.\n\nCommunication studies integrate aspects of both social sciences and the humanities. As a social science, the discipline often overlaps with sociology, psychology, anthropology, biology, political science, economics, and public policy, among others. From a humanities perspective, communication is concerned with rhetoric and persuasion (traditional graduate programs in communication studies trace their history to the rhetoricians of Ancient Greece). The field applies to outside disciplines as well, including engineering, architecture, mathematics, and information science.[citation needed]\n\nEconomics is a social science that seeks to analyze and describe the production, distribution, and consumption of wealth.[23] The word \"economics\" is from the Ancient Greek οἶκος (oikos, \"family, household, estate\") and νόμος (nomos, \"custom, law\"), and hence means \"household management\" or \"management of the state\". An economist is a person using economic concepts and data in the course of employment, or someone who has earned a degree in the subject. The classic brief definition of economics, set out by Lionel Robbins in 1932, is \"the science which studies human behavior as a relationship between ends and scarce means which have alternative uses\". Without scarcity and alternative uses, there is no economic problem. Briefer yet is \"the study of how people seek to satisfy needs and wants\" and \"the study of the financial aspects of human behavior\".[citation needed]\n\nEconomics has two broad branches: microeconomics, where the unit of analysis is the individual agent, such as a household or firm, and macroeconomics, where the unit of analysis is an economy as a whole. Another division of the subject distinguishes positive economics, which seeks to predict and explain economic phenomena, from normative economics, which orders choices and actions by some criterion; such orderings necessarily involve subjective value judgments. Since the early part of the 20th century, economics has focused largely on measurable quantities, employing both theoretical models and empirical analysis. Quantitative models, however, can be traced as far back as the physiocratic school. Economic reasoning has been increasingly applied in recent decades to other social situations such as politics, law, psychology, history, religion, marriage and family life, and other social interactions.[citation needed]\n\nThe expanding domain of economics in the social sciences has been described as economic imperialism.[17][24]\n\nEducation encompasses teaching and learning specific skills, and also something less tangible but more profound: the imparting of knowledge, positive judgement and well-developed wisdom. Education has as one of its fundamental aspects the imparting of culture from generation to generation (see socialization). To educate means 'to draw out', from the Latin educare, or to facilitate the realization of an individual's potential and talents. It is an application of pedagogy, a body of theoretical and applied research relating to teaching and learning and draws on many disciplines such as psychology, philosophy, computer science, linguistics, neuroscience, sociology and anthropology.[25]\n\nGeography as a discipline can be split broadly into two main sub fields: human geography and physical geography. The former focuses largely on the built environment and how space is created, viewed and managed by humans as well as the influence humans have on the space they occupy. This may involve cultural geography, transportation, health, military operations, and cities. The latter examines the natural environment and how the climate, vegetation and life, soil, oceans, water and landforms are produced and interact (is also commonly regarded as an Earth Science).[26] Physical geography examines phenomena related to the measurement of earth. As a result of the two subfields using different approaches a third field has emerged, which is environmental geography. Environmental geography combines physical and human geography and looks at the interactions between the environment and humans.[27] Other branches of geography include social geography, regional geography, and geomatics.\n\nGeographers attempt to understand the Earth in terms of physical and spatial relationships. The first geographers focused on the science of mapmaking and finding ways to precisely project the surface of the earth. In this sense, geography bridges some gaps between the natural sciences and social sciences. Historical geography is often taught in a college in a unified Department of Geography.[citation needed]\n\nModern geography is an all-encompassing discipline, closely related to Geographic Information Science, that seeks to understand humanity and its natural environment. The fields of urban planning, regional science, and planetology are closely related to geography. Practitioners of geography use many technologies and methods to collect data such as Geographic Information Systems, remote sensing, aerial photography, statistics, and global positioning systems.\n\nHistory is the continuous, systematic narrative and research into past human events as interpreted through historiographical paradigms or theories. When used as the name of a field of study, history refers to the study and interpretation of the record of humans, societies, institutions, and any topic that has changed over time.[citation needed]\n\nTraditionally, the study of history has been considered a part of the humanities. In modern academia, whether or not history remains a humanities-based subject is contested. In the United States the National Endowment for the Humanities includes history in its definition of humanities (as it does for applied linguistics).[28] However, the National Research Council classifies history as a social science.[29] The historical method comprises the techniques and guidelines by which historians use primary sources and other evidence to research and then to write history. The Social Science History Association, formed in 1976, brings together scholars from numerous disciplines interested in social history.[30]\n\nThe social science of law, jurisprudence, in common parlance, means a rule that (unlike a rule of ethics) is capable of enforcement through institutions.[31] However, many laws are based on norms accepted by a community and thus have an ethical foundation. The study of law crosses the boundaries between the social sciences and humanities, depending on one's view of research into its objectives and effects. Law is not always enforceable, especially in the international relations context. It has been defined as a \"system of rules\",[32] as an \"interpretive concept\"[33] to achieve justice, as an \"authority\"[34] to mediate people's interests, and even as \"the command of a sovereign, backed by the threat of a sanction\".[35] However one likes to think of law, it is a completely central social institution. Legal policy incorporates the practical manifestation of thinking from almost every social science and the humanities. Laws are politics, because politicians create them. Law is philosophy, because moral and ethical persuasions shape their ideas. Law tells many of history's stories, because statutes, case law and codifications build up over time. And law is economics, because any rule about contract, tort, property law, labour law, company law and many more can have long-lasting effects on the distribution of wealth. The noun law derives from the Old English lagu, meaning something laid down or fixed[36] and the adjective legal comes from the Latin word lex.[37]\n\nLinguistics investigates the cognitive and social aspects of human language. The field is divided into areas that focus on aspects of the linguistic signal, such as syntax (the study of the rules that govern the structure of sentences), semantics (the study of meaning), morphology (the study of the structure of words), phonetics (the study of speech sounds) and phonology (the study of the abstract sound system of a particular language); however, work in areas like evolutionary linguistics (the study of the origins and evolution of language) and psycholinguistics (the study of psychological factors in human language) cut across these divisions.[38]\n\nThe overwhelming majority of modern research in linguistics takes a predominantly synchronic perspective (focusing on language at a particular point in time), and a great deal of it—partly owing to the influence of Noam Chomsky—aims at formulating theories of the cognitive processing of language. However, language does not exist in a vacuum, or only in the brain, and approaches like contact linguistics, creole studies, discourse analysis, social interactional linguistics, and sociolinguistics explore language in its social context. Sociolinguistics often makes use of traditional quantitative analysis and statistics in investigating the frequency of features, while some disciplines, like contact linguistics, focus on qualitative analysis. While certain areas of linguistics can thus be understood as clearly falling within the social sciences, other areas, like acoustic phonetics and neurolinguistics, draw on the natural sciences. Linguistics draws only secondarily on the humanities, which played a rather greater role in linguistic inquiry in the 19th and early 20th centuries. Ferdinand Saussure is considered the father of modern linguistics.[citation needed]\n\nPolitical science is an academic and research discipline that deals with the theory and practice of politics and the description and analysis of political systems and political behaviour. Fields and subfields of political science include political economy, political theory and philosophy, civics and comparative politics, theory of direct democracy, apolitical governance, participatory direct democracy, national systems, cross-national political analysis, political development, international relations, foreign policy, international law, politics, public administration, administrative behaviour, public law, judicial behaviour, and public policy. Political science also studies power in international relations and the theory of great powers and superpowers.[citation needed]\n\nPolitical science is methodologically diverse, although recent years have witnessed an upsurge in the use of the scientific method,[40][page needed] that is, the proliferation of formal-deductive model building and quantitative hypothesis testing. Approaches to the discipline include rational choice, classical political philosophy, interpretivism, structuralism, and behaviouralism, realism, pluralism, and institutionalism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquiries sought: primary sources such as historical documents, interviews, and official records, as well as secondary sources such as scholarly articles, are used in building and testing theories. Empirical methods include survey research, statistical analysis or econometrics, case studies, experiments, and model building.[citation needed]\n\nPsychology is an academic and applied field involving the study of behaviour and mental processes.[41]  Psychology also refers to the application of such knowledge to various spheres of human activity, including problems of individuals' daily lives and the treatment of mental illness.[42][43] The word psychology comes from the Ancient Greek ψυχή (psyche, \"soul\" or \"mind\") and the suffix logy (\"study\").[44]\n\nPsychology differs from anthropology, economics, political science, and sociology in seeking to capture explanatory generalizations about the mental function and overt behaviour of individuals, while the other disciplines focus on creating descriptive generalizations about the functioning of social groups or situation-specific human behaviour. In practice, however, there is quite a lot of cross-fertilization that takes place among the various fields. Psychology differs from biology and neuroscience in that it is primarily concerned with the interaction of mental processes and behaviour, and of the overall processes of a system, and not simply the biological or neural processes themselves, though the subfield of neuropsychology combines the study of the actual neural processes with the study of the mental effects they have subjectively produced.[45]\n\nMany people associate psychology with clinical psychology,[46] which focuses on assessment and treatment of problems in living and psychopathology. In reality, psychology has myriad specialties including social psychology, developmental psychology, cognitive psychology, educational psychology, industrial-organizational psychology, mathematical psychology, neuropsychology, and quantitative analysis of behaviour.[47]\n\nPsychology is a very broad science that is rarely tackled as a whole, major block. Although some subfields encompass a natural science base and a social science application, others can be clearly distinguished as having little to do with the social sciences or having a lot to do with the social sciences. For example, biological psychology is considered a natural science with a social scientific application (as is clinical medicine), social and occupational psychology are, generally speaking, purely social sciences, whereas neuropsychology is a natural science that lacks application out of the scientific tradition entirely.[citation needed]\n\nIn British universities, emphasis on what tenet of psychology a student has studied and/or concentrated is communicated through the degree conferred: BPsy indicates a balance between natural and social sciences, BSc indicates a strong (or entire) scientific concentration, whereas a BA underlines a majority of social science credits. This is not always necessarily the case however, and in many UK institutions students studying the BPsy, BSc, and BA follow the same curriculum as outlined by The British Psychological Society and have the same options of specialism open to them regardless of whether they choose a balance, a heavy science basis, or heavy social science basis to their degree. If they applied to read the BA. for example, but specialized in heavily science-based modules, then they will still generally be awarded the BA.[citation needed]\n\nSociology is the systematic study of society, individuals' relationship to their societies, the consequences of difference, and other aspects of human social action.[48] The meaning of the word comes from the suffix -logy, which means \"study of\", derived from Ancient Greek, and the stem soci-, which is from the Latin word socius, meaning \"companion\", or society in general.[49][50]\n\nAuguste Comte (1798–1857) coined the term sociology to describe a way to apply natural science principles and techniques to the social world in 1838.[51][52] Comte endeavoured to unify history, psychology and economics through the descriptive understanding of the social realm. He proposed that social ills could be remedied through sociological positivism, an epistemological approach outlined in The Course in Positive Philosophy [1830–1842] and A General View of Positivism (1844). Though Comte is generally regarded as the \"Father of Sociology\", the discipline was formally established by another French thinker, Émile Durkheim (1858–1917), who developed positivism as a foundation to practical social research. Durkheim set up the first European department of sociology at the University of Bordeaux in 1895, publishing his Rules of the Sociological Method. In 1896, he established the journal L'Année sociologique. Durkheim's seminal monograph, Suicide (1897), a case study of suicide rates among Catholic and Protestant populations, distinguished sociological analysis from psychology or philosophy.[53]\n\nKarl Marx rejected Comte's positivism but nevertheless aimed to establish a science of society based on historical materialism, becoming recognized as a founding figure of sociology posthumously as the term gained broader meaning. Around the start of the 20th century, the first wave of German sociologists, including Max Weber and Georg Simmel, developed sociological antipositivism. The field may be broadly recognized as an amalgam of three modes of social thought in particular: Durkheimian positivism and structural functionalism; Marxist historical materialism and conflict theory; and Weberian antipositivism and verstehen analysis. American sociology broadly arose on a separate trajectory, with little Marxist influence, an emphasis on rigorous experimental methodology, and a closer association with pragmatism and social psychology. In the 1920s, the Chicago school developed symbolic interactionism. Meanwhile, in the 1930s, the Frankfurt School pioneered the idea of critical theory, an interdisciplinary form of Marxist sociology drawing upon thinkers as diverse as Sigmund Freud and Friedrich Nietzsche. Critical theory would take on something of a life of its own after World War II, influencing literary criticism and the Birmingham School establishment of cultural studies.[citation needed]\n\nSociology evolved as an academic response to the challenges of modernity, such as industrialization, urbanization, secularization, and a perceived process of enveloping rationalization.[54] The field generally concerns the social rules and processes that bind and separate people not only as individuals, but as members of associations, groups, communities and institutions, and includes the examination of the organization and development of human social life. The sociological field of interest ranges from the analysis of short contacts between anonymous individuals on the street to the study of global social processes. In the terms of sociologists Peter L. Berger and Thomas Luckmann, social scientists seek an understanding of the Social Construction of Reality. Most sociologists work in one or more subfields. One useful way to describe the discipline is as a cluster of sub-fields that examine different dimensions of society. For example, social stratification studies inequality and class structure; demography studies changes in population size or type; criminology examines criminal behaviour and deviance; and political sociology studies the interaction between society and state.[citation needed]\n\nSince its inception, sociological epistemologies, methods, and frames of enquiry, have significantly expanded and diverged.[55] Sociologists use a diversity of research methods, collecting both quantitative and qualitative data, draw upon empirical techniques, and engage critical theory.[52] Common modern methods include case studies, historical research, interviewing, participant observation, social network analysis, survey research, statistical analysis, and model building, among other approaches. Since the late 1970s, many sociologists have tried to make the discipline useful for purposes beyond the academy. The results of sociological research aid educators, lawmakers, administrators, developers, and others interested in resolving social problems and formulating public policy, through subdisciplinary areas such as evaluation research, methodological assessment, and public sociology.[citation needed]\n\nIn the early 1970s, women sociologists began to question sociological paradigms and the invisibility of women in sociological studies, analysis, and courses.[56] In 1969, feminist sociologists challenged the discipline's androcentrism at the American Sociological Association's annual conference.[57] This led to the founding of the organization Sociologists for Women in Society, and, eventually, a new sociology journal, Gender & Society. Today, the sociology of gender is considered to be one of the most prominent sub-fields in the discipline.[58]\n\nNew sociological sub-fields continue to appear — such as community studies, computational sociology, environmental sociology, network analysis, actor-network theory, gender studies, and a growing list, many of which are cross-disciplinary in nature.[59]\n\nAdditional applied or interdisciplinary fields related to the social sciences or are applied social sciences include:\n\nThe origin of the survey can be traced back at least as early as the Domesday Book in 1086,[78][79] while some scholars pinpoint the origin of demography to 1663 with the publication of John Graunt's Natural and Political Observations upon the Bills of Mortality.[80] Social research began most intentionally, however, with the positivist philosophy of science in the 19th century.\n\nIn contemporary usage, \"social research\" is a relatively autonomous term, encompassing the work of practitioners from various disciplines that share in its aims and methods. Social scientists employ a range of methods in order to analyse a vast breadth of social phenomena; from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. The methods originally rooted in classical sociology and statistical mathematics have formed the basis for research in other disciplines, such as political science, media studies, and marketing and market research.\n\nSocial research methods may be divided into two broad schools:\n\nSocial scientists will commonly combine quantitative and qualitative approaches as part of a multi-strategy design. Questionnaires, field-based data collection, archival database information and laboratory-based data collections are some of the measurement techniques used. It is noted the importance of measurement and analysis, focusing on the (difficult to achieve) goal of objective research or statistical hypothesis testing. A mathematical model uses mathematical language to describe a system. The process of developing a mathematical model is termed 'mathematical modelling' (also modeling). A mathematical model is \"a representation of the essential aspects of an existing system (or a system to be constructed) that presents knowledge of that system in usable form\".[81] Mathematical models can take many forms, including but not limited to dynamical systems, statistical models, differential equations, or game theoretic models.\n\nThese and other types of models can overlap, with a given model involving a variety of abstract structures. The system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. The concept of an integrated whole can also be stated in terms of a system embodying a set of relationships that are differentiated from relationships of the set to other elements, and from relationships between an element of the set and elements not a part of the relational regime. A dynamical system modeled as a mathematical formalization has a fixed \"rule\" that describes the time dependence of a point's position in its ambient space. Small changes in the state of the system correspond to small changes in the numbers. The evolution rule of the dynamical system is a fixed rule that describes what future states follow from the current state. The rule is deterministic: for a given time interval only one future state follows from the current state.\n\nSocial scientists often conduct program evaluation, which is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and programs,[82] particularly about their effectiveness and efficiency. In both the public and private sectors, stakeholders often want to know whether the programs they are funding, implementing, voting for, receiving or objecting to are producing the intended effect. While program evaluation first focuses around this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are unintended outcomes, and whether the program goals are appropriate and useful.[83]\n\nSome social theorists emphasize the subjective nature of research. These writers espouse social theory perspectives that include various types of the following:\n\nOther fringe social theorists delve into the alternative nature of research. These writers share social theory perspectives that include various types of the following:\n\nAuthors use the concept of recursivity to foreground the situation in which social scientists find themselves when producing knowledge about the world they are always already part of.[84][85] According to Audrey Alejandro, \"as social scientists, the recursivity of our condition deals with the fact that we are both subjects (as discourses are the medium through which we analyse) and objects of the academic discourses we produce (as we are social agents belonging to the world we analyse).\"[86] From this basis, she identifies in recursivity a fundamental challenge in the production of emancipatory knowledge which calls for the exercise of reflexive efforts:\n\nwe are socialised into discourses and dispositions produced by the socio-political order we aim to challenge, a socio-political order that we may, therefore, reproduce unconsciously while aiming to do the contrary. The recursivity of our situation as scholars – and, more precisely, the fact that the dispositional tools we use to produce knowledge about the world are themselves produced by this world – both evinces the vital necessity of implementing reflexivity in practice and poses the main challenge in doing so.[87]\nMost universities offer degrees in social science fields.[88] The Bachelor of Social Science is a degree targeted at the social sciences in particular, it is often more flexible and in-depth than other degrees that include social science subjects.[a]\n\nIn the United States, a university may offer a student who studies a social sciences field a Bachelor of Arts degree, particularly if the field is within one of the traditional liberal arts such as history, or a BSc: Bachelor of Science degree such as those given by the London School of Economics, as the social sciences constitute one of the two main branches of science (the other being the natural sciences). In addition, some institutions have degrees for a particular social science, such as the Bachelor of Economics degree, though such specialized degrees are relatively rare in the United States.\n\nGraduate students may receive a master's degree (Master of Arts, Master of Science or a field-specific degree such as Master of Public Administration) or a doctoral degree (e.g. PhD).\n\nSocial sciences receive less funding than natural sciences. It has been estimated that only 0.12% of all funding for climate-related research is spent on the social science of climate change mitigation. Vastly more funding is spent on natural science studies of climate change and considerable sums are also spent on studies of the impact of and adaptation to climate change.[89] It has been argued that this is a misallocation of resources, as the most urgent puzzle at the current juncture is to work out how to change human behavior to mitigate climate change, whereas the natural science of climate change is already well established and there will be decades and centuries to handle adaptation.[89]\n\nNevertheless, funding and attention paid to the social sciences varies across countries. For instance, the development of a social scientific community can become a priority entangled with national politics.[90] In the case of Brazil, for example, the institutionalisation of social sciences took place in a political context where the state struggled to assert its territorial power, and the social scientific field was expected to produce investigation but also political inputs towards the construction of a new nation.[91][92] This need was accentuated after the 1932 revolution, in the wake of which the USP was founded and became the biggest university in South America. Subsequently, these developments led to the deployment of university programs and the institution of national associations in anthropology, sociology and political science.[90]\n\n\n"
    },
    {
        "title": "Game theory",
        "content": "\n\nEmpirical methods\n\nPrescriptive and policy\n\nGame theory is the study of mathematical models of strategic interactions.[1] It has applications in many fields of social science, and is used extensively in economics, logic, systems science and computer science.[2] Initially, game theory addressed two-person zero-sum games, in which a participant's gains or losses are exactly balanced by the losses and gains of the other participant. In the 1950s, it was extended to the study of non zero-sum games, and was eventually applied to a wide range of behavioral relations. It is now an umbrella term for the science of rational decision making in humans, animals, and computers.\n\nModern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by Theory of Games and Economic Behavior (1944), co-written with Oskar Morgenstern, which considered cooperative games of several players.[3] The second edition provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.[4]\n\nGame theory was developed extensively in the 1950s, and was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. John Maynard Smith was awarded the Crafoord Prize for his application of evolutionary game theory in 1999, and fifteen game theorists have won the Nobel Prize in economics as of 2020, including most recently Paul Milgrom and Robert B. Wilson.\n\nIn 1713, a letter attributed to Charles Waldegrave, an active Jacobite and uncle to British diplomat James Waldegrave, analyzed a game called \"le her\". Waldegrave provided a minimax mixed strategy solution to a two-person version of the card game, and the problem is now known as the Waldegrave problem.[5][6]\n\nIn 1838, Antoine Augustin Cournot provided a model of competition in oligopolies. Though he did not refer to it as such, he presented a solution that is the Nash equilibrium of the game in his Recherches sur les principes mathématiques de la théorie des richesses (Researches into the Mathematical Principles of the Theory of Wealth). In 1883, Joseph Bertrand critiqued Cournot's model as unrealistic, providing an alternative model of price competition[7] which would later be formalized by Francis Ysidro Edgeworth.[8]\n\nIn 1913, Ernst Zermelo published Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels (On an Application of Set Theory to the Theory of the Game of Chess), which proved that the optimal chess strategy is strictly determined.[9]\n\nThe work of John von Neumann established game theory as its own independent field in the early-to-mid 20th century, with von Neumann publishing his paper On the Theory of Games of Strategy in 1928.[10][11] Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. Von Neumann's work in game theory culminated in his 1944 book Theory of Games and Economic Behavior, co-authored with Oskar Morgenstern.[12] The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of money) as an independent discipline. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. Subsequent work focused primarily on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.[13]\n\nIn his 1938 book Applications aux Jeux de Hasard and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix is symmetric and provided a solution to a non-trivial infinite game (known in English as Blotto game). Borel conjectured the non-existence of mixed-strategy equilibria in finite two-person zero-sum games, a conjecture that was proved false by von Neumann.[14]\n\nIn 1950, John Nash developed a criterion for mutual consistency of players' strategies known as the Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every finite n-player, non-zero-sum (not just two-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium in mixed strategies. \n\nGame theory experienced a flurry of activity in the 1950s, during which the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. The 1950s also saw the first applications of game theory to philosophy and political science. The first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy.[15]\n\nIn 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium. Later he would introduce trembling hand perfection as well. In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.\n\nIn the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection and common knowledge[a] were introduced and analyzed.\n\nIn 1994, John Nash was awarded the Nobel Memorial Prize in the Economic Sciences for his contribution to game theory. Nash's most famous contribution to game theory is the concept of the Nash equilibrium, which is a solution concept for non-cooperative games, published in 1951. A Nash equilibrium is a set of strategies, one for each player, such that no player can improve their payoff by unilaterally changing their strategy.\n\nIn 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten, and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing equilibrium coarsening and correlated equilibria, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.\n\nIn 2007, Leonid Hurwicz, Eric Maskin, and Roger Myerson were awarded the Nobel Prize in Economics \"for having laid the foundations of mechanism design theory\". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: Game Theory, Analysis of Conflict.[1] Hurwicz introduced and formalized the concept of incentive compatibility.\n\nIn 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics \"for the theory of stable allocations and the practice of market design\". In 2014, the Nobel went to game theorist Jean Tirole.\n\nA game is cooperative if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is non-cooperative if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).[16]\n\nCooperative games are often analyzed through the framework of cooperative game theory, which focuses on predicting which coalitions will form, the joint actions that groups take, and the resulting collective payoffs. It is different from non-cooperative game theory which focuses on predicting individual players' actions and payoffs by analyzing Nash equilibria.[17][18]\n\nCooperative game theory provides a high-level approach as it describes only the structure and payoffs of coalitions, whereas non-cooperative game theory also looks at how strategic interaction will affect the distribution of payoffs. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation.\n\nA symmetric game is a game where each player earns the same payoff when making the same choice. In other words, the identity of the player does not change the resulting game facing the other player.[19] Many of the commonly studied 2×2 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games.\n\nThe most commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured in this section's graphic is asymmetric despite having identical strategy sets for both players.\n\nZero-sum games (more generally, constant-sum games) are games in which choices by players can neither increase nor decrease the available resources. In zero-sum games, the total benefit goes to all players in a game, for every combination of strategies, and always adds to zero (more informally, a player benefits only at the equal expense of others).[20] Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.\n\nMany games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.\n\nFurthermore, constant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any constant-sum game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called \"the board\") whose losses compensate the players' net winnings.\n\nSimultaneous games are games where both players move simultaneously, or instead the later players are unaware of the earlier players' actions (making them effectively simultaneous). Sequential games (or dynamic games) are games where players do not make decisions simultaneously, and player's earlier actions affect the outcome and decisions of other players.[21] This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while they do not know which of the other available actions the first player actually performed.\n\nThe difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.\n\nIn short, the differences between sequential and simultaneous games are as follows:\n\nAn important subset of sequential games consists of games of perfect information. A game with perfect information means that all players, at every move in the game, know the previous history of the game and the moves previously made by all other players. An imperfect information game is played when the players do not know all moves already made by the opponent such as a simultaneous move game.[22] Examples of perfect-information games include tic-tac-toe, checkers, chess, and Go.[23][24][25]\n\nMany card games are games of imperfect information, such as poker and bridge.[26] Perfect information is often confused with complete information, which is a similar concept pertaining to the common knowledge of each player's sequence, strategies, and payoffs throughout gameplay.[27] Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken, whereas perfect information is knowledge of all aspects of the game and players.[28] Games of incomplete information can be reduced, however, to games of imperfect information by introducing \"moves by nature\".[29]\n\nOne of the assumptions of the Nash equilibrium is that every player has correct beliefs about the actions of the other players. However, there are many situations in game theory where participants do not fully understand the characteristics of their opponents. Negotiators may be unaware of their opponent's valuation of the object of negotiation, companies may be unaware of their opponent's cost functions, combatants may be unaware of their opponent's strengths, and jurors may be unaware of their colleague's interpretation of the evidence at trial. In some cases, participants may know the character of their opponent well, but may not know how well their opponent knows his or her own character.[30]\n\nBayesian game means a strategic game with incomplete information. For a strategic game, decision makers are players, and every player has a group of actions. A core part of the imperfect information specification is the set of states. Every state completely describes a collection of characteristics relevant to the player such as their preferences and details about them. There must be a state for every set of features that some player believes may exist.[31]\n\nFor example, where Player 1 is unsure whether Player 2 would rather date her or get away from her, while Player 2 understands Player 1's preferences as before. To be specific, supposing that Player 1 believes that Player 2 wants to date her under a probability of 1/2 and get away from her under a probability of 1/2 (this evaluation comes from Player 1's experience probably: she faces players who want to date her half of the time in such a case and players who want to avoid her half of the time). Due to the probability involved, the analysis  of this situation requires to understand the player's preference for the draw, even though people are only interested in pure strategic equilibrium.\n\nGames in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and Go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve some particular problems and answer some general questions.[32]\n\nGames of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including \"loopy\" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or \"economic\") game theory.[33][34] A typical game that has been solved this way is Hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.[35]\n\nResearch in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha–beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.[32][36]\n\nMuch of game theory is concerned with finite, discrete games that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.\n\nDifferential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.\n\nA particular case of differential games are the games with a random time horizon.[37] In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.\n\nEvolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.[38]  In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization, or survival of the fittest.\n\nIn biology, such models can represent evolution, in which offspring adopt their parents' strategies and parents who play more successful strategies (i.e. corresponding to higher payoffs) have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies.[39]\n\nIndividual decision problems with stochastic outcomes are sometimes considered \"one-player games\". They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).[40]\n\nStochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes \"chance moves\" (\"moves by nature\").[41] This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.\n\nFor some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen.[42] (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)\n\nGeneral models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The \"gold standard\" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.[42]\n\nThese are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.\n\nThe term metagame analysis is also used to refer to a practical approach developed by Nigel Howard,[43] whereby a situation is framed as a strategic game in which stakeholders try to realize their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.\n\nMean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines, and by mathematicians Pierre-Louis Lions and Jean-Michel Lasry.\n\nThe games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome. (Eric Rasmusen refers to these four \"essential elements\" by the acronym \"PAPI\".)[44][45][46][47] A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.\n\nMost cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.\n\nThe extensive form can be used to formalize games with a time sequencing of moves. Extensive form games can be visualized using game trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree.[48] To solve any extensive form game, backward induction must be used. It involves working backward up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.[49]\n\nThe game pictured consists of two players.  The way this particular game is structured (i.e., with sequential decision making and perfect information), Player 1 \"moves\" first by choosing either F or U (fair or unfair). Next in the sequence, Player 2, who has now observed Player 1's move, can choose to play either A or R  (accept or reject). Once Player 2 has made their choice, the game is considered finished and each player gets their respective payoff, represented in the image as two numbers, where the first number represents Player 1's payoff, and the second number represents Player 2's payoff.  Suppose that Player 1 chooses U and then Player 2 chooses A: Player 1 then gets a payoff of \"eight\" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and Player 2 gets a payoff of \"two\".\n\nThe extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)\n\nThe normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays Up and that Player 2 plays Left. Then Player 1 gets a payoff of 4, and Player 2 gets 3.\n\nWhen a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.\n\nEvery extensive-form game has an equivalent normal-form game, however, the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.[50]\n\nIn cooperative game theory the characteristic function lists the payoff of each coalition. The origin of this formulation is in John von Neumann and Oskar Morgenstern's book.[citation needed]\n\nFormally, a characteristic function is a function \n\n\n\nv\n:\n\n2\n\nN\n\n\n→\n\nR\n\n\n\n{\\displaystyle v:2^{N}\\to \\mathbb {R} }\n\n [51] from the set of all possible coalitions of players to a set of payments, and also satisfies \n\n\n\nv\n(\n∅\n)\n=\n0\n\n\n{\\displaystyle v(\\emptyset )=0}\n\n. The function describes how much collective payoff a set of players can gain by forming a coalition.\n\nAlternative game representation forms are used for some subclasses of games or adjusted to the needs of interdisciplinary research.[52] In addition to classical game representations, some of the alternative representations also encode time related aspects.\n\nAs a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.[67]\n\nAlthough pre-twentieth-century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name \"game theory\", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his 1982 book Evolution and the Theory of Games.[68]\n\nIn addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior.[69] In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic approaches have also been suggested in the philosophy of language and philosophy of science.[70] Game-theoretic arguments of this type can be found as far back as Plato.[71] An alternative version of game theory, called chemical game theory, represents the player's choices as metaphorical chemical reactant molecules called \"knowlecules\".[72]  Chemical game theory then calculates the outcomes as equilibrium solutions to a system of chemical reactions.\n\nThe primary use of game theory is to describe and model how human populations behave.[citation needed] Some[who?] scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real-world situations. Game theorists usually assume players act rationally, but in practice, human rationality and/or behavior often deviates from the model of rationality as used in game theory. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.[b]\n\nSome game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).\n\nSome scholars see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.[74]\n\n\n\nGame theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents.[c][75][76][77] Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers and acquisitions pricing,[78] fair division, duopolies, oligopolies, social network formation, agent-based computational economics,[79][80] general equilibrium, mechanism design,[81][82][83][84][85] and voting systems;[86] and across such broad areas as experimental economics,[87][88][89][90][91] behavioral economics,[92][93][94][95][96][97] information economics,[44][45][46][47] industrial organization,[98][99][100][101] and political economy.[102][103][104][46]\n\nThis research usually focuses on particular sets of strategies known as \"solution concepts\" or \"equilibria\". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.[105][106]\n\nThe payoffs of the game are generally taken to represent the utility of individual players.\n\nA prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Economists and business professors suggest two primary uses (noted above): descriptive and prescriptive.[69]\n\nGame theory also has an extensive use in a specific branch or stream of economics – Managerial Economics. One important usage of it in the field of managerial economics is in analyzing strategic interactions between firms.[107] For example, firms may be competing in a market with limited resources, and game theory can help managers understand how their decisions impact their competitors and the overall market outcomes. Game theory can also be used to analyze cooperation between firms, such as in forming strategic alliances or joint ventures. Another use of game theory in managerial economics is in analyzing pricing strategies. For example, firms may use game theory to determine the optimal pricing strategy based on how they expect their competitors to respond to their pricing decisions. Overall, game theory serves as a useful tool for analyzing strategic interactions and decision making in the context of managerial economics.\n\nThe Chartered Institute of Procurement & Supply (CIPS) promotes knowledge and use of game theory within the context of business procurement.[108] CIPS and TWS Partners have conducted a series of surveys designed to explore the understanding, awareness and application of game theory among procurement professionals. Some of the main findings in their third annual survey (2019) include:\n\nSensible decision-making is critical for the success of projects.  In project management, game theory is used to model the decision-making process of players, such as investors, project managers, contractors, sub-contractors, governments and customers.  Quite often, these players have competing interests, and sometimes their interests are directly detrimental to other players, making project management scenarios well-suited to be modeled by game theory.\n\nPiraveenan (2019)[110] in his review provides several examples where game theory is used to model project management scenarios. For instance, an investor typically has several investment options, and each option will likely result in a different project, and thus one of the investment options has to be chosen before the project charter can be produced. Similarly, any large project involving subcontractors, for instance, a construction project, has a complex interplay between the main contractor (the project manager) and subcontractors, or among the subcontractors themselves, which typically has several decision points. For example, if there is an ambiguity in the contract between the contractor and subcontractor, each must decide how hard to push their case without jeopardizing the whole project, and thus their own stake in it. Similarly, when projects from competing organizations are launched, the marketing personnel have to decide what is the best timing and strategy to market the project, or its resultant product or service, so that it can gain maximum traction in the face of competition. In each of these scenarios, the required decisions depend on the decisions of other players who, in some way, have competing interests to the interests of the decision-maker, and thus can ideally be modeled using game theory.\n\nPiraveenan[110] summarizes that two-player games are predominantly used to model project management scenarios, and based on the identity of these players, five distinct types of games are used in project management.\n\nIn terms of types of games, both cooperative as well as non-cooperative, normal-form as well as extensive-form, and zero-sum as well as non-zero-sum  are used to model various project management scenarios.\n\nThe application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.[111]\n\nEarly examples of game theory applied to political science are provided by Anthony Downs. In his 1957 book An Economic Theory of Democracy,[112] he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game theory was applied in 1962 to the Cuban Missile Crisis during the presidency of John F. Kennedy.[113]\n\nIt has also been proposed that game theory explains the stability of any form of political government.  Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects.  Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed.  Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime.[114]  Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.[citation needed]\n\nA game-theoretic explanation for democratic peace is that public and open debate in democracies sends clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.[115]\n\nHowever, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.[116]\n\nGame theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example is Peter John Wood's (2013) research looking into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce greenhouse gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma for the nations.[117]\n\nGame theory has been used extensively to model decision-making scenarios relevant to defence applications.[118]  Most studies that has applied game theory in defence settings are concerned with Command and Control Warfare, and can be further classified into studies dealing with (i) Resource Allocation Warfare (ii) Information Warfare (iii) Weapons Control Warfare, and (iv) Adversary Monitoring Warfare.[118]  Many of the problems studied are concerned with sensing and tracking, for example a surface ship trying to track a hostile submarine and the submarine trying to evade being tracked, and the interdependent decision making that takes place with regards to bearing, speed, and the sensor technology activated by both vessels. Ho et al. [118] provides a concise summary of the state-of-the-art with regards to the use of game theory in defence applications and highlights the benefits and limitations of game theory in the considered scenarios.\n\nUnlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best-known equilibrium in biology is known as the evolutionarily stable strategy (ESS), first introduced in (Maynard Smith & Price 1973). Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.\n\nIn biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. (Fisher 1930) suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.\n\nAdditionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication.[119] The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's Butterfly Economics).\n\nBiologists have used the game of chicken to analyze fighting behavior and territoriality.[120]\n\nAccording to Maynard Smith, in the preface to Evolution and the Theory of Games, \"paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed\". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.[121]\n\nOne such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival.[122] All of these actions increase the overall fitness of a group, but occur at a cost to the individual.\n\nEvolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation c < b × r, where the cost c to the altruist must be less than the benefit b to the recipient multiplied by the coefficient of relatedness r. The more closely related two organisms are causes the incidences of altruism to increase because they share many of the same alleles. This means that the altruistic individual, by ensuring that the alleles of its close relative are passed on through survival of its offspring, can forgo the option of having offspring itself because the same number of alleles are passed on. For example, helping a sibling (in diploid animals) has a coefficient of 1⁄2, because (on average) an individual shares half of the alleles in its sibling's offspring. Ensuring that enough of a sibling's offspring survive to adulthood precludes the necessity of the altruistic individual producing offspring.[122] The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a coefficient that was 1⁄2 in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.\n\nGame theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.[123]\n\nSeparately, game theory has played a role in online algorithms; in particular, the k-server problem, which has in the past been referred to as games with moving costs and request-answer games.[124] Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.\n\nThe emergence of the Internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory[85] and within it algorithmic mechanism design[84] combine computational algorithm design and analysis of complex systems with economic theory.[125][126][127]\n\nGame theory has multiple applications in the field of artificial intelligence and machine learning. It is often used in developing autonomous systems that can make complex decisions in uncertain environment.[128] Some other areas of application of game theory in AI/ML context are as follows - multi-agent system formation, reinforcement learning,[129] mechanism design etc.[130] By using game theory to model the behavior of other agents and anticipate their actions, AI/ML systems can make better decisions and operate more effectively.[131]\n\nGame theory has been put to several uses in philosophy. Responding to two papers by W.V.O. Quine (1960, 1967), Lewis (1969) used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis.[132][133] Following Lewis (1969) game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.[134][135]\n\nGame theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from the interactions of agents. Philosophers who have worked in this area include Bicchieri (1989, 1993),[136][137] Skyrms (1990),[138] and Stalnaker (1999).[139]\n\nThe synthesis of game theory with ethics was championed by R. B. Braithwaite.[140] The hope was that rigorous mathematical analysis of game theory might help formalize the more imprecise philosophical discussions. However, this expectation was only materialized to a limited extent.[141]\n\nIn ethics, some (most notably David Gauthier, Gregory Kavka, and Jean Hampton) [who?] authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see Gauthier (1986) and Kavka (1986)).[d]\n\nOther authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., Skyrms (1996, 2004) and Sober and Wilson (1998)).\n\nSince the decision to take a vaccine for a particular disease is often made by individuals, who may consider a range of factors and parameters in making this decision (such as the incidence and prevalence of the disease, perceived and real risks associated with contracting the disease,  mortality rate, perceived and real risks associated with vaccination, and financial cost of vaccination), game theory has been used to model and predict vaccination uptake in a society.[142][143]\n\nWilliam Poundstone described the game in his 1993 book Prisoner's Dilemma:[144]\n\nTwo members of a criminal gang, A and B, are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communication with their partner. The principal charge would lead to a sentence of ten years in prison; however, the police do not have the evidence for a conviction. They plan to sentence both to two years in prison on a lesser charge but offer each prisoner a Faustian bargain: If one of them confesses to the crime of the principal charge, betraying the other, they will be pardoned and free to leave while the other must serve the entirety of the sentence instead of just two years for the lesser charge.\n\nThe dominant strategy (and therefore the best response to any possible opponent strategy), is to betray the other, which aligns with the sure-thing principle.[145] However, both prisoners staying silent would yield a greater reward for both of them than mutual betrayal.\n\nThe \"battle of the sexes\" is a term used to describe the perceived conflict between men and women in various areas of life, such as relationships, careers, and social roles. This conflict is often portrayed in popular culture, such as movies and television shows, as a humorous or dramatic competition between the genders. This conflict can be depicted in a game theory framework. This is an example of non-cooperative games.\n\nAn example of the \"battle of the sexes\" can be seen in the portrayal of relationships in popular media, where men and women are often depicted as being fundamentally different and in conflict with each other. For instance, in some romantic comedies, the male and female protagonists are shown as having opposing views on love and relationships, and they have to overcome these differences in order to be together.[146]\n\nIn this game, there are two pure strategy Nash equilibria: one where both the players choose the same strategy and the other where the players choose different options. If the game is played in mixed strategies, where each player chooses their strategy randomly, then there is an infinite number of Nash equilibria. However, in the context of the \"battle of the sexes\" game, the assumption is usually made that the game is played in pure strategies.[147]\n\nThe ultimatum game is a game that has become a popular instrument of economic experiments. An early description is by Nobel laureate John Harsanyi in 1961.[148]\n\nOne player, the proposer, is endowed with a sum of money. The proposer is tasked with splitting it with another player, the responder (who knows what the total sum is). Once the proposer communicates his decision, the responder may accept it or reject it. If the responder accepts, the money is split per the proposal; if the responder rejects, both players receive nothing.  Both players know in advance the consequences of the responder accepting or rejecting the offer. The game demonstrates how social acceptance, fairness, and generosity influence the players decisions.[149]\n\nUltimatum game has a variant, that is the dictator game. They are mostly identical, except in dictator game the responder has no power to reject the proposer's offer.\n\nThe Trust Game is an experiment designed to measure trust in economic decisions. It is also called \"the investment game\" and is designed to investigate trust and demonstrate its importance rather than \"rationality\" of self-interest. The game was designed by Berg Joyce, John Dickhaut and Kevin McCabe in 1995.[150]\n\nIn the game, one player (the investor) is given a sum of money and must decide how much of it to give to another player (the trustee). The amount given is then tripled by the experimenter. The trustee then decides how much of the tripled amount to return to the investor. If the recipient is completely self interested, then he/she should return nothing. However that is not true as the experiment conduct. The outcome suggest that people are willing to place a trust, by risking some amount of money, in the belief that there would be reciprocity.[151]\n\nThe Cournot competition model involves players choosing quantity of a homogenous product to produce independently and simultaneously, where marginal cost can be different for each firm and the firm's payoff is profit. The production costs are public information and the firm aims to find their profit-maximizing quantity based on what they believe the other firm will produce and behave like monopolies. In this game firms want to produce at the monopoly quantity but there is a high incentive to deviate and produce more, which decreases the market-clearing price.[22] For example, firms may be tempted to deviate from the monopoly quantity if there is a low monopoly quantity and high price, with the aim of increasing production to maximize  profit.[22] However this option does not provide the highest payoff, as a firm's ability to maximize profits depends on its market share and the elasticity of the market demand.[152] The Cournot equilibrium is reached when each firm operates on their reaction function with no incentive to deviate, as they have the best response based on the other firms output.[22] Within the game, firms reach the Nash equilibrium when the Cournot equilibrium is achieved.   \n\nThe Bertrand competition assumes homogenous products and a constant marginal cost and players choose the prices.[22] The equilibrium of price competition is where the price is equal to marginal costs, assuming complete information about the competitors' costs. Therefore, the firms have an incentive to deviate from the equilibrium because a homogenous product with a lower price will gain all of the market share, known as a cost advantage.[153]\n\nLists\n\n\n"
    },
    {
        "title": "Applied mathematics",
        "content": "Applied mathematics is the application of mathematical methods by different fields such as physics, engineering, medicine, biology, finance, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term \"applied mathematics\" also describes the professional specialty in which mathematicians work on practical problems by formulating and studying mathematical models.\n\nIn the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics. \n\nHistorically, applied mathematics consisted principally of applied analysis, most notably differential equations; approximation theory (broadly construed, to include representations, asymptotic methods, variational methods, and numerical analysis); and applied probability. These areas of mathematics related directly to the development of Newtonian physics, and in fact, the distinction between mathematicians and physicists was not sharply drawn before the mid-19th century. This history left a pedagogical legacy in the United States: until the early 20th century, subjects such as classical mechanics were often taught in applied mathematics departments at American universities rather than in physics departments, and fluid mechanics may still be taught in applied mathematics departments.[1] Engineering and computer science departments have traditionally made use of applied mathematics.\n\nAs time passed, Applied Mathematics grew alongside the advancement of science and technology. With the advent of modern times, the application of mathematics in fields such as science, economics, technology, and more became deeper and more timely. The development of computers and other technologies enabled a more detailed study and application of mathematical concepts in various fields.\n\nToday, Applied Mathematics continues to be crucial for societal and technological advancement. It guides the development of new technologies, economic progress, and addresses challenges in various scientific fields and industries. The history of Applied Mathematics continually demonstrates the importance of mathematics in human progress.\n\nToday, the term \"applied mathematics\" is used in a broader sense. It includes the classical areas noted above as well as other areas that have become increasingly important in applications. Even fields such as number theory that are part of pure mathematics are now important in applications (such as cryptography), though they are not generally considered to be part of the field of applied mathematics per se.\n\nThere is no consensus as to what the various branches of applied mathematics are. Such categorizations are made difficult by the way mathematics and science change over time, and also by the way universities organize departments, courses, and degrees.\n\nMany mathematicians distinguish between \"applied mathematics\", which is concerned with mathematical methods, and the \"applications of mathematics\" within science and engineering. A biologist using a population model and applying known mathematics would not be doing applied mathematics, but rather using it; however, mathematical biologists have posed problems that have stimulated the growth of pure mathematics. Mathematicians such as Poincaré and Arnold deny the existence of \"applied mathematics\" and claim that there are only \"applications of mathematics.\" Similarly, non-mathematicians blend applied mathematics and applications of mathematics. The use and development of mathematics to solve industrial problems is also called \"industrial mathematics\".[2]\n\nThe success of modern numerical mathematical methods and software has led to the emergence of computational mathematics, computational science, and computational engineering, which use high-performance computing for the simulation of phenomena and the solution of problems in the sciences and engineering. These are often considered interdisciplinary.\n\nSometimes, the term applicable mathematics is used to distinguish between the traditional applied mathematics that developed alongside physics and the many areas of mathematics that are applicable to real-world problems today, although there is no consensus as to a precise definition.[3]\n\nMathematicians often distinguish between \"applied mathematics\" on the one hand, and the \"applications of mathematics\" or \"applicable mathematics\" both within and outside of science and engineering, on the other.[3] Some mathematicians emphasize the term applicable mathematics to separate or delineate the traditional applied areas from new applications arising from fields that were previously seen as pure mathematics.[4] For example, from this viewpoint, an ecologist or geographer using population models and applying known mathematics would not be doing applied, but rather applicable, mathematics. Even fields such as number theory that are part of pure mathematics are now important in applications (such as cryptography), though they are not generally considered to be part of the field of applied mathematics per se. Such descriptions can lead to applicable mathematics being seen as a collection of mathematical methods such as real analysis, linear algebra, mathematical modelling, optimisation, combinatorics, probability and statistics, which are useful in areas outside traditional mathematics and not specific to mathematical physics.\n\nOther authors prefer describing applicable mathematics as a union of \"new\" mathematical applications with the traditional fields of applied mathematics.[4][5][6] With this outlook, the terms applied mathematics and applicable mathematics are thus interchangeable.\n\nHistorically, mathematics was most important in the natural sciences and engineering. However, since World War II, fields outside the physical sciences have spawned the creation of new areas of mathematics, such as game theory and social choice theory, which grew out of economic considerations. Further, the utilization and development of mathematical methods expanded into other areas leading to the creation of new fields such as mathematical finance and data science.\n\nThe advent of the computer has enabled new applications: studying and using the new computer technology itself (computer science) to study problems arising in other areas of science (computational science) as well as the mathematics of computation (for example, theoretical computer science, computer algebra,[7][8][9][10] numerical analysis[11][12][13][14]). Statistics is probably the most widespread mathematical science used in the social sciences.\n\nAcademic institutions are not consistent in the way they group and label courses, programs, and degrees in applied mathematics. At some schools, there is a single mathematics department, whereas others have separate departments for Applied Mathematics and (Pure) Mathematics. It is very common for Statistics departments to be separated at schools with graduate programs, but many undergraduate-only institutions include statistics under the mathematics department.\n\nMany applied mathematics programs (as opposed to departments) consist primarily of cross-listed courses and jointly appointed faculty in departments representing applications. Some Ph.D. programs in applied mathematics require little or no coursework outside mathematics, while others require substantial coursework in a specific area of application. In some respects this difference reflects the distinction between \"application of mathematics\" and \"applied mathematics\".\n\nSome universities in the U.K. host departments of Applied Mathematics and Theoretical Physics,[15][16][17] but it is now much less common to have separate departments of pure and applied mathematics. A notable exception to this is the Department of Applied Mathematics and Theoretical Physics at the University of Cambridge, housing the Lucasian Professor of Mathematics whose past holders include Isaac Newton, Charles Babbage, James Lighthill, Paul Dirac, and Stephen Hawking.\n\nSchools with separate applied mathematics departments range from Brown University, which has a large Division of Applied Mathematics that offers degrees through the doctorate, to Santa Clara University, which offers only the M.S. in applied mathematics.[20] Research universities dividing their mathematics department into pure and applied sections include MIT. Students in this program also learn another skill (computer science, engineering, physics, pure math, etc.) to supplement their applied math skills.\n\nApplied mathematics is associated with the following mathematical sciences:\n\nWith applications of applied geometry together with applied chemistry.\n\nScientific computing includes applied mathematics (especially numerical analysis[11][12][13][14][21]), computing science (especially high-performance computing[22][23]), and mathematical modelling in a scientific discipline.\n\nComputer science relies on logic, algebra, discrete mathematics such as graph theory,[24][25] and combinatorics.\n\nOperations research[26] and management science are often taught in faculties of engineering, business, and public policy.\n\nApplied mathematics has substantial overlap with the discipline of statistics. Statistical theorists study and improve statistical procedures with mathematics, and statistical research often raises mathematical questions. Statistical theory relies on probability and decision theory, and makes extensive use of scientific computing, analysis, and optimization; for the design of experiments, statisticians use algebra and combinatorial design. Applied mathematicians and statisticians often work in a department of mathematical sciences (particularly at colleges and small universities).\n\nActuarial science applies probability, statistics, and economic theory to assess risk in insurance, finance and other industries and professions.[27]\n\nMathematical economics is the application of mathematical methods to represent theories and analyze problems in economics.[28][29][30] The applied methods usually refer to nontrivial mathematical techniques or approaches. Mathematical economics is based on statistics, probability, mathematical programming (as well as other computational methods), operations research, game theory, and some methods from mathematical analysis. In this regard, it resembles (but is distinct from) financial mathematics, another part of applied mathematics.[31]\n\nAccording to the Mathematics Subject Classification (MSC), mathematical economics falls into the Applied mathematics/other classification of category 91:\n\nwith MSC2010 classifications for 'Game theory' at codes 91Axx Archived 2015-04-02 at the Wayback Machine and for 'Mathematical economics' at codes 91Bxx Archived 2015-04-02 at the Wayback Machine.\n\nThe line between applied mathematics and specific areas of application is often blurred. Many universities teach mathematical and statistical courses outside the respective departments, in departments and areas including business, engineering, physics, chemistry, psychology, biology, computer science, scientific computation, information theory, and mathematical physics.\n\nThe Society for Industrial and Applied Mathematics is an international applied mathematics organization. As of 2024, the society has 14,000 individual members.[32] The American Mathematics Society has its Applied Mathematics Group.[33]\n"
    },
    {
        "title": "Pure mathematics",
        "content": "Pure mathematics is the study of mathematical concepts independently of any application outside mathematics. These concepts may originate in real-world concerns, and the results obtained may later turn out to be useful for practical applications, but pure mathematicians are not primarily motivated by such applications. Instead, the appeal is attributed to the intellectual challenge and aesthetic beauty of working out the logical consequences of basic principles.\n\nWhile pure mathematics has existed as an activity since at least ancient Greece, the concept was elaborated upon around the year 1900,[2] after the introduction of theories with counter-intuitive properties (such as non-Euclidean geometries and Cantor's theory of infinite sets), and the discovery of apparent paradoxes (such as continuous functions that are nowhere differentiable, and Russell's paradox). This introduced the need to renew the concept of mathematical rigor and rewrite all mathematics accordingly, with a systematic use of axiomatic methods. This led many mathematicians to focus on mathematics for its own sake, that is, pure mathematics.\n\nNevertheless, almost all mathematical theories remained motivated by problems coming from the real world or from less abstract mathematical theories. Also, many mathematical theories, which had seemed to be totally pure mathematics, were eventually used in applied areas, mainly physics and computer science. A famous early example is Isaac Newton's demonstration that his law of universal gravitation implied that planets move in orbits that are conic sections, geometrical curves that had been studied in antiquity by Apollonius. Another example is the problem of factoring large integers, which is the basis of the RSA cryptosystem, widely used to secure internet communications.[3]\n\nIt follows that, presently, the distinction between pure and applied mathematics is more a philosophical point of view or a mathematician's preference rather than a rigid subdivision of mathematics.[4]\n\nAncient Greek mathematicians were among the earliest to make a distinction between pure and applied mathematics. Plato helped to create the gap between \"arithmetic\", now called number theory, and \"logistic\", now called arithmetic. Plato regarded logistic (arithmetic) as appropriate for businessmen and men of war who \"must learn the art of numbers or [they] will not know how to array [their] troops\" and arithmetic (number theory) as appropriate for philosophers \"because [they have] to arise out of the sea of change and lay hold of true being.\"[5] Euclid of Alexandria, when asked by one of his students of what use was the study of geometry, asked his slave to give the student threepence, \"since he must make gain of what he learns.\"[6] The Greek mathematician Apollonius of Perga was asked about the usefulness of some of his theorems in Book IV of Conics to which he proudly asserted,[7]\n\nThey are worthy of acceptance for the sake of the demonstrations themselves, in the same way as we accept many other things in mathematics for this and for no other reason.\nAnd since many of his results were not applicable to the science or engineering of his day, Apollonius further argued in the preface of the fifth book of Conics that the subject is one of those that \"...seem worthy of study for their own sake.\"[7]\n\nThe term itself is enshrined in the full title of the Sadleirian Chair, \"Sadleirian Professor of Pure Mathematics\", founded (as a professorship) in the mid-nineteenth century. The idea of a separate discipline of pure mathematics may have emerged at that time. The generation of Gauss made no sweeping distinction of the kind between pure and applied. In the following years, specialisation and professionalisation (particularly in the Weierstrass approach to mathematical analysis) started to make a rift more apparent.\n\nAt the start of the twentieth century mathematicians took up the axiomatic method, strongly influenced by David Hilbert's example. The logical formulation of pure mathematics suggested by Bertrand Russell in terms of a quantifier structure of propositions seemed more and more plausible, as large parts of mathematics became axiomatised and thus subject to the simple criteria of rigorous proof.\n\nPure mathematics, according to a view that can be ascribed to the Bourbaki group, is what is proved. \"Pure mathematician\" became a recognized vocation, achievable through training.\n\nThe case was made that pure mathematics is useful in engineering education:[8]\n\nOne central concept in pure mathematics is the idea of generality; pure mathematics often exhibits a trend towards increased generality. Uses and advantages of generality include the following:\n\nGenerality's impact on intuition is both dependent on the subject and a matter of personal preference or learning style.  Often generality is seen as a hindrance to intuition, although it can certainly function as an aid to it, especially when it provides analogies to material for which one already has good intuition.\n\nAs a prime example of generality, the Erlangen program involved an expansion of geometry to accommodate non-Euclidean geometries as well as the field of topology, and other forms of geometry, by viewing geometry as the study of a space together with a group of transformations.  The study of numbers, called algebra at the beginning undergraduate level, extends to abstract algebra at a more advanced level; and the study of functions, called calculus at the college freshman level becomes mathematical analysis and functional analysis at a more advanced level.  Each of these branches of more abstract mathematics have many sub-specialties, and there are in fact many connections between pure mathematics and applied mathematics disciplines. A steep rise in abstraction was seen mid 20th century.\n\nIn practice, however, these developments led to a sharp divergence from physics, particularly from 1950 to 1983. Later this was criticised, for example by Vladimir Arnold, as too much Hilbert, not enough Poincaré. The point does not yet seem to be settled, in that string theory pulls one way, while discrete mathematics pulls back towards proof as central.\n\nMathematicians have always had differing opinions regarding the distinction between pure and applied mathematics. One of the most famous (but perhaps misunderstood) modern examples of this debate can be found in G.H. Hardy's 1940 essay A Mathematician's Apology.\n\nIt is widely believed that Hardy considered applied mathematics to be ugly and dull. Although it is true that Hardy preferred pure mathematics, which he often compared to painting and poetry, Hardy saw the distinction between pure and applied mathematics to be simply that applied mathematics sought to express physical truth in a mathematical framework, whereas pure mathematics expressed truths that were independent of the physical world. Hardy made a separate distinction in mathematics between what he called \"real\" mathematics, \"which has permanent aesthetic value\", and \"the dull and elementary parts of mathematics\" that have practical use.[9]\n\nHardy considered some physicists, such as Einstein and Dirac, to be among the \"real\" mathematicians, but at the time that he was writing his Apology, he considered general relativity and quantum mechanics to be \"useless\", which allowed him to hold the opinion that only \"dull\" mathematics was useful. Moreover, Hardy briefly admitted that—just as the application of matrix theory and group theory to physics had come unexpectedly—the time may come where some kinds of beautiful, \"real\" mathematics may be useful as well.\n\nAnother insightful view is offered by American mathematician Andy Magid:\n\nI've always thought that a good model here could be drawn from ring theory. In that subject, one has the subareas of commutative ring theory and non-commutative ring theory. An uninformed observer might think that these represent a dichotomy, but in fact the latter subsumes the former: a non-commutative ring is a not-necessarily-commutative ring. If we use similar conventions, then we could refer to applied mathematics and nonapplied mathematics, where by the latter we mean not-necessarily-applied mathematics... [emphasis added][10]\nFriedrich Engels argued in his 1878 book Anti-Dühring that \"it is not at all true that in pure mathematics the mind deals only with its own creations and imaginations. The concepts of number and figure have not been invented from any source other than the world of reality\".[11]: 36  He further argued that \"Before one came upon the idea of deducing the form of a cylinder from the rotation of a rectangle about one of its sides, a number of real rectangles and cylinders, however imperfect in form, must have been examined. Like all other sciences, mathematics arose out of the needs of men...But, as in every department of thought, at a certain stage of development the laws, which were abstracted from the real world, become divorced from the real world, and are set up against it as something independent, as laws coming from outside, to which the world has to conform.\"[11]: 37 \n"
    },
    {
        "title": "Rigour",
        "content": "\n\nRigour (British English) or rigor (American English; see spelling differences) describes a condition of stiffness or strictness.[1] These constraints may be environmentally imposed, such as \"the rigours of famine\"; logically imposed, such as mathematical proofs which must maintain consistent answers; or socially imposed, such as the process of defining ethics and law.\n\n\"Rigour\" comes to English through old French (13th c., Modern French rigueur) meaning \"stiffness\", which itself is based on the Latin rigorem (nominative rigor) \"numbness, stiffness, hardness, firmness; roughness, rudeness\", from the verb rigere \"to be stiff\".[2] The noun was frequently used to describe a condition of strictness or stiffness, which arises from a situation or constraint either chosen or experienced passively. For example, the title of the book Theologia Moralis Inter Rigorem et Laxitatem Medi roughly translates as \"mediating theological morality between rigour and laxness\". The book details, for the clergy, situations in which they are obligated to follow church law exactly, and in which situations they can be more forgiving yet still considered moral.[3] Rigor mortis translates directly as the stiffness (rigor) of death (mortis), again describing a condition which arises from a certain constraint (death).\n\nIntellectual rigour is a process of thought which is consistent, does not contain self-contradiction, and takes into account the entire scope of available knowledge on the topic. It actively avoids logical fallacy. Furthermore, it requires a sceptical assessment of the available knowledge. If a topic or case is dealt with in a rigorous way, it typically means that it is dealt with in a comprehensive, thorough and complete way, leaving no room for inconsistencies.[4]\n\nScholarly method describes the different approaches or methods which may be taken to apply intellectual rigour on an institutional level to ensure the quality of information published. An example of intellectual rigour assisted by a methodical approach is the scientific method, in which a person will produce a hypothesis based on what they believe to be true, then construct experiments in order to prove that hypothesis wrong. This method, when followed correctly, helps to prevent against circular reasoning and other fallacies which frequently plague conclusions within academia. Other disciplines, such as philosophy and mathematics, employ their own structures to ensure intellectual rigour. Each method requires close attention to criteria for logical consistency, as well as to all relevant evidence and possible differences of interpretation. At an institutional level, peer review is used to validate intellectual rigour.\n\nIntellectual rigour is a subset of intellectual honesty—a practice of thought in which ones convictions are kept in proportion to valid evidence.[5] Intellectual honesty is an unbiased approach to the acquisition, analysis, and transmission of ideas. A person is being intellectually honest when he or she, knowing the truth, states that truth, regardless of outside social/environmental pressures. It is possible to doubt whether complete intellectual honesty exists—on the grounds that no one can entirely master his or her own presuppositions—without doubting that certain kinds of intellectual rigour are potentially available. The distinction certainly matters greatly in debate, if one wishes to say that an argument is flawed in its premises.\n\nThe setting for intellectual rigour does tend to assume a principled position from which to advance or argue. An opportunistic tendency to use any argument at hand is not very rigorous, although very common in politics, for example. Arguing one way one day, and another later, can be defended by casuistry, i.e. by saying the cases are different.\n\nIn the legal context, for practical purposes, the facts of cases do always differ. Case law can therefore be at odds with a principled approach; and intellectual rigour can seem to be defeated. This defines a judge's problem with uncodified law. Codified law poses a different problem, of interpretation and adaptation of definite principles without losing the point; here applying the letter of the law, with all due rigour, may on occasion seem to undermine the principled approach.\n\nMathematical rigour can apply to methods of mathematical proof and to  methods of mathematical practice (thus relating to other interpretations of rigour).\n\nMathematical rigour is often cited as a kind of gold standard for mathematical proof. Its history traces back to Greek mathematics, especially to Euclid's Elements.[6]\n\nUntil the 19th century, Euclid's Elements was seen as extremely rigorous and profound, but in the late 19th century, Hilbert (among others) realized that the work left certain assumptions implicit—assumptions that could not be proved from Euclid's Axioms (e.g. two circles can intersect in a point, some point is within an angle, and figures can be superimposed on each other).[7] This was contrary to the idea of rigorous proof where all assumptions need to be stated and nothing can be left implicit. New foundations were developed using the axiomatic method to address this gap in rigour found in the Elements (e.g., Hilbert's axioms, Birkhoff's axioms, Tarski's axioms).\n\nDuring the 19th century, the term \"rigorous\" began to be used to describe increasing levels of abstraction when dealing with calculus which eventually became known as mathematical analysis. The works of Cauchy added rigour to the older works of Euler and Gauss. The works of Riemann added rigour to the works of Cauchy. The works of Weierstrass added rigour to the works of Riemann, eventually culminating in the arithmetization of analysis. Starting in the 1870s, the term gradually came to be associated with Cantorian set theory.\n\nMathematical rigour can be modelled as amenability to algorithmic proof checking. Indeed, with the aid of computers, it is possible to check some proofs mechanically.[8] Formal rigour is the introduction of high degrees of completeness by means of a formal language where such proofs can be codified using set theories such as ZFC (see automated theorem proving).\n\nPublished mathematical arguments have to conform to a standard of rigour, but are written in a mixture of symbolic and natural language. In this sense, written mathematical discourse is a prototype of formal proof. Often, a written proof is accepted as rigorous although it might not be formalised as yet. The reason often cited by mathematicians for writing informally is that completely formal proofs tend to be longer and more unwieldy, thereby obscuring the line of argument. An argument that appears obvious to human intuition may in fact require fairly long formal derivations from the axioms. A particularly well-known example is how in Principia Mathematica, Whitehead and Russell have to expend a number of lines of rather opaque effort in order to establish that, indeed, it is sensical to say: \"1+1=2\". In short, comprehensibility is favoured over formality in written discourse.\n\nStill, advocates of automated theorem provers may argue that the formalisation of proof does improve the mathematical rigour by disclosing gaps or flaws in informal written discourse. When the correctness of a proof is disputed, formalisation is a way to settle such a dispute as it helps to reduce misinterpretations or ambiguity.\n\nThe role of mathematical rigour in relation to physics is twofold:\n\nBoth aspects of mathematical rigour in physics have attracted considerable attention in philosophy of science (see, for example, ref.[10] and ref.[11] and the works quoted therein).\n\nRigour in the classroom is a hotly debated topic amongst educators. Even the semantic meaning of the word is contested.\n\nGenerally speaking, classroom rigour consists of multi-faceted, challenging instruction and correct placement of the student. Students excelling in formal operational thought tend to excel in classes for gifted students.[citation needed]  Students who have not reached that final stage of cognitive development, according to developmental psychologist Jean Piaget, can build upon those skills with the help of a properly trained teacher.\n\nRigour in the classroom is commonly called \"rigorous instruction\". It is instruction that requires students to construct meaning for themselves, impose structure on information, integrate individual skills into processes, operate within but at the outer edge of their abilities, and apply what they learn in more than one context and to unpredictable situations.[12]\n"
    },
    {
        "title": "Greek mathematics",
        "content": "\n\nGreek mathematics refers to mathematics texts and ideas stemming from the Archaic through the Hellenistic and Roman periods, mostly from the 5th century BC to the 6th century AD, around the shores of the Mediterranean.[1][2] Greek mathematicians lived in cities spread over the entire region, from Anatolia to Italy and North Africa, but were united by Greek culture and the Greek language.[3] The development of mathematics as a theoretical discipline and the use of deductive reasoning in proofs is an important difference between Greek mathematics and those of preceding civilizations.[4][5]\n\nGreek mathēmatikē (\"mathematics\") derives from the Ancient Greek: μάθημα, romanized: máthēma, Attic Greek: [má.tʰɛː.ma] Koinē Greek: [ˈma.θi.ma], from the verb manthanein, \"to learn\". Strictly speaking, a máthēma could be any branch of learning, or anything learnt; however, since antiquity certain mathēmata (mainly arithmetic, geometry, astronomy, and harmonics) were granted special status.[6][7]\n\nThe origins of Greek mathematics are not well documented.[8][9] The earliest advanced civilizations in Greece and Europe were the Minoan and later Mycenaean civilizations, both of which flourished during the 2nd millennium BC. While these civilizations possessed writing and were capable of advanced engineering, including four-story palaces with drainage and beehive tombs, they left behind no mathematical documents.\n\nThough no direct evidence is available, it is generally thought that the neighboring Babylonian and Egyptian civilizations had an influence on the younger Greek tradition.[10][11][8] Unlike the flourishing of Greek literature in the span of 800 to 600 BC, not much is known about Greek mathematics in this early period—nearly all of the information was passed down through later authors, beginning in the mid-4th century BC.[12][13]\n\nGreek mathematics allegedly began with Thales of Miletus (c. 624–548 BC). Very little is known about his life, although it is generally agreed that he was one of the Seven Wise Men of Greece. According to Proclus, he traveled to Babylon from where he learned mathematics and other subjects, coming up with the proof of what is now called Thales' Theorem.[14][15]\n\nAn equally enigmatic figure is Pythagoras of Samos (c. 580–500 BC), who supposedly visited Egypt and Babylon,[13][16] and ultimately settled in Croton, Magna Graecia, where he started a kind of brotherhood. Pythagoreans supposedly believed that \"all is number\" and were keen in looking for mathematical relations between numbers and things.[17] Pythagoras himself was given credit for many later discoveries, including the construction of the five regular solids. However, Aristotle refused to attribute anything specifically to Pythagoras and only discussed the work of the Pythagoreans as a group.[18][19]\n\nAlmost half of the material in Euclid's Elements is customarily attributed to the Pythagoreans, including the discovery of irrationals, attributed to Hippasus (c. 530–450 BC) and Theodorus (fl. 450 BC).[20] The greatest mathematician associated with the group, however, may have been Archytas (c. 435-360 BC), who solved the problem of doubling the cube, identified the harmonic mean, and possibly contributed to optics and mechanics.[20][21] Other mathematicians active in this period, not fully affiliated with any school, include Hippocrates of Chios (c. 470–410 BC), Theaetetus (c. 417–369 BC), and Eudoxus (c. 408–355 BC).\n\nGreek mathematics also drew the attention of philosophers during the Classical period. Plato (c. 428–348 BC), the founder of the Platonic Academy, mentions mathematics in several of his dialogues.[22] While not considered a mathematician, Plato seems to have been influenced by Pythagorean ideas about number and believed that the elements of matter could be broken down into geometric solids.[23] He also believed that geometrical proportions bound the cosmos together rather than physical or mechanical forces.[24] Aristotle (c. 384–322 BC), the founder of the Peripatetic school, often used mathematics to illustrate many of his theories, as when he used geometry in his theory of the rainbow and the theory of proportions in his analysis of motion.[24] Much of the knowledge about ancient Greek mathematics in this period is thanks to records referenced by Aristotle in his own works.[13][25]\n\nThe Hellenistic era began in the late 4th century BC, following Alexander the Great's conquest of the Eastern Mediterranean, Egypt, Mesopotamia, the Iranian plateau, Central Asia, and parts of India, leading to the spread of the Greek language and culture across these regions. Greek became the lingua franca of scholarship throughout the Hellenistic world, and the mathematics of the Classical period merged with Egyptian and Babylonian mathematics to give rise to Hellenistic mathematics.[27][28]\n\nGreek mathematics[a] reached its acme during the Hellenistic and early Roman periods, and much of the work represented by authors such as Euclid (fl. 300 BC), Archimedes (c. 287–212 BC), Apollonius (c. 240–190 BC), Hipparchus (c. 190–120 BC), and Ptolemy (c. 100–170 AD) was of a very advanced level and rarely mastered outside a small circle.[29] Examples of applied mathematics around this time include the construction of analogue computers like the Antikythera mechanism,[30][31] the accurate measurement of the circumference of the Earth by Eratosthenes (276–194 BC), and the mathematical and mechanical works of Heron (c. 10–70 AD).[32]\n\nSeveral centers of learning appeared during the Hellenistic period, of which the most important one was the Mouseion in Alexandria, Egypt, which attracted scholars from across the Hellenistic world (mostly Greek, but also Egyptian, Jewish, Persian, among others).[33][34] Although few in number, Hellenistic mathematicians actively communicated with each other; publication consisted of passing and copying someone's work among colleagues.[35]\n\nLater mathematicians in the Roman era include Diophantus (c. 214–298 AD), who wrote on polygonal numbers and a work in pre-modern algebra (Arithmetica),[36][37] Pappus of Alexandria (c. 290–350 AD), who compiled many important results in the Collection,[38] Theon of Alexandria (c. 335–405 AD) and his daughter Hypatia (c. 370–415 AD), who edited Ptolemy's Almagest and other works,[39][40] and Eutocius of Ascalon (c. 480–540 AD), who wrote commentaries on treatises by Archimedes and Apollonius.[41] Although none of these mathematicians, save perhaps Diophantus, had notable original works, they are distinguished for their commentaries and expositions. These commentaries have preserved valuable extracts from works which have perished, or historical allusions which, in the absence of original documents, are precious because of their rarity.[42][43]\n\nMost of the mathematical texts written in Greek survived through the copying of manuscripts over the centuries. While some fragments dating from antiquity have been found above all in Egypt, as a rule they do not add anything significant to our knowledge of Greek mathematics preserved in the manuscript tradition.[29]\n\nGreek mathematics constitutes an important period in the history of mathematics: fundamental in respect of geometry and for the idea of formal proof.[44] Greek mathematicians also contributed to number theory, mathematical astronomy, combinatorics, mathematical physics, and, at times, approached ideas close to the integral calculus.[45][46]\n\nEudoxus of Cnidus developed a theory of proportion that bears resemblance to the modern theory of real numbers using the Dedekind cut, developed by Richard Dedekind, who acknowledged Eudoxus as inspiration.[47][48][49][50]\n\nEuclid, who presumably wrote on optics, astronomy, and harmonics, collected many previous mathematical results and theorems in the Elements, a canon of geometry and elementary number theory for many centuries.[51][52][53] Menelaus, a later geometer and astronomer, wrote a standard work on spherical geometry in the style of the Elements, the Spherics, arguably considered the first treatise in non-Euclidean geometry.[54][55]\n\nArchimedes made use of a technique dependent on a form of proof by contradiction to reach answers to problems with an arbitrary degree of accuracy, while specifying the limits within which the answers lay. Known as the method of exhaustion, Archimedes employed it in several of his works, including an approximation to π (Measurement of the Circle),[56] and a proof that the area enclosed by a parabola and a straight line is 4/3 times the area of a triangle with equal base and height (Quadrature of the Parabola).[57] Archimedes also showed that the number of grains of sand filling the universe was not uncountable, devising his own counting scheme based on the myriad, which denoted 10,000 (The Sand-Reckoner).[58]\n\nThe most characteristic product of Greek mathematics may be the theory of conic sections, which was largely developed in the Hellenistic period, starting with the work of Menaechmus and perfected primarily under Apollonius in his work Conics.[59][60][61] The methods employed in these works made no explicit use of algebra, nor trigonometry, the latter appearing around the time of Hipparchus.[62][63]\n\nAncient Greek mathematics was not limited to theoretical works but was also used in other activities, such as business transactions and in land mensuration, as evidenced by extant texts where computational procedures and practical considerations took more of a central role.[11][64]\n\nAlthough the earliest Greek mathematical texts that have been found were written after the Hellenistic period, most are considered to be copies of works written during and before the Hellenistic period.[65] The two major sources are\n\nDespite the lack of original manuscripts, the dates for some Greek mathematicians are more certain than the dates of surviving Babylonian or Egyptian sources because a number of overlapping chronologies exist, though many dates remain uncertain.\n\nNetz (2011) has counted 144 ancient authors in the mathematical or exact sciences, from whom only 29 works are extant in Greek: Aristarchus, Autolycus, Philo of Byzantium, Biton, Apollonius, Archimedes, Euclid, Theodosius, Hypsicles, Athenaeus, Geminus, Heron, Apollodorus, Theon of Smyrna, Cleomedes, Nicomachus, Ptolemy, Gaudentius, Anatolius, Aristides Quintilian, Porphyry, Diophantus, Alypius, Damianus, Pappus, Serenus, Theon of Alexandria, Anthemius, and Eutocius.[66]\n\nThe following works are extant only in Arabic translations:[67][68]\n"
    },
    {
        "title": "Euclid",
        "content": "\n\nEuclid (/ˈjuːklɪd/; Ancient Greek: Εὐκλείδης; fl. 300 BC) was an ancient Greek mathematician active as a geometer and logician.[2] Considered the \"father of geometry\",[3] he is chiefly known for the Elements treatise, which established the foundations of geometry that largely dominated the field until the early 19th century. His system, now referred to as Euclidean geometry, involved innovations in combination with a synthesis of theories from earlier Greek mathematicians, including Eudoxus of Cnidus, Hippocrates of Chios, Thales and Theaetetus. With Archimedes and Apollonius of Perga, Euclid is generally considered among the greatest mathematicians of antiquity, and one of the most influential in the history of mathematics.\n\nVery little is known of Euclid's life, and most information comes from the scholars Proclus and Pappus of Alexandria many centuries later. Medieval Islamic mathematicians invented a fanciful biography, and medieval Byzantine and early Renaissance scholars mistook him for the earlier philosopher Euclid of Megara. It is now generally accepted that he spent his career in Alexandria and lived around 300 BC, after Plato's students and before Archimedes. There is some speculation that Euclid studied at the Platonic Academy and later taught at the Musaeum; he is regarded as bridging the earlier Platonic tradition in Athens with the later tradition of Alexandria.\n\nIn the Elements, Euclid deduced the theorems from a small set of axioms. He also wrote works on perspective, conic sections, spherical geometry, number theory, and mathematical rigour. In addition to the Elements, Euclid wrote a central early text in the optics field, Optics, and lesser-known works including Data and Phaenomena. Euclid's authorship of On Divisions of Figures and Catoptrics has been questioned. He is thought to have written many lost works.\n\nThe English name 'Euclid' is the anglicized version of the Ancient Greek name Eukleídes (Εὐκλείδης).[4][a] It is derived from 'eu-' (εὖ; 'well') and 'klês' (-κλῆς; 'fame'), meaning \"renowned, glorious\".[6] In English, by metonymy, 'Euclid' can mean his most well-known work, Euclid's Elements, or a copy thereof,[5] and is sometimes synonymous with 'geometry'.[2]\n\nAs with many ancient Greek mathematicians, the details of Euclid's life are mostly unknown.[7] He is accepted as the author of four mostly extant treatises—the Elements, Optics, Data, Phaenomena—but besides this, there is nothing known for certain of him.[8][b] The traditional narrative mainly follows the 5th century AD account by Proclus in his Commentary on the First Book of Euclid's Elements, as well as a few anecdotes from Pappus of Alexandria in the early 4th century.[4][c]\n\nAccording to Proclus, Euclid lived shortly after several of Plato's (d. 347 BC) followers and before the mathematician Archimedes (c. 287 – c. 212 BC);[d] specifically, Proclus placed Euclid during the rule of Ptolemy I (r. 305/304–282 BC).[7][8][e] Euclid's birthdate is unknown; some scholars estimate around 330[11][12] or 325 BC,[2][13] but others refrain from speculating.[14] It is presumed that he was of Greek descent,[11] but his birthplace is unknown.[15][f] Proclus held that Euclid followed the Platonic tradition, but there is no definitive confirmation for this.[17] It is unlikely he was a contemporary of Plato, so it is often presumed that he was educated by Plato's disciples at the Platonic Academy in Athens.[18] Historian Thomas Heath supported this theory, noting that most capable geometers lived in Athens, including many of those whose work Euclid built on;[19] historian Michalis Sialaros considers this a mere conjecture.[4][20] In any event, the contents of Euclid's work demonstrate familiarity with the Platonic geometry tradition.[11]\n\nIn his Collection, Pappus mentions that Apollonius studied with Euclid's students in Alexandria, and this has been taken to imply that Euclid worked and founded a mathematical tradition there.[8][21][19] The city was founded by Alexander the Great in 331 BC,[22] and the rule of Ptolemy I from 306 BC onwards gave it a stability which was relatively unique amid the chaotic wars over dividing Alexander's empire.[23] Ptolemy began a process of hellenization and commissioned numerous constructions, building the massive Musaeum institution, which was a leading center of education.[15][g] Euclid is speculated to have been among the Musaeum's first scholars.[22] Euclid's date of death is unknown; it has been speculated that he died c. 270 BC.[22]\n\nEuclid is often referred to as 'Euclid of Alexandria' to differentiate him from the earlier philosopher Euclid of Megara, a pupil of Socrates included in dialogues of Plato with whom he was historically conflated.[4][14] Valerius Maximus, the 1st century AD Roman compiler of anecdotes, mistakenly substituted Euclid's name for Eudoxus (4th century BC) as the mathematician to whom Plato sent those asking how to double the cube.[26] Perhaps on the basis of this mention of a mathematical Euclid roughly a century early, Euclid became mixed up with Euclid of Megara in medieval Byzantine sources (now lost),[27] eventually leading Euclid the mathematician to be ascribed details of both men's biographies and described as Megarensis (lit. 'of Megara').[4][28] The Byzantine scholar Theodore Metochites (c. 1300) explicitly conflated the two Euclids, as did printer Erhard Ratdolt's 1482 editio princeps of Campanus of Novara's Latin translation of the Elements.[27] After the mathematician Bartolomeo Zamberti [fr; de] appended most of the extant biographical fragments about either Euclid to the preface of his 1505 translation of the Elements, subsequent publications passed on this identification.[27] Later Renaissance scholars, particularly Peter Ramus, reevaluated this claim, proving it false via issues in chronology and contradiction in early sources.[27]\n\nMedieval Arabic sources give vast amounts of information concerning Euclid's life, but are completely unverifiable.[4] Most scholars consider them of dubious authenticity;[8] Heath in particular contends that the fictionalization was done to strengthen the connection between a revered mathematician and the Arab world.[17] There are also numerous anecdotal stories concerning to Euclid, all of uncertain historicity, which \"picture him as a kindly and gentle old man\".[29] The best known of these is Proclus' story about Ptolemy asking Euclid if there was a quicker path to learning geometry than reading his Elements, which Euclid replied with \"there is no royal road to geometry\".[29] This anecdote is questionable since a very similar interaction between Menaechmus and Alexander the Great is recorded from Stobaeus.[30] Both accounts were written in the 5th century AD, neither indicates its source, and neither appears in ancient Greek literature.[31]\n\nAny firm dating of Euclid's activity c. 300 BC is called into question by a lack of contemporary references.[4] The earliest original reference to Euclid is in Apollonius' prefatory letter to the Conics (early 2nd century BC): \"The third book of the Conics contains many astonishing theorems that are useful for both the syntheses and the determinations of number of solutions of solid loci. Most of these, and the finest of them, are novel. And when we discovered them we realized that Euclid had not made the synthesis of the locus on three and four lines but only an accidental fragment of it, and even that was not felicitously done.\"[26] The Elements is speculated to have been at least partly in circulation by the 3rd century BC, as Archimedes and Apollonius take several of its propositions for granted;[4] however, Archimedes employs an older variant of the theory of proportions than the one found in the Elements.[8] The oldest physical copies of material included in the Elements, dating from roughly 100 AD, can be found on papyrus fragments unearthed in an ancient rubbish heap from Oxyrhynchus, Roman Egypt. The oldest extant direct citations to the Elements in works whose dates are firmly known are not until the 2nd century AD, by Galen and Alexander of Aphrodisias; by this time it was a standard school text.[26] Some ancient Greek mathematicians mention Euclid by name, but he is usually referred to as \"ὁ στοιχειώτης\" (\"the author of Elements\").[32] In the Middle Ages, some scholars contended Euclid was not a historical personage and that his name arose from a corruption of Greek mathematical terms.[33]\n\n\nEuclid is best known for his thirteen-book treatise, the Elements (Ancient Greek: Στοιχεῖα; Stoicheia), considered his magnum opus.[3][35] Much of its content originates from earlier mathematicians, including Eudoxus, Hippocrates of Chios, Thales and Theaetetus, while other theorems are mentioned by Plato and Aristotle.[36] It is difficult to differentiate the work of Euclid from that of his predecessors, especially because the Elements essentially superseded much earlier and now-lost Greek mathematics.[37][h] The classicist Markus Asper concludes that \"apparently Euclid's achievement consists of assembling accepted mathematical knowledge into a cogent order and adding new proofs to fill in the gaps\" and the historian Serafina Cuomo described it as a \"reservoir of results\".[38][36] Despite this, Sialaros furthers that \"the remarkably tight structure of the Elements reveals authorial control beyond the limits of a mere editor\".[9]\n\nThe Elements does not exclusively discuss geometry as is sometimes believed.[37] It is traditionally divided into three topics: plane geometry (books 1–6), basic number theory (books 7–10) and solid geometry (books 11–13)—though book 5 (on proportions) and 10 (on irrational lines) do not exactly fit this scheme.[39][40] The heart of the text is the theorems scattered throughout.[35] Using Aristotle's terminology, these may be generally separated into two categories: \"first principles\" and \"second principles\".[41] The first group includes statements labeled as a \"definition\" (Ancient Greek: ὅρος or ὁρισμός), \"postulate\" (αἴτημα), or a \"common notion\" (κοινὴ ἔννοια);[41][42] only the first book includes postulates—later known as axioms—and common notions.[37][i] The second group consists of propositions, presented alongside mathematical proofs and diagrams.[41] It is unknown if Euclid intended the Elements as a textbook, but its method of presentation makes it a natural fit.[9] As a whole, the authorial voice remains general and impersonal.[36]\n\nBook 1 of the Elements is foundational for the entire text.[37] It begins with a series of 20 definitions for basic geometric concepts such as lines, angles and various regular polygons.[44] Euclid then presents 10 assumptions (see table, right), grouped into five postulates (axioms) and five common notions.[45][k] These assumptions are intended to provide the logical basis for every subsequent theorem, i.e. serve as an axiomatic system.[46][l] The common notions exclusively concern the comparison of magnitudes.[48] While postulates 1 through 4 are relatively straightforward,[m] the 5th is known as the parallel postulate and particularly famous.[48][n] Book 1 also includes 48 propositions, which can be loosely divided into those concerning basic theorems and constructions of plane geometry and triangle congruence (1–26); parallel lines (27–34); the area of triangles and parallelograms (35–45); and the Pythagorean theorem (46–48).[48] The last of these includes the earliest surviving proof of the Pythagorean theorem, described by Sialaros as \"remarkably delicate\".[41]\n\nBook 2 is traditionally understood as concerning \"geometric algebra\", though this interpretation has been heavily debated since the 1970s; critics describe the characterization as anachronistic, since the foundations of even nascent algebra occurred many centuries later.[41] The second book has a more focused scope and mostly provides algebraic theorems to accompany various geometric shapes.[37][48] It focuses on the area of rectangles and squares (see Quadrature), and leads up to a geometric precursor of the law of cosines.[50] Book 3 focuses on circles, while the 4th discusses regular polygons, especially the pentagon.[37][51] Book 5 is among the work's most important sections and presents what is usually termed as the \"general theory of proportion\".[52][o] Book 6 utilizes the \"theory of ratios\" in the context of plane geometry.[37] It is built almost entirely of its first proposition:[53] \"Triangles and parallelograms which are under the same height are to one another as their bases\".[54]\n\nFrom Book 7 onwards, the mathematician Benno Artmann [de] notes that \"Euclid starts afresh. Nothing from the preceding books is used\".[55] Number theory is covered by books 7 to 10, the former beginning with a set of 22 definitions for parity, prime numbers and other arithmetic-related concepts.[37] Book 7 includes the Euclidean algorithm, a method for finding the greatest common divisor of two numbers.[55] The 8th book discusses geometric progressions, while book 9 includes the proposition, now called Euclid's theorem, that there are infinitely many prime numbers.[37] Of the Elements, book 10 is by far the largest and most complex, dealing with irrational numbers in the context of magnitudes.[41]\n\nThe final three books (11–13) primarily discuss solid geometry.[39] By introducing a list of 37 definitions, Book 11 contextualizes the next two.[56] Although its foundational character resembles Book 1, unlike the latter it features no axiomatic system or postulates.[56] The three sections of Book 11 include content on solid geometry (1–19), solid angles (20–23) and parallelepipedal solids (24–37).[56]\n\nIn addition to the Elements, at least five works of Euclid have survived to the present day. They follow the same logical structure as Elements, with definitions and proved propositions.\n\nFour other works are credibly attributed to Euclid, but have been lost.[9]\n\nEuclid is generally considered with Archimedes and Apollonius of Perga as among the greatest mathematicians of antiquity.[11] Many commentators cite him as one of the most influential figures in the history of mathematics.[2] The geometrical system established by the Elements long dominated the field; however, today that system is often referred to as 'Euclidean geometry' to distinguish it from other non-Euclidean geometries discovered in the early 19th century.[61] Among Euclid's many namesakes are the European Space Agency's (ESA) Euclid spacecraft,[62] the lunar crater Euclides,[63] and the minor planet 4354 Euclides.[64]\n\nThe Elements is often considered after the Bible as the most frequently translated, published, and studied book in the Western World's history.[61] With Aristotle's Metaphysics, the Elements is perhaps the most successful ancient Greek text, and was the dominant mathematical textbook in the Medieval Arab and Latin worlds.[61]\n\nThe first English edition of the Elements was published in 1570 by Henry Billingsley and John Dee.[27] The mathematician Oliver Byrne published a well-known version of the Elements  in 1847 entitled The First Six Books of the Elements of Euclid in Which Coloured Diagrams and Symbols Are Used Instead of Letters for the Greater Ease of Learners, which included colored diagrams intended to increase its pedagogical effect.[65] David Hilbert authored a modern axiomatization of the Elements.[66] Edna St. Vincent Millay wrote that \"Euclid alone has looked on Beauty bare.\"[67]\n"
    },
    {
        "title": "Euclid's Elements",
        "content": "The Elements (Ancient Greek: Στοιχεῖα Stoikheîa) is a mathematical treatise consisting of 13 books attributed to the ancient Greek mathematician Euclid c. 300 BC. It is a collection of definitions, postulates, propositions (theorems and constructions), and mathematical proofs of the propositions. The books cover plane and solid Euclidean geometry, elementary number theory, and incommensurable lines. Elements is the oldest extant large-scale deductive treatment of mathematics. It has proven instrumental in the development of logic and modern science, and its logical rigor was not surpassed until the 19th century.\n\nEuclid's Elements has been referred to as the most successful[a][b] and influential[c] textbook ever written. It was one of the very earliest mathematical works to be printed after the invention of the printing press and has been estimated to be second only to the Bible in the number of editions published since the first printing in 1482,[1] the number reaching well over one thousand.[d] For centuries, when the quadrivium was included in the curriculum of all university students, knowledge of at least part of Euclid's Elements was required of all students. Not until the 20th century, by which time its content was universally taught through other school textbooks, did it cease to be considered something all educated people had read.[citation needed]\n\nScholars believe that the Elements is largely a compilation of propositions based on books by earlier Greek mathematicians.[3]\n\nProclus (412–485 AD), a Greek mathematician who lived around seven centuries after Euclid, wrote in his commentary on the Elements: \"Euclid, who put together the Elements, collecting many of Eudoxus' theorems, perfecting many of Theaetetus', and also bringing to irrefragable demonstration the things which were only somewhat loosely proved by his predecessors\".\n\nPythagoras (c. 570–495 BC) was probably the source for most of books I and II, Hippocrates of Chios (c. 470–410 BC, not the better known Hippocrates of Kos) for book III, and Eudoxus of Cnidus (c. 408–355 BC) for book V, while books IV, VI, XI, and XII probably came from other Pythagorean or Athenian mathematicians.[4] The Elements may have been based on an earlier textbook by Hippocrates of Chios, who also may have originated the use of letters to refer to figures.[5] Other similar works are also reported to have been written by Theudius of Magnesia, Leon, and Hermotimus of Colophon.[6][7]\n\nIn the 4th century AD, Theon of Alexandria produced an edition of Euclid which was so widely used that it became the only surviving source until François Peyrard's 1808 discovery at the Vatican of a manuscript not derived from Theon's. This manuscript, the Heiberg manuscript, is from a Byzantine workshop around 900 and is the basis of modern editions.[8] Papyrus Oxyrhynchus 29 is a tiny fragment of an even older manuscript, but only contains the statement of one proposition.\n\nAlthough Euclid was known to Cicero, for instance, no record exists of the text having been translated into Latin prior to Boethius in the fifth or sixth century.[2] The Arabs received the Elements from the Byzantines around 760; this version was translated into Arabic under Harun al-Rashid (c. 800).[2] The Byzantine scholar Arethas commissioned the copying of one of the extant Greek manuscripts of Euclid in the late ninth century.[9] Although known in Byzantium, the Elements was lost to Western Europe until about 1120, when the English monk Adelard of Bath translated it into Latin from an Arabic translation.[e] A relatively recent discovery was made of a Greek-to-Latin translation from the 12th century at Palermo, Sicily. The name of the translator is not known other than he was an anonymous medical student from Salerno who was visiting Palermo in order to translate the Almagest to Latin. The Euclid manuscript is extant and quite complete.[11]\n\nAfter the translation by Adelard of Bath (known as Adelard I), there was a flurry of translations from Arabic. Notable translators in this period include Herman of Carinthia who wrote an edition around 1140, Robert of Chester (his manuscripts are referred to collectively as Adelard II, written on or before 1251), Johannes de Tinemue,[12] possibly also known as John of Tynemouth (his manuscripts are referred to collectively as Adelard III), late 12th century, and Gerard of Cremona (sometime after 1120 but before 1187). The exact details concerning these translations is still an active area of research.[13][page needed] Campanus of Novara relied heavily on these Arabic translations to create his edition (sometime before 1260) which ultimately came to dominate Latin editions until the availability of Greek manuscripts in the 16th century. There are more than 100 pre-1482 Campanus manuscripts still available today.[14][15]\n\n The first printed edition appeared in 1482 (based on Campanus's translation),[16] and since then it has been translated into many languages and published in about a thousand different editions. Theon's Greek edition was recovered and published in 1533[17] based on Paris gr. 2343 and Venetus Marcianus 301.[18] In 1570, John Dee provided a widely respected \"Mathematical Preface\", along with copious notes and supplementary material, to the first English edition by Henry Billingsley.\n\nCopies of the Greek text still exist, some of which can be found in the Vatican Library and the Bodleian Library in Oxford. The manuscripts available are of variable quality, and invariably incomplete. By careful analysis of the translations and originals, hypotheses have been made about the contents of the original text (copies of which are no longer available).\n\nAncient texts which refer to the Elements itself, and to other mathematical theories that were current at the time it was written, are also important in this process. Such analyses are conducted by J. L. Heiberg and Sir Thomas Little Heath in their editions of the text.\n\nAlso of importance are the scholia, or annotations to the text. These additions, which often distinguished themselves from the main text (depending on the manuscript), gradually accumulated over time as opinions varied upon what was worthy of explanation or further study.\n\nThe Elements is still considered a masterpiece in the application of logic to mathematics. In historical context, it has proven enormously influential in many areas of science. Scientists Nicolaus Copernicus, Johannes Kepler, Galileo Galilei, Albert Einstein and Sir Isaac Newton were all influenced by the Elements, and applied their knowledge of it to their work.[19][20] Mathematicians and philosophers, such as Thomas Hobbes, Baruch Spinoza, Alfred North Whitehead, and Bertrand Russell, have attempted to create their own foundational \"Elements\" for their respective disciplines, by adopting the axiomatized deductive structures that Euclid's work introduced.\n\nThe austere beauty of Euclidean geometry has been seen by many in western culture as a glimpse of an otherworldly system of perfection and certainty. Abraham Lincoln kept a copy of Euclid in his saddlebag, and studied it late at night by lamplight; he related that he said to himself, \"You never can make a lawyer if you do not understand what demonstrate means; and I left my situation in Springfield, went home to my father's house, and stayed there till I could give any proposition in the six books of Euclid at sight\".[21][22] Edna St. Vincent Millay wrote in her sonnet \"Euclid alone has looked on Beauty bare\", \"O blinding hour, O holy, terrible day, / When first the shaft into his vision shone / Of light anatomized!\". Albert Einstein recalled a copy of the Elements and a magnetic compass as two gifts that had a great influence on him as a boy, referring to the Euclid as the \"holy little geometry book\".[23][24]\n\nThe success of the Elements is due primarily to its logical presentation of most of the mathematical knowledge available to Euclid. Much of the material is not original to him, although many of the proofs are his. However, Euclid's systematic development of his subject, from a small set of axioms to deep results, and the consistency of his approach throughout the Elements, encouraged its use as a textbook for about 2,000 years. The Elements still influences modern geometry books. Furthermore, its logical, axiomatic approach and rigorous proofs remain the cornerstone of mathematics.\n\nOne of the most notable influences of Euclid on modern mathematics is the discussion of the parallel postulate. In Book I, Euclid lists five postulates, the fifth of which stipulates\n\nIf a line segment intersects two straight lines forming two interior angles on the same side that sum to less than two right angles, then the two lines, if extended indefinitely, meet on that side on which the angles sum to less than two right angles.\nThis postulate plagued mathematicians for centuries due to its apparent complexity compared with the other four postulates. Many attempts were made to prove the fifth postulate based on the other four, but they never succeeded. Eventually in 1829, mathematician Nikolai Lobachevsky published a description of acute geometry (or hyperbolic geometry), a geometry which assumed a different form of the parallel postulate. It is in fact possible to create a valid geometry without the fifth postulate entirely, or with different versions of the fifth postulate (elliptic geometry). If one takes the fifth postulate as a given, the result is Euclidean geometry.[citation needed]\n\n• \"To draw a straight line from any point to any point.\"\n• \"To describe a circle with any center and distance.\"\n\nEuclid's axiomatic approach and constructive methods were widely influential.\n\nMany of Euclid's propositions were constructive, demonstrating the existence of some figure by detailing the steps he used to construct the object using a compass and straightedge. His constructive approach appears even in his geometry's postulates, as the first and third postulates stating the existence of a line and circle are constructive. Instead of stating that lines and circles exist per his prior definitions, he states that it is possible to 'construct' a line and circle. It also appears that, for him to use a figure in one of his proofs, he needs to construct it in an earlier proposition. For example, he proves the Pythagorean theorem by first inscribing a square on the sides of a right triangle, but only after constructing a square on a given line one proposition earlier.[27]\n\nAs was common in ancient mathematical texts, when a proposition needed proof in several different cases, Euclid often proved only one of them (often the most difficult), leaving the others to the reader. Later editors such as Theon often interpolated their own proofs of these cases.\n\nEuclid's presentation was limited by the mathematical ideas and notations in common currency in his era, and this causes the treatment to seem awkward to the modern reader in some places. For example, there was no notion of an angle greater than two right angles,[28] the number 1 was sometimes treated separately from other positive integers, and as multiplication was treated geometrically he did not use the product of more than 3 different numbers. The geometrical treatment of number theory may have been because the alternative would have been the extremely awkward Alexandrian system of numerals.[29]\n\nThe presentation of each result is given in a stylized form, which, although not invented by Euclid, is recognized as typically classical. It has six different parts: First is the 'enunciation', which states the result in general terms (i.e., the statement of the proposition). Then comes the 'setting-out', which gives the figure and denotes particular geometrical objects by letters. Next comes the 'definition' or 'specification', which restates the enunciation in terms of the particular figure. Then the 'construction' or 'machinery' follows. Here, the original figure is extended to forward the proof. Then, the 'proof' itself follows. Finally, the 'conclusion' connects the proof to the enunciation by stating the specific conclusions drawn in the proof, in the general terms of the enunciation.[30]\n\nNo indication is given of the method of reasoning that led to the result, although the Data does provide instruction about how to approach the types of problems encountered in the first four books of the Elements.[4] Some scholars have tried to find fault in Euclid's use of figures in his proofs, accusing him of writing proofs that depended on the specific figures drawn rather than the general underlying logic, especially concerning Proposition II of Book I. However, Euclid's original proof of this proposition, is general, valid, and does not depend on the figure used as an example to illustrate one given configuration.[31]\n\nEuclid's Elements contains errors.  Some of the foundational theorems are proved using axioms that Euclid did not state explicitly.  A few proofs have errors, by relying on assumptions that are intuitive but not explicitly proven.  Mathematician and historian W. W. Rouse Ball put the criticisms in perspective, remarking that \"the fact that for two thousand years [the Elements] was the usual text-book on the subject raises a strong presumption that it is not unsuitable for that purpose.\"[28]\n\nLater editors have added Euclid's implicit axiomatic assumptions in their list of formal axioms.[32]  For example, in the first construction of Book 1, Euclid used a premise that was neither postulated nor proved: that two circles with centers at the distance of their radius will intersect in two points.[33]\n\nKnown errors in Euclid date to at least 1882, when Pasch published his missing axiom.\n\nEarly attempts to find all the errors include Hilbert's geometry axioms and Tarski's.  In 2017, Michael Beeson et al. used computer proof assistants to create a new set of axioms similar to Euclid's and generate proofs that were valid with those axioms.[34]\n\nBeeson et al. checked only Book I and found these errors: missing axioms, superfluous axioms, gaps in logic (such as failing to prove points were colinear), missing theorems (such as an angle cannot be less than itself), and outright bad proofs.  The bad proofs were in Book I, Proof 7 and Book I, Proposition 9.\n\nIt was not uncommon in ancient times to attribute to celebrated authors works that were not written by them. It is by these means that the apocryphal books XIV and XV of the Elements were sometimes included in the collection.[35] The spurious Book XIV was probably written by Hypsicles on the basis of a treatise by Apollonius. The book continues Euclid's comparison of regular solids inscribed in spheres, with the chief result being that the ratio of the surfaces of the dodecahedron and icosahedron inscribed in the same sphere is the same as the ratio of their volumes, the ratio being\n\n\n\n\n\n\n\n10\n\n3\n(\n5\n−\n\n\n5\n\n\n)\n\n\n\n\n=\n\n\n\n\n5\n+\n\n\n5\n\n\n\n6\n\n\n\n.\n\n\n{\\displaystyle {\\sqrt {\\frac {10}{3(5-{\\sqrt {5}})}}}={\\sqrt {\\frac {5+{\\sqrt {5}}}{6}}}.}\n\n\n\nThe spurious Book XV was probably written, at least in part, by Isidore of Miletus. This book covers topics such as counting the number of edges and solid angles in the regular solids, and finding the measure of dihedral angles of faces that meet at an edge.[f]\n"
    },
    {
        "title": "Arithmetic",
        "content": "\n\nArithmetic is an elementary branch of mathematics that studies numerical operations like addition, subtraction, multiplication, and division. In a wider sense, it also includes exponentiation, extraction of roots, and taking logarithms.\n\nArithmetic systems can be distinguished based on the type of numbers they operate on. Integer arithmetic is about calculations with positive and negative integers. Rational number arithmetic involves operations on fractions of integers. Real number arithmetic is about calculations with real numbers, which include both rational and irrational numbers.\n\nAnother distinction is based on the numeral system employed to perform calculations. Decimal arithmetic is the most common. It uses the basic numerals from 0 to 9 and their combinations to express numbers. Binary arithmetic, by contrast, is used by most computers and represents numbers as combinations of the basic numerals 0 and 1. Computer arithmetic deals with the specificities of the implementation of binary arithmetic on computers. Some arithmetic systems operate on mathematical objects other than numbers, such as interval arithmetic and matrix arithmetic.\n\nArithmetic operations form the basis of many branches of mathematics, such as algebra, calculus, and statistics. They play a similar role in the sciences, like physics and economics. Arithmetic is present in many aspects of daily life, for example, to calculate change while shopping or to manage personal finances. It is one of the earliest forms of mathematics education that students encounter. Its cognitive and conceptual foundations are studied by psychology and philosophy.\n\nThe practice of arithmetic is at least thousands and possibly tens of thousands of years old. Ancient civilizations like the Egyptians and the Sumerians invented numeral systems to solve practical arithmetic problems in about 3000 BCE. Starting in the 7th and 6th centuries BCE, the ancient Greeks initiated a more abstract study of numbers and introduced the method of rigorous mathematical proofs. The ancient Indians developed the concept of zero and the decimal system, which Arab mathematicians further refined and spread to the Western world during the medieval period. The first mechanical calculators were invented in the 17th century. The 18th and 19th centuries saw the development of modern number theory and the formulation of axiomatic foundations of arithmetic. In the 20th century, the emergence of electronic calculators and computers revolutionized the accuracy and speed with which arithmetic calculations could be performed.\n\nArithmetic is the fundamental branch of mathematics that studies numbers and their operations. In particular, it deals with numerical calculations using the arithmetic operations of addition, subtraction, multiplication, and division.[1] In a wider sense, it also includes exponentiation, extraction of roots, and logarithm.[2] The term arithmetic has its root in the Latin term arithmetica which derives from the Ancient Greek words ἀριθμός (arithmos), meaning 'number', and ἀριθμητική τέχνη (arithmetike tekhne), meaning 'the art of counting'.[3]\n\nThere are disagreements about its precise definition. According to a narrow characterization, arithmetic deals only with natural numbers.[4] However, the more common view is to include operations on integers, rational numbers, real numbers, and sometimes also complex numbers in its scope.[5] Some definitions restrict arithmetic to the field of numerical calculations.[6] When understood in a wider sense, it also includes the study of how the concept of numbers developed, the analysis of properties of and relations between numbers, and the examination of the axiomatic structure of arithmetic operations.[7]\n\nArithmetic is closely related to number theory and some authors use the terms as synonyms.[8] However, in a more specific sense, number theory is restricted to the study of integers and focuses on their properties and relationships such as divisibility, factorization, and primality.[9] Traditionally, it is known as higher arithmetic.[10]\n\nNumbers are mathematical objects used to count quantities and measure magnitudes. They are fundamental elements in arithmetic since all arithmetic operations are performed on numbers. There are different kinds of numbers and different numeral systems to represent them.[11]\n\nThe main kinds of numbers employed in arithmetic are natural numbers, whole numbers, integers, rational numbers, and real numbers.[12] The natural numbers are whole numbers that start from 1 and go to infinity. They exclude 0 and negative numbers. They are also known as counting numbers and can be expressed as \n\n\n\n{\n1\n,\n2\n,\n3\n,\n4\n,\n.\n.\n.\n}\n\n\n{\\displaystyle \\{1,2,3,4,...\\}}\n\n. The symbol of the natural numbers is \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n.[a] The whole numbers are identical to the natural numbers with the only difference being that they include 0. They can be represented as \n\n\n\n{\n0\n,\n1\n,\n2\n,\n3\n,\n4\n,\n.\n.\n.\n}\n\n\n{\\displaystyle \\{0,1,2,3,4,...\\}}\n\n and have the symbol \n\n\n\n\n\nN\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbb {N} _{0}}\n\n.[14][b] Some mathematicians do not draw the distinction between the natural and the whole numbers by including 0 in the set of natural numbers.[16] The set of integers encompasses both positive and negative whole numbers. It has the symbol \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n and can be expressed as \n\n\n\n{\n.\n.\n.\n,\n−\n2\n,\n−\n1\n,\n0\n,\n1\n,\n2\n,\n.\n.\n.\n}\n\n\n{\\displaystyle \\{...,-2,-1,0,1,2,...\\}}\n\n.[17]\n\nBased on how natural and whole numbers are used, they can be distinguished into cardinal and ordinal numbers. Cardinal numbers, like one, two, and three, are numbers that express the quantity of objects. They answer the question \"how many?\". Ordinal numbers, such as first, second, and third, indicate order or placement in a series. They answer the question \"what position?\".[18]\n\nA number is rational if it can be represented as the ratio of two integers. For instance, the rational number \n\n\n\n\n\n\n1\n2\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}}\n\n is formed by dividing the integer 1, called the numerator, by the integer 2, called the denominator. Other examples are \n\n\n\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{4}}}\n\n and \n\n\n\n\n\n\n281\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {281}{3}}}\n\n. The set of rational numbers includes all integers, which are fractions with a denominator of 1. The symbol of the rational numbers is \n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n.[19] Decimal fractions like 0.3 and 25.12 are a special type of rational numbers since their denominator is a power of 10. For instance, 0.3 is equal to \n\n\n\n\n\n\n3\n10\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{10}}}\n\n, and 25.12 is equal to \n\n\n\n\n\n\n2512\n100\n\n\n\n\n\n{\\displaystyle {\\tfrac {2512}{100}}}\n\n.[20] Every rational number corresponds to a finite or a repeating decimal.[21][c]\n\nIrrational numbers are numbers that cannot be expressed through the ratio of two integers. They are often required to describe geometric magnitudes. For example, if a right triangle has legs of the length 1 then the length of its hypotenuse is given by the irrational number \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n. π is another irrational number and describes the ratio of a circle's circumference to its diameter.[22] The decimal representation of an irrational number is infinite without repeating decimals.[23] The set of rational numbers together with the set of irrational numbers makes up the set of real numbers. The symbol of the real numbers is \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n.[24] Even wider classes of numbers include complex numbers and quaternions.[25]\n\nA numeral is a symbol to represent a number and numeral systems are representational frameworks.[26] They usually have a limited amount of basic numerals, which directly refer to certain numbers. The system governs how these basic numerals may be combined to express any number.[27] Numeral systems are either positional or non-positional. All early numeral systems were non-positional.[28] For non-positional numeral systems, the value of a digit does not depend on its position in the numeral.[29]\n\nThe simplest non-positional system is the unary numeral system. It relies on one symbol for the number 1. All higher numbers are written by repeating this symbol. For example, the number 7 can be represented by repeating the symbol for 1 seven times. This system makes it cumbersome to write large numbers, which is why many non-positional systems include additional symbols to directly represent larger numbers.[30] Variations of the unary numeral systems are employed in tally sticks using dents and in tally marks.[31]\n\nEgyptian hieroglyphics had a more complex non-positional numeral system. They have additional symbols for numbers like 10, 100, 1000, and 10,000. These symbols can be combined into a sum to more conveniently express larger numbers. For instance, the numeral for 10,405 uses one time the symbol for 10,000, four times the symbol for 100, and five times the symbol for 1. A similar well-known framework is the Roman numeral system. It has the symbols I, V, X, L, C, D, M as its basic numerals to represent the numbers 1, 5, 10, 50, 100, 500, and 1000.[33]\n\nA numeral system is positional if the position of a basic numeral in a compound expression determines its value. Positional numeral systems have a radix that acts as a multiplicand of the different positions. For each subsequent position, the radix is raised to a higher power. In the common decimal system, also called the Hindu–Arabic numeral system, the radix is 10. This means that the first digit is multiplied by \n\n\n\n\n10\n\n0\n\n\n\n\n{\\displaystyle 10^{0}}\n\n, the next digit is multiplied by \n\n\n\n\n10\n\n1\n\n\n\n\n{\\displaystyle 10^{1}}\n\n, and so on. For example, the decimal numeral 532 stands for \n\n\n\n5\n⋅\n\n10\n\n2\n\n\n+\n3\n⋅\n\n10\n\n1\n\n\n+\n2\n⋅\n\n10\n\n0\n\n\n\n\n{\\displaystyle 5\\cdot 10^{2}+3\\cdot 10^{1}+2\\cdot 10^{0}}\n\n. Because of the effect of the digits' positions, the numeral 532 differs from the numerals 325 and 253 even though they have the same digits.[34]\n\nAnother positional numeral system used extensively in computer arithmetic is the binary system, which has a radix of 2. This means that the first digit is multiplied by \n\n\n\n\n2\n\n0\n\n\n\n\n{\\displaystyle 2^{0}}\n\n, the next digit by \n\n\n\n\n2\n\n1\n\n\n\n\n{\\displaystyle 2^{1}}\n\n, and so on. For example, the number 13 is written as 1101 in the binary notation, which stands for \n\n\n\n1\n⋅\n\n2\n\n3\n\n\n+\n1\n⋅\n\n2\n\n2\n\n\n+\n0\n⋅\n\n2\n\n1\n\n\n+\n1\n⋅\n\n2\n\n0\n\n\n\n\n{\\displaystyle 1\\cdot 2^{3}+1\\cdot 2^{2}+0\\cdot 2^{1}+1\\cdot 2^{0}}\n\n. In computing, each digit in the binary notation corresponds to one bit.[35] The earliest positional system was developed by ancient Babylonians and had a radix of 60.[36]\n\nArithmetic operations are ways of combining, transforming, or manipulating numbers. They are functions that have numbers both as input and output.[37] The most important operations in arithmetic are addition, subtraction, multiplication, and division.[38] Further operations include exponentiation, extraction of roots, and logarithm.[39] If these operations are performed on variables rather than numbers, they are sometimes referred to as algebraic operations.[40]\n\nTwo important concepts in relation to arithmetic operations are identity elements and inverse elements. The identity element or neutral element of an operation does not cause any change if it is applied to another element. For example, the identity element of addition is 0 since any sum of a number and 0 results in the same number. The inverse element is the element that results in the identity element when combined with another element. For instance, the additive inverse of the number 6 is -6 since their sum is 0.[41]\n\nThere are not only inverse elements but also inverse operations. In an informal sense, one operation is the inverse of another operation if it undoes the first operation. For example, subtraction is the inverse of addition since a number returns to its original value if a second number is first added and subsequently subtracted, as in \n\n\n\n13\n+\n4\n−\n4\n=\n13\n\n\n{\\displaystyle 13+4-4=13}\n\n. Defined more formally, the operation \"\n\n\n\n⋆\n\n\n{\\displaystyle \\star }\n\n\" is an inverse of the operation \"\n\n\n\n∘\n\n\n{\\displaystyle \\circ }\n\n\" if it fulfills the following condition: \n\n\n\nt\n⋆\ns\n=\nr\n\n\n{\\displaystyle t\\star s=r}\n\n if and only if \n\n\n\nr\n∘\ns\n=\nt\n\n\n{\\displaystyle r\\circ s=t}\n\n.[42]\n\nCommutativity and associativity are laws governing the order in which some arithmetic operations can be carried out. An operation is commutative if the order of the arguments can be changed without affecting the results. This is the case for addition, for instance, \n\n\n\n7\n+\n9\n\n\n{\\displaystyle 7+9}\n\n is the same as \n\n\n\n9\n+\n7\n\n\n{\\displaystyle 9+7}\n\n. Associativity is a rule that affects the order in which a series of operations can be carried out. An operation is associative if, in a series of two operations, it does not matter which operation is carried out first. This is the case for multiplication, for example, since \n\n\n\n(\n5\n×\n4\n)\n×\n2\n\n\n{\\displaystyle (5\\times 4)\\times 2}\n\n is the same as \n\n\n\n5\n×\n(\n4\n×\n2\n)\n\n\n{\\displaystyle 5\\times (4\\times 2)}\n\n.[43]\n\nAddition is an arithmetic operation in which two numbers, called the addends, are combined into a single number, called the sum. The symbol of addition is \n\n\n\n+\n\n\n{\\displaystyle +}\n\n. Examples are \n\n\n\n2\n+\n2\n=\n4\n\n\n{\\displaystyle 2+2=4}\n\n and \n\n\n\n6.3\n+\n1.26\n=\n7.56\n\n\n{\\displaystyle 6.3+1.26=7.56}\n\n.[44] The term summation is used if several additions are performed in a row.[45] Counting is a type of repeated addition in which the number 1 is continuously added.[46]\n\nSubtraction is the inverse of addition. In it, one number, known as the subtrahend, is taken away from another, known as the minuend. The result of this operation is called the difference. The symbol of subtraction is \n\n\n\n−\n\n\n{\\displaystyle -}\n\n.[47] Examples are \n\n\n\n14\n−\n8\n=\n6\n\n\n{\\displaystyle 14-8=6}\n\n and \n\n\n\n45\n−\n1.7\n=\n43.3\n\n\n{\\displaystyle 45-1.7=43.3}\n\n. Subtraction is often treated as a special case of addition: instead of subtracting a positive number, it is also possible to add a negative number. For instance \n\n\n\n14\n−\n8\n=\n14\n+\n(\n−\n8\n)\n\n\n{\\displaystyle 14-8=14+(-8)}\n\n. This helps to simplify mathematical computations by reducing the number of basic arithmetic operations needed to perform calculations.[48]\n\nThe additive identity element is 0 and the additive inverse of a number is the negative of that number. For instance, \n\n\n\n13\n+\n0\n=\n13\n\n\n{\\displaystyle 13+0=13}\n\n and \n\n\n\n13\n+\n(\n−\n13\n)\n=\n0\n\n\n{\\displaystyle 13+(-13)=0}\n\n. Addition is both commutative and associative.[49]\n\nMultiplication  is an arithmetic operation in which two numbers, called the multiplier and the multiplicand, are combined into a single number called the product.[50][d] The symbols of multiplication are \n\n\n\n×\n\n\n{\\displaystyle \\times }\n\n, \n\n\n\n⋅\n\n\n{\\displaystyle \\cdot }\n\n, and *. Examples are \n\n\n\n2\n×\n3\n=\n6\n\n\n{\\displaystyle 2\\times 3=6}\n\n and \n\n\n\n0.3\n⋅\n5\n=\n1.5\n\n\n{\\displaystyle 0.3\\cdot 5=1.5}\n\n. If the multiplicand is a natural number then multiplication is the same as repeated addition, as in \n\n\n\n2\n×\n3\n=\n2\n+\n2\n+\n2\n\n\n{\\displaystyle 2\\times 3=2+2+2}\n\n.[52]\n\nDivision is the inverse of multiplication. In it, one number, known as the dividend, is split into several equal parts by another number, known as the divisor. The result of this operation is called the quotient. The symbols of division are \n\n\n\n÷\n\n\n{\\displaystyle \\div }\n\n and \n\n\n\n\n/\n\n\n\n{\\displaystyle /}\n\n. Examples are \n\n\n\n48\n÷\n8\n=\n6\n\n\n{\\displaystyle 48\\div 8=6}\n\n and \n\n\n\n29.4\n\n/\n\n1.4\n=\n21\n\n\n{\\displaystyle 29.4/1.4=21}\n\n.[53] Division is often treated as a special case of multiplication: instead of dividing by a number, it is also possible to multiply by its reciprocal. The reciprocal of a number is 1 divided by that number. For instance, \n\n\n\n48\n÷\n8\n=\n48\n×\n\n\n\n1\n8\n\n\n\n\n\n{\\displaystyle 48\\div 8=48\\times {\\tfrac {1}{8}}}\n\n.[54]\n\nThe multiplicative identity element is 1 and the multiplicative inverse of a number is the reciprocal of that number. For example, \n\n\n\n13\n×\n1\n=\n13\n\n\n{\\displaystyle 13\\times 1=13}\n\n and \n\n\n\n13\n×\n\n\n\n1\n13\n\n\n\n=\n1\n\n\n{\\displaystyle 13\\times {\\tfrac {1}{13}}=1}\n\n. Multiplication is both commutative and associative.[55]\n\nExponentiation is an arithmetic operation in which a number, known as the base, is raised to the power of another number, known as the exponent. The result of this operation is called the power. Exponentiation is sometimes expressed using the symbol ^ but the more common way is to write the exponent in superscript right after the base. Examples are \n\n\n\n\n2\n\n4\n\n\n=\n16\n\n\n{\\displaystyle 2^{4}=16}\n\n and \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n^\n\n\n\n3\n=\n27\n\n\n{\\displaystyle 3=27}\n\n. If the exponent is a natural number then exponentiation is the same as repeated multiplication, as in \n\n\n\n\n2\n\n4\n\n\n=\n2\n×\n2\n×\n2\n×\n2\n\n\n{\\displaystyle 2^{4}=2\\times 2\\times 2\\times 2}\n\n.[56][e]\n\nRoots are a special type of exponentiation using a fractional exponent. For example, the square root of a number is the same as raising the number to the power of \n\n\n\n\n\n\n1\n2\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}}\n\n and the cube root of a number is the same as raising the number to the power of \n\n\n\n\n\n\n1\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{3}}}\n\n. Examples are \n\n\n\n\n\n4\n\n\n=\n\n4\n\n\n1\n2\n\n\n\n=\n2\n\n\n{\\displaystyle {\\sqrt {4}}=4^{\\frac {1}{2}}=2}\n\n and \n\n\n\n\n\n27\n\n3\n\n\n\n=\n\n27\n\n\n1\n3\n\n\n\n=\n3\n\n\n{\\displaystyle {\\sqrt[{3}]{27}}=27^{\\frac {1}{3}}=3}\n\n.[58]\n\nLogarithm is the inverse of exponentiation. The logarithm of a number \n\n\n\nx\n\n\n{\\displaystyle x}\n\n to the base \n\n\n\nb\n\n\n{\\displaystyle b}\n\n is the exponent to which \n\n\n\nb\n\n\n{\\displaystyle b}\n\n must be raised to produce \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. For instance, since \n\n\n\n1000\n=\n\n10\n\n3\n\n\n\n\n{\\displaystyle 1000=10^{3}}\n\n, the logarithm base 10 of 1000 is 3. The logarithm of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n to base \n\n\n\nb\n\n\n{\\displaystyle b}\n\n is denoted as \n\n\n\n\nlog\n\nb\n\n\n⁡\n(\nx\n)\n\n\n{\\displaystyle \\log _{b}(x)}\n\n, or without parentheses, \n\n\n\n\nlog\n\nb\n\n\n⁡\nx\n\n\n{\\displaystyle \\log _{b}x}\n\n, or even without the explicit base, \n\n\n\nlog\n⁡\nx\n\n\n{\\displaystyle \\log x}\n\n, when the base can be understood from context. So, the previous example can be written \n\n\n\n\nlog\n\n10\n\n\n⁡\n1000\n=\n3\n\n\n{\\displaystyle \\log _{10}1000=3}\n\n.[59]\n\nExponentiation and logarithm do not have general identity elements and inverse elements like addition and multiplication. The neutral element of exponentiation in relation to the exponent is 1, as in \n\n\n\n\n14\n\n1\n\n\n=\n14\n\n\n{\\displaystyle 14^{1}=14}\n\n. However, exponentiation does not have a general identity element since 1 is not the neutral element for the base.[60] Exponentiation and logarithm are neither commutative nor associative.[61]\n\nDifferent types of arithmetic systems are discussed in the academic literature. They differ from each other based on what type of number they operate on, what numeral system they use to represent them, and whether they operate on mathematical objects other than numbers.[62]\n\nInteger arithmetic is the branch of arithmetic that deals with the manipulation of positive and negative whole numbers.[63] Simple one-digit operations can be performed by following or memorizing a table that presents the results of all possible combinations, like an addition table or a multiplication table. Other common methods are verbal counting and finger-counting.[64]\n\nFor operations on numbers with more than one digit, different techniques can be employed to calculate the result by using several one-digit operations in a row. For example, in the method addition with carries, the two numbers are written one above the other. Starting from the rightmost digit, each pair of digits is added together. The rightmost digit of the sum is written below them. If the sum is a two-digit number then the leftmost digit, called the \"carry\", is added to the next pair of digits to the left. This process is repeated until all digits have been added.[65] Other methods used for integer additions are the number line method, the partial sum method, and the compensation method.[66] A similar technique is utilized for subtraction: it also starts with the rightmost digit and uses a \"borrow\" or a negative carry for the column on the left if the result of the one-digit subtraction is negative.[67]\n\nA basic technique of integer multiplication employs repeated addition. For example, the product of \n\n\n\n3\n×\n4\n\n\n{\\displaystyle 3\\times 4}\n\n can be calculated as \n\n\n\n3\n+\n3\n+\n3\n+\n3\n\n\n{\\displaystyle 3+3+3+3}\n\n.[68] A common technique for multiplication with larger numbers is called long multiplication. This method starts by writing the multiplier above the multiplicand. The calculation begins by multiplying the multiplier only with the rightmost digit of the multiplicand and writing the result below, starting in the rightmost column. The same is done for each digit of the multiplicand and the result in each case is shifted one position to the left. As a final step, all the individual products are added to arrive at the total product of the two multi-digit numbers.[69] Other techniques used for multiplication are the grid method and the lattice method.[70] Computer science is interested in multiplication algorithms with a low computational complexity to be able to efficiently multiply very large integers, such as the Karatsuba algorithm, the Schönhage–Strassen algorithm, and the Toom–Cook algorithm.[71] A common technique used for division is called long division. Other methods include short division and chunking.[72]\n\nInteger arithmetic is not closed under division. This means that when dividing one integer by another integer, the result is not always an integer. For instance, 7 divided by 2 is not a whole number but 3.5.[73] One way to ensure that the result is an integer is to round the result to a whole number. However, this method leads to inaccuracies as the original value is altered.[74] Another method is to perform the division only partially and retain the remainder. For example, 7 divided by 2 is 3 with a remainder of 1. These difficulties are avoided by rational number arithmetic, which allows for the exact representation of fractions.[75]\n\nA simple method to calculate exponentiation is by repeated multiplication. For instance, the exponentiation of \n\n\n\n\n3\n\n4\n\n\n\n\n{\\displaystyle 3^{4}}\n\n can be calculated as \n\n\n\n3\n×\n3\n×\n3\n×\n3\n\n\n{\\displaystyle 3\\times 3\\times 3\\times 3}\n\n.[76] A more efficient technique used for large exponents is exponentiation by squaring. It breaks down the calculation into a number of squaring operations. For example, the exponentiation \n\n\n\n\n3\n\n65\n\n\n\n\n{\\displaystyle 3^{65}}\n\n can be written as \n\n\n\n(\n(\n(\n(\n(\n\n3\n\n2\n\n\n\n)\n\n2\n\n\n\n)\n\n2\n\n\n\n)\n\n2\n\n\n\n)\n\n2\n\n\n\n)\n\n2\n\n\n×\n3\n\n\n{\\displaystyle (((((3^{2})^{2})^{2})^{2})^{2})^{2}\\times 3}\n\n. By taking advantage of repeated squaring operations, only 7 individual operations are needed rather than the 64 operations required for regular repeated multiplication.[77] Methods to calculate logarithms include the Taylor series and continued fractions.[78] Integer arithmetic is not closed under logarithm and under exponentiation with negative exponents, meaning that the result of these operations is not always an integer.[79]\n\nNumber theory studies the structure and properties of integers as well as the relations and laws between them.[80] Some of the main branches of modern number theory include elementary number theory, analytic number theory, algebraic number theory, and geometric number theory.[81] Elementary number theory studies aspects of integers that can be investigated using elementary methods. Its topics include divisibility, factorization, and primality.[82] Analytic number theory, by contrast, relies on techniques from analysis and calculus. It examines problems like how prime numbers are distributed and the claim that every even number is a sum of two prime numbers.[83] Algebraic number theory employs algebraic structures to analyze the properties of and relations between numbers. Examples are the use of fields and rings, as in algebraic number fields like the ring of integers. Geometric number theory uses concepts from geometry to study numbers. For instance, it investigates how lattice points with integer coordinates behave in a plane.[84] Further branches of number theory are probabilistic number theory, which employs methods from probability theory,[85] combinatorial number theory, which relies on the field of combinatorics,[86] computational number theory, which approaches number-theoretic problems with computational methods,[87] and applied number theory, which examines the application of number theory to fields like physics, biology, and cryptography.[88]\n\nInfluential theorems in number theory include the fundamental theorem of arithmetic, Euclid's theorem, and Fermat's Last Theorem.[89] According to the fundamental theorem of arithmetic, every integer greater than 1 is either a prime number or can be represented as a unique product of prime numbers. For example, the number 18 is not a prime number and can be represented as \n\n\n\n2\n×\n3\n×\n3\n\n\n{\\displaystyle 2\\times 3\\times 3}\n\n, all of which are prime numbers. The number 19, by contrast, is a prime number that has no other prime factorization.[90] Euclid's theorem states that there are infinitely many prime numbers.[91] Fermat's Last Theorem is the statement that no positive integer values exist for \n\n\n\na\n\n\n{\\displaystyle a}\n\n, \n\n\n\nb\n\n\n{\\displaystyle b}\n\n, and \n\n\n\nc\n\n\n{\\displaystyle c}\n\n that solve the equation \n\n\n\n\na\n\nn\n\n\n+\n\nb\n\nn\n\n\n=\n\nc\n\nn\n\n\n\n\n{\\displaystyle a^{n}+b^{n}=c^{n}}\n\n if \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is greater than \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n.[92]\n\nRational number arithmetic is the branch of arithmetic that deals with the manipulation of numbers that can be expressed as a ratio of two integers.[93] Most arithmetic operations on rational numbers can be calculated by performing a series of integer arithmetic operations on the numerators and the denominators of the involved numbers. If two rational numbers have the same denominator then they can be added by adding their numerators and keeping the common denominator. For example, \n\n\n\n\n\n\n2\n7\n\n\n\n+\n\n\n\n3\n7\n\n\n\n=\n\n\n\n5\n7\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{7}}+{\\tfrac {3}{7}}={\\tfrac {5}{7}}}\n\n. A similar procedure is used for subtraction. If the two numbers do not have the same denominator then they must be transformed to find a common denominator. This can be achieved by scaling the first number with the denominator of the second number while scaling the second number with the denominator of the first number. For instance, \n\n\n\n\n\n\n1\n3\n\n\n\n+\n\n\n\n1\n2\n\n\n\n=\n\n\n\n\n1\n⋅\n2\n\n\n3\n⋅\n2\n\n\n\n\n+\n\n\n\n\n1\n⋅\n3\n\n\n2\n⋅\n3\n\n\n\n\n=\n\n\n\n2\n6\n\n\n\n+\n\n\n\n3\n6\n\n\n\n=\n\n\n\n5\n6\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{3}}+{\\tfrac {1}{2}}={\\tfrac {1\\cdot 2}{3\\cdot 2}}+{\\tfrac {1\\cdot 3}{2\\cdot 3}}={\\tfrac {2}{6}}+{\\tfrac {3}{6}}={\\tfrac {5}{6}}}\n\n.[94]\n\nTwo rational numbers are multiplied by multiplying their numerators and their denominators respectively, as in \n\n\n\n\n\n\n2\n3\n\n\n\n⋅\n\n\n\n2\n5\n\n\n\n=\n\n\n\n\n2\n⋅\n2\n\n\n3\n⋅\n5\n\n\n\n\n=\n\n\n\n4\n15\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{3}}\\cdot {\\tfrac {2}{5}}={\\tfrac {2\\cdot 2}{3\\cdot 5}}={\\tfrac {4}{15}}}\n\n. Dividing one rational number by another can be achieved by multiplying the first number with the reciprocal of the second number. This means that the numerator and the denominator of the second number change position. For example, \n\n\n\n\n\n\n3\n5\n\n\n\n:\n\n\n\n2\n7\n\n\n\n=\n\n\n\n3\n5\n\n\n\n⋅\n\n\n\n7\n2\n\n\n\n=\n\n\n\n21\n10\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{5}}:{\\tfrac {2}{7}}={\\tfrac {3}{5}}\\cdot {\\tfrac {7}{2}}={\\tfrac {21}{10}}}\n\n.[95] Unlike integer arithmetic, rational number arithmetic is closed under division as long as the divisor is not 0.[96]\n\nBoth integer arithmetic and rational number arithmetic are not closed under exponentiation and logarithm.[97] One way to calculate exponentiation with a fractional exponent is to perform two separate calculations: one exponentiation using the numerator of the exponent followed by drawing the nth root of the result based on the denominator of the exponent. For example, \n\n\n\n\n5\n\n\n2\n3\n\n\n\n=\n\n\n\n5\n\n2\n\n\n\n3\n\n\n\n\n\n{\\displaystyle 5^{\\frac {2}{3}}={\\sqrt[{3}]{5^{2}}}}\n\n. The first operation can be completed using methods like repeated multiplication or exponentiation by squaring. One way to get an approximate result for the second operation is to employ Newton's method, which uses a series of steps to gradually refine an initial guess until it reaches the desired level of accuracy.[98] The Taylor series or the continued fraction method can be utilized to calculate logarithms.[99]\n\nThe decimal fraction notation is a special way of representing rational numbers whose denominator is a power of 10. For instance, the rational numbers \n\n\n\n\n\n\n1\n10\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{10}}}\n\n, \n\n\n\n\n\n\n371\n100\n\n\n\n\n\n{\\displaystyle {\\tfrac {371}{100}}}\n\n, and \n\n\n\n\n\n\n44\n10000\n\n\n\n\n\n{\\displaystyle {\\tfrac {44}{10000}}}\n\n are written as 0.1, 3.71, and 0.0044 in the decimal fraction notation.[100] Modified versions of integer calculation methods like addition with carry and long multiplication can be applied to calculations with decimal fractions.[101] Not all rational numbers have a finite representation in the decimal notation. For example, the rational number \n\n\n\n\n\n\n1\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{3}}}\n\n corresponds to 0.333... with an infinite number of 3s. The shortened notation for this type of repeating decimal is 0.3.[102] Every repeating decimal expresses a rational number.[103]\n\nReal number arithmetic is the branch of arithmetic that deals with the manipulation of both rational and irrational numbers. Irrational numbers are numbers that cannot be expressed through fractions or repeated decimals, like the root of 2 and π.[104] Unlike rational number arithmetic, real number arithmetic is closed under exponentiation as long as it uses a positive number as its base. The same is true for the logarithm of positive real numbers as long as the logarithm base is positive and not 1.[105]\n\nIrrational numbers involve an infinite non-repeating series of decimal digits. Because of this, there is often no simple and accurate way to express the results of arithmetic operations like \n\n\n\n\n\n2\n\n\n+\nπ\n\n\n{\\displaystyle {\\sqrt {2}}+\\pi }\n\n or \n\n\n\ne\n⋅\n\n\n3\n\n\n\n\n{\\displaystyle e\\cdot {\\sqrt {3}}}\n\n.[106] In cases where absolute precision is not required, the problem of calculating arithmetic operations on real numbers is usually addressed by truncation or rounding. For truncation, a certain number of leftmost digits are kept and remaining digits are discarded or replaced by zeros. For example, the number π has an infinite number of digits starting with 3.14159.... If this number is truncated to 4 decimal places, the result is 3.141. Rounding is a similar process in which the last preserved digit is increased by one if the next digit is 5 or greater but remains the same if the next digit is less than 5, so that the rounded number is the best approximation of a given precision for the original number. For instance, if the number π is rounded to 4 decimal places, the result is 3.142 because the following digit is a 5, so 3.142 is closer to π than 3.141.[107] These methods allow computers to efficiently perform approximate calculations on real numbers.[108]\n\nIn science and engineering, numbers represent estimates of physical quantities derived from measurement or modeling. Unlike mathematically exact numbers such as π or ⁠\n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n⁠, scientifically relevant numerical data are inherently inexact, involving some measurement uncertainty.[109] One basic way to express the degree of certainty about each number's value and avoid false precision is to round each measurement to a certain number of digits, called significant digits, which are implied to be accurate. For example, a person's height measured with a tape measure might only be precisely known to the nearest centimeter, so should be presented as 1.62 meters rather than 1.6217 meters. If converted to imperial units, this quantity should be rounded to 64 inches or 63.8 inches rather than 63.7795 inches, to clearly convey the precision of the measurement. When a number is written using ordinary decimal notation, leading zeros are not significant, and trailing zeros of numbers not written with a decimal point are implicitly considered to be non-significant.[110] For example, the numbers 0.056 and 1200 each have only 2 significant digits, but the number 40.00 has 4 significant digits. Representing uncertainty using only significant digits is a relatively crude method, with some unintuitive subtleties; explicitly keeping track of an estimate or upper bound of the approximation error is a more sophisticated approach.[111] In the example, the person's height might be represented as 1.62 ± 0.005 meters or 63.8 ± 0.2 inches.[112]\n\nIn performing calculations with uncertain quantities, the uncertainty should be propagated to calculated quantities. When adding or subtracting two or more quantities, add the absolute uncertainties of each summand together to obtain the absolute uncertainty of the sum. When multiplying or dividing two or more quantities, add the relative uncertainties of each factor together to obtain the relative uncertainty of the product.[113] When representing uncertainty by significant digits, uncertainty can be coarsely propagated by rounding the result of adding or subtracting two or more quantities to the leftmost last significant decimal place among the summands, and by rounding the result of multiplying or dividing two or more quantities to the least number of significant digits among the factors.[114] (See Significant figures § Arithmetic.)\n\nMore sophisticated methods of dealing with uncertain values include interval arithmetic and affine arithmetic. Interval arithmetic describes operations on intervals. Intervals can be used to represent a range of values if one does not know the precise magnitude, for example, because of measurement errors. Interval arithmetic includes operations like addition and multiplication on intervals, as in \n\n\n\n[\n1\n,\n2\n]\n+\n[\n3\n,\n4\n]\n=\n[\n4\n,\n6\n]\n\n\n{\\displaystyle [1,2]+[3,4]=[4,6]}\n\n and \n\n\n\n[\n1\n,\n2\n]\n×\n[\n3\n,\n4\n]\n=\n[\n3\n,\n8\n]\n\n\n{\\displaystyle [1,2]\\times [3,4]=[3,8]}\n\n.[115] It is closely related to affine arithmetic, which aims to give more precise results by performing calculations on affine forms rather than intervals. An affine form is a number together with error terms that describe how the number may deviate from the actual magnitude.[116]\n\nThe precision of numerical quantities can be expressed uniformly using normalized scientific notation, which is also convenient for concisely representing numbers which are much larger or smaller than 1. Using scientific notation, a number is decomposed into the product of a number between 1 and 10, called the significand, and 10 raised to some integer power, called the exponent. The significand consists of the significant digits of the number, and is written as a leading digit 1–9 followed by a decimal point and a sequence of digits 0–9. For example, the normalized scientific notation of the number 8276000 is \n\n\n\n8.276\n×\n\n10\n\n6\n\n\n\n\n{\\displaystyle 8.276\\times 10^{6}}\n\n with significand 8.276 and exponent 6, and the normalized scientific notation of the number 0.00735 is \n\n\n\n7.35\n×\n\n10\n\n−\n3\n\n\n\n\n{\\displaystyle 7.35\\times 10^{-3}}\n\n with significand 7.35 and exponent −3.[117] Unlike ordinary decimal notation, where trailing zeros of large numbers are implicitly considered to be non-significant, in scientific notation every digit in the significand is considered significant, and adding trailing zeros indicates higher precision. For example, while the number 1200 implicitly has only 2 significant digits, the number ⁠\n\n\n\n1.20\n×\n\n10\n\n3\n\n\n\n\n{\\displaystyle 1.20\\times 10^{3}}\n\n⁠ explicitly has 3.[118]\n\nA common method employed by computers to approximate real number arithmetic is called floating-point arithmetic. It represents real numbers similar to the scientific notation through three numbers: a significand, a base, and an exponent.[119] The precision of the significand is limited by the number of bits allocated to represent it. If an arithmetic operation results in a number that requires more bits than are available, the computer rounds the result to the closest representable number. This leads to rounding errors.[120] A consequence of this behavior is that certain laws of arithmetic are violated by floating-point arithmetic. For example, floating-point addition is not associative since the rounding errors introduced can depend on the order of the additions. This means that the result of \n\n\n\n(\na\n+\nb\n)\n+\nc\n\n\n{\\displaystyle (a+b)+c}\n\n is sometimes different from the result of \n\n\n\na\n+\n(\nb\n+\nc\n)\n\n\n{\\displaystyle a+(b+c)}\n\n.[121] The most common technical standard used for floating-point arithmetic is called IEEE 754. Among other things, it determines how numbers are represented, how arithmetic operations and rounding are performed, and how errors and exceptions are handled.[122] In cases where computation speed is not a limiting factor, it is possible to use arbitrary-precision arithmetic, for which the precision of calculations is only restricted by the computer's memory.[123]\n\nForms of arithmetic can also be distinguished by the tools employed to perform calculations and include many approaches besides the regular use of pen and paper. Mental arithmetic relies exclusively on the mind without external tools. Instead, it utilizes visualization, memorization, and certain calculation techniques to solve arithmetic problems.[124] One such technique is the compensation method, which consists in altering the numbers to make the calculation easier and then adjusting the result afterward. For example, instead of calculating \n\n\n\n85\n−\n47\n\n\n{\\displaystyle 85-47}\n\n, one calculates \n\n\n\n85\n−\n50\n\n\n{\\displaystyle 85-50}\n\n which is easier because it uses a round number. In the next step, one adds \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n to the result to compensate for the earlier adjustment.[125] Mental arithmetic is often taught in primary education to train the numerical abilities of the students.[126]\n\nThe human body can also be employed as an arithmetic tool. The use of hands in finger counting is often introduced to young children to teach them numbers and simple calculations. In its most basic form, the number of extended fingers corresponds to the represented quantity and arithmetic operations like addition and subtraction are performed by extending or retracting fingers. This system is limited to small numbers compared to more advanced systems which employ different approaches to represent larger quantities.[127] The human voice is used as an arithmetic aid in verbal counting.[128]\n\nTally marks are a simple system based on external tools other than the body. This system relies on mark making, such as strokes drawn on a surface or notches carved into a wooden stick, to keep track of quantities. Some forms of tally marks arrange the strokes in groups of five to make them easier to read.[129]\n\nThe abacus is a more advanced tool to represent numbers and perform calculations. An abacus usually consists of a series of rods, each holding several beads. Each bead represents a quantity, which is counted if the bead is moved from one end of a rod to the other. Calculations happen by manipulating the positions of beads until the final bead pattern reveals the result.[130] Related aids include counting boards, which use tokens whose value depends on the area on the board in which they are placed,[131] and counting rods, which are arranged in horizontal and vertical patterns to represent different numbers.[132][f]\n\nSectors and slide rules are more refined calculating instruments that rely on geometric relationships between different scales to perform both basic and advanced arithmetic operations.[134][g] Printed tables were particularly relevant as an aid to look up the results of operations like logarithm and trigonometric functions.[136]\n\nMechanical calculators automate manual calculation processes. They present the user with some form of input device to enter numbers by turning dials or pressing keys. They include an internal mechanism usually consisting of gears, levers, and wheels to perform calculations and display the results.[137] For electronic calculators and computers, this procedure is further refined by replacing the mechanical components with electronic circuits like microprocessors that combine and transform electric signals to perform calculations.[138]\n\nThere are many other types of arithmetic. Modular arithmetic operates on a finite set of numbers. If an operation would result in a number outside this finite set then the number is adjusted back into the set, similar to how the hands of clocks start at the beginning again after having completed one cycle. The number at which this adjustment happens is called the modulus. For example, a regular clock has a modulus of 12. In the case of adding 4 to 9, this means that the result is not 13 but 1. The same principle applies also to other operations, such as subtraction, multiplication, and division.[139]\n\nSome forms of arithmetic deal with operations performed on mathematical objects other than numbers. Interval arithmetic describes operations on intervals.[140] Vector arithmetic and matrix arithmetic describe arithmetic operations on vectors and matrices, like vector addition and matrix multiplication.[141]\n\nArithmetic systems can be classified based on the numeral system they rely on. For instance, decimal arithmetic describes arithmetic operations in the decimal system. Other examples are binary arithmetic, octal arithmetic, and hexadecimal arithmetic.[142]\n\nCompound unit arithmetic describes arithmetic operations performed on magnitudes with compound units. It involves additional operations to govern the transformation between single unit and compound unit quantities. For example, the operation of reduction is used to transform the compound quantity 1 h 90 min into the single unit quantity 150 min.[143]\n\nNon-Diophantine arithmetics are arithmetic systems that violate traditional arithmetic intuitions and include equations like \n\n\n\n1\n+\n1\n=\n1\n\n\n{\\displaystyle 1+1=1}\n\n and \n\n\n\n2\n+\n2\n=\n5\n\n\n{\\displaystyle 2+2=5}\n\n.[144] They can be employed to represent some real-world situations in modern physics and everyday life. For instance, the equation \n\n\n\n1\n+\n1\n=\n1\n\n\n{\\displaystyle 1+1=1}\n\n can be used to describe the observation that if one raindrop is added to another raindrop then they do not remain two separate entities but become one.[145]\n\nAxiomatic foundations of arithmetic try to provide a small set of laws, called axioms, from which all fundamental properties of and operations on numbers can be derived. They constitute logically consistent and systematic frameworks that can be used to formulate mathematical proofs in a rigorous manner. Two well-known approaches are the Dedekind–Peano axioms and set-theoretic constructions.[146]\n\nThe Dedekind–Peano axioms provide an axiomatization of the arithmetic of natural numbers. Their basic principles were first formulated by Richard Dedekind and later refined by Giuseppe Peano. They rely only on a small number of primitive mathematical concepts, such as 0, natural number, and successor.[h] The Peano axioms determine how these concepts are related to each other. All other arithmetic concepts can then be defined in terms of these primitive concepts.[147]\n\nNumbers greater than 0 are expressed by repeated application of the successor function \n\n\n\ns\n\n\n{\\displaystyle s}\n\n. For example, \n\n\n\n1\n\n\n{\\displaystyle 1}\n\n is \n\n\n\ns\n(\n0\n)\n\n\n{\\displaystyle s(0)}\n\n and \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n is \n\n\n\ns\n(\ns\n(\ns\n(\n0\n)\n)\n)\n\n\n{\\displaystyle s(s(s(0)))}\n\n. Arithmetic operations can be defined as mechanisms that affect how the successor function is applied. For instance, to add \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n to any number is the same as applying the successor function two times to this number.[150]\n\nVarious axiomatizations of arithmetic rely on set theory. They cover natural numbers but can also be extended to integers, rational numbers, and real numbers. Each natural number is represented by a unique set. 0 is usually defined as the empty set \n\n\n\n∅\n\n\n{\\displaystyle \\varnothing }\n\n. Each subsequent number can be defined as the union of the previous number with the set containing the previous number. For example, \n\n\n\n1\n=\n0\n∪\n{\n0\n}\n=\n{\n0\n}\n\n\n{\\displaystyle 1=0\\cup \\{0\\}=\\{0\\}}\n\n, \n\n\n\n2\n=\n1\n∪\n{\n1\n}\n=\n{\n0\n,\n1\n}\n\n\n{\\displaystyle 2=1\\cup \\{1\\}=\\{0,1\\}}\n\n, and \n\n\n\n3\n=\n2\n∪\n{\n2\n}\n=\n{\n0\n,\n1\n,\n2\n}\n\n\n{\\displaystyle 3=2\\cup \\{2\\}=\\{0,1,2\\}}\n\n.[151] Integers can be defined as ordered pairs of natural numbers where the second number is subtracted from the first one. For instance, the pair (9, 0) represents the number 9 while the pair (0, 9) represents the number -9.[152] Rational numbers are defined as pairs of integers where the first number represents the numerator and the second number represents the denominator. For example, the pair (3, 7) represents the rational number \n\n\n\n\n\n\n3\n7\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{7}}}\n\n.[153] One way to construct the real numbers relies on the concept of Dedekind cuts. According to this approach, each real number is represented by a partition of all rational numbers into two sets, one for all numbers below the represented real number and the other for the rest.[154] Arithmetic operations are defined as functions that perform various set-theoretic transformations on the sets representing the input numbers to arrive at the set representing the result.[155]\n\nThe earliest forms of arithmetic are sometimes traced back to counting and tally marks used to keep track of quantities. Some historians suggest that the Lebombo bone (dated about 43,000 years ago) and the Ishango bone (dated about 22,000 to 30,000 years ago) are the oldest arithmetic artifacts but this interpretation is disputed.[156] However, a basic sense of numbers may predate these findings and might even have existed before the development of language.[157]\n\nIt was not until the emergence of ancient civilizations that a more complex and structured approach to arithmetic began to evolve, starting around 3000 BCE. This became necessary because of the increased need to keep track of stored items, manage land ownership, and arrange exchanges.[158] All the major ancient civilizations developed non-positional numeral systems to facilitate the representation of numbers. They also had symbols for operations like addition and subtraction and were aware of fractions. Examples are Egyptian hieroglyphics as well as the numeral systems invented in Sumeria, China, and India.[159] The first positional numeral system was developed by the Babylonians starting around 1800 BCE. This was a significant improvement over earlier numeral systems since it made the representation of large numbers and calculations on them more efficient.[160] Abacuses have been utilized as hand-operated calculating tools since ancient times as efficient means for performing complex calculations.[161]\n\nEarly civilizations primarily used numbers for concrete practical purposes, like commercial activities and tax records, but lacked an abstract concept of number itself.[162] This changed with the ancient Greek mathematicians, who began to explore the abstract nature of numbers rather than studying how they are applied to specific problems.[163] Another novel feature was their use of proofs to establish mathematical truths and validate theories.[164] A further contribution was their distinction of various classes of numbers, such as even numbers, odd numbers, and prime numbers.[165] This included the discovery that numbers for certain geometrical lengths are irrational and therefore cannot be expressed as a fraction.[166] The works of Thales of Miletus and Pythagoras in the 7th and 6th centuries BCE are often regarded as the inception of Greek mathematics.[167] Diophantus was an influential figure in Greek arithmetic in the 3rd century BCE because of his numerous contributions to number theory and his exploration of the application of arithmetic operations to algebraic equations.[168]\n\nThe ancient Indians were the first to develop the concept of zero as a number to be used in calculations. The exact rules of its operation were written down by Brahmagupta in around 628 CE.[169] The concept of zero or none existed long before, but it was not considered an object of arithmetic operations.[170] Brahmagupta further provided a detailed discussion of calculations with negative numbers and their application to problems like credit and debt.[171] The concept of negative numbers itself is significantly older and was first explored in Chinese mathematics in the first millennium BCE.[172]\n\nIndian mathematicians also developed the positional decimal system used today, in particular the concept of a zero digit instead of empty or missing positions.[173] For example, a detailed treatment of its operations was provided by Aryabhata around the turn of the 6th century CE.[174] The Indian decimal system was further refined and expanded to non-integers during the Islamic Golden Age by Middle Eastern mathematicians such as Al-Khwarizmi. His work was influential in introducing the decimal numeral system to the Western world, which at that time relied on the Roman numeral system.[175] There, it was popularized by mathematicians like Leonardo Fibonacci, who lived in the 12th and 13th centuries and also developed the Fibonacci sequence.[176] During the Middle Ages and Renaissance, many popular textbooks were published to cover the practical calculations for commerce. The use of abacuses also became widespread in this period.[177] In the 16th century, the mathematician Gerolamo Cardano conceived the concept of complex numbers as a way to solve cubic equations.[178]\n\nThe first mechanical calculators were developed in the 17th century and greatly facilitated complex mathematical calculations, such as Blaise Pascal's calculator and Gottfried Wilhelm Leibniz's stepped reckoner.[180] The 17th century also saw the discovery of the logarithm by John Napier.[181]\n\nIn the 18th and 19th centuries, mathematicians such as Leonhard Euler and Carl Friedrich Gauss laid the foundations of modern number theory.[182] Another development in this period concerned work on the formalization and foundations of arithmetic, such as Georg Cantor's set theory and the Dedekind–Peano axioms used as an axiomatization of natural-number arithmetic.[183] Computers and electronic calculators were first developed in the 20th century. Their widespread use revolutionized both the accuracy and speed with which even complex arithmetic computations can be calculated.[184]\n\nArithmetic education forms part of primary education. It is one of the first forms of mathematics education that children encounter. Elementary arithmetic aims to give students a basic sense of numbers and to familiarize them with fundamental numerical operations like addition, subtraction, multiplication, and division.[185] It is usually introduced in relation to concrete scenarios, like counting beads, dividing the class into groups of children of the same size, and calculating change when buying items. Common tools in early arithmetic education are number lines, addition and multiplication tables, counting blocks, and abacuses.[186]\n\nLater stages focus on a more abstract understanding and introduce the students to different types of numbers, such as negative numbers, fractions, real numbers, and complex numbers. They further cover more advanced numerical operations, like exponentiation, extraction of roots, and logarithm.[187] They also show how arithmetic operations are employed in other branches of mathematics, such as their application to describe geometrical shapes and the use of variables in algebra. Another aspect is to teach the students the use of algorithms and calculators to solve complex arithmetic problems.[188]\n\nThe psychology of arithmetic is interested in how humans and animals learn about numbers, represent them, and use them for calculations. It examines how mathematical problems are understood and solved and how arithmetic abilities are related to perception, memory, judgment, and decision making.[189] For example, it investigates how collections of concrete items are first encountered in perception and subsequently associated with numbers.[190] A further field of inquiry concerns the relation between numerical calculations and the use of language to form representations.[191] Psychology also explores the biological origin of arithmetic as an inborn ability. This concerns pre-verbal and pre-symbolic cognitive processes implementing arithmetic-like operations required to successfully represent the world and perform tasks like spatial navigation.[192]\n\nOne of the concepts studied by psychology is numeracy, which is the capability to comprehend numerical concepts, apply them to concrete situations, and reason with them. It includes a fundamental number sense as well as being able to estimate and compare quantities. It further encompasses the abilities to symbolically represent numbers in numbering systems, interpret numerical data, and evaluate arithmetic calculations.[193] Numeracy is a key skill in many academic fields. A lack of numeracy can inhibit academic success and lead to bad economic decisions in everyday life, for example, by misunderstanding mortgage plans and insurance policies.[194]\n\nThe philosophy of arithmetic studies the fundamental concepts and principles underlying numbers and arithmetic operations. It explores the nature and ontological status of numbers, the relation of arithmetic to language and logic, and how it is possible to acquire arithmetic knowledge.[195]\n\nAccording to Platonism, numbers have mind-independent existence: they exist as abstract objects outside spacetime and without causal powers.[196][j] This view is rejected by intuitionists, who claim that mathematical objects are mental constructions.[198] Further theories are logicism, which holds that mathematical truths are reducible to logical truths,[199] and formalism, which states that mathematical principles are rules of how symbols are manipulated without claiming that they correspond to entities outside the rule-governed activity.[200]\n\nThe traditionally dominant view in the epistemology of arithmetic is that arithmetic truths are knowable a priori. This means that they can be known by thinking alone without the need to rely on sensory experience.[201] Some proponents of this view state that arithmetic knowledge is innate while others claim that there is some form of rational intuition through which mathematical truths can be apprehended.[202] A more recent alternative view was suggested by naturalist philosophers like Willard Van Orman Quine, who argue that mathematical principles are high-level generalizations that are ultimately grounded in the sensory world as described by the empirical sciences.[203]\n\nArithmetic is relevant to many fields. In daily life, it is required to calculate change when shopping, manage personal finances, and adjust a cooking recipe for a different number of servings. Businesses use arithmetic to calculate profits and losses and analyze market trends. In the field of engineering, it is used to measure quantities, calculate loads and forces, and design structures.[204] Cryptography relies on arithmetic operations to protect sensitive information by encrypting data and messages.[205]\n\nArithmetic is intimately connected to many branches of mathematics that depend on numerical operations. Algebra relies on arithmetic principles to solve equations using variables. These principles also play a key role in calculus in its attempt to determine rates of change and areas under curves. Geometry uses arithmetic operations to measure the properties of shapes while statistics utilizes them to analyze numerical data.[206] Due to the relevance of arithmetic operations throughout mathematics, the influence of arithmetic extends to most sciences such as physics, computer science, and economics. These operations are used in calculations, problem-solving, data analysis, and algorithms, making them integral to scientific research, technological development, and economic modeling.[207]\n"
    },
    {
        "title": "Natural number",
        "content": "\n\nIn mathematics, the natural numbers are the numbers 0, 1, 2, 3, and so on, possibly excluding 0.[1] Some start counting with 0, defining the natural numbers as the non-negative integers 0, 1, 2, 3, ..., while others start with 1, defining them as the positive integers 1, 2, 3, ... .[a]  Some authors acknowledge both definitions whenever convenient.[2] Sometimes, the whole numbers are the natural numbers plus zero. In other cases, the whole numbers refer to all of the integers, including negative integers.[3] The counting numbers are another term for the natural numbers, particularly in primary school education, and are ambiguous as well although typically start at 1.[4]\n\nThe natural numbers are used for counting things, like \"there are six coins on the table\", in which case they are called cardinal numbers. They are also used to put things in order, like \"this is the third largest city in the country\", which are called ordinal numbers. Natural numbers are also used as labels, like jersey numbers on a sports team, where they serve as nominal numbers and do not have mathematical properties.[5]\n\nThe natural numbers form a set, commonly symbolized as a bold N or blackboard bold ⁠\n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n⁠. Many other number sets are built from the natural numbers. For example, the integers are made by adding 0 and negative numbers. The rational numbers add fractions, and the real numbers add infinite decimals. Complex numbers add the square root of −1. This chain of extensions canonically embeds the natural numbers in the other number systems.[6][7]\n\nNatural numbers are studied in different areas of math. Number theory looks at things like how numbers divide evenly (divisibility), or how prime numbers are spread out. Combinatorics studies counting and arranging numbered objects, such as partitions and enumerations.\n\nThe most primitive method of representing a natural number is to use one's fingers, as in finger counting. Putting down a tally mark for each object is another primitive method. Later, a set of objects could be tested for equality, excess or shortage—by striking out a mark and removing an object from the set.\n\nThe first major advance in abstraction was the use of numerals to represent numbers. This allowed systems to be developed for recording large numbers. The ancient Egyptians developed a powerful system of numerals with distinct hieroglyphs for 1, 10, and all powers of 10 up to over 1 million. A stone carving from Karnak, dating back from around 1500 BCE and now at the Louvre in Paris, depicts 276 as 2 hundreds, 7 tens, and 6 ones; and similarly for the number 4,622. The Babylonians had a place-value system based essentially on the numerals for 1 and 10, using base sixty, so that the symbol for sixty was the same as the symbol for one—its value being determined from context.[11]\n\nA much later advance was the development of the idea that 0 can be considered as a number, with its own numeral. The use of a 0 digit in place-value notation (within other numbers) dates back as early as 700 BCE by the Babylonians, who omitted such a digit when it would have been the last symbol in the number.[b] The Olmec and Maya civilizations used 0 as a separate number as early as the 1st century BCE, but this usage did not spread beyond Mesoamerica.[13][14] The use of a numeral 0 in modern times originated with the Indian mathematician Brahmagupta in 628 CE. However, 0 had been used as a number in the medieval computus (the calculation of the date of Easter), beginning with Dionysius Exiguus in 525 CE, without being denoted by a numeral. Standard Roman numerals do not have a symbol for 0; instead, nulla (or the genitive form nullae) from nullus, the Latin word for \"none\", was employed to denote a 0 value.[15]\n\nThe first systematic study of numbers as abstractions is usually credited to the Greek philosophers Pythagoras and Archimedes. Some Greek mathematicians treated the number 1 differently than larger numbers, sometimes even not as a number at all.[c] Euclid, for example, defined a unit first and then a number as a multitude of units, thus by his definition, a unit is not a number and there are no unique numbers (e.g., any two units from indefinitely many units is a 2).[17] However, in the definition of perfect number which comes shortly afterward, Euclid treats 1 as a number like any other.[18]\n\nIndependent studies on numbers also occurred at around the same time in India, China, and Mesoamerica.[19]\n\nNicolas Chuquet used the term progression naturelle (natural progression) in 1484.[20] The earliest known use of \"natural number\" as a complete English phrase is in 1763.[21][22] The 1771 Encyclopaedia Britannica defines natural numbers in the logarithm article.[22]\n\nStarting at 0 or 1 has long been a matter of definition. In 1727, Bernard Le Bovier de Fontenelle wrote that his notions of distance and element led to defining the natural numbers as including or excluding 0.[23] In 1889, Giuseppe Peano used N for the positive integers and started at 1,[24] but he later changed to using N0 and N1.[25] Historically, most definitions have excluded 0,[22][26][27] but many mathematicians such as George A. Wentworth, Bertrand Russell, Nicolas Bourbaki, Paul Halmos, Stephen Cole Kleene, and John Horton Conway have preferred to include 0.[28][22]\n\nMathematicians have noted tendencies in which definition is used, such as algebra texts including 0,[22][d] number theory and analysis texts excluding 0,[22][29][30] logic and set theory texts including 0,[31][32][33] dictionaries excluding 0,[22][34] school books (through high-school level) excluding 0, and upper-division college-level books including 0.[1] There are exceptions to each of these tendencies and as of 2023 no formal survey has been conducted. Arguments raised include division by zero[29] and the size of the empty set. Computer languages often start from zero when enumerating items like loop counters and string- or array-elements.[35][36] Including 0 began to rise in popularity in the 1960s.[22] The ISO 31-11 standard included 0 in the natural numbers in its first edition in 1978 and this has continued through its present edition as ISO 80000-2.[37]\n\nIn 19th century Europe, there was mathematical and philosophical discussion about the exact nature of the natural numbers. Henri Poincaré stated that axioms can only be demonstrated in their finite application, and concluded that it is \"the power of the mind\" which allows conceiving of the indefinite repetition of the same act.[38] Leopold Kronecker summarized his belief as \"God made the integers, all else is the work of man\".[e]\n\nThe constructivists saw a need to improve upon the logical rigor in the foundations of mathematics.[f] In the 1860s, Hermann Grassmann suggested a recursive definition for natural numbers, thus stating they were not really natural—but a consequence of definitions. Later, two classes of such formal definitions emerged, using set theory and Peano's axioms respectively. Later still, they were shown to be equivalent in most practical applications.\n\nSet-theoretical definitions of natural numbers were initiated by Frege. He initially defined a natural number as the class of all sets that are in one-to-one correspondence with a particular set. However, this definition turned out to lead to paradoxes, including Russell's paradox. To avoid such paradoxes, the formalism was modified so that a natural number is defined as a particular set, and any set that can be put into one-to-one correspondence with that set is said to have that number of elements.[41]\n\nIn 1881, Charles Sanders Peirce provided the first axiomatization of natural-number arithmetic.[42][43] In 1888, Richard Dedekind proposed another axiomatization of natural-number arithmetic,[44] and in 1889, Peano published a simplified version of Dedekind's axioms in his book The principles of arithmetic presented by a new method (Latin: Arithmetices principia, nova methodo exposita). This approach is now called Peano arithmetic. It is based on an axiomatization of the properties of ordinal numbers: each natural number has a successor and every non-zero natural number has a unique predecessor. Peano arithmetic is equiconsistent with several weak systems of set theory. One such system is ZFC with the axiom of infinity replaced by its negation.[45] Theorems that can be proved in ZFC but cannot be proved using the Peano Axioms include Goodstein's theorem.[46]\n\nThe set of all natural numbers is standardly denoted N or \n\n\n\n\nN\n\n.\n\n\n{\\displaystyle \\mathbb {N} .}\n\n[2][47] Older texts have occasionally employed J as the symbol for this set.[48]\n\nSince natural numbers may contain 0 or not, it may be important to know which version is referred to. This is often specified by the context, but may also be done by using a subscript or a superscript in the notation, such as:[37][49]\n\nAlternatively, since the natural numbers naturally form a subset of the integers (often denoted \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n), they may be referred to as the positive, or the non-negative integers, respectively.[50] To be unambiguous about whether 0 is included or not, sometimes a superscript \"\n\n\n\n∗\n\n\n{\\displaystyle *}\n\n\" or \"+\" is added in the former case, and a subscript (or superscript) \"0\" is added in the latter case:[37]\n\nThis section uses the convention \n\n\n\n\nN\n\n=\n\n\nN\n\n\n0\n\n\n=\n\n\nN\n\n\n∗\n\n\n∪\n{\n0\n}\n\n\n{\\displaystyle \\mathbb {N} =\\mathbb {N} _{0}=\\mathbb {N} ^{*}\\cup \\{0\\}}\n\n.\n\nGiven the set \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n of natural numbers and the successor function \n\n\n\nS\n:\n\nN\n\n→\n\nN\n\n\n\n{\\displaystyle S\\colon \\mathbb {N} \\to \\mathbb {N} }\n\n sending each natural number to the next one, one can define addition of natural numbers recursively by setting a + 0 = a and a + S(b) = S(a + b) for all a, b. Thus, a + 1 = a + S(0) = S(a+0) = S(a), a + 2 = a + S(1) = S(a+1) = S(S(a)), and so on. The algebraic structure \n\n\n\n(\n\nN\n\n,\n+\n)\n\n\n{\\displaystyle (\\mathbb {N} ,+)}\n\n is a commutative monoid with identity element 0. It is a free monoid on one generator. This commutative monoid satisfies the cancellation property, so it can be embedded in a group. The smallest group containing the natural numbers is the integers.\n\nIf 1 is defined as S(0), then b + 1 = b + S(0) = S(b + 0) = S(b). That is, b + 1 is simply the successor of b.\n\nAnalogously, given that addition has been defined, a multiplication operator \n\n\n\n×\n\n\n{\\displaystyle \\times }\n\n can be defined via a × 0 = 0 and a × S(b) = (a × b) + a. This turns \n\n\n\n(\n\n\nN\n\n\n∗\n\n\n,\n×\n)\n\n\n{\\displaystyle (\\mathbb {N} ^{*},\\times )}\n\n into a free commutative monoid with identity element 1; a generator set for this monoid is the set of prime numbers.\n\nAddition and multiplication are compatible, which is expressed in the distribution law: a × (b + c) = (a × b) + (a × c). These properties of addition and multiplication make the natural numbers an instance of a commutative semiring. Semirings are an algebraic generalization of the natural numbers where multiplication is not necessarily commutative. The lack of additive inverses, which is equivalent to the fact that \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n is not closed under subtraction (that is, subtracting one natural from another does not always result in another natural), means that \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n is not a ring; instead it is a semiring (also known as a rig).\n\nIf the natural numbers are taken as \"excluding 0\", and \"starting at 1\", the definitions of + and × are as above, except that they begin with a + 1 = S(a) and a × 1 = a. Furthermore, \n\n\n\n(\n\n\nN\n\n∗\n\n\n\n,\n+\n)\n\n\n{\\displaystyle (\\mathbb {N^{*}} ,+)}\n\n has no identity element.\n\nIn this section, juxtaposed variables such as ab indicate the product a × b,[51] and the standard order of operations is assumed.\n\nA total order on the natural numbers is defined by letting a ≤ b if and only if there exists another natural number c where a + c = b. This order is compatible with the arithmetical operations in the following sense: if a, b and c are natural numbers and a ≤ b, then a + c ≤ b + c and ac ≤ bc.\n\nAn important property of the natural numbers is that they are well-ordered: every non-empty set of natural numbers has a least element. The rank among well-ordered sets is expressed by an ordinal number; for the natural numbers, this is denoted as ω (omega).\n\nIn this section, juxtaposed variables such as ab indicate the product a × b, and the standard order of operations is assumed.\n\nWhile it is in general not possible to divide one natural number by another and get a natural number as result, the procedure of division with remainder or Euclidean division is available as a substitute: for any two natural numbers a and b with b ≠ 0 there are natural numbers q and r such that\n\nThe number q is called the quotient and r is called the remainder of the division of a by b. The numbers q and r are uniquely determined by a and b. This Euclidean division is key to the several other properties (divisibility), algorithms (such as the Euclidean algorithm), and ideas in number theory.\n\nThe addition (+) and multiplication (×) operations on natural numbers as defined above have several algebraic properties:\n\nTwo important generalizations of natural numbers arise from the two uses of counting and ordering: cardinal numbers and ordinal numbers.\n\nThe least ordinal of cardinality ℵ0 (that is, the initial ordinal of ℵ0) is ω but many well-ordered sets with cardinal number ℵ0 have an ordinal number greater than ω.\n\nFor finite well-ordered sets, there is a one-to-one correspondence between ordinal and cardinal numbers; therefore they can both be expressed by the same natural number, the number of elements of the set. This number can also be used to describe the position of an element in a larger finite, or an infinite, sequence.\n\nA countable non-standard model of arithmetic satisfying the Peano Arithmetic (that is, the first-order Peano axioms) was developed by Skolem in 1933. The hypernatural numbers are an uncountable model that can be constructed from the ordinary natural numbers via the ultrapower construction. Other generalizations are discussed in Number § Extensions of the concept.\n\nGeorges Reeb used to claim provocatively that \"The naïve integers don't fill up \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n\".[55]\n\nThere are two standard methods for formally defining natural numbers. The first one, named for Giuseppe Peano, consists of an autonomous axiomatic theory called Peano arithmetic, based on few axioms called Peano axioms.\n\nThe second definition is based on set theory. It defines the natural numbers as specific sets. More precisely, each natural number n is defined as an explicitly defined set, whose elements allow counting the elements of other sets, in the sense that the sentence \"a set S has n elements\" means that there exists a one to one correspondence between the two sets n and S.\n\nThe sets used to define natural numbers satisfy Peano axioms. It follows that every theorem that can be stated and proved in Peano arithmetic can also be proved in set theory. However, the two definitions are not equivalent, as there are theorems that can be stated in terms of Peano arithmetic and proved in set theory, which are not provable inside Peano arithmetic. A probable example is Fermat's Last Theorem.\n\nThe definition of the integers as sets satisfying Peano axioms provide a model of Peano arithmetic inside set theory. An important consequence is that, if set theory is consistent (as it is usually guessed), then Peano arithmetic is consistent. In other words, if a contradiction could be proved in Peano arithmetic, then set theory would be contradictory, and every theorem of set theory would be both true and wrong. \n\nThe five Peano axioms are the following:[56][g]\n\nThese are not the original axioms published by Peano, but are named in his honor. Some forms of the Peano axioms have 1 in place of 0. In ordinary arithmetic, the successor of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is \n\n\n\nx\n+\n1\n\n\n{\\displaystyle x+1}\n\n.\n\nIntuitively, the natural number n is the common property of all sets that have n elements. So, it seems natural to define n as an equivalence class under the relation \"can be made in one to one correspondence\". This does not work in all set theories, as such an equivalence class would not be a set[h] (because of Russell's paradox). The standard solution is to define a particular set with n elements that will be called the natural number n.\n\nThe following definition was first published by John von Neumann,[57] although Levy attributes the idea to unpublished work of Zermelo in 1916.[58] As this definition extends to infinite set as a definition of ordinal number, the sets considered below are sometimes called von Neumann ordinals.\n\nThe definition proceeds as follows:\n\nIt follows that the natural numbers are defined iteratively as follows:\n\nIt can be checked that the natural numbers satisfy the Peano axioms.\n\nWith this definition, given a natural number n, the sentence \"a set S has n elements\" can be formally defined as \"there exists a bijection from n to S.\" This formalizes the operation of counting the elements of S. Also, n ≤ m if and only if n is a subset of m. In other words, the set inclusion defines the usual total order on the natural numbers. This order is a well-order.\n\nIt follows from the definition that each natural number is equal to the set of all natural numbers less than it. This definition, can be extended to the von Neumann definition of ordinals for defining all ordinal numbers, including the infinite ones: \"each ordinal is the well-ordered set of all smaller ordinals.\"\n\nIf one does not accept the axiom of infinity, the natural numbers may not form a set. Nevertheless, the natural numbers can still be individually defined as above, and they still satisfy the Peano axioms.\n\nThere are other set theoretical constructions. In particular, Ernst Zermelo provided a construction that is nowadays only of historical interest, and is sometimes referred to as Zermelo ordinals.[58] It consists in defining 0 as the empty set, and S(a) = {a}.\n\nWith this definition each nonzero natural number is a singleton set. So, the property of the natural numbers to represent cardinalities is not directly accessible; only the ordinal property (being the nth element of a sequence) is immediate. Unlike von Neumann's construction, the Zermelo ordinals do not extend to infinite ordinals.\n"
    },
    {
        "title": "Fraction",
        "content": "\n\nA fraction (from Latin: fractus, \"broken\") represents a part of a whole or, more generally, any number of equal parts. When spoken in everyday English, a fraction describes how many parts of a certain size there are, for example, one-half, eight-fifths, three-quarters. A common, vulgar, or simple fraction (examples: \n\n\n\n\n\n\n1\n2\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}}\n\n and \n\n\n\n\n\n\n17\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {17}{3}}}\n\n) consists of an integer numerator, displayed above a line (or before a slash like 1⁄2), and a non-zero integer denominator, displayed below (or after) that line. If these integers are positive, then the numerator represents a number of equal parts, and the denominator indicates how many of those parts make up a unit or a whole. For example, in the fraction ⁠3/4⁠, the numerator 3 indicates that the fraction represents 3 equal parts, and the denominator 4 indicates that 4 parts make up a whole. The picture to the right illustrates ⁠3/4⁠ of a cake.\n\nFractions can be used to represent ratios and division.[1] Thus the fraction ⁠3/4⁠ can be used to represent the ratio 3:4 (the ratio of the part to the whole), and the division 3 ÷ 4 (three divided by four).\n\nWe can also write negative fractions, which represent the opposite of a positive fraction. For example, if ⁠1/2⁠ represents a half-dollar profit, then −⁠1/2⁠ represents a half-dollar loss. Because of the rules of division of signed numbers (which states in part that negative divided by positive is negative), −⁠1/2⁠, ⁠−1/2⁠ and ⁠1/−2⁠ all represent the same fraction –  negative one-half. And because a negative divided by a negative produces a positive, ⁠−1/−2⁠ represents positive one-half.\n\nIn mathematics a rational number is a number that can be represented by a fraction of the form ⁠a/b⁠, where a and b are integers and b is not zero; the set of all rational numbers is commonly represented by the symbol Q or ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠, which stands for quotient. The term fraction and the notation ⁠a/b⁠ can also be used for mathematical expressions that do not represent a rational number (for example \n\n\n\n\n\n\n\n2\n\n2\n\n\n\n\n\n{\\displaystyle \\textstyle {\\frac {\\sqrt {2}}{2}}}\n\n), and even do not represent any number (for example the rational fraction \n\n\n\n\n\n\n1\nx\n\n\n\n\n\n{\\displaystyle \\textstyle {\\frac {1}{x}}}\n\n).\n\nIn a fraction, the number of equal parts being described is the numerator (from Latin: numerātor, \"counter\" or \"numberer\"), and the type or variety of the parts is the denominator (from Latin: dēnōminātor, \"thing that names or designates\").[2][3] As an example, the fraction ⁠8/5⁠ amounts to eight parts, each of which is of the type named \"fifth\". In terms of division, the numerator corresponds to the dividend, and the denominator corresponds to the divisor.\n\nInformally, the numerator and denominator may be distinguished by placement alone, but in formal contexts they are usually separated by a fraction bar. The fraction bar may be horizontal (as in ⁠1/3⁠), oblique (as in 2/5), or diagonal (as in 4⁄9).[4] These marks are respectively known as the horizontal bar; the virgule, slash (US), or stroke (UK); and the fraction bar, solidus,[5] or fraction slash.[n 1] In typography, fractions stacked vertically are also known as \"en\" or \"nut fractions\", and diagonal ones as \"em\" or \"mutton fractions\", based on whether a fraction with a single-digit numerator and denominator occupies the proportion of a narrow en square, or a wider em square.[4] In traditional typefounding, a piece of type bearing a complete fraction (e.g. ⁠1/2⁠) was known as a \"case fraction\", while those representing only part of fraction were called \"piece fractions\".\n\nThe denominators of English fractions are generally expressed as ordinal numbers, in the plural if the numerator is not 1. (For example, ⁠2/5⁠ and ⁠3/5⁠ are both read as a number of \"fifths\".) Exceptions include the denominator 2, which is always read \"half\" or \"halves\", the denominator 4, which may be alternatively expressed as \"quarter\"/\"quarters\" or as \"fourth\"/\"fourths\", and the denominator 100, which may be alternatively expressed as \"hundredth\"/\"hundredths\" or \"percent\".\n\nWhen the denominator is 1, it may be expressed in terms of \"wholes\" but is more commonly ignored, with the numerator read out as a whole number. For example, ⁠3/1⁠ may be described as \"three wholes\", or simply as \"three\". When the numerator is 1, it may be omitted (as in \"a tenth\" or \"each quarter\").\n\nThe entire fraction may be expressed as a single composition, in which case it is hyphenated, or as a number of fractions with a numerator of one, in which case they are not. (For example, \"two-fifths\" is the fraction ⁠2/5⁠ and \"two fifths\" is the same fraction understood as 2 instances of ⁠1/5⁠.) Fractions should always be hyphenated when used as adjectives. Alternatively, a fraction may be described by reading it out as the numerator \"over\" the denominator, with the denominator expressed as a cardinal number. (For example, ⁠3/1⁠ may also be expressed as \"three over one\".) The term \"over\" is used even in the case of solidus fractions, where the numbers are placed left and right of a slash mark. (For example, 1/2 may be read \"one-half\", \"one half\", or \"one over two\".) Fractions with large denominators that are not powers of ten are often rendered in this fashion (e.g., ⁠1/117⁠ as \"one over one hundred seventeen\"), while those with denominators divisible by ten are typically read in the normal ordinal fashion (e.g., ⁠6/1000000⁠ as \"six-millionths\", \"six millionths\", or \"six one-millionths\").\n\nA simple fraction (also known as a common fraction or vulgar fraction, where vulgar is Latin for \"common\") is a rational number written as a/b or ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠, where a and b are both integers.[9] As with other fractions, the denominator (b) cannot be zero. Examples include ⁠1/2⁠, −⁠8/5⁠, ⁠−8/5⁠, and ⁠8/−5⁠. The term was originally used to distinguish this type of fraction from the sexagesimal fraction used in astronomy.[10]\n\nCommon fractions can be positive or negative, and they can be proper or improper (see below). Compound fractions, complex fractions, mixed numerals, and decimals (see below) are not common fractions; though, unless irrational, they can be evaluated to a common fraction.\n\nIn Unicode, precomposed fraction characters are in the Number Forms block.\n\nCommon fractions can be classified as either proper or improper. When the numerator and the denominator are both positive, the fraction is called proper if the numerator is less than the denominator, and improper otherwise.[11] The concept of an \"improper fraction\" is a late development, with the terminology deriving from the fact that \"fraction\" means \"a piece\", so a proper fraction must be less than 1.[10] This was explained in the 17th century textbook The Ground of Arts.[12][13]\n\nIn general, a common fraction is said to be a proper fraction, if the absolute value of the fraction is strictly less than one—that is, if the fraction is greater than −1 and less than 1.[14][15] It is said to be an improper fraction, or sometimes top-heavy fraction,[16] if the absolute value of the fraction is greater than or equal to 1. Examples of proper fractions are 2/3, −3/4, and 4/9, whereas examples of improper fractions are 9/4, −4/3, and 3/3.\n\nThe reciprocal of a fraction is another fraction with the numerator and denominator exchanged. The reciprocal of ⁠3/7⁠, for instance, is ⁠7/3⁠. The product of a non-zero fraction and its reciprocal is 1, hence the reciprocal is the multiplicative inverse of a fraction. The reciprocal of a proper fraction is improper, and the reciprocal of an improper fraction not equal to 1 (that is, numerator and denominator are not equal) is a proper fraction.\n\nWhen the numerator and denominator of a fraction are equal (for example, ⁠7/7⁠), its value is 1, and the fraction therefore is improper. Its reciprocal is identical and hence also equal to 1 and improper.\n\nAny integer can be written as a fraction with the number one as denominator. For example, 17 can be written as ⁠17/1⁠, where 1 is sometimes referred to as the invisible denominator.[17] Therefore, every fraction or integer, except for zero, has a reciprocal. For example, the reciprocal of 17 is ⁠1/17⁠.\n\nA ratio is a relationship between two or more numbers that can be sometimes expressed as a fraction. Typically, a number of items are grouped and compared in a ratio, specifying numerically the relationship between each group. Ratios are expressed as \"group 1 to group 2 ... to group n\". For example, if a car lot had 12 vehicles, of which\n\nthen the ratio of red to white to yellow cars is 6 to 2 to 4. The ratio of yellow cars to white cars is 4 to 2 and may be expressed as 4:2 or 2:1.\n\nA ratio is often converted to a fraction when it is expressed as a ratio to the whole. In the above example, the ratio of yellow cars to all the cars on the lot is 4:12 or 1:3. We can convert these ratios to a fraction, and say that ⁠4/12⁠ of the cars or ⁠1/3⁠ of the cars in the lot are yellow. Therefore, if a person randomly chose one car on the lot, then there is a one in three chance or probability that it would be yellow.\n\nA decimal fraction is a fraction whose denominator is not given explicitly, but is understood to be an integer power of ten. Decimal fractions are commonly expressed using decimal notation in which the implied denominator is determined by the number of digits to the right of a decimal separator, the appearance of which (e.g., a period, an interpunct (·), a comma) depends on the locale (for examples, see Decimal separator). Thus, for 0.75 the numerator is 75 and the implied denominator is 10 to the second power, namely, 100, because there are two digits to the right of the decimal separator. In decimal numbers greater than 1 (such as 3.75), the fractional part of the number is expressed by the digits to the right of the decimal (with a value of 0.75 in this case). 3.75 can be written either as an improper fraction, 375/100, or as a mixed number, ⁠3+75/100⁠.\n\nDecimal fractions can also be expressed using scientific notation with negative exponents, such as 6.023×10−7, which represents 0.0000006023. The 10−7 represents a denominator of 107. Dividing by 107 moves the decimal point 7 places to the left.\n\nDecimal fractions with infinitely many digits to the right of the decimal separator represent an infinite series. For example, ⁠1/3⁠ = 0.333... represents the infinite series 3/10 + 3/100 + 3/1000 + ....\n\nAnother kind of fraction is the percentage (from Latin: per centum, meaning \"per hundred\", represented by the symbol %), in which the implied denominator is always 100. Thus, 51% means 51/100. Percentages greater than 100 or less than zero are treated in the same way, e.g. 311% equals 311/100, and −27% equals −27/100.\n\nThe related concept of permille or parts per thousand (ppt) has an implied denominator of 1000, while the more general parts-per notation, as in 75 parts per million (ppm), means that the proportion is 75/1,000,000.\n\nWhether common fractions or decimal fractions are used is often a matter of taste and context. Common fractions are used most often when the denominator is relatively small. By mental calculation, it is easier to multiply 16 by 3/16 than to do the same calculation using the fraction's decimal equivalent (0.1875). And it is more accurate to multiply 15 by 1/3, for example, than it is to multiply 15 by any decimal approximation of one third. Monetary values are commonly expressed as decimal fractions with denominator 100, i.e., with two decimals, for example $3.75. However, as noted above, in pre-decimal British currency, shillings and pence were often given the form (but not the meaning) of a fraction, as, for example, \"3/6\" (read \"three and six\") meaning 3 shillings and 6 pence, and having no relationship to the fraction 3/6.\n\nA mixed number (also called a mixed fraction or mixed numeral) is the sum of a non-zero integer and a proper fraction, conventionally written by juxtaposition (or concatenation) of the two parts, without the use of an intermediate plus (+) or minus (−) sign. When the fraction is written horizontally, a space is added between the integer and fraction to separate them.\n\nAs a basic example, two entire cakes and three quarters of another cake might be written as \n\n\n\n2\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle 2{\\tfrac {3}{4}}}\n\n cakes or \n\n\n\n2\n \n\n3\n\n/\n\n4\n\n\n{\\displaystyle 2\\ \\,3/4}\n\n cakes, with the numeral \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n representing the whole cakes and the fraction \n\n\n\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{4}}}\n\n representing the additional partial cake juxtaposed; this is more concise than the more explicit notation \n\n\n\n2\n+\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle 2+{\\tfrac {3}{4}}}\n\n cakes. The mixed number ⁠2+3/4⁠ is pronounced \"two and three quarters\", with the integer and fraction portions connected by the word and.[18] Subtraction or negation is applied to the entire mixed numeral, so \n\n\n\n−\n2\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle -2{\\tfrac {3}{4}}}\n\n means \n\n\n\n−\n\n\n(\n\n\n2\n+\n\n\n\n3\n4\n\n\n\n\n\n)\n\n\n.\n\n\n{\\displaystyle -{\\bigl (}2+{\\tfrac {3}{4}}{\\bigr )}.}\n\n\n\nAny mixed number can be converted to an improper fraction by applying the rules of adding unlike quantities. For example, \n\n\n\n2\n+\n\n\n\n3\n4\n\n\n\n=\n\n\n\n8\n4\n\n\n\n+\n\n\n\n3\n4\n\n\n\n=\n\n\n\n11\n4\n\n\n\n.\n\n\n{\\displaystyle 2+{\\tfrac {3}{4}}={\\tfrac {8}{4}}+{\\tfrac {3}{4}}={\\tfrac {11}{4}}.}\n\n Conversely, an improper fraction can be converted to a mixed number using division with remainder, with the proper fraction consisting of the remainder divided by the divisor. For example, since 4 goes into 11 twice, with 3 left over, \n\n\n\n\n\n\n11\n4\n\n\n\n=\n2\n+\n\n\n\n3\n4\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {11}{4}}=2+{\\tfrac {3}{4}}.}\n\n\n\nIn primary school, teachers often insist that every fractional result should be expressed as a mixed number.[19] Outside school, mixed numbers are commonly used for describing measurements, for instance ⁠2+1/2⁠ hours or 5 3/16 inches, and remain widespread in daily life and in trades, especially in regions that do not use the decimalized metric system. However, scientific measurements typically use the metric system, which is based on decimal fractions, and starting from the secondary school level, mathematics pedagogy treats every fraction uniformly as a rational number, the quotient ⁠p/q⁠ of integers, leaving behind the concepts of \"improper fraction\" and \"mixed number\".[20] College students with years of mathematical training are sometimes confused when re-encountering mixed numbers because they are used to the convention that juxtaposition in algebraic expressions means multiplication.[21]\n\nAn Egyptian fraction is the sum of distinct positive unit fractions, for example \n\n\n\n\n\n\n1\n2\n\n\n\n+\n\n\n\n1\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}+{\\tfrac {1}{3}}}\n\n. This definition derives from the fact that the ancient Egyptians expressed all fractions except \n\n\n\n\n\n\n1\n2\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}}\n\n, \n\n\n\n\n\n\n2\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{3}}}\n\n and \n\n\n\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{4}}}\n\n in this manner. Every positive rational number can be expanded as an Egyptian fraction. For example, \n\n\n\n\n\n\n5\n7\n\n\n\n\n\n{\\displaystyle {\\tfrac {5}{7}}}\n\n can be written as \n\n\n\n\n\n\n1\n2\n\n\n\n+\n\n\n\n1\n6\n\n\n\n+\n\n\n\n1\n21\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {1}{2}}+{\\tfrac {1}{6}}+{\\tfrac {1}{21}}.}\n\n Any positive rational number can be written as a sum of unit fractions in infinitely many ways. Two ways to write \n\n\n\n\n\n\n13\n17\n\n\n\n\n\n{\\displaystyle {\\tfrac {13}{17}}}\n\n are \n\n\n\n\n\n\n1\n2\n\n\n\n+\n\n\n\n1\n4\n\n\n\n+\n\n\n\n1\n68\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}+{\\tfrac {1}{4}}+{\\tfrac {1}{68}}}\n\n and \n\n\n\n\n\n\n1\n3\n\n\n\n+\n\n\n\n1\n4\n\n\n\n+\n\n\n\n1\n6\n\n\n\n+\n\n\n\n1\n68\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{3}}+{\\tfrac {1}{4}}+{\\tfrac {1}{6}}+{\\tfrac {1}{68}}}\n\n.\n\nIn a complex fraction, either the numerator, or the denominator, or both, is a fraction or a mixed number,[22][23] corresponding to division of fractions. For example, \n\n\n\n\n\n\n\n1\n\n/\n\n2\n\n\n1\n\n/\n\n3\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {1/2}{1/3}}}\n\n and \n\n\n\n\n\n(\n\n\n12\n\n\n\n3\n4\n\n\n\n\n\n)\n\n\n\n\n/\n\n\n26\n\n\n{\\displaystyle {\\bigl (}12{\\tfrac {3}{4}}{\\bigr )}{\\big /}26}\n\n are complex fractions. To interpret nested fractions written \"stacked\" with a horizontal fraction bars, treat shorter bars as nested inside longer bars. Complex fractions can be simplified using multiplication by the reciprocal, as described below at § Division. For example:\n\nA complex fraction should never be written without an obvious marker showing which fraction is nested inside the other, as such expressions are ambiguous. For example, the expression \n\n\n\n5\n\n/\n\n10\n\n/\n\n20\n\n\n{\\displaystyle 5/10/20}\n\n could be plausibly interpreted as either \n\n\n\n\n\n\n5\n10\n\n\n\n\n\n/\n\n\n20\n=\n\n\n\n1\n40\n\n\n\n\n\n{\\displaystyle {\\tfrac {5}{10}}{\\big /}20={\\tfrac {1}{40}}}\n\n or as \n\n\n\n5\n\n\n/\n\n\n\n\n\n10\n20\n\n\n\n=\n10.\n\n\n{\\displaystyle 5{\\big /}{\\tfrac {10}{20}}=10.}\n\n The meaning can be made explicit by writing the fractions using distinct separators or by adding explicit parentheses, in this instance \n\n\n\n(\n5\n\n/\n\n10\n)\n\n\n/\n\n\n20\n\n\n{\\displaystyle (5/10){\\big /}20}\n\n or \n\n\n\n5\n\n\n/\n\n\n(\n10\n\n/\n\n20\n)\n.\n\n\n{\\displaystyle 5{\\big /}(10/20).}\n\n\n\nA compound fraction is a fraction of a fraction, or any number of fractions connected with the word of,[22][23] corresponding to multiplication of fractions. To reduce a compound fraction to a simple fraction, just carry out the multiplication (see § Multiplication). For example, \n\n\n\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{4}}}\n\n of \n\n\n\n\n\n\n5\n7\n\n\n\n\n\n{\\displaystyle {\\tfrac {5}{7}}}\n\n is a compound fraction, corresponding to \n\n\n\n\n\n\n3\n4\n\n\n\n×\n\n\n\n5\n7\n\n\n\n=\n\n\n\n15\n28\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{4}}\\times {\\tfrac {5}{7}}={\\tfrac {15}{28}}}\n\n. The terms compound fraction and complex fraction are closely related and sometimes one is used as a synonym for the other. (For example, the compound fraction \n\n\n\n\n\n\n3\n4\n\n\n\n×\n\n\n\n5\n7\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{4}}\\times {\\tfrac {5}{7}}}\n\n is equivalent to the complex fraction ⁠\n\n\n\n\n\n\n\n3\n\n/\n\n4\n\n\n7\n\n/\n\n5\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {3/4}{7/5}}}\n\n⁠.)\n\nNevertheless, \"complex fraction\" and \"compound fraction\" may both be considered outdated[24] and now used in no well-defined manner, partly even taken synonymously for each other[25] or for mixed numerals.[26] They have lost their meaning as technical terms and the attributes \"complex\" and \"compound\" tend to be used in their every day meaning of \"consisting of parts\".\n\nLike whole numbers, fractions obey the commutative, associative, and distributive laws, and the rule against division by zero.\n\nMixed-number arithmetic can be performed either by converting each mixed number to an improper fraction, or by treating each as a sum of integer and fractional parts.\n\nMultiplying the numerator and denominator of a fraction by the same (non-zero) number results in a fraction that is equivalent to the original fraction. This is true because for any non-zero number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, the fraction \n\n\n\n\n\n\nn\nn\n\n\n\n\n\n{\\displaystyle {\\tfrac {n}{n}}}\n\n equals 1. Therefore, multiplying by \n\n\n\n\n\n\nn\nn\n\n\n\n\n\n{\\displaystyle {\\tfrac {n}{n}}}\n\n is the same as multiplying by one, and any number multiplied by one has the same value as the original number. By way of an example, start with the fraction ⁠\n\n\n\n\n\n\n1\n2\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}}\n\n⁠. When the numerator and denominator are both multiplied by 2, the result is ⁠2/4⁠, which has the same value (0.5) as ⁠1/2⁠. To picture this visually, imagine cutting a cake into four pieces; two of the pieces together (⁠2/4⁠) make up half the cake (⁠1/2⁠).\n\nDividing the numerator and denominator of a fraction by the same non-zero number yields an equivalent fraction: if the numerator and the denominator of a fraction are both divisible by a number (called a factor) greater than 1, then the fraction can be reduced to an equivalent fraction with a smaller numerator and a smaller denominator. For example, if both the numerator and the denominator of the fraction \n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n are divisible by ⁠\n\n\n\nc\n\n\n{\\displaystyle c}\n\n⁠, then they can be written as \n\n\n\na\n=\nc\nd\n\n\n{\\displaystyle a=cd}\n\n, \n\n\n\nb\n=\nc\ne\n\n\n{\\displaystyle b=ce}\n\n, and the fraction becomes ⁠cd/ce⁠, which can be reduced by dividing both the numerator and denominator by c to give the reduced fraction ⁠d/e⁠.\n\nIf one takes for c the greatest common divisor of the numerator and the denominator, one gets the equivalent fraction whose numerator and denominator have the lowest absolute values. One says that the fraction has been reduced to its lowest terms.\n\nIf the numerator and the denominator do not share any factor greater than 1, the fraction is already reduced to its lowest terms, and it is said to be irreducible, reduced, or in simplest terms. For example, \n\n\n\n\n\n\n3\n9\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{9}}}\n\n is not in lowest terms because both 3 and 9 can be exactly divided by 3. In contrast, \n\n\n\n\n\n\n3\n8\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{8}}}\n\n is in lowest terms—the only positive integer that goes into both 3 and 8 evenly is 1.\n\nUsing these rules, we can show that ⁠5/10⁠ = ⁠1/2⁠ = ⁠10/20⁠ = ⁠50/100⁠, for example.\n\nAs another example, since the greatest common divisor of 63 and 462 is 21, the fraction ⁠63/462⁠ can be reduced to lowest terms by dividing the numerator and denominator by 21:\n\nThe Euclidean algorithm gives a method for finding the greatest common divisor of any two integers.\n\nComparing fractions with the same positive denominator yields the same result as comparing the numerators:\n\nIf the equal denominators are negative, then the opposite result of comparing the numerators holds for the fractions:\n\nIf two positive fractions have the same numerator, then the fraction with the smaller denominator is the larger number. When a whole is divided into equal pieces, if fewer equal pieces are needed to make up the whole, then each piece must be larger. When two positive fractions have the same numerator, they represent the same number of parts, but in the fraction with the smaller denominator, the parts are larger.\n\nOne way to compare fractions with different numerators and denominators is to find a common denominator. To compare \n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n and \n\n\n\n\n\n\nc\nd\n\n\n\n\n\n{\\displaystyle {\\tfrac {c}{d}}}\n\n, these are converted to \n\n\n\n\n\n\n\na\n⋅\nd\n\n\nb\n⋅\nd\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {a\\cdot d}{b\\cdot d}}}\n\n and \n\n\n\n\n\n\n\nb\n⋅\nc\n\n\nb\n⋅\nd\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {b\\cdot c}{b\\cdot d}}}\n\n (where the dot signifies multiplication and is an alternative symbol to ×). Then bd is a common denominator and the numerators ad and bc can be compared. It is not necessary to determine the value of the common denominator to compare fractions – one can just compare ad and bc, without evaluating bd, e.g., comparing \n\n\n\n\n\n\n2\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{3}}}\n\n ? \n\n\n\n\n\n\n1\n2\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}}\n\n gives \n\n\n\n\n\n\n4\n6\n\n\n\n>\n\n\n\n3\n6\n\n\n\n\n\n{\\displaystyle {\\tfrac {4}{6}}>{\\tfrac {3}{6}}}\n\n.\n\nFor the more laborious question \n\n\n\n\n\n\n5\n18\n\n\n\n\n\n{\\displaystyle {\\tfrac {5}{18}}}\n\n ? \n\n\n\n\n\n\n4\n17\n\n\n\n,\n\n\n{\\displaystyle {\\tfrac {4}{17}},}\n\n multiply top and bottom of each fraction by the denominator of the other fraction, to get a common denominator, yielding \n\n\n\n\n\n\n\n5\n×\n17\n\n\n18\n×\n17\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {5\\times 17}{18\\times 17}}}\n\n ? \n\n\n\n\n\n\n\n18\n×\n4\n\n\n18\n×\n17\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {18\\times 4}{18\\times 17}}}\n\n. It is not necessary to calculate \n\n\n\n18\n×\n17\n\n\n{\\displaystyle 18\\times 17}\n\n – only the numerators need to be compared. Since 5×17 (= 85) is greater than 4×18 (= 72), the result of comparing is ⁠\n\n\n\n\n\n\n5\n18\n\n\n\n>\n\n\n\n4\n17\n\n\n\n\n\n{\\displaystyle {\\tfrac {5}{18}}>{\\tfrac {4}{17}}}\n\n⁠.\n\nBecause every negative number, including negative fractions, is less than zero, and every positive number, including positive fractions, is greater than zero, it follows that any negative fraction is less than any positive fraction. This allows, together with the above rules, to compare all possible fractions.\n\nThe first rule of addition is that only like quantities can be added; for example, various quantities of quarters. Unlike quantities, such as adding thirds to quarters, must first be converted to like quantities as described below: Imagine a pocket containing two quarters, and another pocket containing three quarters; in total, there are five quarters. Since four quarters is equivalent to one (dollar), this can be represented as follows:\n\nTo add fractions containing unlike quantities (e.g. quarters and thirds), it is necessary to convert all amounts to like quantities. It is easy to work out the chosen type of fraction to convert to; simply multiply together the two denominators (bottom number) of each fraction. In case of an integer number apply the invisible denominator 1.\n\nFor adding quarters to thirds, both types of fraction are converted to twelfths, thus:\n\nConsider adding the following two quantities:\n\nFirst, convert \n\n\n\n\n\n\n3\n5\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{5}}}\n\n into fifteenths by multiplying both the numerator and denominator by three: ⁠\n\n\n\n\n\n\n3\n5\n\n\n\n×\n\n\n\n3\n3\n\n\n\n=\n\n\n\n9\n15\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{5}}\\times {\\tfrac {3}{3}}={\\tfrac {9}{15}}}\n\n⁠. Since ⁠3/3⁠ equals 1, multiplication by ⁠3/3⁠ does not change the value of the fraction.\n\nSecond, convert ⁠2/3⁠ into fifteenths by multiplying both the numerator and denominator by five: ⁠\n\n\n\n\n\n\n2\n3\n\n\n\n×\n\n\n\n5\n5\n\n\n\n=\n\n\n\n10\n15\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{3}}\\times {\\tfrac {5}{5}}={\\tfrac {10}{15}}}\n\n⁠.\n\nNow it can be seen that\n\nis equivalent to\n\nThis method can be expressed algebraically:\n\nThis algebraic method always works, thereby guaranteeing that the sum of simple fractions is always again a simple fraction. However, if the single denominators contain a common factor, a smaller denominator than the product of these can be used. For example, when adding \n\n\n\n\n\n\n3\n4\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{4}}}\n\n and \n\n\n\n\n\n\n5\n6\n\n\n\n\n\n{\\displaystyle {\\tfrac {5}{6}}}\n\n the single denominators have a common factor 2, and therefore, instead of the denominator 24 (4 × 6), the halved denominator 12 may be used, not only reducing the denominator in the result, but also the factors in the numerator.\n\nThe smallest possible denominator is given by the least common multiple of the single denominators, which results from dividing the rote multiple by all common factors of the single denominators. This is called the least common denominator.\n\nThe process for subtracting fractions is, in essence, the same as that of adding them: find a common denominator, and change each fraction to an equivalent fraction with the chosen common denominator. The resulting fraction will have that denominator, and its numerator will be the result of subtracting the numerators of the original fractions. For instance,\n\nTo subtract a mixed number, an extra one can be borrowed from the minuend, for instance\n\nTo multiply fractions, multiply the numerators and multiply the denominators. Thus:\n\nTo explain the process, consider one third of one quarter. Using the example of a cake, if three small slices of equal size make up a quarter, and four quarters make up a whole, twelve of these small, equal slices make up a whole. Therefore, a third of a quarter is a twelfth. Now consider the numerators. The first fraction, two thirds, is twice as large as one third. Since one third of a quarter is one twelfth, two thirds of a quarter is two twelfth. The second fraction, three quarters, is three times as large as one quarter, so two thirds of three quarters is three times as large as two thirds of one quarter. Thus two thirds times three quarters is six twelfths.\n\nA short cut for multiplying fractions is called \"cancellation\". Effectively the answer is reduced to lowest terms during multiplication. For example:\n\nA two is a common factor in both the numerator of the left fraction and the denominator of the right and is divided out of both. Three is a common factor of the left denominator and right numerator and is divided out of both.\n\nSince a whole number can be rewritten as itself divided by 1, normal fraction multiplication rules can still apply. For example,\n\nThis method works because the fraction 6/1 means six equal parts, each one of which is a whole.\n\nThe product of mixed numbers can be computed by converting each to an improper fraction.[27] For example:\n\nAlternately, mixed numbers can be treated as sums, and multiplied as binomials. In this example,\n\nTo divide a fraction by a whole number, you may either divide the numerator by the number, if it goes evenly into the numerator, or multiply the denominator by the number. For example, \n\n\n\n\n\n\n10\n3\n\n\n\n÷\n5\n\n\n{\\displaystyle {\\tfrac {10}{3}}\\div 5}\n\n equals \n\n\n\n\n\n\n2\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{3}}}\n\n and also equals \n\n\n\n\n\n\n10\n\n3\n⋅\n5\n\n\n\n\n=\n\n\n\n10\n15\n\n\n\n\n\n{\\displaystyle {\\tfrac {10}{3\\cdot 5}}={\\tfrac {10}{15}}}\n\n, which reduces to \n\n\n\n\n\n\n2\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{3}}}\n\n. To divide a number by a fraction, multiply that number by the reciprocal of that fraction. Thus, \n\n\n\n\n\n\n1\n2\n\n\n\n÷\n\n\n\n3\n4\n\n\n\n=\n\n\n\n1\n2\n\n\n\n×\n\n\n\n4\n3\n\n\n\n=\n\n\n\n\n1\n⋅\n4\n\n\n2\n⋅\n3\n\n\n\n\n=\n\n\n\n2\n3\n\n\n\n\n\n{\\displaystyle {\\tfrac {1}{2}}\\div {\\tfrac {3}{4}}={\\tfrac {1}{2}}\\times {\\tfrac {4}{3}}={\\tfrac {1\\cdot 4}{2\\cdot 3}}={\\tfrac {2}{3}}}\n\n.\n\nTo change a common fraction to a decimal, do a long division of the decimal representations of the numerator by the denominator (this is idiomatically also phrased as \"divide the denominator into the numerator\"), and round the answer to the desired accuracy. For example, to change ⁠1/4⁠ to a decimal, divide 1.00 by 4 (\"4 into 1.00\"), to obtain 0.25. To change ⁠1/3⁠ to a decimal, divide 1.000... by 3 (\"3 into 1.000...\"), and stop when the desired accuracy is obtained, e.g., at 4 decimals with 0.3333. The fraction ⁠1/4⁠ can be written exactly with two decimal digits, while the fraction ⁠1/3⁠ cannot be written exactly as a decimal with a finite number of digits. To change a decimal to a fraction, write in the denominator a 1 followed by as many zeroes as there are digits to the right of the decimal point, and write in the numerator all the digits of the original decimal, just omitting the decimal point. Thus \n\n\n\n12.3456\n=\n\n\n\n123456\n10000\n\n\n\n.\n\n\n{\\displaystyle 12.3456={\\tfrac {123456}{10000}}.}\n\n\n\nDecimal numbers, while arguably more useful to work with when performing calculations, sometimes lack the precision that common fractions have. Sometimes an infinite repeating decimal is required to reach the same precision. Thus, it is often useful to convert repeating decimals into fractions.\n\nA conventional way to indicate a repeating decimal is to place a bar (known as a vinculum) over the digits that repeat, for example 0.789 = 0.789789789... For repeating patterns that begin immediately after the decimal point, the result of the conversion is the fraction with the pattern as a numerator, and the same number of nines as a denominator. For example:\n\nIf leading zeros precede the pattern, the nines are suffixed by the same number of trailing zeros:\n\nIf a non-repeating set of decimals precede the pattern (such as 0.1523987), one may write the number as the sum of the non-repeating and repeating parts, respectively:\n\nThen, convert both parts to fractions, and add them using the methods described above:\n\nAlternatively, algebra can be used, such as below:\n\nIn addition to being of great practical importance, fractions are also studied by mathematicians, who check that the rules for fractions given above are consistent and reliable. Mathematicians define a fraction as an ordered pair \n\n\n\n(\na\n,\nb\n)\n\n\n{\\displaystyle (a,b)}\n\n of integers \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n≠\n0\n,\n\n\n{\\displaystyle b\\neq 0,}\n\n for which the operations addition, subtraction, multiplication, and division are defined as follows:[28]\n\nThese definitions agree in every case with the definitions given above; only the notation is different. Alternatively, instead of defining subtraction and division as operations, the \"inverse\" fractions with respect to addition and multiplication might be defined as:\n\nFurthermore, the relation, specified as\n\nis an equivalence relation of fractions. Each fraction from one equivalence class may be considered as a representative for the whole class, and each whole class may be considered as one abstract fraction. This equivalence is preserved by the above defined operations, i.e., the results of operating on fractions are independent of the selection of representatives from their equivalence class. Formally, for addition of fractions\n\nand similarly for the other operations.\n\nIn the case of fractions of integers, the fractions ⁠a/b⁠ with a and b coprime and b > 0 are often taken as uniquely determined representatives for their equivalent fractions, which are considered to be the same rational number. This way the fractions of integers make up the field of the rational numbers.\n\nMore generally, a and b may be elements of any integral domain R, in which case a fraction is an element of the field of fractions of R. For example, polynomials in one indeterminate, with coefficients from some integral domain D, are themselves an integral domain, call it P. So for a and b elements of P, the generated field of fractions is the field of rational fractions (also known as the field of rational functions).\n\nAn algebraic fraction is the indicated quotient of two algebraic expressions. As with fractions of integers, the denominator of an algebraic fraction cannot be zero. Two examples of algebraic fractions are \n\n\n\n\n\n\n3\nx\n\n\n\nx\n\n2\n\n\n+\n2\nx\n−\n3\n\n\n\n\n\n{\\displaystyle {\\frac {3x}{x^{2}+2x-3}}}\n\n and ⁠\n\n\n\n\n\n\nx\n+\n2\n\n\n\nx\n\n2\n\n\n−\n3\n\n\n\n\n\n{\\displaystyle {\\frac {\\sqrt {x+2}}{x^{2}-3}}}\n\n⁠. Algebraic fractions are subject to the same field properties as arithmetic fractions.\n\nIf the numerator and the denominator are polynomials, as in ⁠\n\n\n\n\n\n\n3\nx\n\n\n\nx\n\n2\n\n\n+\n2\nx\n−\n3\n\n\n\n\n\n{\\displaystyle {\\frac {3x}{x^{2}+2x-3}}}\n\n⁠, the algebraic fraction is called a rational fraction (or rational expression). An irrational fraction is one that is not rational, as, for example, one that contains the variable under a fractional exponent or root, as in ⁠\n\n\n\n\n\n\nx\n+\n2\n\n\n\nx\n\n2\n\n\n−\n3\n\n\n\n\n\n{\\displaystyle {\\frac {\\sqrt {x+2}}{x^{2}-3}}}\n\n⁠.\n\nThe terminology used to describe algebraic fractions is similar to that used for ordinary fractions. For example, an algebraic fraction is in lowest terms if the only factors common to the numerator and the denominator are 1 and −1. An algebraic fraction whose numerator or denominator, or both, contain a fraction, such as ⁠\n\n\n\n\n\n\n1\n+\n\n\n\n1\nx\n\n\n\n\n\n1\n−\n\n\n\n1\nx\n\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {1+{\\tfrac {1}{x}}}{1-{\\tfrac {1}{x}}}}}\n\n⁠, is called a complex fraction.\n\nThe field of rational numbers is the field of fractions of the integers, while the integers themselves are not a field but rather an integral domain. Similarly, the rational fractions with coefficients in a field form the field of fractions of polynomials with coefficient in that field. Considering the rational fractions with real coefficients, radical expressions representing numbers, such as ⁠\n\n\n\n\n\n\n2\n\n\n\n/\n\n2\n\n\n\n{\\displaystyle \\textstyle {\\sqrt {2}}/2}\n\n⁠, are also rational fractions, as are a transcendental numbers such as \n\n\n\nπ\n\n/\n\n2\n,\n\n\n{\\textstyle \\pi /2,}\n\n since all of \n\n\n\n\n\n2\n\n\n,\nπ\n,\n\n\n{\\displaystyle {\\sqrt {2}},\\pi ,}\n\n and \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n are real numbers, and thus considered as coefficients. These same numbers, however, are not rational fractions with integer coefficients.\n\nThe term partial fraction is used when decomposing rational fractions into sums of simpler fractions. For example, the rational fraction \n\n\n\n\n\n\n2\nx\n\n\n\nx\n\n2\n\n\n−\n1\n\n\n\n\n\n{\\displaystyle {\\frac {2x}{x^{2}-1}}}\n\n can be decomposed as the sum of two fractions: ⁠\n\n\n\n\n\n1\n\nx\n+\n1\n\n\n\n+\n\n\n1\n\nx\n−\n1\n\n\n\n\n\n{\\displaystyle {\\frac {1}{x+1}}+{\\frac {1}{x-1}}}\n\n⁠. This is useful for the computation of antiderivatives of rational functions (see partial fraction decomposition for more).\n\nA fraction may also contain radicals in the numerator or the denominator. If the denominator contains radicals, it can be helpful to rationalize it (compare Simplified form of a radical expression), especially if further operations, such as adding or comparing that fraction to another, are to be carried out. It is also more convenient if division is to be done manually. When the denominator is a monomial square root, it can be rationalized by multiplying both the top and the bottom of the fraction by the denominator:\n\nThe process of rationalization of binomial denominators involves multiplying the top and the bottom of a fraction by the conjugate of the denominator so that the denominator becomes a rational number. For example:\n\nEven if this process results in the numerator being irrational, like in the examples above, the process may still facilitate subsequent manipulations by reducing the number of irrationals one has to work with in the denominator.\n\nIn computer displays and typography, simple fractions are sometimes printed as a single character, e.g. ½ (one half). See the article on Number Forms for information on doing this in Unicode.\n\nScientific publishing distinguishes four ways to set fractions, together with guidelines on use:[29]\n\nThe earliest fractions were reciprocals of integers: ancient symbols representing one part of two, one part of three, one part of four, and so on.[32] The Egyptians used Egyptian fractions c. 1000 BC. About 4000 years ago, Egyptians divided with fractions using slightly different methods. They used least common multiples with unit fractions. Their methods gave the same answer as modern methods.[33] The Egyptians also had a different notation for dyadic fractions, used for certain systems of weights and measures.[34]\n\nThe Greeks used unit fractions and (later) simple continued fractions. Followers of the Greek philosopher Pythagoras (c. 530 BC) discovered that the square root of two cannot be expressed as a fraction of integers. (This is commonly though probably erroneously ascribed to Hippasus of Metapontum, who is said to have been executed for revealing this fact.) In 150 BC Jain mathematicians in India wrote the \"Sthananga Sutra\", which contains work on the theory of numbers, arithmetical operations, and operations with fractions.\n\nA modern expression of fractions known as bhinnarasi seems to have originated in India in the work of Aryabhatta (c. AD 500),[citation needed] Brahmagupta (c. 628), and Bhaskara (c. 1150).[35] Their works form fractions by placing the numerators (Sanskrit: amsa) over the denominators (cheda), but without a bar between them.[35] In Sanskrit literature, fractions were always expressed as an addition to or subtraction from an integer.[citation needed] The integer was written on one line and the fraction in its two parts on the next line. If the fraction was marked by a small circle ⟨०⟩ or cross ⟨+⟩, it is subtracted from the integer; if no such sign appears, it is understood to be added. For example, Bhaskara I writes:[36]\n\nwhich is the equivalent of\n\nand would be written in modern notation as 6⁠1/4⁠, 1⁠1/5⁠, and 2 − ⁠1/9⁠ (i.e., 1⁠8/9⁠).\n\nThe horizontal fraction bar is first attested in the work of Al-Hassār (fl. 1200),[35] a Muslim mathematician from Fez, Morocco, who specialized in Islamic inheritance jurisprudence. In his discussion he writes: \"for example, if you are told to write three-fifths and a third of a fifth, write thus, \n\n\n\n\n\n\n3\n\n1\n\n\n5\n\n3\n\n\n\n\n\n{\\displaystyle {\\frac {3\\quad 1}{5\\quad 3}}}\n\n\".[37] The same fractional notation—with the fraction given before the integer[35]—appears soon after in the work of Leonardo Fibonacci in the 13th century.[38]\n\nIn discussing the origins of decimal fractions, Dirk Jan Struik states:[39]\n\nThe introduction of decimal fractions as a common computational practice can be dated back to the Flemish pamphlet De Thiende, published at Leyden in 1585, together with a French translation, La Disme, by the Flemish mathematician Simon Stevin (1548–1620), then settled in the Northern Netherlands. It is true that decimal fractions were used by the Chinese many centuries before Stevin and that the Persian astronomer Al-Kāshī used both decimal and sexagesimal fractions with great ease in his Key to arithmetic (Samarkand, early fifteenth century).[40]\n\nWhile the Persian mathematician Jamshīd al-Kāshī claimed to have discovered decimal fractions himself in the 15th century, J. Lennart Berggren notes that he was mistaken, as decimal fractions were first used five centuries before him by the Baghdadi mathematician Abu'l-Hasan al-Uqlidisi as early as the 10th century.[41][n 2]\n\nIn primary schools, fractions have been demonstrated through Cuisenaire rods, Fraction Bars, fraction strips, fraction circles, paper (for folding or cutting), pattern blocks, pie-shaped pieces, plastic rectangles, grid paper, dot paper, geoboards, counters and computer software.\n\nSeveral states in the United States have adopted learning trajectories from the Common Core State Standards Initiative's guidelines for mathematics education. Aside from sequencing the learning of fractions and operations with fractions, the document provides the following definition of a fraction: \"A number expressible in the form ⁠\n\n\n\na\n\n\n{\\displaystyle a}\n\n/\n\n\n\nb\n\n\n{\\displaystyle b}\n\n⁠ where \n\n\n\na\n\n\n{\\displaystyle a}\n\n is a whole number and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n is a positive whole number. (The word fraction in these standards always refers to a non-negative number.)\"[43] The document itself also refers to negative fractions.\n\nWeisstein, Eric (2003). \"CRC Concise Encyclopedia of Mathematics, Second Edition\". CRC Concise Encyclopedia of Mathematics. Chapman & Hall/CRC. p. 1925. ISBN 1-58488-347-2.\n"
    },
    {
        "title": "Timeline of scientific discoveries",
        "content": "\n\n\nThe timeline below shows the date of publication of possible major scientific breakthroughs, theories and discoveries, along with the discoverer.  This article discounts mere speculation as discovery, although imperfect reasoned arguments, arguments based on elegance/simplicity, and numerically/experimentally verified conjectures qualify (as otherwise no scientific discovery before the late 19th century would count). The timeline begins at the Bronze Age, as it is difficult to give even estimates for the timing of events prior to this, such as of the discovery of counting, natural numbers and arithmetic.\n\nTo avoid overlap with timeline of historic inventions, the timeline does not list examples of documentation for manufactured substances and devices unless they reveal a more fundamental leap in the theoretical ideas in a field.\n\nMany early innovations of the Bronze Age were prompted by the increase in trade, and this also applies to the scientific advances of this period. For context, the major civilizations of this period are Egypt, Mesopotamia, and the Indus Valley, with Greece rising in importance towards the end of the third millennium BC. The Indus Valley script remains undeciphered and there are very little surviving fragments of its writing, thus any inference about scientific discoveries in that region must be made based only on archaeological digs. The following dates are approximations.\n\nThe following dates are approximations.\n\nThe following dates are approximations.\n\nMathematics and astronomy flourish during the Golden Age of India (4th to 6th centuries AD) under the Gupta Empire. Meanwhile, Greece and its colonies have entered the Roman period in the last few decades of the preceding millennium, and Greek science is negatively impacted by the Fall of the Western Roman Empire and the economic decline that follows.\n\nThe Golden Age of Indian mathematics and astronomy continues after the end of the Gupta empire, especially in Southern India during the era of the Rashtrakuta, Western Chalukya and Vijayanagara empires of Karnataka, which variously patronised Hindu and Jain mathematicians. In addition, the Middle East enters the Islamic Golden Age through contact with other civilisations, and China enters a golden period during the Tang and Song dynasties.\n\nThe Scientific Revolution occurs in Europe around this period, greatly accelerating the progress of science and contributing to the rationalization of the natural sciences. \n"
    },
    {
        "title": "Foundations of mathematics",
        "content": "Foundations of mathematics are the logical and mathematical framework that allows the development of mathematics without generating self-contradictory theories, and, in particular, to have reliable concepts of theorems, proofs, algorithms, etc. This may also include the philosophical study of the relation of this framework with reality.[1]\n\nThe term \"foundations of mathematics\" was not coined before the end of the 19th century, although foundations were first established by the ancient Greek philosophers under the name of Aristotle's logic and systematically applied in Euclid's Elements. A mathematical assertion is considered as truth only if it is a theorem that is proved from true premises by means of a sequence of syllogisms (inference rules), the premises being either already proved theorems or self-evident assertions called axioms or postulates.\n\nThese foundations were tacitly assumed to be definitive until the introduction of infinitesimal calculus by Isaac Newton and Gottfried Wilhelm Leibniz in the 17th century. This new area of mathematics involved new methods of reasoning and new basic concepts (continuous functions, derivatives, limits) that were not well founded, but had astonishing consequences, such as the deduction from Newton's law of gravitation that the orbits of the planets are ellipses. \n\nDuring the 19th century, progress was made towards elaborating precise definitions of the basic concepts of infinitesimal calculus, notably the natural and real numbers. This led, near the end of the 19th century, to a series of seemingly paradoxical mathematical results that challenged the general confidence in the reliability and truth of mathematical results. This has been called the foundational crisis of mathematics. \n\nThe resolution of this crisis involved the rise of a new mathematical discipline called mathematical logic that includes set theory, model theory, proof theory, computability and computational complexity theory, and more recently, parts of computer science. Subsequent discoveries in the 20th century then stabilized the foundations of mathematics into a coherent framework valid for all mathematics. This framework is based on a systematic use of axiomatic method and on set theory, specifically ZFC, the Zermelo–Fraenkel set theory with the axiom of choice.\n\nIt results from this that the basic mathematical concepts, such as numbers, points, lines, and geometrical spaces are not defined as abstractions from reality but from basic properties (axioms). Their adequation with their physical origins does not belong to mathematics anymore, although their relation with reality is still used for guiding mathematical intuition: physical reality is still used by mathematicians to choose axioms, find which theorems are interesting to prove, and obtain indications of possible proofs.\n\nMost civilisations developed some mathematics, mainly for practical purposes, such as counting (merchants), surveying (delimitation of fields), prosody, astronomy, and astrology. It seems that ancient Greek philosophers were the first to study the nature of mathematics and its relation with the real world.\n\nZeno of Elea (490 –  c. 430 BC) produced several paradoxes he used to support his thesis that movement does not exist. These paradoxes involve mathematical infinity, a concept that was outside the mathematical foundations of that time and was not well understood before the end of the 19th century.\n\nThe Pythagorean school of mathematics originally insisted that the only numbers are natural numbers and ratios of natural numbers. The discovery (around 5th century BC) that the ratio of the diagonal of a square to its side is not the ratio of two natural numbers was a shock to them which they only reluctantly accepted. A testimony of this is the modern terminology of irrational number for referring to a number that is not the quotient of two integers, since \"irrational\" means originally \"not reasonable\" or \"not accessible with reason\".\n\nThe fact that length ratios are not represented by rational numbers was resolved by Eudoxus of Cnidus (408–355 BC), a student of Plato, who reduced the comparison of two irrational ratios to comparisons of integer multiples of the magnitudes involved. His method anticipated that of Dedekind cuts in the modern definition of real numbers by Richard Dedekind (1831–1916);[2] see Eudoxus of Cnidus § Eudoxus' proportions.\n\nIn the Posterior Analytics, Aristotle (384–322 BC) laid down the logic for organizing a field of knowledge by means of primitive concepts, axioms, postulates, definitions, and theorems. Aristotle took a majority of his examples for this from arithmetic and from geometry, and his logic served as the foundation of mathematics for centuries. This method resembles the modern axiomatic method but with a big philosophical difference: axioms and postulates were supposed to be true, being either self-evident or resulting from experiments, while no other truth than the correctness of the proof is involved in the axiomatic method. So, for Aristotle, a proved theorem is true, while in the axiomatic methods, the proof says only that the axioms imply the statement of the theorem.\n\nAristotle's logic reached its high point with Euclid's Elements (300 BC), a treatise on mathematics structured with very high standards of rigor: Euclid justifies each proposition by a demonstration in the form of chains of syllogisms (though they do not always conform strictly to Aristotelian templates).\nAristotle's syllogistic logic, together with its exemplification by Euclid's Elements, are recognized as scientific achievements of ancient Greece, and remained as the foundations of mathematics for centuries.\n\nDuring Middle Ages, Euclid's Elements stood as a perfectly solid foundation for mathematics, and philosophy of mathematics concentrated on the ontological status of mathematical concepts; the question was whether they exist independently of perception (realism) or within the mind only (conceptualism); or even whether they are simply names of collection of individual objects (nominalism). \n\nIn Elements, the only numbers that are considered are natural numbers and ratios of lengths. This geometrical view of non-integer numbers remained dominant until the end of Middle Ages, although the rise of algebra led to consider them independently from geometry, which implies implicitly that there are foundational primitives of mathematics. For example, the transformations of equations introduced by Al-Khwarizmi and the cubic and quartic formulas discovered in the 16th century result from algebraic manipulations that have no geometric counterpart.\n\nNevertheless, this did not challenge the classical foundations of mathematics since all properties of numbers that were used can be deduced from their geometrical definition.\n\nIn 1637, René Descartes published La Géométrie, in which he showed that geometry can be reduced to algebra by means coordinates, which are numbers determining the position of a point. This gives to the numbers that he called real numbers a more foundational role (before him, numbers were defined as the ratio of two lengths). Descartes' book became famous after 1649 and paved the way to infinitesimal calculus.\n\nIsaac Newton (1642–1727) in England and Leibniz (1646–1716) in Germany independently developed the infinitesimal calculus for dealing with mobile points (such as planets in the sky) and variable quantities.\n\nThis needed the introduction of new concepts such as continuous functions, derivatives and limits. For dealing with these concepts in a logical way, they were defined in terms of infinitesimals that are hypothetical numbers that are infinitely close to zero. The strong implications of infinitesimal calculus on foundations of mathematics is illustrated by a pamphlet of the Protestant philosopher George Berkeley (1685–1753), who wrote \"[Infinitesimals] are neither finite quantities, nor quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?\".[3]\n\nAlso, a lack of rigor has been frequently invoked, because infinitesimals and the associated concepts were not formally defined (lines and planes were not formally defined either, but people were more accustomed to them). Real numbers, continuous functions, derivatives were not formally defined before the 19th century, as well as Euclidean geometry. It is only in the 20th century that a formal definition of infinitesimals has been given, with the proof that the whole infinitesimal can be deduced from them.\n\nDespite its lack of firm logical foundations, infinitesimal calculus was quickly adopted by mathematicians, and validated by its numerous applications; in particular the fact that the planet trajectories can be deduced from the Newton's law of gravitation.\n\nIn the 19th century, mathematics developed quickly in many directions. Several of the problems that were considered led to questions on the foundations of mathematics. Frequently, the proposed solutions led to further questions that were often simultaneously of philosophical and mathematical nature. All these questions led, at the end of the 19th century and the beginning of the 20th century, to debates which have been called the foundational crisis of mathematics. The following subsections describe the main such foundational problems revealed during the 19th century.\n\nCauchy (1789–1857) started the project of giving rigorous bases to infinitesimal calculus. In particular, he rejected the heuristic principle that he called the generality of algebra, which consisted to apply properties of algebraic operations to infinite sequences without proper proofs. In his Cours d'Analyse (1821), he considered very small quantities, which could presently be called \"sufficiently small quantities\"; that is, a sentence such that \"if x is very small then ...\" must be understood as \"there is a (sufficiently large) natural number n such that |x| < 1/n\". In the proofs he used this in a way that predated the modern (ε, δ)-definition of limit.[4]\n\nThe modern (ε, δ)-definition of limits and continuous functions was first developed by Bolzano in 1817, but remained relatively unknown, and Cauchy probably did know Bolzano's work.\n\nKarl Weierstrass (1815–1897) formalized and popularized the (ε, δ)-definition of limits, and discovered some pathological functions that seemed paradoxical at this time, such as continuous, nowhere-differentiable functions. Indeed, such functions contradict previous conceptions of a function as a rule for computation or a smooth graph. \n\nAt this point, the program of arithmetization of analysis (reduction of mathematical analysis to arithmetic and algebraic operations) advocated by Weierstrass was essentially completed, except for two points.\n\nFirstly, a formal definition of real numbers was still lacking. Indeed, beginning with Richard Dedekind in 1858, several mathematicians worked on the definition of the real numbers, including Hermann Hankel, Charles Méray, and Eduard Heine, but this is only in 1872 that two independent complete definitions of real numbers were published: one by Dedekind, by means of Dedekind cuts; the other one by Georg Cantor as equivalence classes of Cauchy sequences.[5]\n\nSeveral problems were left open by these definitions, which contributed to the foundational crisis of mathematics. Firstly both definitions suppose that rational numbers and thus natural numbers are rigorously defined; this was done a few years later with Peano axioms. Secondly, both definitions involve infinite sets (Dedekind cuts and sets of the elements of a Cauchy sequence), and Cantor's set theory was published several years later.\n\nThe third problem is more subtle: and is related to the foundations of logic: classical logic is a first order logic; that is, quantifiers apply to variables representing individual elements, not to variables representing (infinite) sets of elements. The basic property of the completeness of the real numbers that is required for defining and using real numbers involves a quantification on infinite sets. Indeed, this property may be expressed either as for every infinite sequence of real numbers, if it is a Cauchy sequence, it has a limit that is a real number, or as every subset of the real numbers that is bounded has a least upper bound that is a real number. This need of quantification over infinite sets is one of the motivation of the development of higher-order logics during the first half of the 20th century.\n\nBefore the 19th century, there were many failed attempts to derive the parallel postulate from other axioms of geometry. In an attempt to prove that its negation leads to a contradiction, Johann Heinrich Lambert (1728–1777) started to build hyperbolic geometry and introduced the hyperbolic functions and computed the area of a hyperbolic triangle (where the sum of angles is less than 180°). \n\nContinuing the construction of this new geometry, several mathematicians proved independently that if it is inconsistent, then Euclidean geometry is also inconsistent and thus that the parallel postulate cannot be proved. This was proved by Nikolai Lobachevsky in 1826, János Bolyai (1802–1860) in 1832 and Carl Friedrich Gauss (unpublished).\n\nLater in the 19th century, the German mathematician Bernhard Riemann developed Elliptic geometry, another non-Euclidean geometry where no parallel can be found and the sum of angles in a triangle is more than 180°. It was proved consistent by defining points as pairs of antipodal points on a sphere (or hypersphere), and lines as great circles on the sphere. \n\nThese proofs of unprovability of the parallel postulate lead to several philosophical problems, the main one being that before this discovery, the parallel postulate and all its consequences were considered as true. So, the non-Euclidean geometries challenged the concept of mathematical truth.\n\nSince the introduction of analytic geometry by René Descartes in the 17th century, there were two approaches to geometry, the old one called synthetic geometry, and the new one, where everything is specified in terms of real numbers called coordinates.\n\nMathematicians did not worry much about the contradiction between these two approaches before the mid-nineteenth century, where there was \"an acrimonious controversy between the proponents of synthetic and analytic methods in projective geometry, the two sides accusing each other of mixing projective and metric concepts\".[6] Indeed, there is no concept of distance in a projective space, and the cross-ratio, which is a number, is a basic concept of synthetic projective geometry. \n\nKarl von Staudt developed a purely geometric approach to this problem by introducing \"throws\" that form what is presently called a field, in which the cross ratio can be expressed.\n\nApparently, the problem of the equivalence between analytic and synthetic approach was completely solved only with Emil Artin's book Geometric Algebra published in 1957. It was well known that, given a field k, one may define affine and projective spaces over k in terms of k-vector spaces. In these spaces, the Pappus hexagon theorem holds. Conversely, if the Pappus hexagon theorem is included in the axioms of a plane geometry, then one can define a field k such that the geometry is the same as the affine or projective geometry over k.\n\nThe work of making rigorous real analysis and the definition of real numbers, consisted of reducing everything to rational numbers and thus to natural numbers, since positive rational numbers are fractions of natural numbers. There was therefore a need of a formal definition of natural numbers, which imply as axiomatic theory of arithmetic. This was started with Charles Sanders Peirce in 1881 and Richard Dedekind in 1888, who defined a natural numbers as the cardinality of a finite set.[citation needed]. However, this involves set theory, which was not formalized at this time.\n\nGiuseppe Peano provided in 1888 a complete axiomatisation based on the ordinal property of the natural numbers. The last Peano's axiom is the only one that induces logical difficulties, as it begin with either \"if S is a set then\" or \"if \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n is a predicate then\". So, Peano's axioms induce a quantification on infinite sets, and this means that Peano arithmetic is what is presently called a Second-order logic.\n\nThis was not well understood at that times, but the fact that infinity occurred in the definition of the natural numbers was a problem for many mathematicians of this time. For example, Henri Poincaré stated that axioms can only be demonstrated in their finite application, and concluded that it is \"the power of the mind\" which allows conceiving of the indefinite repetition of the same act.[7] This applies in particular to the use of the last Peano axiom for showing that the successor function generates all natural numbers. Also, Leopold Kronecker said \"God made the integers, all else is the work of man\".[a] This may be interpreted as \"the integers cannot be mathematically defined\".\n\nBefore the second half of the 19th century, infinity was a philosophical concept that did not belong to mathematics. However, with the rise of infinitesimal calculus, mathematicians became to be accustomed  to infinity, mainly through potential infinity, that is, as the result of an endless process, such as the definition of an infinite sequence, an infinite series or a limit. The possibility of an actual infinity was the subject of many philosophical disputes.\n\nSets, and more specially infinite sets were not considered as a mathematical concept; in particular, there was no fixed term for them. A dramatic change arose with the work of Georg Cantor who was the first mathematician to systematically study infinite sets. In particular, he introduced cardinal numbers that measure the size of infinite sets, and ordinal numbers that, roughly speaking, allow one to continue to count after having reach infinity. One of his major results is the discovery that there are strictly more real numbers than natural numbers (the cardinal of the continuum of the real numbers is greater than that of the natural numbers).\n\nThese results were rejected by many mathematicians and philosophers, and led to debates that are a part of the foundational crisis of mathematics.\n\nThe crisis was amplified with the Russel's paradox that asserts that the phrase \"the set of all sets\" is self-contradictory. This condradiction introduced a doubt on the consistency of all mathematics.\n\nWith the introduction of the Zermelo–Fraenkel set theory (c. 1925) and its adoption by the mathematical community, the doubt about the consistency was essentially removed, although consistency of set theory cannot be proved because of Gödel's incompleteness theorem.\n\nIn 1847, De Morgan published his laws and George Boole devised an algebra, now called Boolean algebra,   that allows expressing Aristotle's logic in terms of formulas and algebraic operations. Boolean algebra is the starting point of mathematization  logic and the basis of propositional calculus\n\nIndependently, in the 1870's, Charles Sanders Peirce and Gottlob Frege extended propositional calculus by introducing  quantifiers, for building predicate logic.\n\nFrege pointed out three desired properties of a logical theory:[citation needed]consistency (impossibility of proving contradictory statements), completeness (any statement is either provable or refutable; that is, its negation is provable), and decidability (there is a decision procedure to test every statement).\n\nBy near the turn of the century, Bertrand Russell popularized Frege's work and discovered Russel's paradox which implies that the phrase \"the set of all sets\" is self-contradictory. This paradox seemed to make the whole mathematics inconsistent  and is one of the major causes of the foundational crisis of mathematics.\n\nThe foundational crisis of mathematics\narose at the end of the 19th century and the beginning of the 20th century with the discovery of several paradoxes or counter-intuitive results. \n\nThe first one was the proof that the parallel postulate cannot be proved. This results from a construction of a non-Euclidean geometry inside Euclidean geometry, whose inconsistency would imply the inconsistency of Euclidean geometry. A well known paradox is Russell's paradox, which shows that the phrase \"the set of all sets that do not contain themselves\" is self-contradictory. Other philosophical problems were the proof of the existence of mathematical objects that cannot be computed or explicitly described, and the proof of the existence of theorems of arithmetic that cannot be proved with Peano arithmetic. \n\nSeveral schools of philosophy of mathematics were challenged with these problems in the 20th century, and are described below. \n\nThese problems were also studied by mathematicians, and this led to establish mathematical logic as a new area of mathematics, consisting of providing mathematical definitions to logics (sets of inference rules), mathematical and logical theories, theorems, and proofs, and of using mathematical methods to prove theorems about these concepts. \n\nThis led to unexpected results, such as Gödel's incompleteness theorems, which, roughly speaking, assert that, if a theory contains the standard arithmetic, it cannot be used to prove that it itself is not self-contradictory; and, if it is not self-contradictory, there are theorems that cannot be proved inside the theory, but are nevertheless true in some technical sense. \n\nZermelo–Fraenkel set theory with the axiom of choice (ZFC) is a logical theory established by Ernst Zermelo and Abraham Fraenkel. It became the standard foundation of modern mathematics, and, unless the contrary is explicitly specified, it is used in all modern mathematical texts, generally implicitly.\n\nSimultaneously, the axiomatic method became a de facto standard: the proof of a theorem must result from explicit axioms and previously proved theorems by the application of clearly defined inference rules. The axioms need not correspond to some reality. Nevertheless, it is an open philosophical problem to explain why the axiom systems that lead to rich and useful theories are those resulting from abstraction from the physical reality or other mathematical theory.\n\nIn summary, the foundational crisis is essentially resolved, and this opens new philosophical problems. In particular, it cannot be proved that the new foundation (ZFC) is not self-contradictory. It is a general consensus that, if this would happen, the problem could be solved by a mild modification of ZFC. \n\nWhen the foundational crisis arose, there was much debate among mathematicians and logicians about what should be done for restoring confidence in mathematics. This involved philosophical questions about mathematical truth, the relationship of mathematics with reality, the reality of mathematical objects, and the nature of mathematics.\n\nFor the problem of foundations, there was two main options for trying to avoid paradoxes. The first one led to intuitionism and constructivism, and consisted to restrict the logical rules for remaining closer to intuition, while the second, which has been called formalism, considers that a theorem is true if it can be deduced from axioms by applying inference rules (formal proof), and that no \"trueness\" of the axioms is needed for the validity of a theorem.\n\nIt has been claimed[by whom?] that formalists, such as David Hilbert (1862–1943), hold that mathematics is only a language and a series of games. Hilbert insisted that formalism, called \"formula game\" by him, is a fundamental part of mathematics, but that mathematics must not be reduced to formalism. Indeed, he used the words \"formula game\" in his 1927 response to L. E. J. Brouwer's criticisms:\n\nAnd to what extent has the formula game thus made possible been successful? This formula game enables us to express the entire thought-content of the science of mathematics in a uniform manner and develop it in such a way that, at the same time, the interconnections between the individual propositions and facts become clear ... The formula game that Brouwer so deprecates has, besides its mathematical value, an important general philosophical significance. For this formula game is carried out according to certain definite rules, in which the technique of our thinking is expressed. These rules form a closed system that can be discovered and definitively stated.[10]\nThus Hilbert is insisting that mathematics is not an arbitrary game with arbitrary rules; rather it must agree with how our thinking, and then our speaking and writing, proceeds.[10]\n\nWe are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather, it is a conceptual system possessing internal necessity that can only be so and by no means otherwise.[11]\nThe foundational philosophy of formalism, as exemplified by David Hilbert, is a response to the paradoxes of set theory, and is based on formal logic. Virtually all mathematical theorems today can be formulated as theorems of set theory. The truth of a mathematical statement, in this view, is represented by the fact that the statement can be derived from the axioms of set theory using the rules of formal logic.\n\nMerely the use of formalism alone does not explain several issues: why we should use the axioms we do and not some others, why we should employ the logical rules we do and not some others, why \"true\" mathematical statements (e.g., the laws of arithmetic) appear to be true, and so on. Hermann Weyl posed these very questions to Hilbert:\n\nWhat \"truth\" or objectivity can be ascribed to this theoretic construction of the world, which presses far beyond the given, is a profound philosophical problem. It is closely connected with the further question: what impels us to take as a basis precisely the particular axiom system developed by Hilbert? Consistency is indeed a necessary but not a sufficient condition. For the time being we probably cannot answer this question ...[12]\nIn some cases these questions may be sufficiently answered through the study of formal theories, in disciplines such as reverse mathematics and computational complexity theory. As noted by Weyl, formal logical systems also run the risk of inconsistency; in Peano arithmetic, this arguably has already been settled with several proofs of consistency, but there is debate over whether or not they are sufficiently finitary to be meaningful. Gödel's second incompleteness theorem establishes that logical systems of arithmetic can never contain a valid proof of their own consistency. What Hilbert wanted to do was prove a logical system S was consistent, based on principles P that only made up a small part of S. But Gödel proved that the principles P could not even prove P to be consistent, let alone S.\n\nIntuitionists, such as L. E. J. Brouwer (1882–1966), hold that mathematics is a creation of the human mind. Numbers, like fairy tale characters, are merely mental entities, which would not exist if there were never any human minds to think about them.\n\nThe foundational philosophy of intuitionism or constructivism, as exemplified in the extreme by Brouwer and Stephen Kleene, requires proofs to be \"constructive\" in nature –  the existence of an object must be demonstrated rather than inferred from a demonstration of the impossibility of its non-existence. For example, as a consequence of this the form of proof known as reductio ad absurdum is suspect.\n\nSome modern theories in the philosophy of mathematics deny the existence of foundations in the original sense. Some theories tend to focus on mathematical practice, and aim to describe and analyze the actual working of mathematicians as a social group. Others try to create a cognitive science of mathematics, focusing on human cognition as the origin of the reliability of mathematics when applied to the real world. These theories would propose to find foundations only in human thought, not in any objective outside construct. The matter remains controversial.\n\nLogicism is a school of thought, and research programme, in the philosophy of mathematics, based on the thesis that mathematics is an extension of logic or that some or all mathematics may be derived in a suitable formal system whose axioms and rules of inference are 'logical' in nature. Bertrand Russell and Alfred North Whitehead championed this theory initiated by Gottlob Frege and influenced by Richard Dedekind.\n\nMany researchers in axiomatic set theory have subscribed to what is known as set-theoretic Platonism, exemplified by Kurt Gödel.\n\nSeveral set theorists followed this approach and actively searched for axioms that may be considered as true for heuristic reasons and that would decide the continuum hypothesis. Many large cardinal axioms were studied, but the hypothesis always remained independent from them and it is now considered unlikely that CH can be resolved by a new large cardinal axiom. Other types of axioms were considered, but none of them has reached consensus on the continuum hypothesis yet. Recent work by Hamkins proposes a more flexible alternative: a set-theoretic multiverse allowing free passage between set-theoretic universes that satisfy the continuum hypothesis and other universes that do not.\n\nThis argument by Willard Quine and Hilary Putnam says (in Putnam's shorter words),\n\n... quantification over mathematical entities is indispensable for science ... therefore we should accept such quantification; but this commits us to accepting the existence of the mathematical entities in question.\nHowever, Putnam was not a Platonist.\n\nFew mathematicians are typically concerned on a daily, working basis over logicism, formalism or any other philosophical position. Instead, their primary concern is that the mathematical enterprise as a whole always remains productive. Typically, they see this as ensured by remaining open-minded, practical and busy; as potentially threatened by becoming overly-ideological, fanatically reductionistic or lazy.\n\nSuch a view has also been expressed by some well-known physicists.\n\nFor example, the Physics Nobel Prize laureate Richard Feynman said\n\nPeople say to me, \"Are you looking for the ultimate laws of physics?\" No, I'm not ... If it turns out there is a simple ultimate law which explains everything, so be it – that would be very nice to discover. If it turns out it's like an onion with millions of layers ... then that's the way it is. But either way there's Nature and she's going to come out the way She is. So therefore when we go to investigate we shouldn't predecide what it is we're looking for only to find out more about it.[13]\nAnd Steven Weinberg:[14]\n\nThe insights of philosophers have occasionally benefited physicists, but generally in a negative fashion – by protecting them from the preconceptions of other philosophers. ... without some guidance from our preconceptions one could do nothing at all. It is just that philosophical principles have not generally provided us with the right preconceptions.\nWeinberg believed that any undecidability in mathematics, such as the continuum hypothesis, could be potentially resolved despite the incompleteness theorem, by finding suitable further axioms to add to set theory.\n\nGödel's completeness theorem establishes an equivalence in first-order logic between the formal provability of a formula and its truth in all possible models. Precisely, for any consistent first-order theory it gives an \"explicit construction\" of a model described by the theory; this model will be countable if the language of the theory is countable. However this \"explicit construction\" is not algorithmic. It is based on an iterative process of completion of the theory, where each step of the iteration consists in adding a formula to the axioms if it keeps the theory consistent; but this consistency question is only semi-decidable (an algorithm is available to find any contradiction but if there is none this consistency fact can remain unprovable).\n\nThe following lists some notable results in metamathematics. Zermelo–Fraenkel set theory is the most widely studied axiomatization of set theory. It is abbreviated ZFC when it includes the axiom of choice and ZF when the axiom of choice is excluded.\n\nStarting in 1935, the Bourbaki group of French mathematicians started publishing a series of books to formalize many areas of mathematics on the new foundation of set theory.\n\nThe intuitionistic school did not attract many adherents, and it was not until Bishop's work in 1967 that constructive mathematics was placed on a sounder footing.[16]\n\nOne may consider that Hilbert's program has been partially completed, so that the crisis is essentially resolved, satisfying ourselves with lower requirements than Hilbert's original ambitions. His ambitions were expressed in a time when nothing was clear: it was not clear whether mathematics could have a rigorous foundation at all.\n\nThere are many possible variants of set theory, which differ in consistency strength, where stronger versions (postulating higher types of infinities) contain formal proofs of the consistency of weaker versions, but none contains a formal proof of its own consistency. Thus the only thing we do not have is a formal proof of consistency of whatever version of set theory we may prefer, such as ZF.\n\nIn practice, most mathematicians either do not work from axiomatic systems, or if they do, do not doubt the consistency of ZFC, generally their preferred axiomatic system. In most of mathematics as it is practiced, the incompleteness and paradoxes of the underlying formal theories never played a role anyway, and in those branches in which they do or whose formalization attempts would run the risk of forming inconsistent theories (such as logic and category theory), they may be treated carefully.\n\nThe development of category theory in the middle of the 20th century showed the usefulness of set theories guaranteeing the existence of larger classes than does ZFC, such as Von Neumann–Bernays–Gödel set theory or Tarski–Grothendieck set theory, albeit that in very many cases the use of large cardinal axioms or Grothendieck universes is formally eliminable.\n\nOne goal of the reverse mathematics program is to identify whether there are areas of \"core mathematics\" in which foundational issues may again provoke a crisis.\n"
    },
    {
        "title": "Axiomatic system",
        "content": "In mathematics and logic, an axiomatic system is any set of primitive notions and axioms to logically derive theorems. A theory is a consistent, relatively-self-contained body of knowledge which usually contains an axiomatic system and all its derived theorems. An axiomatic system that is completely described is a special kind of formal system. A formal theory is an axiomatic system (usually formulated within model theory) that describes a set of sentences that is closed under logical implication.[1] A formal proof is a complete rendition of a mathematical proof within a formal system.\n\nAn axiomatic system is said to be consistent if it lacks contradiction. That is, it is impossible to derive both a statement and its negation from the system's axioms. Consistency is a key requirement for most axiomatic systems, as the presence of contradiction would allow any statement to be proven (principle of explosion).\n\nIn an axiomatic system, an axiom is called independent if it cannot be proven or disproven from other axioms in the system. A system is called independent if each of its underlying axioms is independent. Unlike consistency, independence is not a necessary requirement for a functioning axiomatic system — though it is usually sought after to minimize the number of axioms in the system.\n\nAn axiomatic system is called complete if for every statement, either itself or its negation is derivable from the system's axioms (equivalently, every statement is capable of being proven true or false).[2]\n\nBeyond consistency, relative consistency is also the mark of a worthwhile axiom system. This describes the scenario where the undefined terms of a first axiom system are provided definitions from a second, such that the axioms of the first are theorems of the second.\n\nA good example is the relative consistency of absolute geometry with respect to the theory of the real number system.  Lines and points are undefined terms (also called primitive notions) in absolute geometry, but assigned meanings in the theory of real numbers in a way that is consistent with both axiom systems.[citation needed]\n\nA model for an axiomatic system is a well-defined set, which assigns meaning for the undefined terms presented in the system, in a manner that is correct with the relations defined in the system. The existence of a concrete model proves the consistency of a system[disputed – discuss]. A model is called concrete if the meanings assigned are objects and relations from the real world[clarification needed], as opposed to an abstract model which is based on other axiomatic systems.\n\nModels can also be used to show the independence of an axiom in the system. By constructing a valid model for a subsystem without a specific axiom, we show that the omitted axiom is independent if its correctness does not necessarily follow from the subsystem.\n\nTwo models are said to be isomorphic if a one-to-one correspondence can be found between their elements, in a manner that preserves their relationship.[3] An axiomatic system for which every model is isomorphic to another is called categorial (sometimes categorical). The property of categoriality (categoricity) ensures the completeness of a system, however the converse is not true: Completeness does not ensure the categoriality (categoricity) of a system, since two models can differ in properties that cannot be expressed by the semantics of the system.\n\nAs an example, observe the following axiomatic system, based on first-order logic with additional semantics of the following countably infinitely many axioms added (these can be easily formalized as an axiom schema):\n\nInformally, this infinite set of axioms states that there are infinitely many different items. However, the concept of an infinite set cannot be defined within the system — let alone the cardinality of such a set.\n\nThe system has at least two different models – one is the natural numbers (isomorphic to any other countably infinite set), and another is the real numbers (isomorphic to any other set with the cardinality of the continuum). In fact, it has an infinite number of models, one for each cardinality of an infinite set. However, the property distinguishing these models is their cardinality — a property which cannot be defined within the system. Thus the system is not categorial. However it can be shown to be complete.\n\nStating definitions and propositions in a way such that each new term can be formally eliminated by the priorly introduced terms requires primitive notions (axioms) to avoid infinite regress. This way of doing mathematics is called the axiomatic method.[4]\n\nA common attitude towards the axiomatic method is logicism. In their book Principia Mathematica, Alfred North Whitehead and Bertrand Russell attempted to show that all mathematical theory could be reduced to some collection of axioms. More generally, the reduction of a body of propositions to a particular collection of axioms underlies the mathematician's research program. This was very prominent in the mathematics of the twentieth century, in particular in subjects based around homological algebra.\n\nThe explication of the particular axioms used in a theory can help to clarify a suitable level of abstraction that the mathematician would like to work with. For example, mathematicians opted that rings need not be commutative, which differed from Emmy Noether's original formulation. Mathematicians decided to consider topological spaces more generally without the separation axiom which Felix Hausdorff originally formulated.\n\nThe Zermelo–Fraenkel set theory, a result of the axiomatic method applied to set theory, allowed the \"proper\" formulation of set-theory problems and helped avoid the paradoxes of naïve set theory. One such problem was the continuum hypothesis. Zermelo–Fraenkel set theory, with the historically controversial axiom of choice included, is commonly abbreviated ZFC, where \"C\" stands for \"choice\". Many authors use ZF to refer to the axioms of Zermelo–Fraenkel set theory with the axiom of choice excluded.[5] Today ZFC is the standard form of axiomatic set theory and as such is the most common foundation of mathematics.\n\nMathematical methods developed to some degree of sophistication in ancient Egypt, Babylon, India, and China, apparently without employing the axiomatic method.\n\nEuclid of Alexandria authored the earliest extant axiomatic presentation of Euclidean geometry and number theory. His idea begins with five undeniable geometric assumptions called axioms. Then, using these axioms, he established the truth of other propositions by proofs, hence the axiomatic method.[6]\n\nMany axiomatic systems were developed in the nineteenth century, including non-Euclidean geometry, the foundations of real analysis, Cantor's set theory, Frege's work on foundations, and Hilbert's 'new' use of axiomatic method as a research tool. For example, group theory was first put on an axiomatic basis towards the end of that century. Once the axioms were clarified (that inverse elements should be required, for example), the subject could proceed autonomously, without reference to the transformation group origins of those studies.\n\nNot every consistent body of propositions can be captured by a describable collection of axioms. In recursion theory, a collection of axioms is called recursive if a computer program can recognize whether a given proposition in the language is a theorem. Gödel's first incompleteness theorem then tells us that there are certain consistent bodies of propositions with no recursive axiomatization. Typically, the computer can recognize the axioms and logical rules for deriving theorems, and the computer can recognize whether a proof is valid, but to determine whether a proof exists for a statement is only soluble by \"waiting\" for the proof or disproof to be generated. The result is that one will not know which propositions are theorems and the axiomatic method breaks down. An example of such a body of propositions is the theory of the natural numbers, which is only partially axiomatized by the Peano axioms (described below).\n\nIn practice, not every proof is traced back to the axioms. At times, it is not even clear which collection of axioms a proof appeals to. For example, a number-theoretic statement might be expressible in the language of arithmetic (i.e. the language of the Peano axioms) and a proof might be given that appeals to topology or complex analysis. It might not be immediately clear whether another proof can be found that derives itself solely from the Peano axioms.\n\nAny more-or-less arbitrarily chosen system of axioms is the basis of some mathematical theory, but such an arbitrary axiomatic system will not necessarily be free of contradictions, and even if it is, it is not likely to shed light on anything. Philosophers of mathematics sometimes assert that mathematicians choose axioms \"arbitrarily\", but it is possible that although they may appear arbitrary when viewed only from the point of view of the canons of deductive logic, that appearance is due to a limitation on the purposes that deductive logic serves.\n\nThe mathematical system of natural numbers  0, 1, 2, 3, 4, ... is based on an axiomatic system first devised by the mathematician Giuseppe Peano in 1889. He chose the axioms, in the language of a single unary function symbol S (short for \"successor\"), for the set of natural numbers to be:\n\nIn mathematics, axiomatization is the process of taking a body of knowledge and working backwards towards its axioms. It is the formulation of a system of statements (i.e. axioms) that relate a number of primitive terms — in order that a consistent body of propositions may be derived deductively from these statements. Thereafter, the proof of any proposition should be, in principle, traceable back to these axioms.\n"
    },
    {
        "title": "Mathematics Subject Classification",
        "content": "The Mathematics Subject Classification (MSC) is an alphanumerical classification scheme that has collaboratively been produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2020.\n\nThe MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.\n\nThe first level is represented by a two-digit number, the second by a letter, and the third by another two-digit number. For example:\n\nAt the top level, 64 mathematical disciplines are labeled with a unique two-digit number. In addition to the typical areas of mathematical research, there are top-level categories for \"History and Biography\", \"Mathematics Education\", and for the overlap with different sciences. Physics (i.e. mathematical physics) is particularly well represented in the classification scheme with a number of different categories including:\n\nAll valid MSC classification codes must have at least the first-level identifier.\n\nThe second-level codes are a single letter from the Latin alphabet. These represent specific areas covered by the first-level discipline. The second-level codes vary from discipline to discipline.\n\nFor example, for differential geometry, the top-level code is 53, and the second-level codes are:\n\nIn addition, the special second-level code \"-\" is used for specific kinds of materials. These codes are of the form:\n\nThe second and third level of these codes are always the same - only the first level changes. For example, it is not valid to use 53- as a classification. Either 53 on its own or, better yet, a more specific code should be used.\n\nThird-level codes are the most specific, usually corresponding to a specific kind of mathematical object or a well-known problem or research area.\n\nThe third-level code 99 exists in every category and means none of the above, but in this section.\n\nThe AMS recommends that papers submitted to its journals for publication have one primary classification and one or more optional secondary classifications. A typical MSC subject class line on a research paper looks like\n\nMSC Primary 03C90; Secondary 03-02;\n\nAccording to the American Mathematical Society (AMS) help page about MSC,[1] the MSC has been revised a number of times since 1940. Based on a scheme to organize AMS's Mathematical Offprint Service (MOS scheme), the AMS Classification was established for the classification of reviews in Mathematical Reviews in the 1960s. It saw various ad-hoc changes. Despite its shortcomings, Zentralblatt für Mathematik started to use it as well in the 1970s. In the late 1980s, a jointly revised scheme with more formal rules was agreed upon by Mathematical Reviews and Zentralblatt für Mathematik under the new name Mathematics Subject Classification. It saw various revisions as MSC1990, MSC2000 and MSC2010.[2] In July 2016, Mathematical Reviews and zbMATH started collecting input from the mathematical community on the next revision of MSC,[3] which was released as MSC2020 in January 2020.[4]\n\nThe original classification of older items has not been changed. This can sometimes make it difficult to search for older works dealing with particular topics. Changes at the first level involved the subjects with (present) codes 03, 08, 12-20, 28, 37, 51, 58, 74, 90, 91, 92.\n\nFor physics papers the Physics and Astronomy Classification Scheme (PACS) is often used. Due to the large overlap between mathematics and physics research it is quite common to see both PACS and MSC codes on research papers, particularly for multidisciplinary journals and repositories such as the arXiv.\n\nThe ACM Computing Classification System (CCS) is a similar hierarchical classification scheme for computer science. There is some overlap between the AMS and ACM classification schemes, in subjects related to both mathematics and computer science, however the two schemes differ in the details of their organization of those topics.\n\nThe classification scheme used on the arXiv is chosen to reflect the papers submitted. As arXiv is multidisciplinary its classification scheme does not fit entirely with the MSC, ACM or PACS classification schemes. It is common to see codes from one or more of these schemes on individual papers.\n"
    },
    {
        "title": "Renaissance",
        "content": "\n\nThe Renaissance (UK: /rɪˈneɪsəns/ rin-AY-sənss, US: /ˈrɛnəsɑːns/ ⓘ REN-ə-sahnss)[1][2][a] is a period of history and a European cultural movement covering the 15th and 16th centuries. It marked the transition from the Middle Ages to modernity and was characterized by an effort to revive and surpass the ideas and achievements of classical antiquity. Associated with great social change in most fields and disciplines, including art, architecture, politics, literature, exploration and science, the Renaissance was first centered in the Republic of Florence, then spread to the rest of Italy and later throughout Europe. The term rinascita (\"rebirth\") first appeared in Lives of the Artists (c. 1550) by Giorgio Vasari, while the corresponding French word renaissance was adopted into English as the term for this period during the 1830s.[4][b]\n\nThe Renaissance's intellectual basis was founded in its version of humanism, derived from the concept of Roman humanitas and the rediscovery of classical Greek philosophy, such as that of Protagoras, who said that \"man is the measure of all things\". Although the invention of metal movable type sped the dissemination of ideas from the later 15th century, the changes of the Renaissance were not uniform across Europe: the first traces appear in Italy as early as the late 13th century, in particular with the writings of Dante and the paintings of Giotto.\n\nAs a cultural movement, the Renaissance encompassed innovative flowering of literary Latin and an explosion of vernacular literatures, beginning with the 14th-century resurgence of learning based on classical sources, which contemporaries credited to Petrarch; the development of linear perspective and other techniques of rendering a more natural reality in painting; and gradual but widespread educational reform. It saw myriad artistic developments and contributions from such polymaths as Leonardo da Vinci and Michelangelo, who inspired the term \"Renaissance man\".[5][6] In politics, the Renaissance contributed to the development of the customs and conventions of diplomacy, and in science to an increased reliance on observation and inductive reasoning. The period also saw revolutions in other intellectual and social scientific pursuits, as well as the introduction of modern banking and the field of accounting.[7]\n\nThe Renaissance period started during the crisis of the Late Middle Ages and conventionally ends by the 1600s with the waning of humanism, and the advents of the Reformation and Counter-Reformation, and in art the Baroque period. It had a different period and characteristics in different regions, such as the Italian Renaissance, the Northern Renaissance, the Spanish Renaissance, etc.\n\nIn addition to the standard periodization, proponents of a \"long Renaissance\" may put its beginning in the 14th century and its end in the 17th century.[c]\n\nThe traditional view focuses more on the Renaissance's early modern aspects and argues that it was a break from the past, but many historians today focus more on its medieval aspects and argue that it was an extension of the Middle Ages.[11][12] The beginnings of the period—the early Renaissance of the 15th century and the Italian Proto-Renaissance from around 1250 or 1300—overlap considerably with the Late Middle Ages, conventionally dated to c. 1350–1500, and the Middle Ages themselves were a long period filled with gradual changes, like the modern age; as a transitional period between both, the Renaissance has close similarities to both, especially the late and early sub-periods of either.\n\nThe Renaissance began in Florence, one of the many states of Italy.[13] Various theories have been proposed to account for its origins and characteristics, focusing on a variety of factors, including Florence's social and civic peculiarities at the time: its political structure, the patronage of its dominant family, the Medici,[14] and the migration of Greek scholars and their texts to Italy following the fall of Constantinople to the Ottoman Empire.[15][16][17] Other major centers were Venice, Genoa, Milan, Rome during the Renaissance Papacy, and Naples. From Italy, the Renaissance spread throughout Europe and also to American, African and Asian territories ruled by the European colonial powers of the time or where Christian missionaries were active.\n\nThe Renaissance has a long and complex historiography, and in line with general skepticism of discrete periodizations, there has been much debate among historians reacting to the 19th-century glorification of the \"Renaissance\" and individual cultural heroes as \"Renaissance men\", questioning the usefulness of Renaissance as a term and as a historical delineation.[18]\n\nSome observers have questioned whether the Renaissance was a cultural \"advance\" from the Middle Ages, instead seeing it as a period of pessimism and nostalgia for classical antiquity,[19] while social and economic historians, especially of the longue durée, have instead focused on the continuity between the two eras,[20] which are linked, as Panofsky observed, \"by a thousand ties\".[21][d]\n\nThe word has also been extended to other historical and cultural movements, such as the Carolingian Renaissance (8th and 9th centuries), Ottonian Renaissance (10th and 11th century), and the Renaissance of the 12th century.[23]\n\nThe Renaissance was a cultural movement that profoundly affected European intellectual life in the early modern period. Beginning in Italy, and spreading to the rest of Europe by the 16th century, its influence was felt in art, architecture, philosophy, literature, music, science, technology, politics, religion, and other aspects of intellectual inquiry. Renaissance scholars employed the humanist method in study, and searched for realism and human emotion in art.[24]\n\nRenaissance humanists such as Poggio Bracciolini sought out in Europe's monastic libraries the Latin literary, historical, and oratorical texts of antiquity, while the fall of Constantinople (1453) generated a wave of émigré Greek scholars bringing precious manuscripts in ancient Greek, many of which had fallen into obscurity in the West. It was in their new focus on literary and historical texts that Renaissance scholars differed so markedly from the medieval scholars of the Renaissance of the 12th century, who had focused on studying Greek and Arabic works of natural sciences, philosophy, and mathematics, rather than on such cultural texts.[citation needed]\n\nIn the revival of neoplatonism, Renaissance humanists did not reject Christianity; on the contrary, many of the Renaissance's greatest works were devoted to it, and the Church patronized many works of Renaissance art.[citation needed] But a subtle shift took place in the way that intellectuals approached religion that was reflected in many other areas of cultural life.[25][better source needed] In addition, many Greek Christian works, including the Greek New Testament, were brought back from Byzantium to Western Europe and engaged Western scholars for the first time since late antiquity. This new engagement with Greek Christian works, and particularly the return to the original Greek of the New Testament promoted by humanists Lorenzo Valla and Erasmus, helped pave the way for the Reformation.[citation needed]\n\nWell after the first artistic return to classicism had been exemplified in the sculpture of Nicola Pisano, Florentine painters led by Masaccio strove to portray the human form realistically, developing techniques to render perspective and light more naturally. Political philosophers, most famously Niccolò Machiavelli, sought to describe political life as it really was, that is to understand it rationally. A critical contribution to Italian Renaissance humanism, Giovanni Pico della Mirandola wrote De hominis dignitate (Oration on the Dignity of Man, 1486), a series of theses on philosophy, natural thought, faith, and magic defended against any opponent on the grounds of reason. In addition to studying classical Latin and Greek, Renaissance authors also began increasingly to use vernacular languages; combined with the introduction of the printing press, this allowed many more people access to books, especially the Bible.[26]\n\nIn all, the Renaissance can be viewed as an attempt by intellectuals to study and improve the secular and worldly, both through the revival of ideas from antiquity and through novel approaches to thought. Political philosopher Hans Kohn describes it as an age where \"Men looked for new foundations\"; some like Erasmus and Thomas More envisioned new reformed spiritual foundations, others. in the words of Machiavelli, una lunga sperienza delle cose moderne ed una continua lezione delle antiche (a long experience with modern life and a continuous learning from antiquity).[27]\n\nSociologist Rodney Stark, plays down the Renaissance in favor of the earlier innovations of the Italian city-states in the High Middle Ages, which married responsive government, Christianity and the birth of capitalism.[28] This analysis argues that, whereas the great European states (France and Spain) were absolute monarchies, and others were under direct Church control, the independent city-republics of Italy took over the principles of capitalism invented on monastic estates and set off a vast unprecedented Commercial Revolution that preceded and financed the Renaissance.[citation needed]\n\nHistorian Leon Poliakov offers a critical view in his seminal study of European racist thought: The Aryan Myth. According to Poliakov, the use of ethnic origin myths are first used by Renaissance humanists \"in the service of a new born chauvinism\".[29][30]\n\nMany argue that the ideas characterizing the Renaissance had their origin in Florence at the turn of the 13th and 14th centuries, in particular with the writings of Dante Alighieri (1265–1321) and Petrarch (1304–1374), as well as the paintings of Giotto di Bondone (1267–1337). Some writers date the Renaissance quite precisely; one proposed starting point is 1401, when the rival geniuses Lorenzo Ghiberti and Filippo Brunelleschi competed for the contract to build the bronze doors for the Baptistery of the Florence Cathedral (Ghiberti won).[31] Others see more general competition between artists and polymaths such as Brunelleschi, Ghiberti, Donatello, and Masaccio for artistic commissions as sparking the creativity of the Renaissance.\n\nYet it remains much debated why the Renaissance began in Italy, and why it began when it did. Accordingly, several theories have been put forward to explain its origins. Peter Rietbergen posits that various influential Proto-Renaissance movements started from roughly 1300 onwards across many regions of Europe.[32]\n\nIn stark contrast to the High Middle Ages, when Latin scholars focused almost entirely on studying Greek and Arabic works of natural science, philosophy and mathematics,[e] Renaissance scholars were most interested in recovering and studying Latin and Greek literary, historical, and oratorical texts. Broadly speaking, this began in the 14th century with a Latin phase, when Renaissance scholars such as Petrarch, Coluccio Salutati (1331–1406), Niccolò de' Niccoli (1364–1437), and Poggio Bracciolini (1380–1459) scoured the libraries of Europe in search of works by such Latin authors as Cicero, Lucretius, Livy, and Seneca.[33] By the early 15th century, the bulk of the surviving such Latin literature had been recovered; the Greek phase of Renaissance humanism was under way, as Western European scholars turned to recovering ancient Greek literary, historical, oratorical and theological texts.[34]\n\nUnlike with Latin texts, which had been preserved and studied in Western Europe since late antiquity, the study of ancient Greek texts was very limited in medieval Western Europe. Ancient Greek works on science, mathematics, and philosophy had been studied since the High Middle Ages in Western Europe and in the Islamic Golden Age (normally in translation), but Greek literary, oratorical and historical works (such as Homer, the Greek dramatists, Demosthenes and Thucydides) were not studied in either the Latin or medieval Islamic worlds; in the Middle Ages these sorts of texts were only studied by Byzantine scholars. Some argue that the Timurid Renaissance in Samarkand and Herat, whose magnificence toned with Florence as the center of a cultural rebirth,[35][36] were linked to the Ottoman Empire, whose conquests led to the migration of Greek scholars to Italian cities.[37][full citation needed][38][full citation needed][15][39] One of the greatest achievements of Renaissance scholars was to bring this entire class of Greek cultural works back into Western Europe for the first time since late antiquity.\n\nMuslim logicians, most notably Avicenna and Averroes, had inherited Greek ideas after they had invaded and conquered Egypt and the Levant. Their translations and commentaries on these ideas worked their way through the Arab West into Iberia and Sicily, which became important centers for this transmission of ideas. Between the 11th and 13th centuries, many schools dedicated to the translation of philosophical and scientific works from Classical Arabic to Medieval Latin were established in Iberia, most notably the Toledo School of Translators. This work of translation from Islamic culture, though largely unplanned and disorganized, constituted one of the greatest transmissions of ideas in history.[40]\n\nThe movement to reintegrate the regular study of Greek literary, historical, oratorical, and theological texts back into the Western European curriculum is usually dated to the 1396 invitation from Coluccio Salutati to the Byzantine diplomat and scholar Manuel Chrysoloras (c. 1355–1415) to teach Greek in Florence.[41] This legacy was continued by a number of expatriate Greek scholars, from Basilios Bessarion to Leo Allatius.\n\nThe unique political structures of Italy during the Late Middle Ages have led some to theorize that its unusual social climate allowed the emergence of a rare cultural efflorescence. Italy did not exist as a political entity in the early modern period. Instead, it was divided into smaller city-states and territories: the Neapolitans controlled the south, the Florentines and the Romans at the center, the Milanese and the Genoese to the north and west respectively, and the Venetians to the north east. 15th-century Italy was one of the most urbanized areas in Europe.[42] Many of its cities stood among the ruins of ancient Roman buildings; it seems likely that the classical nature of the Renaissance was linked to its origin in the Roman Empire's heartland.[43]\n\nHistorian and political philosopher Quentin Skinner points out that Otto of Freising (c. 1114–1158), a German bishop visiting north Italy during the 12th century, noticed a widespread new form of political and social organization, observing that Italy appeared to have exited from feudalism so that its society was based on merchants and commerce. Linked to this was anti-monarchical thinking, represented in the famous early Renaissance fresco cycle The Allegory of Good and Bad Government by Ambrogio Lorenzetti (painted 1338–1340), whose strong message is about the virtues of fairness, justice, republicanism and good administration. Holding both Church and Empire at bay, these city republics were devoted to notions of liberty. Skinner reports that there were many defences of liberty such as the Matteo Palmieri (1406–1475) celebration of Florentine genius not only in art, sculpture and architecture, but \"the remarkable efflorescence of moral, social and political philosophy that occurred in Florence at the same time\".[44]\n\nEven cities and states beyond central Italy, such as the Republic of Florence at this time, were also notable for their merchant republics, especially the Republic of Venice. Although in practice these were oligarchical, and bore little resemblance to a modern democracy, they did have democratic features and were responsive states, with forms of participation in governance and belief in liberty.[44][45][46] The relative political freedom they afforded was conducive to academic and artistic advancement.[47] Likewise, the position of Italian cities such as Venice as great trading centres made them intellectual crossroads. Merchants brought with them ideas from far corners of the globe, particularly the Levant. Venice was Europe's gateway to trade with the East, and a producer of fine glass, while Florence was a capital of textiles. The wealth such business brought to Italy meant large public and private artistic projects could be commissioned and individuals had more leisure time for study.[47]\n\nOne theory that has been advanced is that the devastation in Florence caused by the Black Death, which hit Europe between 1348 and 1350, resulted in a shift in the world view of people in 14th century Italy. Italy was particularly badly hit by the plague, and it has been speculated that the resulting familiarity with death caused thinkers to dwell more on their lives on Earth, rather than on spirituality and the afterlife.[48] It has also been argued that the Black Death prompted a new wave of piety, manifested in the sponsorship of religious works of art.[49] However, this does not fully explain why the Renaissance occurred specifically in Italy in the 14th century. The Black Death was a pandemic that affected all of Europe in the ways described, not only Italy. The Renaissance's emergence in Italy was most likely the result of the complex interaction of the above factors.[18]\n\nThe plague was carried by fleas on sailing vessels returning from the ports of Asia, spreading quickly due to lack of proper sanitation: the population of England, then about 4.2 million, lost 1.4 million people to the bubonic plague. Florence's population was nearly halved in the year 1348. As a result of the decimation in the populace the value of the working class increased, and commoners came to enjoy more freedom. To answer the increased need for labor, workers traveled in search of the most favorable position economically.[50]\n\nThe demographic decline due to the plague had economic consequences: the prices of food dropped and land values declined by 30–40% in most parts of Europe between 1350 and 1400.[51] Landholders faced a great loss, but for ordinary men and women it was a windfall. The survivors of the plague found not only that the prices of food were cheaper but also that lands were more abundant, and many of them inherited property from their dead relatives.\n\nThe spread of disease was significantly more rampant in areas of poverty. Epidemics ravaged cities, particularly children. Plagues were easily spread by lice, unsanitary drinking water, armies, or by poor sanitation. Children were hit the hardest because many diseases, such as typhus and congenital syphilis, target the immune system, leaving young children without a fighting chance. Children in city dwellings were more affected by the spread of disease than the children of the wealthy.[52]\n\nThe Black Death caused greater upheaval to Florence's social and political structure than later epidemics. Despite a significant number of deaths among members of the ruling classes, the government of Florence continued to function during this period. Formal meetings of elected representatives were suspended during the height of the epidemic due to the chaotic conditions in the city, but a small group of officials was appointed to conduct the affairs of the city, which ensured continuity of government.[53]\n\nIt has long been a matter of debate why the Renaissance began in Florence, and not elsewhere in Italy. Scholars have noted several features unique to Florentine cultural life that may have caused such a cultural movement. Many have emphasized the role played by the Medici, a banking family and later ducal ruling house, in patronizing and stimulating the arts. Some historians have postulated that Florence was the birthplace of the Renaissance as a result of luck, i.e., because \"Great Men\" were born there by chance:[54] Leonardo, Botticelli and Michelangelo were all born in Tuscany. Arguing that such chance seems improbable, other historians have contended that these \"Great Men\" were only able to rise to prominence because of the prevailing cultural conditions at the time.[55]\n\nLorenzo de' Medici (1449–1492) was the catalyst for an enormous amount of arts patronage, encouraging his countrymen to commission works from the leading artists of Florence, including Leonardo da Vinci, Sandro Botticelli, and Michelangelo Buonarroti.[14] Works by Neri di Bicci, Botticelli, Leonardo, and Filippino Lippi had been commissioned additionally by the Convent of San Donato in Scopeto in Florence.[56]\n\nThe Renaissance was certainly underway before Lorenzo de' Medici came to power – indeed, before the Medici family itself achieved hegemony in Florentine society.\n\nIn some ways, Renaissance humanism was not a philosophy but a method of learning. In contrast to the medieval scholastic mode, which focused on resolving contradictions between authors, Renaissance humanists would study ancient texts in their original languages and appraise them through a combination of reasoning and empirical evidence. Humanist education was based on the programme of Studia Humanitatis, the study of five humanities: poetry, grammar, history, moral philosophy, and rhetoric. Although historians have sometimes struggled to define humanism precisely, most have settled on \"a middle of the road definition... the movement to recover, interpret, and assimilate the language, literature, learning and values of ancient Greece and Rome\".[57] Above all, humanists asserted \"the genius of man ... the unique and extraordinary ability of the human mind\".[58]\n\nHumanist scholars shaped the intellectual landscape throughout the early modern period. Political philosophers such as Niccolò Machiavelli and Thomas More revived the ideas of Greek and Roman thinkers and applied them in critiques of contemporary government, following the Islamic steps of Ibn Khaldun.[60][61] Pico della Mirandola wrote the \"manifesto\" of the Renaissance, the Oration on the Dignity of Man, a vibrant defence of thinking.[citation needed] Matteo Palmieri (1406–1475), another humanist, is most known for his work Della vita civile (\"On Civic Life\"; printed 1528), which advocated civic humanism, and for his influence in refining the Tuscan vernacular to the same level as Latin. Palmieri drew on Roman philosophers and theorists, especially Cicero, who, like Palmieri, lived an active public life as a citizen and official, as well as a theorist and philosopher and also Quintilian. Perhaps the most succinct expression of his perspective on humanism is in a 1465 poetic work La città di vita, but an earlier work, Della vita civile, is more wide-ranging. Composed as a series of dialogues set in a country house in the Mugello countryside outside Florence during the plague of 1430, Palmieri expounds on the qualities of the ideal citizen. The dialogues include ideas about how children develop mentally and physically, how citizens can conduct themselves morally, how citizens and states can ensure probity in public life, and an important debate on the difference between that which is pragmatically useful and that which is honest.[citation needed]\n\nThe humanists believed that it is important to transcend to the afterlife with a perfect mind and body, which could be attained with education. The purpose of humanism was to create a universal man whose person combined intellectual and physical excellence and who was capable of functioning honorably in virtually any situation.[62] This ideology was referred to as the uomo universale, an ancient Greco-Roman ideal. Education during the Renaissance was mainly composed of ancient literature and history as it was thought that the classics provided moral instruction and an intensive understanding of human behavior.\n\nA unique characteristic of some Renaissance libraries is that they were open to the public. These libraries were places where ideas were exchanged and where scholarship and reading were considered both pleasurable and beneficial to the mind and soul. As freethinking was a hallmark of the age, many libraries contained a wide range of writers. Classical texts could be found alongside humanist writings. These informal associations of intellectuals profoundly influenced Renaissance culture. An essential tool of Renaissance librarianship was the catalog that listed, described, and classified a library's books.[63] Some of the richest \"bibliophiles\" built libraries as temples to books and knowledge. A number of libraries appeared as manifestations of immense wealth joined with a love of books. In some cases, cultivated library builders were also committed to offering others the opportunity to use their collections. Prominent aristocrats and princes of the Church created great libraries for the use of their courts, called \"court libraries\", and were housed in lavishly designed monumental buildings decorated with ornate woodwork, and the walls adorned with frescoes (Murray, Stuart A.P.).\n\nRenaissance art marks a cultural rebirth at the close of the Middle Ages and rise of the Modern world. One of the distinguishing features of Renaissance art was its development of highly realistic linear perspective. Giotto di Bondone (1267–1337) is credited with first treating a painting as a window into space, but it was not until the demonstrations of architect Filippo Brunelleschi (1377–1446) and the subsequent writings of Leon Battista Alberti (1404–1472) that perspective was formalized as an artistic technique.[64]\n\nThe development of perspective was part of a wider trend toward realism in the arts.[65] Painters developed other techniques, studying light, shadow, and, famously in the case of Leonardo da Vinci, human anatomy. Underlying these changes in artistic method was a renewed desire to depict the beauty of nature and to unravel the axioms of aesthetics, with the works of Leonardo, Michelangelo and Raphael representing artistic pinnacles that were much imitated by other artists.[66] Other notable artists include Sandro Botticelli, working for the Medici in Florence, Donatello, another Florentine, and Titian in Venice, among others.\n\nIn the Low Countries, a particularly vibrant artistic culture developed. The work of Hugo van der Goes and Jan van Eyck was particularly influential on the development of painting in Italy, both technically with the introduction of oil paint and canvas, and stylistically in terms of naturalism in representation. Later, the work of Pieter Brueghel the Elder would inspire artists to depict themes of everyday life.[67]\n\nIn architecture, Filippo Brunelleschi was foremost in studying the remains of ancient classical buildings. With rediscovered knowledge from the 1st-century writer Vitruvius and the flourishing discipline of mathematics, Brunelleschi formulated the Renaissance style that emulated and improved on classical forms. His major feat of engineering was building the dome of Florence Cathedral.[68] Another building demonstrating this style is the Basilica of Sant'Andrea, Mantua, built by Alberti. The outstanding architectural work of the High Renaissance was the rebuilding of St. Peter's Basilica, combining the skills of Bramante, Michelangelo, Raphael, Sangallo and Maderno.\n\nDuring the Renaissance, architects aimed to use columns, pilasters, and entablatures as an integrated system. The Roman orders types of columns are used: Tuscan and Composite. These can either be structural, supporting an arcade or architrave, or purely decorative, set against a wall in the form of pilasters. One of the first buildings to use pilasters as an integrated system was in the Old Sacristy (1421–1440) by Brunelleschi.[69] Arches, semi-circular or (in the Mannerist style) segmental, are often used in arcades, supported on piers or columns with capitals. There may be a section of entablature between the capital and the springing of the arch. Alberti was one of the first to use the arch on a monumental. Renaissance vaults do not have ribs; they are semi-circular or segmental and on a square plan, unlike the Gothic vault, which is frequently rectangular.\n\nRenaissance artists were not pagans, although they admired antiquity and kept some ideas and symbols of the medieval past. Nicola Pisano (c. 1220 – c. 1278) imitated classical forms by portraying scenes from the Bible. His Annunciation, from the Pisa Baptistry, demonstrates that classical models influenced Italian art before the Renaissance took root as a literary movement.[70]\n\nApplied innovation extended to commerce. At the end of the 15th century, Luca Pacioli published the first work on bookkeeping, making him the founder of accounting.[7]\n\nThe rediscovery of ancient texts and the invention of the printing press in about 1440 democratized learning and allowed a faster propagation of more widely distributed ideas. In the first period of the Italian Renaissance, humanists favored the study of humanities over natural philosophy or applied mathematics, and their reverence for classical sources further enshrined the Aristotelian and Ptolemaic views of the universe. Writing around 1450, Nicholas of Cusa anticipated the heliocentric worldview of Copernicus, but in a philosophical fashion.\n\nScience and art were intermingled in the early Renaissance, with polymath artists such as Leonardo da Vinci making observational drawings of anatomy and nature. Leonardo set up controlled experiments in water flow, medical dissection, and systematic study of movement and aerodynamics, and he devised principles of research method that led Fritjof Capra to classify him as the \"father of modern science\".[g] Other examples of Da Vinci's contribution during this period include machines designed to saw marbles and lift monoliths, and new discoveries in acoustics, botany, geology, anatomy, and mechanics.[73]\n\nA suitable environment had developed to question classical scientific doctrine. The discovery in 1492 of the New World by Christopher Columbus challenged the classical worldview. The works of Ptolemy (in geography) and Galen (in medicine) were found to not always match everyday observations. As the Reformation and Counter-Reformation clashed, the Northern Renaissance showed a decisive shift in focus from Aristotelean natural philosophy to chemistry and the biological sciences (botany, anatomy, and medicine).[74] The willingness to question previously held truths and search for new answers resulted in a period of major scientific advancements.\n\nSome view this as a \"scientific revolution\", heralding the beginning of the modern age,[75] others as an acceleration of a continuous process stretching from the ancient world to the present day.[76] Significant scientific advances were made during this time by Galileo Galilei, Tycho Brahe, and Johannes Kepler.[77] Copernicus, in De revolutionibus orbium coelestium (On the Revolutions of the Heavenly Spheres), posited that the Earth moved around the Sun. De humani corporis fabrica (On the Workings of the Human Body) by Andreas Vesalius, gave a new confidence to the role of dissection, observation, and the mechanistic view of anatomy.[78]\n\nAnother important development was in the process for discovery, the scientific method,[78] focusing on empirical evidence and the importance of mathematics, while discarding much of Aristotelian science. Early and influential proponents of these ideas included Copernicus, Galileo, and Francis Bacon.[79][80] The new scientific method led to great contributions in the fields of astronomy, physics, biology, and anatomy.[h][81]\n\nDuring the Renaissance, extending from 1450 to 1650,[82] every continent was visited and mostly mapped by Europeans, except the south polar continent now known as Antarctica. This development is depicted in the large world map Nova Totius Terrarum Orbis Tabula made by the Dutch cartographer Joan Blaeu in 1648 to commemorate the Peace of Westphalia.\n\nIn 1492, Christopher Columbus sailed across the Atlantic Ocean from Spain seeking a direct route to India of the Delhi Sultanate. He accidentally stumbled upon the Americas, but believed he had reached the East Indies.\n\nIn 1606, the Dutch navigator Willem Janszoon sailed from the East Indies in the Dutch East India Company ship Duyfken and landed in Australia. He charted about 300 km of the west coast of Cape York Peninsula in Queensland. More than thirty Dutch expeditions followed, mapping sections of the north, west, and south coasts. In 1642–1643, Abel Tasman circumnavigated the continent, proving that it was not joined to the imagined south polar continent.\n\nBy 1650, Dutch cartographers had mapped most of the coastline of the continent, which they named New Holland, except the east coast which was charted in 1770 by James Cook.\n\nThe long-imagined south polar continent was eventually sighted in 1820. Throughout the Renaissance it had been known as Terra Australis, or 'Australia' for short. However, after that name was transferred to New Holland in the nineteenth century, the new name of 'Antarctica' was bestowed on the south polar continent.[83]\n\nFrom this changing society emerged a common, unifying musical language, in particular the polyphonic style of the Franco-Flemish school. The development of printing made distribution of music possible on a wide scale. Demand for music as entertainment and as an activity for educated amateurs increased with the emergence of a bourgeois class. Dissemination of chansons, motets, and masses throughout Europe coincided with the unification of polyphonic practice into the fluid style that culminated in the second half of the sixteenth century in the work of composers such as Giovanni Pierluigi da Palestrina, Orlande de Lassus, Tomás Luis de Victoria, and William Byrd.\n\nThe new ideals of humanism, although more secular in some aspects, developed against a Christian backdrop, especially in the Northern Renaissance. Much, if not most, of the new art was commissioned by or in dedication to the Roman Catholic Church.[25] However, the Renaissance had a profound effect on contemporary theology, particularly in the way people perceived the relationship between man and God.[25] Many of the period's foremost theologians were followers of the humanist method, including Erasmus, Huldrych Zwingli, Thomas More, Martin Luther, and John Calvin.\n\nThe Renaissance began in times of religious turmoil. The Late Middle Ages was a period of political intrigue surrounding the Papacy, culminating in the Western Schism, in which three men simultaneously claimed to be true Bishop of Rome.[84] While the schism was resolved by the Council of Constance (1414), a resulting reform movement known as Conciliarism sought to limit the power of the pope. Although the papacy eventually emerged supreme in ecclesiastical matters by the Fifth Council of the Lateran (1511), it was dogged by continued accusations of corruption, most famously in the person of Pope Alexander VI, who was accused variously of simony, nepotism, and fathering children (most of whom were married off, presumably for the consolidation of power) while a cardinal.[85]\n\nChurchmen such as Erasmus and Luther proposed reform to the Church, often based on humanist textual criticism of the New Testament.[25] In October 1517, Luther published the Ninety-five Theses, challenging papal authority and criticizing its perceived corruption, particularly with regard to instances of sold indulgences.[i] The 95 Theses led to the Reformation, a break with the Roman Catholic Church that previously claimed hegemony in Western Europe. Humanism and the Renaissance therefore played a direct role in sparking the Reformation, as well as in many other contemporaneous religious debates and conflicts.\n\nPope Paul III came to the papal throne (1534–1549) after the sack of Rome in 1527, with uncertainties prevalent in the Catholic Church following the Reformation. Nicolaus Copernicus dedicated De revolutionibus orbium coelestium (On the Revolutions of the Celestial Spheres) to Paul III, who became the grandfather of Alessandro Farnese, who had paintings by Titian, Michelangelo, and Raphael, as well as an important collection of drawings, and who commissioned the masterpiece of Giulio Clovio, arguably the last major illuminated manuscript, the Farnese Hours.\n\nBy the 15th century, writers, artists, and architects in Italy were well aware of the transformations that were taking place and were using phrases such as modi antichi (in the antique manner) or alle romana et alla antica (in the manner of the Romans and the ancients) to describe their work. In the 1330s Petrarch referred to pre-Christian times as antiqua (ancient) and to the Christian period as nova (new).[86] From Petrarch's Italian perspective, this new period (which included his own time) was an age of national eclipse.[86] Leonardo Bruni was the first to use tripartite periodization in his History of the Florentine People (1442).[87] Bruni's first two periods were based on those of Petrarch, but he added a third period because he believed that Italy was no longer in a state of decline. Flavio Biondo used a similar framework in Decades of History from the Deterioration of the Roman Empire (1439–1453).\n\nHumanist historians argued that contemporary scholarship restored direct links to the classical period, thus bypassing the Medieval period, which they then named for the first time the \"Middle Ages\". The term first appears in Latin in 1469 as media tempestas (middle times).[88] The term rinascita (rebirth) first appeared, however, in its broad sense in Giorgio Vasari's Lives of the Artists, 1550, revised 1568.[89][90] Vasari divides the age into three phases: the first phase contains Cimabue, Giotto, and Arnolfo di Cambio; the second phase contains Masaccio, Brunelleschi, and Donatello; the third centers on Leonardo da Vinci and culminates with Michelangelo. It was not just the growing awareness of classical antiquity that drove this development, according to Vasari, but also the growing desire to study and imitate nature.[91]\n\nIn the 15th century, the Renaissance spread rapidly from its birthplace in Florence to the rest of Italy and soon to the rest of Europe. The invention of the printing press by German printer Johannes Gutenberg allowed the rapid transmission of these new ideas. As it spread, its ideas diversified and changed, being adapted to local culture. In the 20th century, scholars began to break the Renaissance into regional and national movements.\n\nThe Elizabethan era in the second half of the 16th century is usually regarded as the height of the English Renaissance. Many scholars see its beginnings in the early 16th century during the reign of Henry VIII.[92]\n\nThe English Renaissance is different from the Italian Renaissance in several ways. The dominant art forms of the English Renaissance were literature and music, which had a rich flowering.[93] Visual arts in the English Renaissance were much less significant than in the Italian Renaissance. The English Renaissance period in art began far later than the Italian, which had moved into Mannerism by the 1530s.[94]\n\nIn literature the later part of the 16th century saw the flowering of Elizabethan literature, with poetry heavily influenced by Italian Renaissance literature but Elizabethan theatre a distinctive native style. Writers include William Shakespeare (1564–1616), Christopher Marlowe (1564–1593), Edmund Spenser (1552–1599), Sir Thomas More (1478–1535), and Sir Philip Sidney (1554–1586). English Renaissance music competed with that in Europe with composers such as Thomas Tallis (1505–1585), John Taverner (1490–1545), and William Byrd (1540–1623). Elizabethan architecture produced the large prodigy houses of courtiers, and in the next century Inigo Jones (1573–1652), who introduced Palladian architecture to England.[95]\n\nElsewhere, Sir Francis Bacon (1561–1626) was the pioneer of modern scientific thought, and is commonly regarded as one of the founders of the Scientific Revolution.[96][97]\n\nThe word \"Renaissance\" is borrowed from the French language, where it means \"re-birth\". It was first used in the eighteenth century and was later popularized by French historian Jules Michelet (1798–1874) in his 1855 work, Histoire de France (History of France).[98][99]\n\nIn 1495 the Italian Renaissance arrived in France, imported by King Charles VIII after his invasion of Italy. A factor that promoted the spread of secularism was the inability of the Church to offer assistance against the Black Death. Francis I imported Italian art and artists, including Leonardo da Vinci, and built ornate palaces at great expense. Writers such as François Rabelais, Pierre de Ronsard, Joachim du Bellay, and Michel de Montaigne, painters such as Jean Clouet, and musicians such as Jean Mouton also borrowed from the spirit of the Renaissance.\n\nIn 1533, a fourteen-year-old Catherine de' Medici (1519–1589), born in Florence to Lorenzo de' Medici, Duke of Urbino and Madeleine de La Tour d'Auvergne, married Henry II of France, second son of King Francis I and Queen Claude. Though she became famous and infamous for her role in the French Wars of Religion, she made a direct contribution in bringing arts, sciences, and music (including the origins of ballet) to the French court from her native Florence.\n\nIn the second half of the 15th century, the Renaissance spirit spread to Germany and the Low Countries, where the development of the printing press (ca. 1450) and Renaissance artists such as Albrecht Dürer (1471–1528) predated the influence from Italy. In the early Protestant areas of the country humanism became closely linked to the turmoil of the Reformation, and the art and writing of the German Renaissance frequently reflected this dispute.[100] However, the Gothic style and medieval scholastic philosophy remained exclusively until the turn of the 16th century. Emperor Maximilian I of Habsburg (ruling 1493–1519) was the first truly Renaissance monarch of the Holy Roman Empire.\n\nAfter Italy, Hungary was the first European country where the Renaissance appeared.[101] The Renaissance style came directly from Italy during the Quattrocento (1400s) to Hungary first in the Central European region, thanks to the development of early Hungarian-Italian relationships — not only in dynastic connections, but also in cultural, humanistic and commercial relations – growing in strength from the 14th century. The relationship between Hungarian and Italian Gothic styles was a second reason – exaggerated breakthrough of walls is avoided, preferring clean and light structures. Large-scale building schemes provided ample and long term work for the artists, for example, the building of the Friss (New) Castle in Buda, the castles of Visegrád, Tata, and Várpalota. In Sigismund's court there were patrons such as Pippo Spano, a descendant of the Scolari family of Florence, who invited Manetto Ammanatini and Masolino da Pannicale to Hungary.[102]\n\nThe new Italian trend combined with existing national traditions to create a particular local Renaissance art. Acceptance of Renaissance art was furthered by the continuous arrival of humanist thought in the country. Many young Hungarians studying at Italian universities came closer to the Florentine humanist center, so a direct connection with Florence evolved. The growing number of Italian traders moving to Hungary, specially to Buda, helped this process. New thoughts were carried by the humanist prelates, among them Vitéz János, archbishop of Esztergom, one of the founders of Hungarian humanism.[103] During the long reign of Emperor Sigismund of Luxemburg the Royal Castle of Buda became probably the largest Gothic palace of the late Middle Ages. King Matthias Corvinus (r. 1458–1490) rebuilt the palace in early Renaissance style and further expanded it.[104][105]\n\nAfter the marriage in 1476 of King Matthias to Beatrice of Naples, Buda became one of the most important artistic centers of the Renaissance north of the Alps.[106] The most important humanists living in Matthias' court were Antonio Bonfini and the famous Hungarian poet Janus Pannonius.[106] András Hess set up a printing press in Buda in 1472. Matthias Corvinus's library, the Bibliotheca Corviniana, was Europe's greatest collections of secular books: historical chronicles, philosophic and scientific works in the 15th century. His library was second only in size to the Vatican Library. (However, the Vatican Library mainly contained Bibles and religious materials.)[107] In 1489, Bartolomeo della Fonte of Florence wrote that Lorenzo de' Medici founded his own Greek-Latin library encouraged by the example of the Hungarian king. Corvinus's library is part of UNESCO World Heritage.[108]\n\nMatthias started at least two major building projects.[109] The works in Buda and Visegrád began in about 1479.[110] Two new wings and a hanging garden were built at the royal castle of Buda, and the palace at Visegrád was rebuilt in Renaissance style.[110][111] Matthias appointed the Italian Chimenti Camicia and the Dalmatian Giovanni Dalmata to direct these projects. [110] Matthias commissioned the leading Italian artists of his age to embellish his palaces: for instance, the sculptor Benedetto da Majano and the painters Filippino Lippi and Andrea Mantegna worked for him.[112] A copy of Mantegna's portrait of Matthias survived.[113] Matthias also hired the Italian military engineer Aristotele Fioravanti to direct the rebuilding of the forts along the southern frontier.[114] He had new monasteries built in Late Gothic style for the Franciscans in Kolozsvár, Szeged and Hunyad, and for the Paulines in Fejéregyháza.[115][116] In the spring of 1485, Leonardo da Vinci travelled to Hungary on behalf of Sforza to meet King Matthias Corvinus, and was commissioned by him to paint a Madonna.[117]\n\nMatthias enjoyed the company of Humanists and had lively discussions on various topics with them.[118] The fame of his magnanimity encouraged many scholars—mostly Italian—to settle in Buda.[119] Antonio Bonfini, Pietro Ranzano, Bartolomeo Fonzio, and Francesco Bandini spent many years in Matthias's court.[120][118] This circle of educated men introduced the ideas of Neoplatonism to Hungary.[121][122] Like all intellectuals of his age, Matthias was convinced that the movements and combinations of the stars and planets exercised influence on individuals' life and on the history of nations.[123] Martius Galeotti described him as \"king and astrologer\", and Antonio Bonfini said Matthias \"never did anything without consulting the stars\".[124] Upon his request, the famous astronomers of the age, Johannes Regiomontanus and Marcin Bylica, set up an observatory in Buda and installed it with astrolabes and celestial globes.[125] Regiomontanus dedicated his book on navigation that was used by Christopher Columbus to Matthias.[119]\n\nOther important figures of Hungarian Renaissance include Bálint Balassi (poet), Sebestyén Tinódi Lantos (poet), Bálint Bakfark (composer and lutenist), and Master MS (fresco painter).\n\nCulture in the Netherlands at the end of the 15th century was influenced by the Italian Renaissance through trade via Bruges, which made Flanders wealthy. Its nobles commissioned artists who became known across Europe.[126] In science, the anatomist Andreas Vesalius led the way; in cartography, Gerardus Mercator's map assisted explorers and navigators. In art, Dutch and Flemish Renaissance painting ranged from the strange work of Hieronymus Bosch[127] to the everyday life depictions of Pieter Brueghel the Elder.[126]\n\nErasmus was arguably the Netherlands' best known humanist and Catholic intellectual during the Renaissance.[32]\n\nThe Renaissance in Northern Europe has been termed the \"Northern Renaissance\". While Renaissance ideas were moving north from Italy, there was a simultaneous southward spread of some areas of innovation, particularly in music.[128] The music of the 15th-century Burgundian School defined the beginning of the Renaissance in music, and the polyphony of the Netherlanders, as it moved with the musicians themselves into Italy, formed the core of the first true international style in music since the standardization of Gregorian Chant in the 9th century.[128] The culmination of the Netherlandish school was in the music of the Italian composer Giovanni Pierluigi da Palestrina. At the end of the 16th century Italy again became a center of musical innovation, with the development of the polychoral style of the Venetian School, which spread northward into Germany around 1600. In Denmark, the Renaissance sparked the translation of the works of Saxo Grammaticus into Danish as well as Frederick II and Christian IV ordering the redecoration or construction of several important works of architecture, i.e. Kronborg, Rosenborg and Børsen.[129] Danish astronomer Tycho Brahe greatly contributed to turn astronomy into the first modern science and also helped launch the Scientific Revolution.[130][131]\n\nThe paintings of the Italian Renaissance differed from those of the Northern Renaissance. Italian Renaissance artists were among the first to paint secular scenes, breaking away from the purely religious art of medieval painters. Northern Renaissance artists initially remained focused on religious subjects, such as the contemporary religious upheaval portrayed by Albrecht Dürer. Later, the works of Pieter Bruegel the Elder influenced artists to paint scenes of daily life rather than religious or classical themes. It was also during the Northern Renaissance that Flemish brothers Hubert and Jan van Eyck perfected the oil painting technique, which enabled artists to produce strong colors on a hard surface that could survive for centuries.[132] A feature of the Northern Renaissance was its use of the vernacular in place of Latin or Greek, which allowed greater freedom of expression. This movement had started in Italy with the decisive influence of Dante Alighieri on the development of vernacular languages; in fact the focus on writing in Italian has neglected a major source of Florentine ideas expressed in Latin.[133] The spread of the printing press technology boosted the Renaissance in Northern Europe as elsewhere, with Venice becoming a world center of printing.\n\nThe Polish Renaissance lasted from the late 15th to the late 16th century and was the Golden Age of Polish culture. Ruled by the Jagiellonian dynasty, the Kingdom of Poland (from 1569 known as the Polish–Lithuanian Commonwealth) actively participated in the broad European Renaissance. An early Italian humanist who came to Poland in the mid-15th century was Filippo Buonaccorsi, who was employed as royal advisor and councillor. The tomb of John I Albert, completed in 1505 by Francesco Fiorentino, is the first example of a Renaissance composition in the country.[134][135] Many Italian artists subsequently came to Poland with Bona Sforza of Milan, when she married King Sigismund I in 1518.[136] This was supported by temporarily strengthened monarchies in both areas, as well as by newly established universities.[137]\n\nThe Renaissance was a period when the multi-national Polish state experienced a substantial period of cultural growth thanks in part to a century without major wars, aside from conflicts in the sparsely populated eastern and southern borderlands. Architecture became more refined and decorative. Mannerism played an important part in shaping what is now considered to be the truly Polish architectural style – high attics above the cornice with pinnacles and pilasters.[138] It was also the time when the first major works of Polish literature were published, particularly those of Mikołaj Rey and Jan Kochanowski, and the Polish language became the lingua franca of East-Central Europe.[139] The Jagiellonian University transformed into a major institution of higher education for the region and hosted many notable scholars, chiefly Nicolaus Copernicus and Conrad Celtes. Three more academies were founded at Königsberg (1544), Vilnius (1579), and Zamość (1594). The Reformation spread peacefully throughout the country, giving rise to the Nontrinitarian Polish Brethren.[140] Living conditions improved, cities grew, and exports of agricultural products enriched the population, especially the nobility (szlachta) and magnates. The nobles gained dominance in the new political system of Golden Liberty, a counterweight to monarchical absolutism.[141]\n\nAlthough Italian Renaissance had a modest impact in Portuguese arts, Portugal was influential in broadening the European worldview,[142] stimulating humanist inquiry. Renaissance arrived through the influence of wealthy Italian and Flemish merchants who invested in the profitable commerce overseas. As the pioneer headquarters of European exploration, Lisbon flourished in the late 15th century, attracting experts who made several breakthroughs in mathematics, astronomy and naval technology, including Pedro Nunes, João de Castro, Abraham Zacuto, and Martin Behaim. Cartographers Pedro Reinel, Lopo Homem, Estêvão Gomes, and Diogo Ribeiro made crucial advances in mapping the world. Apothecary Tomé Pires and physicians Garcia de Orta and Cristóvão da Costa collected and published works on plants and medicines, soon translated by Flemish pioneer botanist Carolus Clusius.\n\nIn architecture, the huge profits of the spice trade financed a sumptuous composite style in the first decades of the 16th century, the Manueline, incorporating maritime elements.[143] The primary painters were Nuno Gonçalves, Gregório Lopes, and Vasco Fernandes. In music, Pedro de Escobar and Duarte Lobo produced four songbooks, including the Cancioneiro de Elvas.\n\nIn literature, Luís de Camões inscribed the Portuguese feats overseas in the epic poem Os Lusíadas. Sá de Miranda introduced Italian forms of verse and Bernardim Ribeiro developed pastoral romance, while plays by Gil Vicente fused it with popular culture, reporting the changing times. Travel literature especially flourished: João de Barros, Fernão Lopes de Castanheda, António Galvão, Gaspar Correia, Duarte Barbosa, and Fernão Mendes Pinto, among others, described new lands and were translated and spread with the new printing press.[142] After joining the Portuguese exploration of Brazil in 1500, Amerigo Vespucci coined the term New World,[144] in his letters to Lorenzo di Pierfrancesco de' Medici.\n\nThe intense international exchange produced several cosmopolitan humanist scholars, including Francisco de Holanda, André de Resende, and Damião de Góis, a friend of Erasmus who wrote with rare independence on the reign of King Manuel I. Diogo de Gouveia and André de Gouveia made relevant teaching reforms via France. Foreign news and products in the Portuguese factory in Antwerp attracted the interest of Thomas More[145] and Albrecht Dürer to the wider world.[146] There, profits and know-how helped nurture the Dutch Renaissance and Golden Age, especially after the arrival of the wealthy cultured Jewish community expelled from Portugal.\n\nThe Renaissance arrived in the Iberian peninsula through the Mediterranean possessions of the Crown of Aragon and the city of Valencia. Many early Spanish Renaissance writers come from the Crown of Aragon, including Ausiàs March and Joanot Martorell. In the Crown of Castile, the early Renaissance was heavily influenced by the Italian humanism, starting with writers and poets such as Íñigo López de Mendoza, marqués de Santillana, who introduced the new Italian poetry to Spain in the early 15th century. Other writers, such as Jorge Manrique, Fernando de Rojas, Juan del Encina, Juan Boscán Almogáver, and Garcilaso de la Vega, kept a close resemblance to the Italian canon. Miguel de Cervantes's masterpiece Don Quixote is credited as the first Western novel. Renaissance humanism flourished in the early 16th century, with influential writers such as philosopher Juan Luis Vives, grammarian Antonio de Nebrija and natural historian Pedro de Mexía. The poet and philosopher Luisa de Medrano, celebrated among her Renaissance contemporaries as one of the puellae doctae (\"learned girls\"), was the first female professor in Europe at the University of Salamanca.\n\nLater Spanish Renaissance tended toward religious themes and mysticism, with poets such as Luis de León, Teresa of Ávila, and John of the Cross, and treated issues related to the exploration of the New World, with chroniclers and writers such as Inca Garcilaso de la Vega and Bartolomé de las Casas, giving rise to a body of work, now known as Spanish Renaissance literature. The late Renaissance in Spain produced political and religious authors such as Tomás Fernández de Medrano and artists such as El Greco and composers such as Tomás Luis de Victoria and Antonio de Cabezón.\n\nThe Italian artist and critic Giorgio Vasari (1511–1574) first used the term rinascita in his book The Lives of the Artists (published 1550). In the book Vasari attempted to define what he described as a break with the barbarities of Gothic art: the arts (he held) had fallen into decay with the collapse of the Roman Empire and only the Tuscan artists, beginning with Cimabue (1240–1301) and Giotto (1267–1337) began to reverse this decline in the arts. Vasari saw ancient art as central to the rebirth of Italian art.[147]\n\nHowever, only in the 19th century did the French word renaissance achieve popularity in describing the self-conscious cultural movement based on revival of Roman models that began in the late 13th century. French historian Jules Michelet (1798–1874) defined \"The Renaissance\" in his 1855 work Histoire de France as an entire historical period, whereas previously it had been used in a more limited sense.[23] For Michelet, the Renaissance was more a development in science than in art and culture. He asserted that it spanned the period from Columbus to Copernicus to Galileo; that is, from the end of the 15th century to the middle of the 17th century.[98] Moreover, Michelet distinguished between what he called, \"the bizarre and monstrous\" quality of the Middle Ages and the democratic values that he, as a vocal Republican, chose to see in its character.[18] A French nationalist, Michelet also sought to claim the Renaissance as a French movement.[18]\n\nThe Swiss historian Jacob Burckhardt (1818–1897) in his The Civilization of the Renaissance in Italy (1860), by contrast, defined the Renaissance as the period between Giotto and Michelangelo in Italy, that is, the 14th to mid-16th centuries. He saw in the Renaissance the emergence of the modern spirit of individuality, which the Middle Ages had stifled.[148] His book was widely read and became influential in the development of the modern interpretation of the Italian Renaissance.[149]\n\nMore recently, some historians have been much less keen to define the Renaissance as a historical age, or even as a coherent cultural movement. The historian Randolph Starn, of the University of California Berkeley, stated in 1998:\n\nRather than a period with definitive beginnings and endings and consistent content in between, the Renaissance can be (and occasionally has been) seen as a movement of practices and ideas to which specific groups and identifiable persons variously responded in different times and places. It would be in this sense a network of diverse, sometimes converging, sometimes conflicting cultures, not a single, time-bound culture.[20]\n\nThere is debate about the extent to which the Renaissance improved on the culture of the Middle Ages. Both Michelet and Burckhardt were keen to describe the progress made in the Renaissance toward the modern age. Burckhardt likened the change to a veil being removed from man's eyes, allowing him to see clearly.[54]\n\nIn the Middle Ages both sides of human consciousness – that which was turned within as that which was turned without – lay dreaming or half awake beneath a common veil. The veil was woven of faith, illusion, and childish prepossession, through which the world and history were seen clad in strange hues.[150]\nOn the other hand, many historians now point out that most of the negative social factors popularly associated with the medieval period – poverty, warfare, religious and political persecution, for example – seem to have worsened in this era, which saw the rise of Machiavellian politics, the Wars of Religion, the corrupt Borgia Popes, and the intensified witch-hunts of the 16th century. Many people who lived during the Renaissance did not view it as the \"golden age\" imagined by certain 19th-century authors, but were concerned by these social maladies.[151] Significantly, though, the artists, writers, and patrons involved in the cultural movements in question believed they were living in a new era that was a clean break from the Middle Ages.[89] Some Marxist historians prefer to describe the Renaissance in material terms, holding the view that the changes in art, literature, and philosophy were part of a general economic trend from feudalism toward capitalism, resulting in a bourgeois class with leisure time to devote to the arts.[152]\n\nJohan Huizinga (1872–1945) acknowledged the existence of the Renaissance but questioned whether it was a positive change. In his book The Autumn of the Middle Ages, he argued that the Renaissance was a period of decline from the High Middle Ages, destroying much that was important.[19] The Medieval Latin language, for instance, had evolved greatly from the classical period and was still a living language used in the church and elsewhere. The Renaissance obsession with classical purity halted its further evolution and saw Latin revert to its classical form. This view is however somewhat contested by recent studies. Robert S. Lopez has contended that it was a period of deep economic recession.[153] Meanwhile, George Sarton and Lynn Thorndike have both argued that scientific progress was perhaps less original than has traditionally been supposed.[154] Finally, Joan Kelly argued that the Renaissance led to greater gender dichotomy, lessening the agency women had had during the Middle Ages.[155]\n\nSome historians have begun to consider the word Renaissance to be unnecessarily loaded, implying an unambiguously positive rebirth from the supposedly more primitive \"Dark Ages\", the Middle Ages. Most political and economic historians now prefer to use the term \"early modern\" for this period (and a considerable period afterwards), a designation intended to highlight the period as a transitional one between the Middle Ages and the modern era.[156] Others such as Roger Osborne have come to consider the Italian Renaissance as a repository of the myths and ideals of western history in general, and instead of rebirth of ancient ideas as a period of great innovation.[157]\n\nThe art historian Erwin Panofsky observed of this resistance to the concept of \"Renaissance\":\n\nIt is perhaps no accident that the factuality of the Italian Renaissance has been most vigorously questioned by those who are not obliged to take a professional interest in the aesthetic aspects of civilization – historians of economic and social developments, political and religious situations, and, most particularly, natural science – but only exceptionally by students of literature and hardly ever by historians of Art.[158]\nThe term Renaissance has also been used to define periods outside of the 15th and 16th centuries in the earlier Medieval period. Charles H. Haskins (1870–1937), for example, made a case for a Renaissance of the 12th century.[159] Other historians have argued for a Carolingian Renaissance in the 8th and 9th centuries, Ottonian Renaissance in the 10th century and for the Timurid Renaissance of the 14th century. The Islamic Golden Age has been also sometimes termed with the Islamic Renaissance.[160] The Macedonian Renaissance is a term used for a period in the Roman Empire in the 9th-11th centuries CE.\n\nOther periods of cultural rebirth in Modern times have also been termed \"renaissances\", such as the Bengal Renaissance, Tamil Renaissance, Nepal Bhasa renaissance, al-Nahda or the Harlem Renaissance. The term can also be used in cinema. In animation, the Disney Renaissance is a period that spanned the years from 1989 to 1999 which saw the studio return to the level of quality not witnessed since their Golden Age of Animation. The San Francisco Renaissance was a vibrant period of exploratory poetry and fiction writing in San Francisco in the mid-20th century.\n\nRapid accumulation of knowledge, which has characterized the development of science since the 17th century, had never occurred before that time. The new kind of scientific activity emerged only in a few countries of Western Europe, and it was restricted to that small area for about two hundred years. (Since the 19th century, scientific knowledge has been assimilated by the rest of the world)."
    },
    {
        "title": "Pseudoscience",
        "content": "\n\n\n\n\n\nPseudoscience consists of statements, beliefs, or practices that claim to be both scientific and factual but are incompatible with the scientific method.[Note 1] Pseudoscience is often characterized by contradictory, exaggerated or unfalsifiable claims; reliance on confirmation bias rather than rigorous attempts at refutation; lack of openness to evaluation by other experts; absence of systematic practices when developing hypotheses; and continued adherence long after the pseudoscientific hypotheses have been experimentally discredited.[4] It is not the same as junk science.[7]\n\nThe demarcation between science and pseudoscience has scientific, philosophical, and political implications.[8] Philosophers debate the nature of science and the general criteria for drawing the line between scientific theories and pseudoscientific beliefs, but there is widespread agreement \"that creationism, astrology, homeopathy, Kirlian photography, dowsing, ufology, ancient astronaut theory, Holocaust denialism, Velikovskian catastrophism, and climate change denialism are pseudosciences.\"[9] There are implications for health care, the use of expert testimony, and weighing environmental policies.[9] Recent empirical research has shown that individuals who indulge in pseudoscientific beliefs generally show lower evidential criteria, meaning they often require significantly less evidence before coming to conclusions. This can be coined as a 'jump-to-conclusions' bias that can increase the spread of pseudoscientific beliefs.[10] Addressing pseudoscience is part of science education and developing scientific literacy.[11][12]\n\nPseudoscience can have dangerous effects. For example, pseudoscientific anti-vaccine activism and promotion of homeopathic remedies as alternative disease treatments can result in people forgoing important medical treatments with demonstrable health benefits, leading to ill-health and deaths.[13][14][15] Furthermore, people who refuse legitimate medical treatments for contagious diseases may put others at risk. Pseudoscientific theories about racial and ethnic classifications have led to racism and genocide.\n\nThe term pseudoscience is often considered pejorative, particularly by its purveyors, because it suggests something is being presented as science inaccurately or even deceptively. Therefore, practitioners and advocates of pseudoscience frequently dispute the characterization.[4][16]\n\nThe word pseudoscience is derived from the Greek root pseudo meaning \"false\"[17][18] and the English word science, from the Latin word scientia, meaning \"knowledge\". Although the term has been in use since at least the late 18th century (e.g., in 1796 by James Pettit Andrews in reference to alchemy[19][20]), the concept of pseudoscience as distinct from real or proper science seems to have become more widespread during the mid-19th century. Among the earliest uses of \"pseudo-science\" was in an 1844 article in the Northern Journal of Medicine, issue 387:\n\nThat opposite kind of innovation which pronounces what has been recognized as a branch of science, to have been a pseudo-science, composed merely of so-called facts, connected together by misapprehensions under the disguise of principles.\nAn earlier use of the term was in 1843 by the French physiologist François Magendie, that refers to phrenology as \"a pseudo-science of the present day\".[3][21][22] During the 20th century, the word was used pejoratively to describe explanations of phenomena which were claimed to be scientific, but which were not in fact supported by reliable experimental evidence.\n\nFrom time to time, however, the usage of the word occurred in a more formal, technical manner in response to a perceived threat to individual and institutional security in a social and cultural setting.[25]\n\nPseudoscience is differentiated from science because – although it usually claims to be science – pseudoscience does not adhere to scientific standards, such as the scientific method, falsifiability of claims, and Mertonian norms.\n\nA number of basic principles are accepted by scientists as standards for determining whether a body of knowledge, method, or practice is scientific. Experimental results should be reproducible and verified by other researchers.[26] These principles are intended to ensure experiments can be reproduced measurably given the same conditions, allowing further investigation to determine whether a hypothesis or theory related to given phenomena is valid and reliable. Standards require the scientific method to be applied throughout, and bias to be controlled for or eliminated through randomization, fair sampling procedures, blinding of studies, and other methods. All gathered data, including the experimental or environmental conditions, are expected to be documented for scrutiny and made available for peer review, allowing further experiments or studies to be conducted to confirm or falsify results. Statistical quantification of significance, confidence, and error[27] are also important tools for the scientific method.\n\nDuring the mid-20th century, the philosopher Karl Popper emphasized the criterion of falsifiability to distinguish science from non-science.[28] Statements, hypotheses, or theories have falsifiability or refutability if there is the inherent possibility that they can be proven false, that is, if it is possible to conceive of an observation or an argument that negates them. Popper used astrology and psychoanalysis as examples of pseudoscience and Einstein's theory of relativity as an example of science. He subdivided non-science into philosophical, mathematical, mythological, religious and metaphysical formulations on one hand, and pseudoscientific formulations on the other.[29]\n\nAnother example which shows the distinct need for a claim to be falsifiable was stated in Carl Sagan's publication The Demon-Haunted World when he discusses an invisible dragon that he has in his garage. The point is made that there is no physical test to refute the claim of the presence of this dragon. Whatever test one thinks can be devised, there is a reason why it does not apply to the invisible dragon, so one can never prove that the initial claim is wrong. Sagan concludes; \"Now, what's the difference between an invisible, incorporeal, floating dragon who spits heatless fire and no dragon at all?\". He states that \"your inability to invalidate my hypothesis is not at all the same thing as proving it true\",[30] once again explaining that even if such a claim were true, it would be outside the realm of scientific inquiry.\n\nDuring 1942, Robert K. Merton identified a set of five \"norms\" which characterize real science. If any of the norms were violated, Merton considered the enterprise to be non-science. His norms were:\n\nIn 1978, Paul Thagard proposed that pseudoscience is primarily distinguishable from science when it is less progressive than alternative theories over a long period of time, and its proponents fail to acknowledge or address problems with the theory.[32] In 1983, Mario Bunge suggested the categories of \"belief fields\" and \"research fields\" to help distinguish between pseudoscience and science, where the former is primarily personal and subjective and the latter involves a certain systematic method.[33] The 2018 book about scientific skepticism by Steven Novella, et al. The Skeptics' Guide to the Universe lists hostility to criticism as one of the major features of pseudoscience.[34]\n\nLarry Laudan has suggested pseudoscience has no scientific meaning and is mostly used to describe human emotions: \"If we would stand up and be counted on the side of reason, we ought to drop terms like 'pseudo-science' and 'unscientific' from our vocabulary; they are just hollow phrases which do only emotive work for us\".[35] Likewise, Richard McNally states, \"The term 'pseudoscience' has become little more than an inflammatory buzzword for quickly dismissing one's opponents in media sound-bites\" and \"When therapeutic entrepreneurs make claims on behalf of their interventions, we should not waste our time trying to determine whether their interventions qualify as pseudoscientific. Rather, we should ask them: How do you know that your intervention works? What is your evidence?\"[36]\n\nFor philosophers Silvio Funtowicz and Jerome R. Ravetz \"pseudo-science may be defined as one where the uncertainty of its inputs must be suppressed, lest they render its outputs totally indeterminate\". The definition, in the book Uncertainty and Quality in Science for Policy,[37] alludes to the loss of craft skills in handling quantitative information, and to the bad practice of achieving precision in prediction (inference) only at the expenses of ignoring uncertainty in the input which was used to formulate the prediction. This use of the term is common among practitioners of post-normal science. Understood in this way, pseudoscience can be fought using good practices to assess uncertainty in quantitative information, such as NUSAP and – in the case of mathematical modelling – sensitivity auditing.\n\nThe history of pseudoscience is the study of pseudoscientific theories over time. A pseudoscience is a set of ideas that presents itself as science, while it does not meet the criteria to be properly called such.[38][39]\n\nDistinguishing between proper science and pseudoscience is sometimes difficult.[40] One proposal for demarcation between the two is the falsification criterion, attributed most notably to the philosopher Karl Popper.[41] In the history of science and the history of pseudoscience it can be especially difficult to separate the two, because some sciences developed from pseudosciences. An example of this transformation is the science of chemistry, which traces its origins to the pseudoscientific or pre-scientific study of alchemy.\n\nThe vast diversity in pseudosciences further complicates the history of science. Some modern pseudosciences, such as astrology and acupuncture, originated before the scientific era. Others developed as part of an ideology, such as Lysenkoism, or as a response to perceived threats to an ideology. Examples of this ideological process are creation science and intelligent design, which were developed in response to the scientific theory of evolution.[42]\n\nA topic, practice, or body of knowledge might reasonably be termed pseudoscientific when it is presented as consistent with the norms of scientific research, but it demonstrably fails to meet these norms.[43][44]\n\nThe Ministry of AYUSH in the Government of India is purposed with developing education, research and propagation of indigenous alternative medicine systems in India. The ministry has faced significant criticism for funding systems that lack biological plausibility and are either untested or conclusively proven as ineffective. Quality of research has been poor, and drugs have been launched without any rigorous pharmacological studies and meaningful clinical trials on Ayurveda or other alternative healthcare systems.[69][70] There is no credible efficacy or scientific basis of any of these forms of treatment.[71]\n\nIn his book The Demon-Haunted World, Carl Sagan discusses the government of China and the Chinese Communist Party's concern about Western pseudoscience developments and certain ancient Chinese practices in China. He sees pseudoscience occurring in the United States as part of a worldwide trend and suggests its causes, dangers, diagnosis and treatment may be universal.[72]\n\nA large percentage of the United States population lacks scientific literacy, not adequately understanding scientific principles and method.[Note 6][Note 7][75][Note 8] In the Journal of College Science Teaching, Art Hobson writes, \"Pseudoscientific beliefs are surprisingly widespread in our culture even among public school science teachers and newspaper editors, and are closely related to scientific illiteracy.\"[77] However, a 10,000-student study in the same journal concluded there was no strong correlation between science knowledge and belief in pseudoscience.[78]\n\nDuring 2006, the U.S. National Science Foundation (NSF) issued an executive summary of a paper on science and engineering which briefly discussed the prevalence of pseudoscience in modern times. It said, \"belief in pseudoscience is widespread\" and, referencing a Gallup Poll,[79][80] stated that belief in the 10 commonly believed examples of paranormal phenomena listed in the poll were \"pseudoscientific beliefs\".[81] The items were \"extrasensory perception (ESP), that houses can be haunted, ghosts, telepathy, clairvoyance, astrology, that people can mentally communicate with the dead, witches, reincarnation, and channelling\".[81] Such beliefs in pseudoscience represent a lack of knowledge of how science works. The scientific community may attempt to communicate information about science out of concern for the public's susceptibility to unproven claims.[81] The NSF stated that pseudoscientific beliefs in the U.S. became more widespread during the 1990s, peaked about 2001, and then decreased slightly since with pseudoscientific beliefs remaining common. According to the NSF report, there is a lack of knowledge of pseudoscientific issues in society and pseudoscientific practices are commonly followed.[82] Surveys indicate about a third of adult Americans consider astrology to be scientific.[83][84][85]\n\nIn Russia, in the late 20th and early 21st century, significant budgetary funds were spent on programs for the experimental study of \"torsion fields\",[86] the extraction of energy from granite,[87] the study of \"cold nuclear fusion\", and astrological and extrasensory \"research\" by the Ministry of Defense, the Ministry of Emergency Situations, the Ministry of Internal Affairs, and the State Duma[86] (see Military Unit 10003). In 2006, Deputy Chairman of the Security Council of the Russian Federation Nikolai Spassky published an article in Rossiyskaya Gazeta, where among the priority areas for the development of the Russian energy sector, the task of extracting energy from a vacuum was in the first place.[88] The Clean Water project was adopted as a United Russia party project; in the version submitted to the government, the program budget for 2010–2017 exceeded $14 billion.[89][88]\n\nThere have been many connections between pseudoscientific writers and researchers and their anti-semitic, racist and neo-Nazi backgrounds. They often use pseudoscience to reinforce their beliefs. One of the most predominant pseudoscientific writers is Frank Collin, a self-proclaimed Nazi who goes by Frank Joseph in his writings.[90] The majority of his works include the topics of Atlantis, extraterrestrial encounters, and Lemuria as well as other ancient civilizations, often with white supremacist undertones. For example, he posited that European peoples migrated to North America before Columbus, and that all Native American civilizations were initiated by descendants of white people.[91]\n\nThe Alt-Right using pseudoscience to base their ideologies on is not a new issue. The entire foundation of anti-semitism is based on pseudoscience, or scientific racism. In an article from Newsweek by Sander Gilman, Gilman describes the pseudoscience community's anti-semitic views. \"Jews as they appear in this world of pseudoscience are an invented group of ill, stupid or stupidly smart people who use science to their own nefarious ends. Other groups, too, are painted similarly in 'race science', as it used to call itself: African-Americans, the Irish, the Chinese and, well, any and all groups that you want to prove inferior to yourself\".[92] Neo-Nazis and white supremacist often try to support their claims with studies that \"prove\" that their claims are more than just harmful stereotypes. For example Bret Stephens published a column in The New York Times where he claimed that Ashkenazi Jews had the highest IQ among any ethnic group.[93] However, the scientific methodology and conclusions reached by the article Stephens cited has been called into question repeatedly since its publication. It has been found that at least one of that study's authors has been identified by the Southern Poverty Law Center as a white nationalist.[94]\n\nThe journal Nature has published a number of editorials in the last few years warning researchers about extremists looking to abuse their work, particularly population geneticists and those working with ancient DNA. One article in Nature, titled \"Racism in Science: The Taint That Lingers\" notes that early-twentieth-century eugenic pseudoscience has been used to influence public policy, such as the Immigration Act of 1924 in the United States, which sought to prevent immigration from Asia and parts of Europe.[95]\n\nIn a 1981 report Singer and Benassi wrote that pseudoscientific beliefs have their origin from at least four sources.[96]\n\nA 1990 study by Eve and Dunn supported the findings of Singer and Benassi and found pseudoscientific belief being promoted by high school life science and biology teachers.[97]\n\nThe psychology of pseudoscience attempts to explore and analyze pseudoscientific thinking by means of thorough clarification on making the distinction of what is considered scientific vs. pseudoscientific. The human proclivity for seeking confirmation rather than refutation (confirmation bias),[98] the tendency to hold comforting beliefs, and the tendency to overgeneralize have been proposed as reasons for pseudoscientific thinking. According to Beyerstein, humans are prone to associations based on resemblances only, and often prone to misattribution in cause-effect thinking.[99]\n\nMichael Shermer's theory of belief-dependent realism is driven by the belief that the brain is essentially a \"belief engine\" which scans data perceived by the senses and looks for patterns and meaning. There is also the tendency for the brain to create cognitive biases, as a result of inferences and assumptions made without logic and based on instinct – usually resulting in patterns in cognition. These tendencies of patternicity and agenticity are also driven \"by a meta-bias called the bias blind spot, or the tendency to recognize the power of cognitive biases in other people but to be blind to their influence on our own beliefs\".[100]\nLindeman states that social motives (i.e., \"to comprehend self and the world, to have a sense of control over outcomes, to belong, to find the world benevolent and to maintain one's self-esteem\") are often \"more easily\" fulfilled by pseudoscience than by scientific information. Furthermore, pseudoscientific explanations are generally not analyzed rationally, but instead experientially. Operating within a different set of rules compared to rational thinking, experiential thinking regards an explanation as valid if the explanation is \"personally functional, satisfying and sufficient\", offering a description of the world that may be more personal than can be provided by science and reducing the amount of potential work involved in understanding complex events and outcomes.[101]\n\nAnyone searching for psychological help that is based in science should seek a licensed therapist whose techniques are not based in pseudoscience. Hupp and Santa Maria provide a complete explanation of what that person should look for.[102]\n\nThere is a trend to believe in pseudoscience more than scientific evidence.[103] Some people believe the prevalence of pseudoscientific beliefs is due to widespread scientific illiteracy.[104] Individuals lacking scientific literacy are more susceptible to wishful thinking, since they are likely to turn to immediate gratification powered by System 1, our default operating system which requires little to no effort. This system encourages one to accept the conclusions they believe, and reject the ones they do not. Further analysis of complex pseudoscientific phenomena require System 2, which follows rules, compares objects along multiple dimensions and weighs options. These two systems have several other differences which are further discussed in the dual-process theory.[105] The scientific and secular systems of morality and meaning are generally unsatisfying to most people. Humans are, by nature, a forward-minded species pursuing greater avenues of happiness and satisfaction, but we are all too frequently willing to grasp at unrealistic promises of a better life.[106]\n\nPsychology has much to discuss about pseudoscience thinking, as it is the illusory perceptions of causality and effectiveness of numerous individuals that needs to be illuminated. Research suggests that illusionary thinking happens in most people when exposed to certain circumstances such as reading a book, an advertisement or the testimony of others are the basis of pseudoscience beliefs. It is assumed that illusions are not unusual, and given the right conditions, illusions are able to occur systematically even in normal emotional situations. One of the things pseudoscience believers quibble most about is that academic science usually treats them as fools. Minimizing these illusions in the real world is not simple.[107] To this aim, designing evidence-based educational programs can be effective to help people identify and reduce their own illusions.[107]\n\nPhilosophers classify types of knowledge. In English, the word science is used to indicate specifically the natural sciences and related fields, which are called the social sciences.[108] Different philosophers of science may disagree on the exact limits – for example, is mathematics a formal science that is closer to the empirical ones, or is pure mathematics closer to the philosophical study of logic and therefore not a science?[109] – but all agree that all of the ideas that are not scientific are non-scientific. The large category of non-science includes all matters outside the natural and social sciences, such as the study of history, metaphysics, religion, art, and the humanities.[108] Dividing the category again, unscientific claims are a subset of the large category of non-scientific claims. This category specifically includes all matters that are directly opposed to good science.[108] Un-science includes both \"bad science\" (such as an error made in a good-faith attempt at learning something about the natural world) and pseudoscience.[108] Thus pseudoscience is a subset of un-science, and un-science, in turn, is subset of non-science.\n\nScience is also distinguishable from revelation, theology, or spirituality in that it offers insight into the physical world obtained by empirical research and testing.[110][111] The most notable disputes concern the evolution of living organisms, the idea of common descent, the geologic history of the Earth, the formation of the Solar System, and the origin of the universe.[112] Systems of belief that derive from divine or inspired knowledge are not considered pseudoscience if they do not claim either to be scientific or to overturn well-established science. Moreover, some specific religious claims, such as the power of intercessory prayer to heal the sick, although they may be based on untestable beliefs, can be tested by the scientific method.\n\nSome statements and common beliefs of popular science may not meet the criteria of science. \"Pop\" science may blur the divide between science and pseudoscience among the general public, and may also involve science fiction.[113] Indeed, pop science is disseminated to, and can also easily emanate from, persons not accountable to scientific methodology and expert peer review.\n\nIf claims of a given field can be tested experimentally and standards are upheld, it is not pseudoscience, regardless of how odd, astonishing, or counterintuitive those claims are. If claims made are inconsistent with existing experimental results or established theory, but the method is sound, caution should be used, since science consists of testing hypotheses which may turn out to be false. In such a case, the work may be better described as ideas that are \"not yet generally accepted\". Protoscience is a term sometimes used to describe a hypothesis that has not yet been tested adequately by the scientific method, but which is otherwise consistent with existing science or which, where inconsistent, offers reasonable account of the inconsistency. It may also describe the transition from a body of practical knowledge into a scientific field.[28]\n\nKarl Popper stated it is insufficient to distinguish science from pseudoscience, or from metaphysics (such as the philosophical question of what existence means), by the criterion of rigorous adherence to the empirical method, which is essentially inductive, based on observation or experimentation.[46] He proposed a method to distinguish between genuine empirical, nonempirical or even pseudoempirical methods. The latter case was exemplified by astrology, which appeals to observation and experimentation. While it had empirical evidence based on observation, on horoscopes and biographies, it crucially failed to use acceptable scientific standards.[46] Popper proposed falsifiability as an important criterion in distinguishing science from pseudoscience.\n\nTo demonstrate this point, Popper[46] gave two cases of human behavior and typical explanations from Sigmund Freud and Alfred Adler's theories: \"that of a man who pushes a child into the water with the intention of drowning it; and that of a man who sacrifices his life in an attempt to save the child.\"[46] From Freud's perspective, the first man would have suffered from psychological repression, probably originating from an Oedipus complex, whereas the second man had attained sublimation. From Adler's perspective, the first and second man suffered from feelings of inferiority and had to prove himself, which drove him to commit the crime or, in the second case, drove him to rescue the child. Popper was not able to find any counterexamples of human behavior in which the behavior could not be explained in the terms of Adler's or Freud's theory. Popper argued[46] it was that the observation always fitted or confirmed the theory which, rather than being its strength, was actually its weakness. In contrast, Popper[46] gave the example of Einstein's gravitational theory, which predicted \"light must be attracted by heavy bodies (such as the Sun), precisely as material bodies were attracted.\"[46] Following from this, stars closer to the Sun would appear to have moved a small distance away from the Sun, and away from each other. This prediction was particularly striking to Popper because it involved considerable risk. The brightness of the Sun prevented this effect from being observed under normal circumstances, so photographs had to be taken during an eclipse and compared to photographs taken at night. Popper states, \"If observation shows that the predicted effect is definitely absent, then the theory is simply refuted.\"[46] Popper summed up his criterion for the scientific status of a theory as depending on its falsifiability, refutability, or testability.\n\nPaul R. Thagard used astrology as a case study to distinguish science from pseudoscience and proposed principles and criteria to delineate them.[114] First, astrology has not progressed in that it has not been updated nor added any explanatory power since Ptolemy. Second, it has ignored outstanding problems such as the precession of equinoxes in astronomy. Third, alternative theories of personality and behavior have grown progressively to encompass explanations of phenomena which astrology statically attributes to heavenly forces. Fourth, astrologers have remained uninterested in furthering the theory to deal with outstanding problems or in critically evaluating the theory in relation to other theories. Thagard intended this criterion to be extended to areas other than astrology. He believed it would delineate as pseudoscientific such practices as witchcraft and pyramidology, while leaving physics, chemistry, astronomy, geoscience, biology, and archaeology in the realm of science.[114]\n\nIn the philosophy and history of science, Imre Lakatos stresses the social and political importance of the demarcation problem, the normative methodological problem of distinguishing between science and pseudoscience. His distinctive historical analysis of scientific methodology based on research programmes suggests: \"scientists regard the successful theoretical prediction of stunning novel facts – such as the return of Halley's comet or the gravitational bending of light rays – as what demarcates good scientific theories from pseudo-scientific and degenerate theories, and in spite of all scientific theories being forever confronted by 'an ocean of counterexamples'\".[8] Lakatos offers a \"novel fallibilist analysis of the development of Newton's celestial dynamics, [his] favourite historical example of his methodology\" and argues in light of this historical turn, that his account answers for certain inadequacies in those of Karl Popper and Thomas Kuhn.[8] \"Nonetheless, Lakatos did recognize the force of Kuhn's historical criticism of Popper – all important theories have been surrounded by an 'ocean of anomalies', which on a falsificationist view would require the rejection of the theory outright...Lakatos sought to reconcile the rationalism of Popperian falsificationism with what seemed to be its own refutation by history\".[115]\n\nMany philosophers have tried to solve the problem of demarcation in the following terms: a statement constitutes knowledge if sufficiently many people believe it sufficiently strongly. But the history of thought shows us that many people were totally committed to absurd beliefs. If the strengths of beliefs were a hallmark of knowledge, we should have to rank some tales about demons, angels, devils, and of heaven and hell as knowledge. Scientists, on the other hand, are very sceptical even of their best theories. Newton's is the most powerful theory science has yet produced, but Newton himself never believed that bodies attract each other at a distance. So no degree of commitment to beliefs makes them knowledge. Indeed, the hallmark of scientific behaviour is a certain scepticism even towards one's most cherished theories. Blind commitment to a theory is not an intellectual virtue: it is an intellectual crime.\nThus a statement may be pseudoscientific even if it is eminently 'plausible' and everybody believes in it, and it may be scientifically valuable even if it is unbelievable and nobody believes in it. A theory may even be of supreme scientific value even if no one understands it, let alone believes in it.[8]\nThe boundary between science and pseudoscience is disputed and difficult to determine analytically, even after more than a century of study by philosophers of science and scientists, and despite some basic agreements on the fundamentals of the scientific method.[43][116][117] The concept of pseudoscience rests on an understanding that the scientific method has been misrepresented or misapplied with respect to a given theory, but many philosophers of science maintain that different kinds of methods are held as appropriate across different fields and different eras of human history. According to Lakatos, the typical descriptive unit of great scientific achievements is not an isolated hypothesis but \"a powerful problem-solving machinery, which, with the help of sophisticated mathematical techniques, digests anomalies and even turns them into positive evidence\".[8]\n\nTo Popper, pseudoscience uses induction to generate theories, and only performs experiments to seek to verify them. To Popper, falsifiability is what determines the scientific status of a theory. Taking a historical approach, Kuhn observed that scientists did not follow Popper's rule, and might ignore falsifying data, unless overwhelming. To Kuhn, puzzle-solving within a paradigm is science. Lakatos attempted to resolve this debate, by suggesting history shows that science occurs in research programmes, competing according to how progressive they are. The leading idea of a programme could evolve, driven by its heuristic to make predictions that can be supported by evidence. Feyerabend claimed that Lakatos was selective in his examples, and the whole history of science shows there is no universal rule of scientific method, and imposing one on the scientific community impedes progress.[118]\n Laudan maintained that the demarcation between science and non-science was a pseudo-problem, preferring to focus on the more general distinction between reliable and unreliable knowledge.[119]\n\n[Feyerabend] regards Lakatos's view as being closet anarchism disguised as methodological rationalism. Feyerabend's claim was not that standard methodological rules should never be obeyed, but rather that sometimes progress is made by abandoning them. In the absence of a generally accepted rule, there is a need for alternative methods of persuasion. According to Feyerabend, Galileo employed stylistic and rhetorical techniques to convince his reader, while he also wrote in Italian rather than Latin and directed his arguments to those already temperamentally inclined to accept them.[115]\nThe demarcation problem between science and pseudoscience brings up debate in the realms of science, philosophy and politics. Imre Lakatos, for instance, points out that the Communist Party of the Soviet Union at one point declared that Mendelian genetics was pseudoscientific and had its advocates, including well-established scientists such as Nikolai Vavilov, sent to a Gulag and that the \"liberal Establishment of the West\" denies freedom of speech to topics it regards as pseudoscience, particularly where they run up against social mores.[8]\n\nSomething becomes pseudoscientific when science cannot be separated from ideology, scientists misrepresent scientific findings to promote or draw attention for publicity, when politicians, journalists and a nation's intellectual elite distort the facts of science for short-term political gain, or when powerful individuals of the public conflate causation and cofactors by clever wordplay. These ideas reduce the authority, value, integrity and independence of science in society.[120]\n\nDistinguishing science from pseudoscience has practical implications in the case of health care, expert testimony, environmental policies, and science education. Treatments with a patina of scientific authority which have not actually been subjected to actual scientific testing may be ineffective, expensive and dangerous to patients and confuse health providers, insurers, government decision makers and the public as to what treatments are appropriate. Claims advanced by pseudoscience may result in government officials and educators making bad decisions in selecting curricula.[Note 9]\n\nThe extent to which students acquire a range of social and cognitive thinking skills related to the proper usage of science and technology determines whether they are scientifically literate. Education in the sciences encounters new dimensions with the changing landscape of science and technology, a fast-changing culture and a knowledge-driven era. A reinvention of the school science curriculum is one that shapes students to contend with its changing influence on human welfare. Scientific literacy, which allows a person to distinguish science from pseudosciences such as astrology, is among the attributes that enable students to adapt to the changing world. Its characteristics are embedded in a curriculum where students are engaged in resolving problems, conducting investigations, or developing projects.[11]\n\nAlan J. Friedman mentions why most scientists avoid educating about pseudoscience, including that paying undue attention to pseudoscience could dignify it.[121]\n\nOn the other hand, Robert L. Park emphasizes how pseudoscience can be a threat to society and considers that scientists have a responsibility to teach how to distinguish science from pseudoscience.[122]\n\nPseudosciences such as homeopathy, even if generally benign, are used by charlatans. This poses a serious issue because it enables incompetent practitioners to administer health care. True-believing zealots may pose a more serious threat than typical con men because of their delusion to homeopathy's ideology. Irrational health care is not harmless and it is careless to create patient confidence in pseudomedicine.[123]\n\nOn 8 December 2016, journalist Michael V. LeVine pointed out the dangers posed by the Natural News website: \"Snake-oil salesmen have pushed false cures since the dawn of medicine, and now websites like Natural News flood social media with dangerous anti-pharmaceutical, anti-vaccination and anti-GMO pseudoscience that puts millions at risk of contracting preventable illnesses.\"[124]\n\nThe anti-vaccine movement has persuaded large numbers of parents not to vaccinate their children, citing pseudoscientific research that links childhood vaccines with the onset of autism.[125] These include the study by Andrew Wakefield, which claimed that a combination of gastrointestinal disease and developmental regression, which are often seen in children with ASD, occurred within two weeks of receiving vaccines.[126][127] The study was eventually retracted by its publisher, and Wakefield was stripped of his license to practice medicine.[125]\n\nAlkaline water is water that has a pH of higher than 7, purported to host numerous health benefits, with no empirical backing. A practitioner known as Robert O. Young who promoted alkaline water and an \"Alkaline diet\" was sent to jail for 3 years in 2017 for practicing medicine without a license.[128]\n"
    },
    {
        "title": "Numerology",
        "content": "\n\nNumerology (known before the 20th century as arithmancy) is the belief in an occult, divine or mystical relationship between a number and one or more coinciding events. It is also the study of the numerical value, via an alphanumeric system, of the letters in words and names. When numerology is applied to a person's name, it is a form of onomancy. It is often associated with astrology and other divinatory arts.[2]\n\nThe term numerologist can be used for those who place faith in numerical patterns and draw  inferences from them, even if those people do not practice traditional numerology. For example, in his 1997 book Numerology: Or What Pythagoras Wrought (Dudley 1997), mathematician Underwood Dudley uses the term to discuss practitioners of the Elliott wave principle of stock market analysis.\n\nThe term arithmancy is derived from two Greek words – arithmos (meaning number) and manteia (meaning divination). \"Αριθμομαντεία\" Arithmancy is thus the study of divination through numbers.[3] Although the word \"arithmancy\" dates to the 1570s,[4] the word \"numerology\" is not recorded in English before c. 1907.[5]\n\nThe practice of gematria, assigning numerical values to words and names and imputing those values with religious meaning, dates back to antiquity. An Assyrian inscription from the 8th century BC, commissioned by Sargon II declares \"the king built the wall of Khorsabad 16,283 cubits long to correspond with the numerical value of his name\".[6] Rabbinic literature used gematria to interpret passages in the Hebrew Bible.\n\nThe practice of using alphabetic letters to represent numbers developed in the Greek city of Miletus, and is thus known as the Milesian system.[7] Early examples include vase graffiti dating to the 6th century BCE.[8] Aristotle wrote that the Pythgoraean tradition, founded in the 6th century by Pythagoras of Samos, practiced isopsephy,[9] the Greek predecessor of Hebrew gematria. Pythagoras was a contemporary of the philosophers Anaximander, Anaximenes, and the historian Hecataeus, all of whom lived in Miletus, across the sea from Samos.[10] The Milesian system was in common use by the reign of Alexander the Great (336–323 BCE) and was adopted by other cultures during the subsequent Hellenistic period.[7] It was officially adopted in Egypt during the reign of Ptolemy II Philadelphus (284–246 BCE).[7]\n\nIn 325 AD, following the First Council of Nicaea, departures from the beliefs of the state church were classified as civil violations within the Roman Empire. Numerology, referred to as isopsephy, remained in use in conservative Greek Orthodox circles.[citation needed]\n\nSome alchemical theories were closely related to numerology. For example, Arab alchemist Jabir ibn Hayyan (died c. 806−816) framed his experiments in an elaborate numerology based on the names of substances in the Arabic language.[11]\n\nNumerology is prominent in Sir Thomas Browne's 1658 literary discourse The Garden of Cyrus. Throughout its pages, the author attempts to demonstrate that the number five and the related quincunx pattern can be found throughout the arts, in design, and in nature –  particularly botany.[citation needed]\n\nSome approaches to understanding the meanings of the Qur'an (the book of Muslims) include the understanding of numerical meanings, numerical symbols and their combination with purely textual approaches.[12][13]\n\nThere are various numerology systems which assign numerical value to the letters of an alphabet. Examples include the Abjad numerals in Arabic, Hebrew numerals, Armenian numerals, and Greek numerals. The practice within Jewish tradition of assigning mystical meaning to words based on their numerical values, and on connections between words of equal value, is known as gematria.[14]\n\nThe Mandaean number alphasyllabary is also used for numerology (Mandaic: gmaṭ aria). The Book of the Zodiac is an important Mandaean text on numerology.[15]\n\nIn the Pythagorean method (which uses a kind of place-value for number-letter attributions, as does the ancient Hebrew and Greek systems), the letters of the modern Latin alphabet are assigned numerical values 1 through 9.[16]\n\nHeinrich Cornelius Agrippa applied the concept of arithmancy to the classical Latin alphabet in the 16th century in Three Books of Occult Philosophy. He mapped the letters as follows (in accordance with the Latin alphabet's place-value at that time):[17]\n\nNote that the letters U, J, and W were not commonly considered part of the Latin alphabet at the time.\n\nA lesser known method, more popular in the nineteenth and early twentieth century, is the Chaldean method; in this context, \"Chaldean\" is an old-fashioned name for the Aramaic languages. In the Chaldean method number 9 is not used in the calculations, at least in practice. It is left out because it is thought to be divine and sacred, and therefore unassignable.\n\nThis method is radically different from the Pythagorean (as well as both the ancient Greek and Hebrew systems) as letters are assigned values based on equating Latin letters with letters of the Hebrew alphabet in accordance with sound equivalents (then number associations being derived via its gematria) rather than applying the ancient system of place-value used by the Hebrew and Greek gematria (although 'place-value' is almost universally interpreted in the ancient world according to units, tens and hundreds, which nonetheless have the same digital root as place value); in consequence of this there are several slightly different versions, there being disagreements over some of the letter-sound equivalents.[18]\n\nAngel numbers, as defined by Doreen Virtue and Lynnette Brown in 2004, are numbers consisting of repeating digits, such as 111 or 444.[19] As of 2023[update], a number of popular media publications have published articles suggesting that these numbers have numerological significance.[20] Doreen Virtue has since renounced the concept of angel numbers in a 2024 interview with The Cut declaring that \"It’s garbage. I regret it, and I’m sorry that I made them.\"[21]\n\nThere are various systems of English gematria or numerology.[22] These systems interpret the letters of the Roman script or English alphabet via an assigned set of numerological significances.[23][24] English Qaballa, on the other hand, refers specifically to a Qabalah supported by a system discovered by James Lees in 1976.\n\nThe first system of English gematria was used by the poet John Skelton in 1523 in his poem \"The Garland of Laurel\".[25] The next reference to an English gematria found in the literature was made by Willis F. Whitehead in 1899 in his book, The Mystic Thesaurus, in which he describes a system he called \"English Cabala\".[26]\n\nIn 1952, John P. L. Hughes published The Hidden Numerical Significance of the English Language, or, Suggestive Gematria, based on his lecture delivered at Holden Research Circle on July 4, 1952.[27] A system related to the Spiritualist Agasha Temple of Wisdom was described by William Eisen in his two volume The English Cabalah (1980–82).[28][29][30]\n\nWilliam G. Gray proposes another system in his 1984 book, Concepts of Qabalah,[31] more recently republished as Qabalistic Concepts.[32] This system includes correspondence attributions of the English letters to the positions on the Tree of Life. Michael Bertiaux described a system called Angelic Gematria in his The Voudon Gnostic Workbook (1989).[33] David Rankine described a system of English gematria[34] using prime numbers which he calls Prime Qabalah in his book Becoming Magick (2004).[35]\n\nScientific theories are sometimes labeled \"numerology\" if their primary inspiration appears to be a set of patterns rather than scientific observations. This colloquial use of the term is quite common within the scientific community and it is mostly used to dismiss a theory as questionable science.[citation needed]\n\nThe best known example of \"numerology\" in science involves the coincidental resemblance of certain large numbers that intrigued mathematical physicist Paul Dirac, mathematician Hermann Weyl and astronomer Arthur Stanley Eddington.[36] These numerical coincidences refer to such quantities as the ratio of the age of the universe to the atomic unit of time, the number of electrons in the universe, and the difference in strengths between gravity and the electric force for the electron and proton.[37] (See also Fine-tuned universe).\n\nWolfgang Pauli was also fascinated by the appearance of certain numbers, including 137 (a prime number), in physics.[38]\n\nBritish mathematician I. J. Good wrote:\n\nThere have been a few examples of numerology that have led to theories that transformed society: see the mention of Kirchhoff and Balmer in Good (1962), p. 316 [...] and one can well include Kepler on account of his third law. It would be fair enough to say that numerology was the origin of the theories of electromagnetism, quantum mechanics, gravitation. [...] So I intend no disparagement when I describe a formula as numerological.\n\nWhen a numerological formula is proposed, then we may ask whether it is correct. [...] I think an appropriate definition of correctness is that the formula has a good explanation, in a Platonic sense, that is, the explanation could be based on a good theory that is not yet known but 'exists' in the universe of possible reasonable ideas.[39]\n"
    },
    {
        "title": "Astrology",
        "content": "\n\nAstrology is a range of divinatory practices, recognized as pseudoscientific since the 18th century,[1][2] that propose that information about human affairs and terrestrial events may be discerned by studying the apparent positions of celestial objects.[3][4][5][6][7] Different cultures have employed forms of astrology since at least the 2nd millennium BCE, these practices having originated in calendrical systems used to predict seasonal shifts and to interpret celestial cycles as signs of divine communications.[8] Most, if not all, cultures have attached importance to what they observed in the sky, and some—such as the Hindus, Chinese, and the Maya—developed elaborate systems for predicting terrestrial events from celestial observations. Western astrology, one of the oldest astrological systems still in use, can trace its roots to 19th–17th century BCE Mesopotamia, from where it spread to Ancient Greece, Rome, the Islamic world, and eventually Central and Western Europe. Contemporary Western astrology is often associated with systems of horoscopes that purport to explain aspects of a person's personality and predict significant events in their lives based on the positions of celestial objects; the majority of professional astrologers rely on such systems.[9]\n\nThroughout its history, astrology has had its detractors, competitors and skeptics who opposed it for moral, religious, political, and empirical reasons.[10][11][12] Nonetheless, prior to the Enlightenment, astrology was generally considered a scholarly tradition and was common in learned circles, often in close relation with astronomy, meteorology, medicine, and alchemy.[13] It was present in political circles and is mentioned in various works of literature, from Dante Alighieri and Geoffrey Chaucer to William Shakespeare, Lope de Vega, and Pedro Calderón de la Barca. During the Enlightenment, however, astrology lost its status as an area of legitimate scholarly pursuit.[14][15] Following the end of the 19th century and the wide-scale adoption of the scientific method, researchers have successfully challenged astrology on both theoretical[16][17] and experimental grounds,[18][19] and have shown it to have no scientific validity or explanatory power.[20] Astrology thus lost its academic and theoretical standing in the western world, and common belief in it largely declined, until a continuing resurgence starting in the 1960s.[21]\n\nThe word astrology comes from the early Latin word astrologia,[22] which derives from the Greek ἀστρολογία—from ἄστρον astron (\"star\") and -λογία -logia, (\"study of\"—\"account of the stars\"). The word entered the English language via Latin and medieval French, and its use overlapped considerably with that of astronomy (derived from the Latin astronomia). By the 17th century, astronomy became established as the scientific term, with astrology referring to divinations and schemes for predicting human affairs.[23]\n\nMany cultures have attached importance to astronomical events, and the Indians, Chinese, and Maya developed elaborate systems for predicting terrestrial events from celestial observations. A form of astrology was practised in the Old Babylonian period of Mesopotamia, c. 1800 BCE.[24][8] Vedāṅga Jyotiṣa is one of earliest known Hindu texts on astronomy and astrology (Jyotisha). The text is dated between 1400 BCE to final centuries BCE by various scholars according to astronomical and linguistic evidences. Chinese astrology was elaborated in the Zhou dynasty (1046–256 BCE). Hellenistic astrology after 332 BCE mixed Babylonian astrology with Egyptian Decanic astrology in Alexandria, creating horoscopic astrology. Alexander the Great's conquest of Asia allowed astrology to spread to Ancient Greece and Rome. In Rome, astrology was associated with \"Chaldean wisdom\". After the conquest of Alexandria in the 7th century, astrology was taken up by Islamic scholars, and Hellenistic texts were translated into Arabic and Persian. In the 12th century, Arabic texts were imported to Europe and translated into Latin. Major astronomers including Tycho Brahe, Johannes Kepler and Galileo practised as court astrologers. Astrological references appear in literature in the works of poets such as Dante Alighieri and Geoffrey Chaucer, and of playwrights such as Christopher Marlowe and William Shakespeare.\n\nThroughout most of its history, astrology was considered a scholarly tradition. It was accepted in political and academic contexts, and was connected with other studies, such as astronomy, alchemy, meteorology, and medicine.[13] At the end of the 17th century, new scientific concepts in astronomy and physics (such as heliocentrism and Newtonian mechanics) called astrology into question. Astrology thus lost its academic and theoretical standing, and common belief in astrology has largely declined.[21]\n\nAstrology, in its broadest sense, is the search for meaning in the sky.[25] Early evidence for humans making conscious attempts to measure, record, and predict seasonal changes by reference to astronomical cycles, appears as markings on bones and cave walls, which show that lunar cycles were being noted as early as 25,000 years ago.[26] This was a first step towards recording the Moon's influence upon tides and rivers, and towards organising a communal calendar.[26] Farmers addressed agricultural needs with increasing knowledge of the constellations that appear in the different seasons—and used the rising of particular star-groups to herald annual floods or seasonal activities.[27] By the 3rd millennium BCE, civilisations had sophisticated awareness of celestial cycles, and may have oriented temples in alignment with heliacal risings of the stars.[28]\n\nScattered evidence suggests that the oldest known astrological references are copies of texts made in the ancient world. The Venus tablet of Ammisaduqa is thought to have been compiled in Babylon around 1700 BCE.[29] A scroll documenting an early use of electional astrology is doubtfully ascribed to the reign of the Sumerian ruler Gudea of Lagash (c. 2144 – 2124 BCE). This describes how the gods revealed to him in a dream the constellations that would be most favourable for the planned construction of a temple.[30] However, there is controversy about whether these were genuinely recorded at the time or merely ascribed to ancient rulers by posterity. The oldest undisputed evidence of the use of astrology as an integrated system of knowledge is therefore attributed to the records of the first dynasty of Babylon (1950–1651 BCE). This astrology had some parallels with Hellenistic Greek (western) astrology, including the zodiac, a norming point near 9 degrees in Aries, the trine aspect, planetary exaltations, and the dodekatemoria (the twelve divisions of 30 degrees each).[31] The Babylonians viewed celestial events as possible signs rather than as causes of physical events.[31]\n\nThe system of Chinese astrology was elaborated during the Zhou dynasty (1046–256 BCE) and flourished during the Han dynasty (2nd century BCE to 2nd century CE), during which all the familiar elements of traditional Chinese culture – the Yin-Yang philosophy, theory of the five elements, Heaven and Earth, Confucian morality – were brought together to formalise the philosophical principles of Chinese medicine and divination, astrology, and alchemy.[32]\n\nThe ancient Arabs that inhabited the Arabian Peninsula before the advent of Islam used to profess a widespread belief in fatalism (ḳadar) alongside a fearful consideration for the sky and the stars, which they held to be ultimately responsible for every phenomena that occurs on Earth and for the destiny of humankind.[33] Accordingly, they shaped their entire lives in accordance with their interpretations of astral configurations and phenomena.[33]\n\nThe Hellenistic schools of philosophical skepticism criticized the rationality of astrology.[clarification needed] Criticism of astrology by academic skeptics such as Cicero, Carneades, and Favorinus; and Pyrrhonists such as Sextus Empiricus has been preserved.\n\nCarneades argued that belief in fate denies free will and morality; that people born at different times can all die in the same accident or battle; and that contrary to uniform influences from the stars, tribes and cultures are all different.[12]\n\nCicero, in De Divinatione, leveled a critique of astrology that some modern philosophers consider to be the first working definition of pseudoscience and the answer to the demarcation problem.[11] Philosopher of Science Massimo Pigliucci, building on the work of Historian of Science, Damien Fernandez-Beanato, argues that Cicero outlined a \"convincing distinction between astrology and astronomy that remains valid in the twenty-first century.\"[10] Cicero stated the twins objection (that with close birth times, personal outcomes can be very different), later developed by Augustine.[34] He argued that since the other planets are much more distant from the Earth than the Moon, they could have only very tiny influence compared to the Moon's.[35] He also argued that if astrology explains everything about a person's fate, then it wrongly ignores the visible effect of inherited ability and parenting, changes in health worked by medicine, or the effects of the weather on people.[36]\n\nFavorinus argued that it was absurd to imagine that stars and planets would affect human bodies in the same way as they affect the tides,[37] and equally absurd that small motions in the heavens cause large changes in people's fates.\n\nSextus Empiricus argued that it was absurd to link human attributes with myths about the signs of the zodiac,[38] and wrote an entire book, Against the Astrologers (Πρὸς ἀστρολόγους, Pros astrologous), compiling arguments against astrology. Against the Astrologers was the fifth section of a larger work arguing against philosophical and scientific inquiry in general, Against the Professors (Πρὸς μαθηματικούς, Pros mathematikous).\n\nPlotinus, a neoplatonist, argued that since the fixed stars are much more distant than the planets, it is laughable to imagine the planets' effect on human affairs should depend on their position with respect to the zodiac. He also argues that the interpretation of the Moon's conjunction with a planet as good when the moon is full, but bad when the moon is waning, is clearly wrong, as from the Moon's point of view, half of its surface is always in sunlight; and from the planet's point of view, waning should be better, as then the planet sees some light from the Moon, but when the Moon is full to us, it is dark, and therefore bad, on the side facing the planet in question.[39]\n\nIn 525 BCE, Egypt was conquered by the Persians. The 1st century BCE Egyptian Dendera Zodiac shares two signs – the Balance and the Scorpion – with Mesopotamian astrology.[40]\n\nWith the occupation by Alexander the Great in 332 BCE, Egypt became Hellenistic. The city of Alexandria was founded by Alexander after the conquest, becoming the place where Babylonian astrology was mixed with Egyptian Decanic astrology to create Horoscopic astrology. This contained the Babylonian zodiac with its system of planetary exaltations, the triplicities of the signs and the importance of eclipses. It used the Egyptian concept of dividing the zodiac into thirty-six decans of ten degrees each, with an emphasis on the rising decan, and the Greek system of planetary Gods, sign rulership and four elements.[41] 2nd century BCE texts predict positions of planets in zodiac signs at the time of the rising of certain decans, particularly Sothis.[42] The astrologer and astronomer Ptolemy lived in Alexandria. Ptolemy's work the Tetrabiblos formed the basis of Western astrology, and, \"...enjoyed almost the authority of a Bible among the astrological writers of a thousand years or more.\"[43]\n\nThe conquest of Asia by Alexander the Great exposed the Greeks to ideas from Syria, Babylon, Persia and central Asia.[44] Around 280 BCE, Berossus, a priest of Bel from Babylon, moved to the Greek island of Kos, teaching astrology and Babylonian culture.[45] By the 1st century BCE, there were two varieties of astrology, one using horoscopes to describe the past, present and future; the other, theurgic, emphasising the soul's ascent to the stars.[46] Greek influence played a crucial role in the transmission of astrological theory to Rome.[47]\n\nThe first definite reference to astrology in Rome comes from the orator Cato, who in 160 BCE warned farm overseers against consulting with Chaldeans,[48] who were described as Babylonian 'star-gazers'.[49] Among both Greeks and Romans, Babylonia (also known as Chaldea) became so identified with astrology that 'Chaldean wisdom' became synonymous with divination using planets and stars.[50] The 2nd-century Roman poet and satirist Juvenal complains about the pervasive influence of Chaldeans, saying, \"Still more trusted are the Chaldaeans; every word uttered by the astrologer they will believe has come from Hammon's fountain.\"[51]\n\nOne of the first astrologers to bring Hermetic astrology to Rome was Thrasyllus, astrologer to the emperor Tiberius,[47] the first emperor to have had a court astrologer,[52] though his predecessor Augustus had used astrology to help legitimise his Imperial rights.[53]\n\nThe main texts upon which classical Indian astrology is based are early medieval compilations, notably the Bṛhat Parāśara Horāśāstra, and Sārāvalī by Kalyāṇavarma.\nThe Horāshastra is a composite work of 71 chapters, of which the first part (chapters 1–51) dates to the 7th to early 8th centuries and the second part (chapters 52–71) to the later 8th century. The Sārāvalī likewise dates to around 800 CE.[54] English translations of these texts were published by N.N. Krishna Rau and V.B. Choudhari in 1963 and 1961, respectively.\n\nAstrology was taken up by Islamic scholars[55] following the collapse of Alexandria to the Arabs in the 7th century, and the founding of the Abbasid empire in the 8th. The second Abbasid caliph, Al Mansur (754–775) founded the city of Baghdad to act as a centre of learning, and included in its design a library-translation centre known as Bayt al-Hikma 'House of Wisdom', which continued to receive development from his heirs and was to provide a major impetus for Arabic-Persian translations of Hellenistic astrological texts. The early translators included Mashallah, who helped to elect the time for the foundation of Baghdad,[56] and Sahl ibn Bishr, (a.k.a. Zael), whose texts were directly influential upon later European astrologers such as Guido Bonatti in the 13th century, and William Lilly in the 17th century.[57] Knowledge of Arabic texts started to become imported into Europe during the Latin translations of the 12th century.\n\nIn the seventh century, Isidore of Seville argued in his Etymologiae that astronomy described the movements of the heavens, while astrology had two parts: one was scientific, describing the movements of the Sun, the Moon and the stars, while the other, making predictions, was theologically erroneous.[58][59]\n\nThe first astrological book published in Europe was the Liber Planetis et Mundi Climatibus (\"Book of the Planets and Regions of the World\"), which appeared between 1010 and 1027 AD, and may have been authored by Gerbert of Aurillac.[60] Ptolemy's second century AD Tetrabiblos was translated into Latin by Plato of Tivoli in 1138.[60] The Dominican theologian Thomas Aquinas followed Aristotle in proposing that the stars ruled the imperfect 'sublunary' body, while attempting to reconcile astrology with Christianity by stating that God ruled the soul.[61] The thirteenth century mathematician Campanus of Novara is said to have devised a system of astrological houses that divides the prime vertical into 'houses' of equal 30° arcs,[62] though the system was used earlier in the East.[63] The thirteenth century astronomer Guido Bonatti wrote a textbook, the Liber Astronomicus, a copy of which King Henry VII of England owned at the end of the fifteenth century.[62]\n\nIn Paradiso, the final part of the Divine Comedy, the Italian poet Dante Alighieri referred \"in countless details\"[64] to the astrological planets, though he adapted traditional astrology to suit his Christian viewpoint,[64] for example using astrological thinking in his prophecies of the reform of Christendom.[65]\n\nJohn Gower in the fourteenth century defined astrology as essentially limited to the making of predictions.[58][66] The influence of the stars was in turn divided into natural astrology, with for example effects on tides and the growth of plants, and judicial astrology, with supposedly predictable effects on people.[67][68] The fourteenth-century sceptic Nicole Oresme however included astronomy as a part of astrology in his Livre de divinacions.[69] Oresme argued that current approaches to prediction of events such as plagues, wars, and weather were inappropriate, but that such prediction was a valid field of inquiry. However, he attacked the use of astrology to choose the timing of actions (so-called interrogation and election) as wholly false, and rejected the determination of human action by the stars on grounds of free will.[69][70] The friar Laurens Pignon (c. 1368–1449)[71] similarly rejected all forms of divination and determinism, including by the stars, in his 1411 Contre les Devineurs.[72] This was in opposition to the tradition carried by the Arab astronomer Albumasar (787–886) whose Introductorium in Astronomiam and De Magnis Coniunctionibus argued the view that both individual actions and larger scale history are determined by the stars.[73]\n\nIn the late 15th century, Giovanni Pico della Mirandola forcefully attacked astrology in Disputationes contra Astrologos, arguing that the heavens neither caused, nor heralded earthly events.[74] His contemporary, Pietro Pomponazzi, a \"rationalistic and critical thinker\", was much more sanguine about astrology and critical of Pico's attack.[75]\n\nRenaissance scholars commonly practised astrology. Gerolamo Cardano cast the horoscope of king Edward VI of England, while John Dee was the personal astrologer to queen Elizabeth I of England. Catherine de Medici paid Michael Nostradamus in 1566 to verify the prediction of the death of her husband, king Henry II of France made by her astrologer Lucus Gauricus. Major astronomers who practised as court astrologers included Tycho Brahe in the royal court of Denmark, Johannes Kepler to the Habsburgs, Galileo Galilei to the Medici, and Giordano Bruno who was burnt at the stake for heresy in Rome in 1600.[76] The distinction between astrology and astronomy was not entirely clear. Advances in astronomy were often motivated by the desire to improve the accuracy of astrology.[77] Kepler, for example, was driven by a belief in harmonies between Earthly and celestial affairs, yet he disparaged the activities of most astrologers as \"evil-smelling dung\".[78]\n\nEphemerides with complex astrological calculations, and almanacs interpreting celestial events for use in medicine and for choosing times to plant crops, were popular in Elizabethan England.[79] In 1597, the English mathematician and physician Thomas Hood made a set of paper instruments that used revolving overlays to help students work out relationships between fixed stars or constellations, the midheaven, and the twelve astrological houses.[80] Hood's instruments also illustrated, for pedagogical purposes, the supposed relationships between the signs of the zodiac, the planets, and the parts of the human body adherents believed were governed by the planets and signs.[80][81] While Hood's presentation was innovative, his astrological information was largely standard and was taken from Gerard Mercator's astrological disc made in 1551, or a source used by Mercator.[82][83] Despite its popularity, Renaissance astrology had what historian Gabor Almasi calls \"elite debate\", exemplified by the polemical letters of Swiss physician Thomas Erastus who fought against astrology, calling it \"vanity\" and \"superstition.\" Then around the time of the new star of 1572 and the comet of 1577 there began what Almasi calls an \"extended epistemological reform\" which began the process of excluding religion, astrology and anthropocentrism from scientific debate.[84] By 1679, the yearly publication La Connoissance des temps eschewed astrology as a legitimate topic.[85]\n\nDuring the Enlightenment, intellectual sympathy for astrology fell away, leaving only a popular following supported by cheap almanacs.[14][15] One English almanac compiler, Richard Saunders, followed the spirit of the age by printing a derisive Discourse on the Invalidity of Astrology, while in France Pierre Bayle's Dictionnaire of 1697 stated that the subject was puerile.[14] The Anglo-Irish satirist Jonathan Swift ridiculed the Whig political astrologer John Partridge.[14]\n\nIn the second half of the 17th century, the Society of Astrologers (1647–1684), a trade, educational, and social organization, sought to unite London's often fractious astrologers in the task of revitalizing astrology. Following the template of the popular \"Feasts of Mathematicians\" they endeavored to defend their art in the face of growing religious criticism. The Society hosted banquets, exchanged \"instruments and manuscripts\", proposed research projects, and funded the publication of sermons that depicted astrology as a legitimate biblical pursuit for Christians. They commissioned sermons that argued Astrology was divine, Hebraic, and scripturally supported by Bible passages about the Magi and the sons of Seth. According to historian Michelle Pfeffer, \"The society's public relations campaign ultimately failed.\" Modern historians have mostly neglected the Society of Astrologers in favor of the still extant Royal Society (1660), even though both organizations initially had some of the same members.[86]\n\nAstrology saw a popular revival starting in the 19th century, as part of a general revival of spiritualism and—later, New Age philosophy,[87] and through the influence of mass media such as newspaper horoscopes.[88] Early in the 20th century the psychiatrist Carl Jung developed some concepts concerning astrology,[89] which led to the development of psychological astrology.[90][91][92]\n\nAdvocates have defined astrology as a symbolic language, an art form, a science, and a method of divination.[93][94] Though most cultural astrology systems share common roots in ancient philosophies that influenced each other, many use methods that differ from those in the West. These include Hindu astrology (also known as \"Indian astrology\" and in modern times referred to as \"Vedic astrology\") and Chinese astrology, both of which have influenced the world's cultural history.\n\nWestern astrology is a form of divination based on the construction of a horoscope for an exact moment, such as a person's birth.[95] It uses the tropical zodiac, which is aligned to the equinoctial points.[96]\n\nWestern astrology is founded on the movements and relative positions of celestial bodies such as the Sun, Moon and planets, which are analysed by their movement through signs of the zodiac (twelve spatial divisions of the ecliptic) and by their aspects (based on geometric angles) relative to one another. They are also considered by their placement in houses (twelve spatial divisions of the sky).[97] Astrology's modern representation in western popular media is usually reduced to sun sign astrology, which considers only the zodiac sign of the Sun at an individual's date of birth, and represents only 1/12 of the total chart.[98]\n\nThe horoscope visually expresses the set of relationships for the time and place of the chosen event. These relationships are between the seven 'planets', signifying tendencies such as war and love; the twelve signs of the zodiac; and the twelve houses. Each planet is in a particular sign and a particular house at the chosen time, when observed from the chosen place, creating two kinds of relationship.[99] A third kind is the aspect of each planet to every other planet, where for example two planets 120° apart (in 'trine') are in a harmonious relationship, but two planets 90° apart ('square') are in a conflicted relationship.[100][101] Together these relationships and their interpretations are said to form \"...the language of the heavens speaking to learned men.\"[99]\n\nAlong with tarot divination, astrology is one of the core studies of Western esotericism, and as such has influenced systems of magical belief not only among Western esotericists and Hermeticists, but also belief systems such as Wicca, which have borrowed from or been influenced by the Western esoteric tradition. Tanya Luhrmann has said that \"all magicians know something about astrology,\" and refers to a table of correspondences in Starhawk's The Spiral Dance, organised by planet, as an example of the astrological lore studied by magicians.[102]\n\nThe earliest Vedic text on astronomy is the Vedanga Jyotisha; Vedic thought later came to include astrology as well.[103]\n\nHindu natal astrology originated with Hellenistic astrology by the 3rd century BCE,[104][105] though incorporating the Hindu lunar mansions.[106] The names of the signs (e.g. Greek 'Krios' for Aries, Hindi 'Kriya'), the planets (e.g. Greek 'Helios' for Sun, astrological Hindi 'Heli'), and astrological terms (e.g. Greek 'apoklima' and 'sunaphe' for declination and planetary conjunction, Hindi 'apoklima' and 'sunapha' respectively) in Varaha Mihira's texts are considered conclusive evidence of a Greek origin for Hindu astrology.[107] The Indian techniques may also have been augmented with some of the Babylonian techniques.[108]\n\nChinese astrology has a close relation with Chinese philosophy (theory of the three harmonies: heaven, earth and man) and uses concepts such as yin and yang, the Five phases, the 10 Celestial stems, the 12 Earthly Branches, and shichen (時辰 a form of timekeeping used for religious purposes). The early use of Chinese astrology was mainly confined to political astrology, the observation of unusual phenomena, identification of portents and the selection of auspicious days for events and decisions.[109]\n\nThe constellations of the Zodiac of western Asia and Europe were not used; instead the sky is divided into Three Enclosures (三垣 sān yuán), and Twenty-Eight Mansions (二十八宿 èrshíbā xiù) in twelve Ci (十二次).[110] The Chinese zodiac of twelve animal signs is said to represent twelve different types of personality. It is based on cycles of years, lunar months, and two-hour periods of the day (the shichen). The zodiac traditionally begins with the sign of the Rat, and the cycle proceeds through 11 other animal signs: the Ox, Tiger, Rabbit, Dragon, Snake, Horse, Goat, Monkey, Rooster, Dog, and Pig.[111] Complex systems of predicting fate and destiny based on one's birthday, birth season, and birth hours, such as ziping and Zi Wei Dou Shu (simplified Chinese: 紫微斗数; traditional Chinese: 紫微斗數; pinyin: zǐwēidǒushù) are still used regularly in modern-day Chinese astrology. They do not rely on direct observations of the stars.[112]\n\nThe Korean zodiac is identical to the Chinese one. The Vietnamese zodiac is almost identical to the Chinese, except for second animal being the Water Buffalo instead of the Ox, and the fourth animal the Cat instead of the Rabbit. The Japanese have since 1873 celebrated the beginning of the new year on 1 January as per the Gregorian calendar. The Thai zodiac begins, not at Chinese New Year, but either on the first day of the fifth month in the Thai lunar calendar, or during the Songkran festival (now celebrated every 13–15 April), depending on the purpose of the use.[113]\n\nAugustine (354–430) believed that the determinism of astrology conflicted with the Christian doctrines of man's free will and responsibility, and God not being the cause of evil,[114] but he also grounded his opposition philosophically, citing the failure of astrology to explain twins who behave differently although conceived at the same moment and born at approximately the same time.[115]\n\n\nSome of the practices of astrology were contested on theological grounds by medieval Muslim astronomers such as Al-Farabi (Alpharabius), Ibn al-Haytham (Alhazen) and Avicenna. They said that the methods of astrologers conflicted with orthodox religious views of Islamic scholars, by suggesting that the Will of God can be known and predicted.[116] For example, Avicenna's 'Refutation against astrology', Risāla fī ibṭāl aḥkām al-nojūm, argues against the practice of astrology while supporting the principle that planets may act as agents of divine causation. Avicenna considered that the movement of the planets influenced life on earth in a deterministic way, but argued against the possibility of determining the exact influence of the stars.[117] Essentially, Avicenna did not deny the core dogma of astrology, but denied our ability to understand it to the extent that precise and fatalistic predictions could be made from it.[118] Ibn Qayyim al-Jawziyya (1292–1350), in his Miftah Dar al-SaCadah, also used physical arguments in astronomy to question the practice of judicial astrology.[119] He recognised that the stars are much larger than the planets, and argued: \nAnd if you astrologers answer that it is precisely because of this distance and smallness that their influences are negligible, then why is it that you claim a great influence for the smallest heavenly body, Mercury? Why is it that you have given an influence to al-Ra's [the head] and al-Dhanab [the tail], which are two imaginary points [ascending and descending nodes]?[119]\nMartin Luther denounced astrology in his Table Talk. He asked why twins like Esau and Jacob had two different natures yet were born at the same time. Luther also compared astrologers to those who say their dice will always land on a certain number. Although the dice may roll on the number a couple of times, the predictor is silent for all the times the dice fails to land on that number.[120]\n\nWhat is done by God, ought not to be ascribed to the stars. The upright and true Christian religion opposes and confutes all such fables.[120]\nThe Catechism of the Catholic Church maintains that divination, including predictive astrology, is incompatible with modern Catholic beliefs[121] such as free will:[115]\n\nAll forms of divination are to be rejected: recourse to Satan or demons, conjuring up the dead or other practices falsely supposed to \"unveil\" the future. Consulting horoscopes, astrology, palm reading, interpretation of omens and lots, the phenomena of clairvoyance, and recourse to mediums all conceal a desire for power over time, history, and, in the last analysis, other human beings, as well as a wish to conciliate hidden powers. They contradict the honor, respect, and loving fear that we owe to God alone.[122]\nThe scientific community rejects astrology as having no explanatory power for describing the universe, and considers it a pseudoscience.[123][124][125] Scientific testing of astrology has been conducted, and no evidence has been found to support any of the premises or purported effects outlined in astrological traditions.[126][127][128] There is no proposed mechanism of action by which the positions and motions of stars and planets could affect people and events on Earth that does not contradict basic and well understood aspects of biology and physics.[16][17] Those who have faith in astrology have been characterised by scientists including Bart J. Bok as doing so \"...in spite of the fact that there is no verified scientific basis for their beliefs, and indeed that there is strong evidence to the contrary\".[129]\n\nConfirmation bias is a form of cognitive bias, a psychological factor that contributes to belief in astrology.[130][131][132][133][a] Astrology believers tend to selectively remember predictions that turn out to be true, and do not remember those that turn out false. Another, separate, form of confirmation bias also plays a role, where believers often fail to distinguish between messages that demonstrate special ability and those that do not.[131] Thus there are two distinct forms of confirmation bias that are under study with respect to astrological belief.[131]\n\nUnder the criterion of falsifiability, first proposed by the philosopher of science Karl Popper, astrology is a pseudoscience.[134] Popper regarded astrology as \"pseudo-empirical\" in that \"it appeals to observation and experiment,\" but \"nevertheless does not come up to scientific standards.\"[135] In contrast to scientific disciplines, astrology has not responded to falsification through experiment.[136]: 206 \n\nIn contrast to Popper, the philosopher Thomas Kuhn argued that it was not lack of falsifiability that makes astrology unscientific, but rather that the process and concepts of astrology are non-empirical.[137]: 401  Kuhn thought that, though astrologers had, historically, made predictions that categorically failed, this in itself does not make astrology unscientific, nor do attempts by astrologers to explain away failures by saying that creating a horoscope is very difficult. Rather, in Kuhn's eyes, astrology is not science because it was always more akin to medieval medicine; astrologers followed a sequence of rules and guidelines for a seemingly necessary field with known shortcomings, but they did no research because the fields are not amenable to research,[138]: 8  and so \"they had no puzzles to solve and therefore no science to practise.\"[137]: 401,  [138]: 8  While an astronomer could correct for failure, an astrologer could not. An astrologer could only explain away failure but could not revise the astrological hypothesis in a meaningful way. As such, to Kuhn, even if the stars could influence the path of humans through life, astrology is not scientific.[138]: 8 \n\nThe philosopher Paul Thagard asserts that astrology cannot be regarded as falsified in this sense until it has been replaced with a successor. In the case of predicting behaviour, psychology is the alternative.[6]: 228  To Thagard a further criterion of demarcation of science from pseudoscience is that the state-of-the-art must progress and that the community of researchers should be attempting to compare the current theory to alternatives, and not be \"selective in considering confirmations and disconfirmations.\"[6]: 227–228  Progress is defined here as explaining new phenomena and solving existing problems, yet astrology has failed to progress having only changed little in nearly 2000 years.[6]: 228 [139]: 549  To Thagard, astrologers are acting as though engaged in normal science believing that the foundations of astrology were well established despite the \"many unsolved problems\", and in the face of better alternative theories (psychology). For these reasons Thagard views astrology as pseudoscience.[6][139]: 228 \n\nFor the philosopher Edward W. James, astrology is irrational not because of the numerous problems with mechanisms and falsification due to experiments, but because an analysis of the astrological literature shows that it is infused with fallacious logic and poor reasoning.[140]: 34 \n\nWhat if throughout astrological writings we meet little appreciation of coherence, blatant insensitivity to evidence, no sense of a hierarchy of reasons, slight command over the contextual force of critieria, stubborn unwillingness to pursue an argument where it leads, stark naivete concerning the efficacy of explanation and so on? In that case, I think, we are perfectly justified in rejecting astrology as irrational. ... Astrology simply fails to meet the multifarious demands of legitimate reasoning.\nAstrology has not demonstrated its effectiveness in controlled studies and has no scientific validity.[141][19] Where it has made falsifiable predictions under controlled conditions, they have been falsified.[126] One famous experiment included 28 astrologers who were asked to match over a hundred natal charts to psychological profiles generated by the California Psychological Inventory (CPI) questionnaire.[142][143] The double-blind experimental protocol used in this study was agreed upon by a group of physicists and a group of astrologers[19] nominated by the National Council for Geocosmic Research, who advised the experimenters, helped ensure that the test was fair[18]: 420,  [143]: 117  and helped draw the central proposition of natal astrology to be tested.[18]: 419  They also chose 26 out of the 28 astrologers for the tests (two more volunteered afterwards).[18]: 420  The study, published in Nature in 1985, found that predictions based on natal astrology were no better than chance, and that the testing \"...clearly refutes the astrological hypothesis.\"[18]\n\nIn 1955, the astrologer and psychologist Michel Gauquelin stated that though he had failed to find evidence that supported indicators like zodiacal signs and planetary aspects in astrology, he did find positive correlations between the diurnal positions of some planets and success in professions that astrology traditionally associates with those planets.[144][145] The best-known of Gauquelin's findings is based on the positions of Mars in the natal charts of successful athletes and became known as the Mars effect.[146]: 213  A study conducted by seven French scientists attempted to replicate the claim, but found no statistical evidence.[146]: 213–214  They attributed the effect to selective bias on Gauquelin's part, accusing him of attempting to persuade them to add or delete names from their study.[147]\n\nGeoffrey Dean has suggested that the effect may be caused by self-reporting of birth dates by parents rather than any issue with the study by Gauquelin. The suggestion is that a small subset of the parents may have had changed birth times to be consistent with better astrological charts for a related profession.  The number of births under astrologically undesirable conditions was also lower, indicating that parents choose dates and times to suit their beliefs. The sample group was taken from a time where belief in astrology was more common. Gauquelin had failed to find the Mars effect in more recent populations, where a nurse or doctor recorded the birth information.[143]: 116 \n\nDean, a scientist and former astrologer, and psychologist Ivan Kelly conducted a large scale scientific test that involved more than one hundred cognitive, behavioural, physical, and other variables—but found no support for astrology.[148][149] Furthermore, a meta-analysis pooled 40 studies that involved 700 astrologers and over 1,000 birth charts. Ten of the tests—which involved 300 participants—had the astrologers pick the correct chart interpretation out of a number of others that were not the astrologically correct chart interpretation (usually three to five others). When date and other obvious clues were removed, no significant results suggested there was any preferred chart.[149]: 190 \n\nTesting the validity of astrology can be difficult, because there is no consensus amongst astrologers as to what astrology is or what it can predict.[9] Most professional astrologers are paid to predict the future or describe a person's personality and life, but most horoscopes only make vague untestable statements that can apply to almost anyone.[20][150]\n\nMany astrologers believe that astrology is scientific,[151] while some have proposed conventional causal agents such as electromagnetism and gravity.[151] Scientists reject these mechanisms as implausible[151] since, for example, the magnetic field, when measured from Earth, of a large but distant planet such as Jupiter is far smaller than that produced by ordinary household appliances.[152]\n\nWestern astrology has taken the earth's axial precession (also called precession of the equinoxes) into account since Ptolemy's Almagest, so the \"first point of Aries\", the start of the astrological year, continually moves against the background of the stars.[153] The tropical zodiac has no connection to the stars; tropical astrologers distinguish the constellations from their historically associated sign, thereby avoiding complications involving precession.[154] Charpak and Broch, noting this, referred to astrology based on the tropical zodiac as being \"...empty boxes that have nothing to do with anything and are devoid of any consistency or correspondence with the stars.\"[154] Sole use of the tropical zodiac is inconsistent with references made, by the same astrologers, to the Age of Aquarius, which depends on when the vernal point enters the constellation of Aquarius.[19]\n\nAstrologers usually have only a small knowledge of astronomy, and often do not take into account basic principles—such as the precession of the equinoxes, which changes the position of the sun with time. They commented on the example of Élizabeth Teissier, who wrote that, \"The sun ends up in the same place in the sky on the same date each year\", as the basis for the idea that two people with the same birthday, but a number of years apart, should be under the same planetary influence. Charpak and Broch noted that, \"There is a difference of about twenty-two thousand miles between Earth's location on any specific date in two successive years\", and that thus they should not be under the same influence according to astrology. Over a 40-year period there would be a difference greater than 780,000 miles.[154]\n\nThe general consensus of astronomers and other natural scientists is that astrology is a pseudoscience which carries no predictive capability, with many philosophers of science considering it a \"paradigm or prime example of pseudoscience.\"[155] Some scholars in the social sciences have cautioned against categorizing astrology, especially ancient astrology, as \"just\" a pseudoscience or projecting the distinction backwards into the past.[156] Thagard, while demarcating it as a pseudoscience, notes that astrology \"should be judged as not pseudoscientific in classical or Renaissance times...Only when the historical and social aspects of science are neglected does it become plausible that pseudoscience is an unchanging category.\"[157] Historians of science such as Tamsyn Barton, Roger Beck, Francesca Rochberg, and Wouter J. Hanegraaff argue that such a wholesale description is anachronistic when applied to historical contexts, stressing that astrology was not pseudoscience before the 18th century and the importance of the discipline to the development of medieval science.[158][159][156][160][161] R. J. Hakinson writes in the context of Hellenistic astrology that \"the belief in the possibility of [astrology] was, at least some of the time, the result of careful reflection on the nature and structure of the universe.\"[162]\n\nNicholas Campion, both an astrologer and academic historian of astrology, argues that Indigenous astronomy is largely used as a synonym for astrology in academia, and that modern Indian and Western astrology are better understood as modes of cultural astronomy or ethnoastronomy.[163] Roy Willis and Patrick Curry draw a distinction between propositional episteme and metaphoric metis in the ancient world, identifying astrology with the latter and noting that the central concern of astrology \"is not knowledge (factual, let alone scientific) but wisdom (ethical, spiritual and pragmatic)\".[164] Similarly, historian of science Justin Niermeier-Dohoney writes that astrology was \"more than simply a science of prediction using the stars and comprised a vast body of beliefs, knowledge, and practices with the overarching theme of understanding the relationship between humanity and the rest of the cosmos through an interpretation of stellar, solar, lunar, and planetary movement.\" Scholars such as Assyriologist Matthew Rutz have begun using the term \"astral knowledge\" rather than astrology \"to better describe a category of beliefs and practices much broader than the term 'astrology' can capture.\"[165][166]\n\nIn the West, political leaders have sometimes consulted astrologers. For example, the British intelligence agency MI5 employed Louis de Wohl as an astrologer after it was reported that Adolf Hitler used astrology to time his actions. The War Office was \"...interested to know what Hitler's own astrologers would be telling him from week to week.\"[167] In fact, de Wohl's predictions were so inaccurate that he was soon labelled a \"complete charlatan\", and later evidence showed that Hitler considered astrology \"complete nonsense\".[168] After John Hinckley's attempted assassination of US President Ronald Reagan, first lady Nancy Reagan commissioned astrologer Joan Quigley to act as the secret White House astrologer. However, Quigley's role ended in 1988 when it became public through the memoirs of former chief of staff, Donald Regan.[169][170][171]\n\nThere was a boom in interest in astrology in the late 1960s. The sociologist Marcello Truzzi described three levels of involvement of \"Astrology-believers\" to account for its revived popularity in the face of scientific discrediting. He found that most astrology-believers did not think that it was a scientific explanation with predictive power. Instead, those superficially involved, knowing \"next to nothing\" about astrology's 'mechanics', read newspaper astrology columns, and could benefit from \"tension-management of anxieties\" and \"a cognitive belief-system that transcends science.\"[172] Those at the second level usually had their horoscopes cast and sought advice and predictions. They were much younger than those at the first level, and could benefit from knowledge of the language of astrology and the resulting ability to belong to a coherent and exclusive group. Those at the third level were highly involved and usually cast horoscopes for themselves. Astrology provided this small minority of astrology-believers with a \"meaningful view of their universe and [gave] them an understanding of their place in it.\"[b] This third group took astrology seriously, possibly as an overarching religious worldview (a sacred canopy, in Peter L. Berger's phrase), whereas the other two groups took it playfully and irreverently.[172]\n\nIn 1953, the sociologist Theodor W. Adorno conducted a study of the astrology column of a Los Angeles newspaper as part of a project examining mass culture in capitalist society.[173]: 326  Adorno believed that popular astrology, as a device, invariably leads to statements that encouraged conformity—and that astrologers who go against conformity, by discouraging performance at work etc., risk losing their jobs.[173]: 327  Adorno concluded that astrology is a large-scale manifestation of systematic irrationalism, where individuals are subtly led—through flattery and vague generalisations—to believe that the author of the column is addressing them directly.[174] Adorno drew a parallel with the phrase opium of the people, by Karl Marx, by commenting, \"occultism is the metaphysic of the dopes.\"[173]: 329 \n\nA 2005 Gallup poll and a 2009 survey by the Pew Research Center reported that 25% of US adults believe in astrology,[175][176] while a 2018 Pew survey found a figure of 29%.[177] According to data released in the National Science Foundation's 2014 Science and Engineering Indicators study, \"Fewer Americans rejected astrology in 2012 than in recent years.\"[178] The NSF study noted that in 2012, \"slightly more than half of Americans said that astrology was 'not at all scientific,' whereas nearly two-thirds gave this response in 2010. The comparable percentage has not been this low since 1983.\"[178] Astrology apps became popular in the late 2010s, some receiving millions of dollars in Silicon Valley venture capital.[179]\n\nIn India, there is a long-established and widespread belief in astrology. It is commonly used for daily life, particularly in matters concerning marriage and career, and makes extensive use of electional, horary and karmic astrology.[180][181] Indian politics have also been influenced by astrology.[182] It is still considered a branch of the Vedanga.[183][184] In 2001, Indian scientists and politicians debated and critiqued a proposal to use state money to fund research into astrology,[185] resulting in permission for Indian universities to offer courses in Vedic astrology.[186]\n\nIn February 2011, the Bombay High Court reaffirmed astrology's standing in India when it dismissed a case that challenged its status as a science.[187]\n\nIn Japan, strong belief in astrology has led to dramatic changes in the fertility rate and the number of abortions in the years of Fire Horse. Adherents believe that women born in hinoeuma years are unmarriageable and bring bad luck to their father or husband. In 1966, the number of babies born in Japan dropped by over 25% as parents tried to avoid the stigma of having a daughter born in the hinoeuma year.[188][189]\n\nThe fourteenth-century English poets John Gower and Geoffrey Chaucer both referred to astrology in their works, including Gower's Confessio Amantis and Chaucer's The Canterbury Tales.[190] Chaucer commented explicitly on astrology in his Treatise on the Astrolabe, demonstrating personal knowledge of one area, judicial astrology, with an account of how to find the ascendant or rising sign.[191]\n\nIn the fifteenth century, references to astrology, such as with similes, became \"a matter of course\" in English literature.[190]\n\nIn the sixteenth century, John Lyly's 1597 play, The Woman in the Moon, is wholly motivated by astrology,[192] while Christopher Marlowe makes astrological references in his plays Doctor Faustus and Tamburlaine (both c. 1590),[192] and Sir Philip Sidney refers to astrology at least four times in his romance The Countess of Pembroke's Arcadia (c. 1580).[192] Edmund Spenser uses astrology both decoratively and causally in his poetry, revealing \"...unmistakably an abiding interest in the art, an interest shared by a large number of his contemporaries.\"[192] George Chapman's play, Byron's Conspiracy (1608), similarly uses astrology as a causal mechanism in the drama.[193] William Shakespeare's attitude towards astrology is unclear, with contradictory references in plays including King Lear, Antony and Cleopatra, and Richard II.[193] Shakespeare was familiar with astrology and made use of his knowledge of astrology in nearly every play he wrote,[193] assuming a basic familiarity with the subject in his commercial audience.[193] Outside theatre, the physician and mystic Robert Fludd practised astrology, as did the quack doctor Simon Forman.[193] In Elizabethan England, \"The usual feeling about astrology ... [was] that it is the most useful of the sciences.\"[193]\n\nIn seventeenth century Spain, Lope de Vega, with a detailed knowledge of astronomy, wrote plays that ridicule astrology. In his pastoral romance La Arcadia (1598), it leads to absurdity; in his novela Guzman el Bravo (1624), he concludes that the stars were made for man, not man for the stars.[194] Calderón de la Barca wrote the 1641 comedy Astrologo Fingido (The Pretended Astrologer); the plot was borrowed by the French playwright Thomas Corneille for his 1651 comedy Feint Astrologue.[195]\n\nThe most famous piece of music influenced by astrology is the orchestral suite The Planets. Written by the British composer Gustav Holst (1874–1934), and first performed in 1918, the framework of The Planets is based upon the astrological symbolism of the planets.[196] Each of the seven movements of the suite is based upon a different planet, though the movements are not in the order of the planets from the Sun. The composer Colin Matthews wrote an eighth movement entitled Pluto, the Renewer, first performed in 2000, as the suite was written prior to Pluto's discovery.[197] In 1937, another British composer, Constant Lambert, wrote a ballet on astrological themes, called Horoscope.[198] In 1974, the New Zealand composer Edwin Carr wrote The Twelve Signs: An Astrological Entertainment for orchestra without strings.[199] Camille Paglia acknowledges astrology as an influence on her work of literary criticism Sexual Personae (1990).[200] The American comedian Harvey Sid Fisher is best known for his comedic songs about astrology.[201]\n\nAstrology features strongly in Eleanor Catton's The Luminaries, recipient of the 2013 Man Booker Prize.[202]\n"
    },
    {
        "title": "Mathematical notation",
        "content": "\n\nMathematical notation consists of using symbols for representing operations, unspecified numbers, relations, and any other mathematical objects and assembling them into expressions and formulas. Mathematical notation is widely used in mathematics, science, and engineering for representing complex concepts and properties in a concise, unambiguous, and accurate way.\n\nFor example, the physicist Albert Einstein's formula \n\n\n\nE\n=\nm\n\nc\n\n2\n\n\n\n\n{\\displaystyle E=mc^{2}}\n\n is the quantitative representation in mathematical notation of mass–energy equivalence.[1]\n\nMathematical notation was first introduced by François Viète at the end of the 16th century and largely expanded during the 17th and 18th centuries by René Descartes, Isaac Newton, Gottfried Wilhelm Leibniz, and overall Leonhard Euler.\n\nThe use of many symbols is the basis of mathematical notation. They play a similar role as words in natural languages. They may play different roles in mathematical notation similarly as verbs, adjective and nouns play different roles in a sentence.\n\nLetters are typically used for naming—in mathematical jargon, one says representing—mathematical objects. The Latin and Greek alphabets are used extensively, but a few letters of other alphabets are also used sporadically, such as the Hebrew ⁠\n\n\n\nℵ\n\n\n{\\displaystyle \\aleph }\n\n⁠, Cyrillic Ш, and Hiragana よ. Uppercase and lowercase letters are considered as different symbols. For Latin alphabet, different typefaces also provide different symbols. For example, \n\n\n\nr\n,\nR\n,\n\nR\n\n,\n\n\nR\n\n\n,\n\n\nr\n\n\n,\n\n\n{\\displaystyle r,R,\\mathbb {R} ,{\\mathcal {R}},{\\mathfrak {r}},}\n\n and \n\n\n\n\n\nR\n\n\n\n\n{\\displaystyle {\\mathfrak {R}}}\n\n could theoretically appear in the same mathematical text with six different meanings. Normally, roman upright typeface is not used for symbols, except for symbols representing a standard function, such as the symbol \"\n\n\n\nsin\n\n\n{\\displaystyle \\sin }\n\n\" of the sine function.[2]\n\nIn order to have more symbols, and for allowing related mathematical objects to be represented by related symbols, diacritics, subscripts and superscripts are often used. For example, \n\n\n\n\n\n\n\nf\n\n1\n\n′\n\n^\n\n\n\n\n\n{\\displaystyle {\\hat {f'_{1}}}}\n\n may denote the Fourier transform of the derivative of a function called \n\n\n\n\nf\n\n1\n\n\n.\n\n\n{\\displaystyle f_{1}.}\n\n\n\nSymbols are not only used for naming mathematical objects. They can be used for operations \n\n\n\n(\n+\n,\n−\n,\n\n/\n\n,\n⊕\n,\n…\n)\n,\n\n\n{\\displaystyle (+,-,/,\\oplus ,\\ldots ),}\n\n for relations \n\n\n\n(\n=\n,\n<\n,\n≤\n,\n∼\n,\n≡\n,\n…\n)\n,\n\n\n{\\displaystyle (=,<,\\leq ,\\sim ,\\equiv ,\\ldots ),}\n\n for logical connectives \n\n\n\n(\n\n⟹\n\n,\n∧\n,\n∨\n,\n…\n)\n,\n\n\n{\\displaystyle (\\implies ,\\land ,\\lor ,\\ldots ),}\n\n for quantifiers \n\n\n\n(\n∀\n,\n∃\n)\n,\n\n\n{\\displaystyle (\\forall ,\\exists ),}\n\n and for other purposes.\n\nSome symbols are similar to Latin or Greek letters, some are obtained by deforming letters, some are traditional typographic symbols, but many have been specially designed for mathematics.\n\nAn expression is a finite combination of symbols that is well-formed according to rules that depend on the context. In general, an expression denotes or names a mathematical object, and plays therefore in the language of mathematics the role of a noun phrase in the natural language.\n\nAn expression contains often some operators, and may therefore be evaluated by the action of the operators in it. For example, \n\n\n\n3\n+\n2\n\n\n{\\displaystyle 3+2}\n\n is an expression in which the operator \n\n\n\n+\n\n\n{\\displaystyle +}\n\n can be evaluated for giving the result \n\n\n\n5.\n\n\n{\\displaystyle 5.}\n\n So, \n\n\n\n3\n+\n2\n\n\n{\\displaystyle 3+2}\n\n and \n\n\n\n5\n\n\n{\\displaystyle 5}\n\n are two different expressions that represent the same number. This is the meaning of the equality \n\n\n\n3\n+\n2\n=\n5.\n\n\n{\\displaystyle 3+2=5.}\n\n\n\nA more complicated example is given by the expression\n\n\n\n\n∫\n\na\n\n\nb\n\n\nx\nd\nx\n\n\n{\\textstyle \\int _{a}^{b}xdx}\n\n that can be evaluated to \n\n\n\n\n\n\nb\n\n2\n\n\n2\n\n\n−\n\n\n\na\n\n2\n\n\n2\n\n\n.\n\n\n{\\textstyle {\\frac {b^{2}}{2}}-{\\frac {a^{2}}{2}}.}\n\n Although the resulting expression contains the operators of division, subtraction and exponentiation, it cannot be evaluated further because a and b denote unspecified numbers.\n\nIt is believed that a notation to represent numbers was first developed at least 50,000 years ago.[3] Early mathematical ideas such as finger counting[4] have also been represented by collections of rocks, sticks, bone, clay, stone, wood carvings, and knotted ropes. The tally stick is a way of counting dating back to the Upper Paleolithic. Perhaps the oldest known mathematical texts are those of ancient Sumer. The Census Quipu of the Andes and the Ishango Bone from Africa both used the tally mark method of accounting for numerical concepts.\n\nThe concept of zero and the introduction of a notation for it are important developments in early mathematics, which predates for centuries the concept of zero as a number. It was used as a placeholder by the Babylonians and Greek Egyptians, and then as an integer by the Mayans, Indians and Arabs (see the history of zero).\n\nUntil the 16th century, mathematics was essentially rhetorical, in the sense that everything but explicit numbers was expressed in words. However, some authors such as Diophantus used some symbols as abbreviations. \n\nThe first systematic use of formulas, and, in particular the use of symbols (variables) for unspecified numbers is generally attributed to François Viète (16th century). However, he used different symbols than those that are now standard.\n\nLater, René Descartes (17th century) introduced the modern notation for variables and equations; in particular, the use of \n\n\n\nx\n,\ny\n,\nz\n\n\n{\\displaystyle x,y,z}\n\n for unknown quantities and \n\n\n\na\n,\nb\n,\nc\n\n\n{\\displaystyle a,b,c}\n\n for known ones (constants). He introduced also the notation i and the term \"imaginary\" for the imaginary unit.\n\nThe 18th and 19th centuries saw the standardization of mathematical notation as used today. Leonhard Euler was responsible for many of the notations currently in use: the functional notation \n\n\n\nf\n(\nx\n)\n,\n\n\n{\\displaystyle f(x),}\n\n e for the base of the natural logarithm, \n\n\n\n∑\n\n\n{\\textstyle \\sum }\n\n for summation, etc.[5] He also popularized the use of π for the Archimedes constant (proposed by William Jones, based on an earlier notation of William Oughtred).[6]\n\nSince then many new notations have been introduced, often specific to a particular area of mathematics. Some notations are named after their inventors, such as Leibniz's notation, Legendre symbol, the Einstein summation convention, etc.\n\nGeneral typesetting systems are generally not well suited for mathematical notation. One of the reasons is that, in mathematical notation, the symbols are often arranged in two-dimensional figures, such as in:\n\nTeX is a mathematically oriented typesetting system that was created in 1978 by Donald Knuth. It is widely used in mathematics, through its extension called LaTeX, and is a de facto standard. (The above expression is written in LaTeX.)\n\nMore recently, another approach for mathematical typesetting is provided by MathML. However, it is not well supported in web browsers, which is its primary target.\n\nThe international standard ISO 80000-2 (previously, ISO 31-11) specifies symbols for use in mathematical equations. The standard requires use of italic fonts for variables (e.g., E = mc2) and roman (upright) fonts for mathematical constants (e.g., e or π).\n\nModern Arabic mathematical notation is based mostly on the Arabic alphabet and is used widely in the Arab world, especially in pre-tertiary education.  (Western notation uses Arabic numerals, but the Arabic notation also replaces Latin letters and related symbols with Arabic script.)\n\nIn addition to Arabic notation, mathematics also makes use of Greek letters to denote a wide variety of mathematical objects and variables. On some occasions, certain Hebrew letters are also used (such as in the context of infinite cardinals).\n\nSome mathematical notations are mostly diagrammatic, and so are almost entirely script independent. Examples are Penrose graphical notation and Coxeter–Dynkin diagrams.\n\nBraille-based mathematical notations used by blind people include Nemeth Braille and GS8 Braille.\n"
    },
    {
        "title": "Formula",
        "content": "In science, a formula is a concise way of expressing information symbolically, as in a mathematical formula or a chemical formula. The informal use of the term formula in science refers to the general construct of a relationship between given quantities.  \n\nThe plural of formula can be either formulas (from the most common English plural noun form) or, under the influence of scientific Latin, formulae (from the original Latin).[2]\n\nIn mathematics, a formula generally refers to an equation or inequality relating one mathematical expression to another, with the most important ones being mathematical theorems. For example, determining the volume of a sphere requires a significant amount of integral calculus or its geometrical analogue, the method of exhaustion.[3] However, having done this once in terms of some parameter (the radius for example), mathematicians have produced a formula to describe the volume of a sphere in terms of its radius:\n\nHaving obtained this result, the volume of any sphere can be computed as long as its radius is known. Here, notice that the volume V and the radius r are expressed as single letters instead of words or phrases. This convention, while less important in a relatively simple formula, means that mathematicians can more quickly manipulate formulas which are larger and more complex.[4] Mathematical formulas are often algebraic, analytical or in closed form.[5]\n\nIn a general context, formulas often represent mathematical models of real world phenomena, and as such can be used to provide solutions (or approximate solutions) to real world problems, with some being more general than others. For example, the formula\n\nis an expression of Newton's second law, and is applicable to a wide range of physical situations. Other formulas, such as the use of the equation of a sine curve to model the movement of the tides in a bay, may be created to solve a particular problem. In all cases, however, formulas form the basis for calculations.\n\nExpressions are distinct from formulas in the sense that they don't usually contain relations like equality (=) or inequality (<).  Expressions denote a mathematical object, where as formulas denote a statement about mathematical objects.[6][7][dubious – discuss] This is analogous to natural language, where a noun phrase refers to an object, and a whole sentence refers to a fact. For example, \n\n\n\n8\nx\n−\n5\n\n\n{\\displaystyle 8x-5}\n\n is an expression, while \n\n\n\n8\nx\n−\n5\n≥\n3\n\n\n{\\displaystyle 8x-5\\geq 3}\n\n is a formula.\n\nHowever, in some areas mathematics, and in particular in computer algebra, formulas are viewed as expressions that can be evaluated to true or false, depending on the values that are given to the variables occurring in the expressions. For example \n\n\n\n8\nx\n−\n5\n≥\n3\n\n\n{\\displaystyle 8x-5\\geq 3}\n\n takes the value false if x is given a value less than 1, and the value true otherwise. (See Boolean expression)\n\nIn mathematical logic, a formula (often referred to as a well-formed formula) is an entity constructed using the symbols and formation rules of a given logical language.[8] For example, in first-order logic,\n\nis a formula, provided that \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is a unary function symbol, \n\n\n\nP\n\n\n{\\displaystyle P}\n\n a unary predicate symbol, and \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n a ternary predicate symbol.\n\nIn modern chemistry, a chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using a single line of chemical element symbols, numbers, and sometimes other symbols, such as parentheses, brackets, and plus (+) and minus (−) signs.[9] For example, H2O is the chemical formula for water, specifying that each molecule consists of two hydrogen (H) atoms and one oxygen (O) atom. Similarly, O−3 denotes an ozone molecule consisting of three oxygen atoms[10] and a net negative charge.\n\nA chemical formula identifies each constituent element by its chemical symbol, and indicates the proportionate number of atoms of each element.\n\nIn empirical formulas, these proportions begin with a key element and then assign numbers of atoms of the other elements in the compound—as ratios to the key element. For molecular compounds, these ratio numbers can always be expressed as whole numbers. For example, the empirical formula of ethanol may be written C2H6O,[11] because the molecules of ethanol all contain two carbon atoms, six hydrogen atoms, and one oxygen atom. Some types of ionic compounds, however, cannot be written as empirical formulas which contains only the whole numbers. An example is boron carbide, whose formula of CBn is a variable non-whole number ratio, with n ranging from over 4 to more than 6.5.\n\nWhen the chemical compound of the formula consists of simple molecules, chemical formulas often employ ways to suggest the structure of the molecule. There are several types of these formulas, including molecular formulas and condensed formulas. A molecular formula enumerates the number of atoms to reflect those in the molecule, so that the molecular formula for glucose is C6H12O6 rather than the glucose empirical formula, which is CH2O. Except for the very simple substances, molecular chemical formulas generally lack needed structural information, and might even be ambiguous in occasions.\n\nA structural formula is a drawing that shows the location of each atom, and which atoms it binds to.\n\nIn computing, a formula typically describes a calculation, such as addition, to be performed on one or more variables. A formula is often implicitly provided in the form of a computer instruction such as.\n\nIn computer spreadsheet software, a formula indicating how to compute the value of a cell, say A3, could be written as\n\nwhere A1 and A2 refer to other cells (column A, row 1 or 2) within the spreadsheet. This is a shortcut for the \"paper\" form A3 = A1+A2, where A3 is, by convention, omitted because the result is always stored in the cell itself, making the stating of the name redundant.\n\nFormulas used in science almost always require a choice of units.[12] Formulas are used to express relationships between various quantities, such as temperature, mass, or charge in physics; supply, profit, or demand in economics; or a wide range of other quantities in other disciplines.\n\nAn example of a formula used in science is Boltzmann's entropy formula. In statistical thermodynamics, it is a probability equation relating the entropy S of an ideal gas to the quantity W, which is the number of microstates corresponding to a given macrostate:\n\nwhere k is the Boltzmann constant, equal to 1.380649×10−23 J⋅K−1, and W is the number of microstates consistent with the given macrostate.\n"
    },
    {
        "title": "Differential calculus",
        "content": "In mathematics, differential calculus is a subfield of calculus that studies the rates at which quantities change.[1] It is one of the two traditional divisions of calculus, the other being integral calculus—the study of the area beneath a curve.[2]\n\nThe primary objects of study in differential calculus are the derivative of a function, related notions such as the differential, and their applications. The derivative of a function at a chosen input value describes the rate of change of the function near that input value. The process of finding a derivative is called differentiation. Geometrically, the derivative at a point is the slope of the tangent line to the graph of the function at that point, provided that the derivative exists and is defined at that point. For a real-valued function of a single real variable, the derivative of a function at a point generally determines the best linear approximation to the function at that point.\n\nDifferential calculus and integral calculus are connected by the fundamental theorem of calculus. This states that differentiation is the reverse process to integration.\n\nDifferentiation has applications in nearly all quantitative disciplines. In physics, the derivative of the displacement of a moving body with respect to time is the velocity of the body, and the derivative of the velocity with respect to time is acceleration. The derivative of the momentum of a body with respect to time equals the force applied to the body; rearranging this derivative statement leads to the famous F = ma equation associated with Newton's second law of motion. The reaction rate of a chemical reaction is a derivative. In operations research, derivatives determine the most efficient ways to transport materials and design factories.\n\nDerivatives are frequently used to find the maxima and minima of a function. Equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. Derivatives and their generalizations appear in many fields of mathematics, such as complex analysis, functional analysis, differential geometry, measure theory, and abstract algebra.\n\nThe derivative of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n at the point \n\n\n\nx\n=\na\n\n\n{\\displaystyle x=a}\n\n is the slope of the tangent to \n\n\n\n(\na\n,\nf\n(\na\n)\n)\n\n\n{\\displaystyle (a,f(a))}\n\n.[3] In order to gain an intuition for this, one must first be familiar with finding the slope of a linear equation, written in the form \n\n\n\ny\n=\nm\nx\n+\nb\n\n\n{\\displaystyle y=mx+b}\n\n. The slope of an equation is its steepness. It can be found by picking any two points and dividing the change in \n\n\n\ny\n\n\n{\\displaystyle y}\n\n by the change in \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, meaning that \n\n\n\n\nslope \n\n=\n\n\n\n\n change in \n\ny\n\n\n\nchange in \n\nx\n\n\n\n\n\n{\\displaystyle {\\text{slope }}={\\frac {{\\text{ change in }}y}{{\\text{change in }}x}}}\n\n. For, the graph of \n\n\n\ny\n=\n−\n2\nx\n+\n13\n\n\n{\\displaystyle y=-2x+13}\n\n has a slope of \n\n\n\n−\n2\n\n\n{\\displaystyle -2}\n\n, as shown in the diagram below:\n\nFor brevity, \n\n\n\n\n\n\n\nchange in \n\ny\n\n\n\nchange in \n\nx\n\n\n\n\n\n{\\displaystyle {\\frac {{\\text{change in }}y}{{\\text{change in }}x}}}\n\n is often written as \n\n\n\n\n\n\nΔ\ny\n\n\nΔ\nx\n\n\n\n\n\n{\\displaystyle {\\frac {\\Delta y}{\\Delta x}}}\n\n, with \n\n\n\nΔ\n\n\n{\\displaystyle \\Delta }\n\n being the Greek letter delta, meaning 'change in'. The slope of a linear equation is constant, meaning that the steepness is the same everywhere. However, many graphs such as \n\n\n\ny\n=\n\nx\n\n2\n\n\n\n\n{\\displaystyle y=x^{2}}\n\n vary in their steepness. This means that you can no longer pick any two arbitrary points and compute the slope. Instead, the slope of the graph can be computed by considering the tangent line—a line that 'just touches' a particular point.[a] The slope of a curve at a particular point is equal to the slope of the tangent to that point. For example, \n\n\n\ny\n=\n\nx\n\n2\n\n\n\n\n{\\displaystyle y=x^{2}}\n\n has a slope of \n\n\n\n4\n\n\n{\\displaystyle 4}\n\n at \n\n\n\nx\n=\n2\n\n\n{\\displaystyle x=2}\n\n because the slope of the tangent line to that point is equal to \n\n\n\n4\n\n\n{\\displaystyle 4}\n\n:\n\n\nThe derivative of a function is then simply the slope of this tangent line.[b] Even though the tangent line only touches a single point at the point of tangency, it can be approximated by a line that goes through two points. This is known as a secant line. If the two points that the secant line goes through are close together, then the secant line closely resembles the tangent line, and, as a result, its slope is also very similar:\n\n\nThe advantage of using a secant line is that its slope can be calculated directly. Consider the two points on the graph \n\n\n\n(\nx\n,\nf\n(\nx\n)\n)\n\n\n{\\displaystyle (x,f(x))}\n\n and \n\n\n\n(\nx\n+\nΔ\nx\n,\nf\n(\nx\n+\nΔ\nx\n)\n)\n\n\n{\\displaystyle (x+\\Delta x,f(x+\\Delta x))}\n\n, where \n\n\n\nΔ\nx\n\n\n{\\displaystyle \\Delta x}\n\n is a small number. As before, the slope of the line passing through these two points can be calculated with the formula \n\n\n\n\nslope \n\n=\n\n\n\nΔ\ny\n\n\nΔ\nx\n\n\n\n\n\n{\\displaystyle {\\text{slope }}={\\frac {\\Delta y}{\\Delta x}}}\n\n. This gives\n\nAs \n\n\n\nΔ\nx\n\n\n{\\displaystyle \\Delta x}\n\n gets closer and closer to \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n, the slope of the secant line gets closer and closer to the slope of the tangent line. This is formally written as\n\nThe above expression means 'as \n\n\n\nΔ\nx\n\n\n{\\displaystyle \\Delta x}\n\n gets closer and closer to 0, the slope of the secant line gets closer and closer to a certain value'. The value that is being approached is the derivative of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n; this can be written as \n\n\n\n\nf\n′\n\n(\nx\n)\n\n\n{\\displaystyle f'(x)}\n\n. If \n\n\n\ny\n=\nf\n(\nx\n)\n\n\n{\\displaystyle y=f(x)}\n\n, the derivative can also be written as \n\n\n\n\n\n\nd\ny\n\n\nd\nx\n\n\n\n\n\n{\\displaystyle {\\frac {dy}{dx}}}\n\n, with \n\n\n\nd\n\n\n{\\displaystyle d}\n\n representing an infinitesimal change. For example, \n\n\n\nd\nx\n\n\n{\\displaystyle dx}\n\n represents an infinitesimal change in x.[c] In summary, if \n\n\n\ny\n=\nf\n(\nx\n)\n\n\n{\\displaystyle y=f(x)}\n\n, then the derivative of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is\n\nprovided such a limit exists.[4][d]  We have thus succeeded in properly defining the derivative of a function, meaning that the 'slope of the tangent line' now has a precise mathematical meaning. Differentiating a function using the above definition is known as differentiation from first principles. Here is a proof, using differentiation from first principles, that the derivative of \n\n\n\ny\n=\n\nx\n\n2\n\n\n\n\n{\\displaystyle y=x^{2}}\n\n is \n\n\n\n2\nx\n\n\n{\\displaystyle 2x}\n\n:\n\nAs \n\n\n\nΔ\nx\n\n\n{\\displaystyle \\Delta x}\n\n approaches \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n, \n\n\n\n2\nx\n+\nΔ\nx\n\n\n{\\displaystyle 2x+\\Delta x}\n\n approaches \n\n\n\n2\nx\n\n\n{\\displaystyle 2x}\n\n. Therefore, \n\n\n\n\n\n\nd\ny\n\n\nd\nx\n\n\n\n=\n2\nx\n\n\n{\\displaystyle {\\frac {dy}{dx}}=2x}\n\n. This proof can be generalised to show that \n\n\n\n\n\n\nd\n(\na\n\nx\n\nn\n\n\n)\n\n\nd\nx\n\n\n\n=\na\nn\n\nx\n\nn\n−\n1\n\n\n\n\n{\\displaystyle {\\frac {d(ax^{n})}{dx}}=anx^{n-1}}\n\n if \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nn\n\n\n{\\displaystyle n}\n\n are constants. This is known as the power rule. For example, \n\n\n\n\n\nd\n\nd\nx\n\n\n\n(\n5\n\nx\n\n4\n\n\n)\n=\n5\n(\n4\n)\n\nx\n\n3\n\n\n=\n20\n\nx\n\n3\n\n\n\n\n{\\displaystyle {\\frac {d}{dx}}(5x^{4})=5(4)x^{3}=20x^{3}}\n\n. However, many other functions cannot be differentiated as easily as polynomial functions, meaning that sometimes further techniques are needed to find the derivative of a function. These techniques include the chain rule, product rule, and quotient rule. Other functions cannot be differentiated at all, giving rise to the concept of differentiability.\n\nA closely related concept to the derivative of a function is its differential. When x and y are real variables, the derivative of f at x is the slope of the tangent line to the graph of f at x. Because the source and target of f are one-dimensional, the derivative of f is a real number. If x and y are vectors, then the best linear approximation to the graph of f depends on how f changes in several directions at once. Taking the best linear approximation in a single direction determines a partial derivative, which is usually denoted ⁠∂y/∂x⁠. The linearization of f in all directions at once is called the total derivative.\n\nThe concept of a derivative in the sense of a tangent line is a very old one, familiar to ancient Greek mathematicians such as Euclid (c. 300 BC), Archimedes (c. 287–212 BC), and Apollonius of Perga (c. 262–190 BC).[5] Archimedes also made use of indivisibles, although these were primarily used to study areas and volumes rather than derivatives and tangents (see The Method of Mechanical Theorems). \nThe use of infinitesimals to compute rates of change was developed significantly by Bhāskara II (1114–1185); indeed, it has been argued[6] that many of the key notions of differential calculus can be found in his work, such as \"Rolle's theorem\".[7]\n\nThe mathematician, Sharaf al-Dīn al-Tūsī (1135–1213), in his Treatise on Equations, established conditions for some cubic equations to have solutions, by finding the maxima of appropriate cubic polynomials. He obtained, for example, that the maximum (for positive x) of the cubic  ax2 – x3 occurs when x = 2a / 3, and concluded therefrom that the equation  ax2 = x3 + c has exactly one positive solution when c = 4a3 / 27, and two positive solutions whenever 0 < c < 4a3 / 27.[8][page needed] The historian of science, Roshdi Rashed,[8][page needed] has argued that al-Tūsī must have used the derivative of the cubic to obtain this result. Rashed's conclusion has been contested by other scholars, however, who argue that he could have obtained the result by other methods which do not require the derivative of the function to be known.[8][page needed]\n\nThe modern development of calculus is usually credited to Isaac Newton (1643–1727) and Gottfried Wilhelm Leibniz (1646–1716), who provided independent[e] and unified approaches to differentiation and derivatives. The key insight, however, that earned them this credit, was the fundamental theorem of calculus relating differentiation and integration: this rendered obsolete most previous methods for computing areas and volumes.[f] For their ideas on derivatives, both Newton and Leibniz built on significant earlier work by mathematicians such as Pierre de Fermat (1607-1665), Isaac Barrow (1630–1677), René Descartes (1596–1650), Christiaan Huygens (1629–1695), Blaise Pascal (1623–1662) and John Wallis (1616–1703). Regarding Fermat's influence, Newton once wrote in a letter that \"I had the hint of this method [of fluxions] from Fermat's way of drawing tangents, and by applying it to abstract equations, directly and invertedly, I made it general.\"[9] Isaac Barrow is generally given credit for the early development of the derivative.[10] Nevertheless, Newton and Leibniz remain key figures in the history of differentiation, not least because Newton was the first to apply differentiation to theoretical physics, while Leibniz systematically developed much of the notation still used today.\n\nSince the 17th century many mathematicians have contributed to the theory of differentiation. In the 19th century, calculus was put on a much more rigorous footing by mathematicians such as Augustin Louis Cauchy (1789–1857), Bernhard Riemann (1826–1866), and Karl Weierstrass (1815–1897). It was also during this period that the differentiation was generalized to Euclidean space and the complex plane.\n\nThe 20th century brought two major steps towards our present understanding and practice of derivation : Lebesgue integration, besides extending integral calculus to many more functions, clarified the relation between derivation and integration with the notion of absolute continuity. Later the theory of distributions (after Laurent Schwartz) extended derivation to generalized functions (e.g., the Dirac delta function previously introduced in Quantum Mechanics) and became fundamental to nowadays applied analysis especially by the use of weak solutions to partial differential equations.\n\nIf f is a differentiable function on ℝ (or an open interval) and x is a local maximum or a local minimum of f, then the derivative of f at x is zero. Points where f'(x) = 0 are called critical points or stationary points (and the value of f at x is called a critical value). If f is not assumed to be everywhere differentiable, then points at which it fails to be differentiable are also designated critical points.\n\nIf f is twice differentiable, then conversely, a critical point x of f can be analysed by considering the second derivative of f at x :\n\nThis is called the second derivative test. An alternative approach, called the first derivative test, involves considering the sign of the f' on each side of the critical point.\n\nTaking derivatives and solving for critical points is therefore often a simple way to find local minima or maxima, which can be useful in optimization. By the extreme value theorem, a continuous function on a closed interval must attain its minimum and maximum values at least once. If the function is differentiable, the minima and maxima can only occur at critical points or endpoints.\n\nThis also has applications in graph sketching: once the local minima and maxima of a differentiable function have been found, a rough plot of the graph can be obtained from the observation that it will be either increasing or decreasing between critical points.\n\nIn higher dimensions, a critical point of a scalar valued function is a point at which the gradient is zero. The second derivative test can still be used to analyse critical points by considering the eigenvalues of the Hessian matrix of second partial derivatives of the function at the critical point. If all of the eigenvalues are positive, then the point is a local minimum; if all are negative, it is a local maximum. If there are some positive and some negative eigenvalues, then the critical point is called a \"saddle point\", and if none of these cases hold (i.e., some of the eigenvalues are zero) then the test is considered to be inconclusive.\n\nOne example of an optimization problem is: Find the shortest curve between two points on a surface, assuming that the curve must also lie on the surface. If the surface is a plane, then the shortest curve is a line. But if the surface is, for example, egg-shaped, then the shortest path is not immediately clear. These paths are called geodesics, and one of the most fundamental problems in the calculus of variations is finding geodesics. Another example is: Find the smallest area surface filling in a closed curve in space. This surface is called a minimal surface and it, too, can be found using the calculus of variations.\n\nCalculus is of vital importance in physics: many physical processes are described by equations involving derivatives, called differential equations. Physics is particularly concerned with the way quantities change and develop over time, and the concept of the \"time derivative\" — the rate of change over time — is essential for the precise definition of several important concepts. In particular, the time derivatives of an object's position are significant in Newtonian physics:\n\nFor example, if an object's position on a line is given by\n\nthen the object's velocity is\n\nand the object's acceleration is\n\nwhich is constant.\n\nA differential equation is a relation between a collection of functions and their derivatives. An ordinary differential equation is a differential equation that relates functions of one variable to their derivatives with respect to that variable. A partial differential equation is a differential equation that relates functions of more than one variable to their partial derivatives. Differential equations arise naturally in the physical sciences, in mathematical modelling, and within mathematics itself. For example, Newton's second law, which describes the relationship between acceleration and force, can be stated as the ordinary differential equation\n\nThe heat equation in one space variable, which describes how heat diffuses through a straight rod, is the partial differential equation\n\nHere u(x,t) is the temperature of the rod at position x and time t and α is a constant that depends on how fast heat diffuses through the rod.\n\nThe mean value theorem gives a relationship between values of the derivative and values of the original function. If f(x) is a real-valued function and a and b are numbers with a < b, then the mean value theorem says that under mild hypotheses, the slope between the two points (a, f(a)) and (b, f(b)) is equal to the slope of the tangent line to f at some point c between a and b. In other words,\n\nIn practice, what the mean value theorem does is control a function in terms of its derivative. For instance, suppose that f has derivative equal to zero at each point. This means that its tangent line is horizontal at every point, so the function should also be horizontal. The mean value theorem proves that this must be true: The slope between any two points on the graph of f must equal the slope of one of the tangent lines of f. All of those slopes are zero, so any line from one point on the graph to another point will also have slope zero. But that says that the function does not move up or down, so it must be a horizontal line. More complicated conditions on the derivative lead to less precise but still highly useful information about the original function.\n\nThe derivative gives the best possible linear approximation of a function at a given point, but this can be very different from the original function. One way of improving the approximation is to take a quadratic approximation. That is to say, the linearization of a real-valued function f(x) at the point x0 is a linear polynomial a + b(x − x0), and it may be possible to get a better approximation by considering a quadratic polynomial a + b(x − x0) + c(x − x0)2. Still better might be a cubic polynomial a + b(x − x0) + c(x − x0)2 + d(x − x0)3, and this idea can be extended to arbitrarily high degree polynomials. For each one of these polynomials, there should be a best possible choice of coefficients a, b, c, and d that makes the approximation as good as possible.\n\nIn the neighbourhood of x0, for a the best possible choice is always f(x0), and for b the best possible choice is always f'(x0). For c, d, and higher-degree coefficients, these coefficients are determined by higher derivatives of f. c should always be ⁠f''(x0)/2⁠, and d should always be ⁠f'''(x0)/3!⁠. Using these coefficients gives the Taylor polynomial of f. The Taylor polynomial of degree d is the polynomial of degree d which best approximates f, and its coefficients can be found by a generalization of the above formulas. Taylor's theorem gives a precise bound on how good the approximation is. If f is a polynomial of degree less than or equal to d, then the Taylor polynomial of degree d equals f.\n\nThe limit of the Taylor polynomials is an infinite series called the Taylor series. The Taylor series is frequently a very good approximation to the original function. Functions which are equal to their Taylor series are called analytic functions. It is impossible for functions with discontinuities or sharp corners to be analytic; moreover, there exist smooth functions which are also not analytic.\n\nSome natural geometric shapes, such as circles, cannot be drawn as the graph of a function. For instance, if f(x, y) = x2 + y2 − 1, then the circle is the set of all pairs (x, y) such that f(x, y) = 0. This set is called the zero set of f, and is not the same as the graph of f, which is a paraboloid. The implicit function theorem converts relations such as f(x, y) = 0 into functions. It states that if f is continuously differentiable, then around most points, the zero set of f looks like graphs of functions pasted together. The points where this is not true are determined by a condition on the derivative of f. The circle, for instance, can be pasted together from the graphs of the two functions ± √1 - x2. In a neighborhood of every point on the circle except (−1, 0) and (1, 0), one of these two functions has a graph that looks like the circle. (These two functions also happen to meet (−1, 0) and (1, 0), but this is not guaranteed by the implicit function theorem.)\n\nThe implicit function theorem is closely related to the inverse function theorem, which states when a function looks like graphs of invertible functions pasted together.\n"
    },
    {
        "title": "Integral",
        "content": "In mathematics, an integral is the continuous analog of a sum, which is used to calculate areas, volumes, and their generalizations. Integration, the process of computing an integral, is one of the two fundamental operations of calculus,[a] the other being differentiation. Integration was initially used to solve problems in mathematics and physics, such as finding the area under a curve, or determining displacement from velocity. Usage of integration expanded to a wide variety of scientific fields thereafter.\n\nA definite integral computes the signed area of the region in the plane that is bounded by the graph of a given function between two points in the real line. Conventionally, areas above the horizontal axis of the plane are positive while areas below are negative. Integrals also refer to the concept of an antiderivative, a function whose derivative is the given function; in this case, they are also called indefinite integrals. The fundamental theorem of calculus relates definite integration to differentiation and provides a method to compute the definite integral of a function when its antiderivative is known; differentiation and integration are inverse operations.\n\nAlthough methods of calculating areas and volumes dated from ancient Greek mathematics, the principles of integration were formulated independently by Isaac Newton and Gottfried Wilhelm Leibniz in the late 17th century, who thought of the area under a curve as an infinite sum of rectangles of infinitesimal width. Bernhard Riemann later gave a rigorous definition of integrals, which is based on a limiting procedure that approximates the area of a curvilinear region by breaking the region into infinitesimally thin vertical slabs. In the early 20th century, Henri Lebesgue generalized Riemann's formulation by introducing what is now referred to as the Lebesgue integral; it is more general than Riemann's in the sense that a wider class of functions are Lebesgue-integrable.\n\nIntegrals may be generalized depending on the type of the function as well as the domain over which the integration is performed. For example, a line integral is defined for functions of two or more variables, and the interval of integration is replaced by a curve connecting two points in space. In a surface integral, the curve is replaced by a piece of a surface in three-dimensional space.\n\nThe first documented systematic technique capable of determining integrals is the method of exhaustion of the ancient Greek astronomer Eudoxus and philosopher Democritus (ca. 370 BC), which sought to find areas and volumes by breaking them up into an infinite number of divisions for which the area or volume was known.[1] This method was further developed and employed by Archimedes in the 3rd century BC and used to calculate the area of a circle, the surface area and volume of a sphere, area of an ellipse, the area under a parabola, the volume of a segment of a paraboloid of revolution, the volume of a segment of a hyperboloid of revolution, and the area of a spiral.[2]\n\nA similar method was independently developed in China around the 3rd century AD by Liu Hui, who used it to find the area of the circle. This method was later used in the 5th century by Chinese father-and-son mathematicians Zu Chongzhi and Zu Geng to find the volume of a sphere.[3]\n\nIn the Middle East, Hasan Ibn al-Haytham, Latinized as Alhazen (c. 965 – c. 1040 AD) derived a formula for the sum of fourth powers.[4] Alhazen determined the equations to calculate the area enclosed by the curve represented by \n\n\n\ny\n=\n\nx\n\nk\n\n\n\n\n{\\displaystyle y=x^{k}}\n\n (which translates to the integral \n\n\n\n∫\n\nx\n\nk\n\n\n\nd\nx\n\n\n{\\displaystyle \\int x^{k}\\,dx}\n\n in contemporary notation), for any given non-negative integer value of \n\n\n\nk\n\n\n{\\displaystyle k}\n\n.[5] He used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid.[6]\n\nThe next significant advances in integral calculus did not begin to appear until the 17th century. At this time, the work of Cavalieri with his method of indivisibles, and work by Fermat, began to lay the foundations of modern calculus,[7] with Cavalieri computing the integrals of xn up to degree n = 9 in Cavalieri's quadrature formula.[8] The case n = −1 required the invention of a function, the hyperbolic logarithm, achieved by quadrature of the hyperbola in 1647.\n\nFurther steps were made in the early 17th century by Barrow and Torricelli, who provided the first hints of a connection between integration and differentiation. Barrow provided the first proof of the fundamental theorem of calculus.[9] Wallis generalized Cavalieri's method, computing integrals of x to a general power, including negative powers and fractional powers.[10]\n\nThe major advance in integration came in the 17th century with the independent discovery of the fundamental theorem of calculus by Leibniz and Newton.[11] The theorem demonstrates a connection between integration and differentiation. This connection, combined with the comparative ease of differentiation, can be exploited to calculate integrals. In particular, the fundamental theorem of calculus allows one to solve a much broader class of problems. Equal in importance is the comprehensive mathematical framework that both Leibniz and Newton developed. Given the name infinitesimal calculus, it allowed for precise analysis of functions with continuous domains. This framework eventually became modern calculus, whose notation for integrals is drawn directly from the work of Leibniz.\n\nWhile Newton and Leibniz provided a systematic approach to integration, their work lacked a degree of rigour. Bishop Berkeley memorably attacked the vanishing increments used by Newton, calling them \"ghosts of departed quantities\".[12] Calculus acquired a firmer footing with the development of limits. Integration was first rigorously formalized, using limits, by Riemann.[13] Although all bounded piecewise continuous functions are Riemann-integrable on a bounded interval, subsequently more general functions were considered—particularly in the context of Fourier analysis—to which Riemann's definition does not apply, and Lebesgue formulated a different definition of integral, founded in measure theory (a subfield of real analysis). Other definitions of integral, extending Riemann's and Lebesgue's approaches, were proposed. These approaches based on the real number system are the ones most common today, but alternative approaches exist, such as a definition of integral as the standard part of an infinite Riemann sum, based on the hyperreal number system.\n\nThe notation for the indefinite integral was introduced by Gottfried Wilhelm Leibniz in 1675.[14] He adapted the integral symbol, ∫, from the letter ſ (long s), standing for summa (written as ſumma; Latin for \"sum\" or \"total\"). The modern notation for the definite integral, with limits above and below the integral sign, was first used by Joseph Fourier in Mémoires of the French Academy around 1819–1820, reprinted in his book of 1822.[15]\n\nIsaac Newton used a small vertical bar above a variable to indicate integration, or placed the variable inside a box. The vertical bar was easily confused with .x or x′, which are used to indicate differentiation, and the box notation was difficult for printers to reproduce, so these notations were not widely adopted.[16]\n\nThe term was first printed in Latin by Jacob Bernoulli in 1690: \"Ergo et horum Integralia aequantur\".[17]\n\nIn general, the integral of a real-valued function f(x) with respect to a real variable x on an interval [a, b] is written as\n\nThe integral sign ∫ represents integration. The symbol dx, called the differential of the variable x, indicates that the variable of integration is x.  The function f(x) is called the integrand, the points a and b are called the limits (or bounds) of integration, and the integral is said to be over the interval [a, b], called the interval of integration.[18] \nA function is said to be integrable if its integral over its domain is finite. If limits are specified, the integral is called a definite integral.\n\nWhen the limits are omitted, as in\n\nthe integral is called an indefinite integral, which represents a class of functions (the antiderivative) whose derivative is the integrand.[19] The fundamental theorem of calculus relates the evaluation of definite integrals to indefinite integrals. There are several extensions of the notation for integrals to encompass integration on unbounded domains and/or in multiple dimensions (see later sections of this article).\n\nIn advanced settings, it is not uncommon to leave out dx when only the simple Riemann integral is being used, or the exact type of integral is immaterial. For instance, one might write \n\n\n\n\n∫\n\na\n\n\nb\n\n\n(\n\nc\n\n1\n\n\nf\n+\n\nc\n\n2\n\n\ng\n)\n=\n\nc\n\n1\n\n\n\n∫\n\na\n\n\nb\n\n\nf\n+\n\nc\n\n2\n\n\n\n∫\n\na\n\n\nb\n\n\ng\n\n\n{\\textstyle \\int _{a}^{b}(c_{1}f+c_{2}g)=c_{1}\\int _{a}^{b}f+c_{2}\\int _{a}^{b}g}\n\n to express the linearity of the integral, a property shared by the Riemann integral and all generalizations thereof.[20]\n\nIntegrals appear in many practical situations. For instance, from the length, width and depth of a swimming pool which is rectangular with a flat bottom, one can determine the volume of water it can contain, the area of its surface, and the length of its edge. But if it is oval with a rounded bottom, integrals are required to find exact and rigorous values for these quantities. In each case, one may divide the sought quantity into infinitely many infinitesimal pieces, then sum the pieces to achieve an accurate approximation.\n\nAs another example, to find the area of the region bounded by the graph of the function f(x) = \n\n\n\n\n\nx\n\n\n\n\n{\\textstyle {\\sqrt {x}}}\n\n between x = 0 and x = 1, one can divide the interval into five pieces (0, 1/5, 2/5, ..., 1), then construct rectangles using the right end height of each piece (thus √0, √1/5, √2/5, ..., √1) and sum their areas to get the approximation\n\nwhich is larger than the exact value. Alternatively, when replacing these subintervals by ones with the left end height of each piece, the approximation one gets is too low: with twelve such subintervals the approximated area is only 0.6203. However, when the number of pieces increases to infinity, it will reach a limit which is the exact value of the area sought (in this case, 2/3). One writes\n\nwhich means 2/3 is the result of a weighted sum of function values, √x, multiplied by the infinitesimal step widths, denoted by dx, on the interval [0, 1].\n\nThere are many ways of formally defining an integral, not all of which are equivalent. The differences exist mostly to deal with differing special cases which may not be integrable under other definitions, but are also occasionally for pedagogical reasons. The most commonly used definitions are Riemann integrals and Lebesgue integrals.\n\nThe Riemann integral is defined in terms of Riemann sums of functions with respect to tagged partitions of an interval.[21] A tagged partition of a closed interval [a, b] on the real line is a finite sequence\n\nThis partitions the interval [a, b] into n sub-intervals [xi−1, xi] indexed by i, each of which is \"tagged\" with a specific point ti ∈ [xi−1, xi]. A Riemann sum of a function f with respect to such a tagged partition is defined as\n\nthus each term of the sum is the area of a rectangle with height equal to the function value at the chosen point of the given sub-interval, and width the same as the width of sub-interval, Δi = xi−xi−1. The mesh of such a tagged partition is the width of the largest sub-interval formed by the partition, maxi=1...n Δi. The Riemann integral of a function f over the interval [a, b] is equal to S if:[22]\n\nWhen the chosen tags are the maximum (respectively, minimum) value of the function in each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.\n\nIt is often of interest, both in theory and applications, to be able to pass to the limit under the integral. For instance, a sequence of functions can frequently be constructed that approximate, in a suitable sense, the solution to a problem. Then the integral of the solution function should be the limit of the integrals of the approximations. However, many functions that can be obtained as limits are not Riemann-integrable, and so such limit theorems do not hold with the Riemann integral. Therefore, it is of great importance to have a definition of the integral that allows a wider class of functions to be integrated.[23]\n\nSuch an integral is the Lebesgue integral, that exploits the following fact to enlarge the class of integrable functions: if the values of a function are rearranged over the domain, the integral of a function should remain the same. Thus Henri Lebesgue introduced the integral bearing his name, explaining this integral thus in a letter to Paul Montel:[24]\n\nI have to pay a certain sum, which I have collected in my pocket. I take the bills and coins out of my pocket and give them to the creditor in the order I find them until I have reached the total sum. This is the Riemann integral. But I can proceed differently. After I have taken all the money out of my pocket I order the bills and coins according to identical values and then I pay the several heaps one after the other to the creditor. This is my integral.\nAs Folland puts it, \"To compute the Riemann integral of f, one partitions the domain [a, b] into subintervals\", while in the Lebesgue integral, \"one is in effect partitioning the range of f \".[25] The definition of the Lebesgue integral thus begins with a measure, μ. In the simplest case, the Lebesgue measure μ(A) of an interval A = [a, b] is its width, b − a, so that the Lebesgue integral agrees with the (proper) Riemann integral when both exist.[26] In more complicated cases, the sets being measured can be highly fragmented, with no continuity and no resemblance to intervals.\n\nUsing the \"partitioning the range of f \" philosophy, the integral of a non-negative function f : R → R should be the sum over t of the areas between a thin horizontal strip between y = t and y = t + dt. This area is just μ{ x : f(x) > t} dt. Let f∗(t) = μ{ x : f(x) > t }. The Lebesgue integral of f is then defined by\n\nwhere the integral on the right is an ordinary improper Riemann integral (f∗ is a strictly decreasing positive function, and therefore has a well-defined improper Riemann integral).[27] For a suitable class of functions (the measurable functions) this defines the Lebesgue integral.\n\nA general measurable function f is Lebesgue-integrable if the sum of the absolute values of the areas of the regions between the graph of f and the x-axis is finite:[28]\n\nIn that case, the integral is, as in the Riemannian case, the difference between the area above the x-axis and the area below the x-axis:[29]\n\nwhere\n\nAlthough the Riemann and Lebesgue integrals are the most widely used definitions of the integral, a number of others exist, including:\n\nThe collection of Riemann-integrable functions on a closed interval [a, b] forms a vector space under the operations of pointwise addition and multiplication by a scalar, and the operation of integration\n\nis a linear functional on this vector space. Thus, the collection of integrable functions is closed under taking linear combinations, and the integral of a linear combination is the linear combination of the integrals:[30]\n\nSimilarly, the set of real-valued Lebesgue-integrable functions on a given measure space E with measure μ is closed under taking linear combinations and hence form a vector space, and the Lebesgue integral\n\nis a linear functional on this vector space, so that:[29]\n\nMore generally, consider the vector space of all measurable functions on a measure space (E,μ), taking values in a locally compact complete topological vector space V over a locally compact topological field K, f : E → V. Then one may define an abstract integration map assigning to each function f an element of V or the symbol ∞,\n\nthat is compatible with linear combinations.[31] In this situation, the linearity holds for the subspace of functions whose integral is an element of V (i.e. \"finite\"). The most important special cases arise when K is R, C, or a finite extension of the field Qp of p-adic numbers, and V is a finite-dimensional vector space over K, and when K = C and V is a complex Hilbert space.\n\nLinearity, together with some natural continuity properties and normalization for a certain class of \"simple\" functions, may be used to give an alternative definition of the integral. This is the approach of Daniell for the case of real-valued functions on a set X, generalized by Nicolas Bourbaki to functions with values in a locally compact topological vector space. See Hildebrandt 1953 for an axiomatic characterization of the integral.\n\nA number of general inequalities hold for Riemann-integrable functions defined on a closed and bounded interval [a, b] and can be generalized to other notions of integral (Lebesgue and Daniell).\n\nIn this section, f is a real-valued Riemann-integrable function. The integral\n\nover an interval [a, b] is defined if a < b. This means that the upper and lower sums of the function f are evaluated on a partition a = x0 ≤ x1 ≤ . . . ≤ xn = b whose values xi are increasing. Geometrically, this signifies that integration takes place \"left to right\", evaluating f within intervals [x i , x i +1] where an interval with a higher index lies to the right of one with a lower index. The values a and b, the end-points of the interval, are called the limits of integration of f. Integrals can also be defined if a > b:[18]\n\nWith a = b, this implies:\n\nThe first convention is necessary in consideration of taking integrals over subintervals of [a, b]; the second says that an integral taken over a degenerate interval, or a point, should be zero. One reason for the first convention is that the integrability of f on an interval [a, b] implies that f is integrable on any subinterval [c, d], but in particular integrals have the property that if c is any element of [a, b], then:[30]\n\nWith the first convention, the resulting relation\n\nis then well-defined for any cyclic permutation of a, b, and c.\n\nThe fundamental theorem of calculus is the statement that differentiation and integration are inverse operations: if a continuous function is first integrated and then differentiated, the original function is retrieved.[34] An important consequence, sometimes called the second fundamental theorem of calculus, allows one to compute integrals by using an antiderivative of the function to be integrated.[35]\n\nLet f be a continuous real-valued function defined on a closed interval [a, b]. Let F be the function defined, for all x in [a, b], by[36]\n\nThen, F is continuous on [a, b], differentiable on the open interval (a, b), and\n\nfor all x in (a, b).\n\nLet f be a real-valued function defined on a closed interval [a, b] that admits an antiderivative F on [a, b]. That is, f and F are functions such that for all x in [a, b],\n\nIf f is integrable on [a, b] then\n\nA \"proper\" Riemann integral assumes the integrand is defined and finite on a closed and bounded interval, bracketed by the limits of integration. An improper integral occurs when one or more of these conditions is not satisfied. In some cases such integrals may be defined by considering the limit of a sequence of proper Riemann integrals on progressively larger intervals.\n\nIf the interval is unbounded, for instance at its upper end, then the improper integral is the limit as that endpoint goes to infinity:[37]\n\nIf the integrand is only defined or finite on a half-open interval, for instance (a, b], then again a limit may provide a finite result:[38]\n\nThat is, the improper integral is the limit of proper integrals as one endpoint of the interval of integration approaches either a specified real number, or ∞, or −∞. In more complicated cases, limits are required at both endpoints, or at interior points.\n\nJust as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the x-axis, the double integral of a positive function of two variables represents the volume of the region between the surface defined by the function and the plane that contains its domain.[39] For example, a function in two dimensions depends on two real variables, x and y, and the integral of a function f over the rectangle R given as the Cartesian product of two intervals \n\n\n\nR\n=\n[\na\n,\nb\n]\n×\n[\nc\n,\nd\n]\n\n\n{\\displaystyle R=[a,b]\\times [c,d]}\n\n can be written\n\nwhere the differential dA indicates that integration is taken with respect to area. This double integral can be defined using Riemann sums, and represents the (signed) volume under the graph of z = f(x,y) over the domain R.[40] Under suitable conditions (e.g., if f is continuous), Fubini's theorem states that this integral can be expressed as an equivalent iterated integral[41]\n\nThis reduces the problem of computing a double integral to computing one-dimensional integrals. Because of this, another notation for the integral over R uses a double integral sign:[40]\n\nIntegration over more general domains is possible. The integral of a function f, with respect to volume, over an n-dimensional region D of \n\n\n\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{n}}\n\n is denoted by symbols such as:\n\nThe concept of an integral can be extended to more general domains of integration, such as curved lines and surfaces inside higher-dimensional spaces. Such integrals are known as line integrals and surface integrals respectively. These have important applications in physics, as when dealing with vector fields.\n\nA line integral (sometimes called a path integral) is an integral where the function to be integrated is evaluated along a curve.[42] Various different line integrals are in use. In the case of a closed curve it is also called a contour integral.\n\nThe function to be integrated may be a scalar field or a vector field. The value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve).[43] This weighting distinguishes the line integral from simpler integrals defined on intervals. Many simple formulas in physics have natural continuous analogs in terms of line integrals; for example, the fact that work is equal to force, F, multiplied by displacement, s, may be expressed (in terms of vector quantities) as:[44]\n\nFor an object moving along a path C in a vector field F such as an electric field or gravitational field, the total work done by the field on the object is obtained by summing up the differential work done in moving from s to s + ds. This gives the line integral[45]\n\nA surface integral generalizes double integrals to integration over a surface (which may be a curved set in space); it can be thought of as the double integral analog of the line integral. The function to be integrated may be a scalar field or a vector field. The value of the surface integral is the sum of the field at all points on the surface. This can be achieved by splitting the surface into surface elements, which provide the partitioning for Riemann sums.[46]\n\nFor an example of applications of surface integrals, consider a vector field v on a surface S; that is, for each point x in S, v(x) is a vector. Imagine that a fluid flows through S, such that v(x) determines the velocity of the fluid at x. The flux is defined as the quantity of fluid flowing through S in unit amount of time. To find the flux, one need to take the dot product of v with the unit surface normal to S at each point, which will give a scalar field, which is integrated over the surface:[47]\n\nThe fluid flux in this example may be from a physical fluid such as water or air, or from electrical or magnetic flux. Thus surface integrals have applications in physics, particularly with the classical theory of electromagnetism.\n\nIn complex analysis, the integrand is a complex-valued function of a complex variable z instead of a real function of a real variable x. When a complex function is integrated along a curve \n\n\n\nγ\n\n\n{\\displaystyle \\gamma }\n\n in the complex plane, the integral is denoted as follows\n\nThis is known as a contour integral.\n\nA differential form is a mathematical concept in the fields of multivariable calculus, differential topology, and tensors. Differential forms are organized by degree. For example, a one-form is a weighted sum of the differentials of the coordinates, such as:\n\nwhere E, F, G are functions in three dimensions. A differential one-form can be integrated over an oriented path, and the resulting integral is just another way of writing a line integral. Here the basic differentials dx, dy, dz measure infinitesimal oriented lengths parallel to the three coordinate axes.\n\nA differential two-form is a sum of the form\n\nHere the basic two-forms \n\n\n\nd\nx\n∧\nd\ny\n,\nd\nz\n∧\nd\nx\n,\nd\ny\n∧\nd\nz\n\n\n{\\displaystyle dx\\wedge dy,dz\\wedge dx,dy\\wedge dz}\n\n measure oriented areas parallel to the coordinate two-planes. The symbol \n\n\n\n∧\n\n\n{\\displaystyle \\wedge }\n\n denotes the wedge product, which is similar to the cross product in the sense that the wedge product of two forms representing oriented lengths represents an oriented area. A two-form can be integrated over an oriented surface, and the resulting integral is equivalent to the surface integral giving the flux of \n\n\n\nE\n\ni\n\n+\nF\n\nj\n\n+\nG\n\nk\n\n\n\n{\\displaystyle E\\mathbf {i} +F\\mathbf {j} +G\\mathbf {k} }\n\n.\n\nUnlike the cross product, and the three-dimensional vector calculus, the wedge product and the calculus of differential forms makes sense in arbitrary dimension and on more general manifolds (curves, surfaces, and their higher-dimensional analogs). The exterior derivative plays the role of the gradient and curl of vector calculus, and Stokes' theorem simultaneously generalizes the three theorems of vector calculus: the divergence theorem, Green's theorem, and the Kelvin-Stokes theorem.\n\nThe discrete equivalent of integration is summation. Summations and integrals can be put on the same foundations using the theory of Lebesgue integrals or time-scale calculus.\n\nAn integration that is performed not over a variable (or, in physics, over a space or time dimension), but over a space of functions, is referred to as a functional integral.\n\nIntegrals are used extensively in many areas. For example, in probability theory, integrals are used to determine the probability of some random variable falling within a certain range.[48] Moreover, the integral under an entire probability density function must equal 1, which provides a test of whether a function with no negative values could be a density function or not.[49]\n\nIntegrals can be used for computing the area of a two-dimensional region that has a curved boundary, as well as computing the volume of a three-dimensional object that has a curved boundary. The area of a two-dimensional region can be calculated using the aforementioned definite integral.[50] The volume of a three-dimensional object such as a disc or washer can be computed by disc integration using the equation for the volume of a cylinder, \n\n\n\nπ\n\nr\n\n2\n\n\nh\n\n\n{\\displaystyle \\pi r^{2}h}\n\n, where \n\n\n\nr\n\n\n{\\displaystyle r}\n\n is the radius. In the case of a simple disc created by rotating a curve about the x-axis, the radius is given by f(x), and its height is the differential dx. Using an integral with bounds a and b, the volume of the disc is equal to:[51]\n\n\n\nπ\n\n∫\n\na\n\n\nb\n\n\n\nf\n\n2\n\n\n(\nx\n)\n\nd\nx\n.\n\n\n{\\displaystyle \\pi \\int _{a}^{b}f^{2}(x)\\,dx.}\n\nIntegrals are also used in physics, in areas like kinematics to find quantities like displacement, time, and velocity. For example, in rectilinear motion, the displacement of an object over the time interval \n\n\n\n[\na\n,\nb\n]\n\n\n{\\displaystyle [a,b]}\n\n is given by\n\nwhere \n\n\n\nv\n(\nt\n)\n\n\n{\\displaystyle v(t)}\n\n is the velocity expressed as a function of time.[52] The work done by a force \n\n\n\nF\n(\nx\n)\n\n\n{\\displaystyle F(x)}\n\n (given as a function of position) from an initial position \n\n\n\nA\n\n\n{\\displaystyle A}\n\n to a final position \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is:[53]\n\nIntegrals are also used in thermodynamics, where thermodynamic integration is used to calculate the difference in free energy between two given states.\n\nThe most basic technique for computing definite integrals of one real variable is based on the fundamental theorem of calculus. Let f(x) be the function of x to be integrated over a given interval [a, b]. Then, find an antiderivative of f; that is, a function F such that F′ = f on the interval. Provided the integrand and integral have no singularities on the path of integration, by the fundamental theorem of calculus,\n\nSometimes it is necessary to use one of the many techniques that have been developed to evaluate integrals. Most of these techniques rewrite one integral as a different one which is hopefully more tractable. Techniques include integration by substitution, integration by parts, integration by trigonometric substitution, and integration by partial fractions.\n\nAlternative methods exist to compute more complex integrals. Many nonelementary integrals can be expanded in a Taylor series and integrated term by term. Occasionally, the resulting infinite series can be summed analytically. The method of convolution using Meijer G-functions can also be used, assuming that the integrand can be written as a product of Meijer G-functions. There are also many less common ways of calculating definite integrals; for instance, Parseval's identity can be used to transform an integral over a rectangular region into an infinite sum. Occasionally, an integral can be evaluated by a trick; for an example of this, see Gaussian integral.\n\nComputations of volumes of solids of revolution can usually be done with disk integration or shell integration.\n\nSpecific results which have been worked out by various techniques are collected in the list of integrals.\n\nMany problems in mathematics, physics, and engineering involve integration where an explicit formula for the integral is desired. Extensive tables of integrals have been compiled and published over the years for this purpose. With the spread of computers, many professionals, educators, and students have turned to computer algebra systems that are specifically designed to perform difficult or tedious tasks, including integration. Symbolic integration has been one of the motivations for the development of the first such systems, like Macsyma and Maple.\n\nA major mathematical difficulty in symbolic integration is that in many cases, a relatively simple function does not have integrals that can be expressed in closed form involving only elementary functions, include rational and exponential functions, logarithm, trigonometric functions and inverse trigonometric functions, and the operations of multiplication and composition. The Risch algorithm provides a general criterion to determine whether the antiderivative of an elementary function is elementary and to compute the integral if is elementary. However, functions with closed expressions of antiderivatives are the exception, and consequently, computerized algebra systems have no hope of being able to find an antiderivative for a randomly constructed elementary function. On the positive side, if the 'building blocks' for antiderivatives are fixed in advance, it may still be possible to decide whether the antiderivative of a given function can be expressed using these blocks and operations of multiplication and composition and to find the symbolic answer whenever it exists. The Risch algorithm, implemented in Mathematica, Maple and other computer algebra systems, does just that for functions and antiderivatives built from rational functions, radicals, logarithm, and exponential functions.\n\nSome special integrands occur often enough to warrant special study. In particular, it may be useful to have, in the set of antiderivatives, the special functions (like the Legendre functions, the hypergeometric function, the gamma function, the incomplete gamma function and so on). Extending Risch's algorithm to include such functions is possible but challenging and has been an active research subject.\n\nMore recently a new approach has emerged, using  D-finite functions, which are the solutions of linear differential equations with polynomial coefficients. Most of the elementary and special functions are D-finite, and the integral of a D-finite function is also a D-finite function. This provides an algorithm to express the antiderivative of a D-finite function as the solution of a differential equation. This theory also allows one to compute the definite integral of a D-function as the sum of a series given by the first coefficients and provides an algorithm to compute any coefficient.\n\nRule-based integration systems facilitate integration.  Rubi, a computer algebra system rule-based integrator, pattern matches an extensive system of symbolic integration rules to integrate a wide variety of integrands.  This system uses over 6600 integration rules to compute integrals.[54]  The  method of brackets is a generalization of Ramanujan's master theorem that can be applied to a wide range of univariate and multivariate integrals.  A set of rules are applied to the coefficients and exponential terms of the integrand's power series expansion to determine the integral.  The method is closely related to the Mellin transform.[55]\n\nDefinite integrals may be approximated using several methods of numerical integration. The rectangle method relies on dividing the region under the function into a series of rectangles corresponding to function values and multiplies by the step width to find the sum. A better approach, the trapezoidal rule, replaces the rectangles used in a Riemann sum with trapezoids. The trapezoidal rule weights the first and last values by one half, then multiplies by the step width to obtain a better approximation.[56] The idea behind the trapezoidal rule, that more accurate approximations to the function yield better approximations to the integral, can be carried further: Simpson's rule approximates the integrand by a piecewise quadratic function.[57]\n\nRiemann sums, the trapezoidal rule, and Simpson's rule are examples of a family of quadrature rules called the Newton–Cotes formulas. The degree n Newton–Cotes quadrature rule approximates the polynomial on each subinterval by a degree n polynomial. This polynomial is chosen to interpolate the values of the function on the interval.[58] Higher degree Newton–Cotes approximations can be more accurate, but they require more function evaluations, and they can suffer from numerical inaccuracy due to Runge's phenomenon. One solution to this problem is Clenshaw–Curtis quadrature, in which the integrand is approximated by expanding it in terms of Chebyshev polynomials.\n\nRomberg's method halves the step widths incrementally, giving trapezoid approximations denoted by T(h0), T(h1), and so on, where hk+1 is half of hk. For each new step size, only half the new function values need to be computed; the others carry over from the previous size. It then interpolate a polynomial through the approximations, and extrapolate to T(0). Gaussian quadrature evaluates the function at the roots of a set of orthogonal polynomials.[59] An n-point Gaussian method is exact for polynomials of degree up to 2n − 1.\n\nThe computation of higher-dimensional integrals (for example, volume calculations) makes important use of such alternatives as Monte Carlo integration.[60]\n\nThe area of an arbitrary two-dimensional shape can be determined using a measuring instrument called planimeter. The volume of irregular objects can be measured with precision by the fluid displaced as the object is submerged.\n\nArea can sometimes be found via geometrical compass-and-straightedge constructions of an equivalent square.\n\nKempf, Jackson and Morales demonstrated mathematical relations that allow an integral to be calculated by means of differentiation. Their calculus involves the Dirac delta function and the partial derivative operator \n\n\n\n\n∂\n\nx\n\n\n\n\n{\\displaystyle \\partial _{x}}\n\n. This can also be applied to functional integrals, allowing them to be computed by functional differentiation.[61]\n\nThe fundamental theorem of calculus allows straightforward calculations of basic functions:\n"
    },
    {
        "title": "Continuous function",
        "content": "In mathematics, a continuous function is a function such that a small variation of the argument induces a small variation of the value of the function. This implies there are no abrupt changes in value, known as discontinuities. More precisely, a function is continuous if arbitrarily small changes in its value can be assured by restricting to sufficiently small changes of its argument. A discontinuous function is a function that is not continuous. Until the 19th century, mathematicians largely relied on intuitive notions of continuity and considered only continuous functions. The epsilon–delta definition of a limit was introduced to formalize the definition of continuity.\n\nContinuity is one of the core concepts of calculus and mathematical analysis, where arguments and values of functions are real and complex numbers. The concept has been generalized to functions between metric spaces and between topological spaces. The latter are the most general continuous functions, and their definition is the basis of topology.\n\nA stronger form of continuity is uniform continuity. In order theory, especially in domain theory, a related concept of continuity is Scott continuity.\n\nAs an example, the function H(t) denoting the height of a growing flower at time t would be considered continuous. In contrast, the function M(t) denoting the amount of money in a bank account at time t would be considered discontinuous since it \"jumps\" at each point in time when money is deposited or withdrawn.\n\nA form of the epsilon–delta definition of continuity was first given by Bernard Bolzano in 1817. Augustin-Louis Cauchy defined continuity of \n\n\n\ny\n=\nf\n(\nx\n)\n\n\n{\\displaystyle y=f(x)}\n\n as follows: an infinitely small increment \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n of the independent variable x always produces an infinitely small change \n\n\n\nf\n(\nx\n+\nα\n)\n−\nf\n(\nx\n)\n\n\n{\\displaystyle f(x+\\alpha )-f(x)}\n\n of the dependent variable y (see e.g. Cours d'Analyse, p. 34). Cauchy defined infinitely small quantities in terms of variable quantities, and his definition of continuity closely parallels the infinitesimal definition used today (see microcontinuity). The formal definition and the distinction between pointwise continuity and uniform continuity were first given by Bolzano in the 1830s, but the work wasn't published until the 1930s. Like Bolzano,[1] Karl Weierstrass[2] denied continuity of a function at a point c unless it was defined at and on both sides of c, but Édouard Goursat[3] allowed the function to be defined only at and on one side of c, and Camille Jordan[4] allowed it even if the function was defined only at c. All three of those nonequivalent definitions of pointwise continuity are still in use.[5] Eduard Heine provided the first published definition of uniform continuity in 1872, but based these ideas on lectures given by Peter Gustav Lejeune Dirichlet in 1854.[6]\n\nA real function that is a function from real numbers to real numbers can be represented by a graph in the Cartesian plane; such a function is continuous if, roughly speaking, the graph is a single unbroken curve whose domain is the entire real line. A more mathematically rigorous definition is given below.[8]\n\nContinuity of real functions is usually defined in terms of limits. A function f with variable x is continuous at the real number c, if the limit of \n\n\n\nf\n(\nx\n)\n,\n\n\n{\\displaystyle f(x),}\n\n as x tends to c, is equal to \n\n\n\nf\n(\nc\n)\n.\n\n\n{\\displaystyle f(c).}\n\n\n\nThere are several different definitions of the (global) continuity of a function, which depend on the nature of its domain. \n\nA function is continuous on an open interval if the interval is contained in the function's domain and the function is continuous at every interval point. A function that is continuous on the interval \n\n\n\n(\n−\n∞\n,\n+\n∞\n)\n\n\n{\\displaystyle (-\\infty ,+\\infty )}\n\n (the whole real line) is often called simply a continuous function; one also says that such a function is continuous everywhere. For example, all polynomial functions are continuous everywhere.\n\nA function is continuous on a semi-open or a closed interval; if the interval is contained in the domain of the function, the function is continuous at every interior point of the interval, and the value of the function at each endpoint that belongs to the interval is the limit of the values of the function when the variable tends to the endpoint from the interior of the interval. For example, the function \n\n\n\nf\n(\nx\n)\n=\n\n\nx\n\n\n\n\n{\\displaystyle f(x)={\\sqrt {x}}}\n\n is continuous on its whole domain, which is the closed interval \n\n\n\n[\n0\n,\n+\n∞\n)\n.\n\n\n{\\displaystyle [0,+\\infty ).}\n\n\n\nMany commonly encountered functions are partial functions that have a domain formed by all real numbers, except some isolated points. Examples include the reciprocal function \n\n\n\nx\n↦\n\n\n1\nx\n\n\n\n\n{\\textstyle x\\mapsto {\\frac {1}{x}}}\n\n and the tangent function \n\n\n\nx\n↦\ntan\n⁡\nx\n.\n\n\n{\\displaystyle x\\mapsto \\tan x.}\n\n When they are continuous on their domain, one says, in some contexts, that they are continuous, although they are not continuous everywhere. In other contexts, mainly when one is interested in their behavior near the exceptional points, one says they are discontinuous.\n\nA partial function is discontinuous at a point if the point belongs to the topological closure of its domain, and either the point does not belong to the domain of the function or the function is not continuous at the point. For example, the functions \n\n\n\nx\n↦\n\n\n1\nx\n\n\n\n\n{\\textstyle x\\mapsto {\\frac {1}{x}}}\n\n and \n\n\n\nx\n↦\nsin\n⁡\n(\n\n\n1\nx\n\n\n)\n\n\n{\\textstyle x\\mapsto \\sin({\\frac {1}{x}})}\n\n are discontinuous at 0, and remain discontinuous whichever value is chosen for defining them at 0. A point where a function is discontinuous is called a discontinuity.\n\nUsing mathematical notation, several ways exist to define continuous functions in the three senses mentioned above.\n\nLet \n\n\n\nf\n:\nD\n→\n\nR\n\n\n\n{\\displaystyle f:D\\to \\mathbb {R} }\n\n be a function defined on a subset \n\n\n\nD\n\n\n{\\displaystyle D}\n\n of the set \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n of real numbers.\n\nThis subset \n\n\n\nD\n\n\n{\\displaystyle D}\n\n is the domain of f. Some possible choices include \n\nIn the case of the domain \n\n\n\nD\n\n\n{\\displaystyle D}\n\n being defined as an open interval, \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n do not belong to \n\n\n\nD\n\n\n{\\displaystyle D}\n\n, and the values of \n\n\n\nf\n(\na\n)\n\n\n{\\displaystyle f(a)}\n\n and \n\n\n\nf\n(\nb\n)\n\n\n{\\displaystyle f(b)}\n\n do not matter for continuity on \n\n\n\nD\n\n\n{\\displaystyle D}\n\n.\n\nThe function f is continuous at some point c of its domain if the limit of \n\n\n\nf\n(\nx\n)\n,\n\n\n{\\displaystyle f(x),}\n\n as x approaches c through the domain of f,  exists and is equal to \n\n\n\nf\n(\nc\n)\n.\n\n\n{\\displaystyle f(c).}\n\n[9] In mathematical notation, this is written as\n\n\n\n\n\nlim\n\nx\n→\nc\n\n\n\nf\n(\nx\n)\n\n=\nf\n(\nc\n)\n.\n\n\n{\\displaystyle \\lim _{x\\to c}{f(x)}=f(c).}\n\n\nIn detail this means three conditions: first, f has to be defined at c (guaranteed by the requirement that c is in the domain of f). Second, the limit of that equation has to exist. Third, the value of this limit must equal \n\n\n\nf\n(\nc\n)\n.\n\n\n{\\displaystyle f(c).}\n\n\n\n(Here, we have assumed that the domain of f does not have any isolated points.)\n\nA neighborhood of a point c is a set that contains, at least, all points within some fixed distance of c. Intuitively, a function is continuous at a point c if the range of f over the neighborhood of c shrinks to a single point \n\n\n\nf\n(\nc\n)\n\n\n{\\displaystyle f(c)}\n\n as the width of the neighborhood around c shrinks to zero. More precisely, a function f is continuous at a point c of its domain if, for any neighborhood \n\n\n\n\nN\n\n1\n\n\n(\nf\n(\nc\n)\n)\n\n\n{\\displaystyle N_{1}(f(c))}\n\n there is a neighborhood \n\n\n\n\nN\n\n2\n\n\n(\nc\n)\n\n\n{\\displaystyle N_{2}(c)}\n\n in its domain such that \n\n\n\nf\n(\nx\n)\n∈\n\nN\n\n1\n\n\n(\nf\n(\nc\n)\n)\n\n\n{\\displaystyle f(x)\\in N_{1}(f(c))}\n\n whenever \n\n\n\nx\n∈\n\nN\n\n2\n\n\n(\nc\n)\n.\n\n\n{\\displaystyle x\\in N_{2}(c).}\n\n\n\nAs neighborhoods are defined in any topological space, this definition of a continuous function applies not only for real functions but also when the domain and the codomain are topological spaces and is thus the most general definition. It follows that a function is automatically continuous at every isolated point of its domain. For example, every real-valued function on the integers is continuous.\n\nOne can instead require that for any sequence \n\n\n\n(\n\nx\n\nn\n\n\n\n)\n\nn\n∈\n\nN\n\n\n\n\n\n{\\displaystyle (x_{n})_{n\\in \\mathbb {N} }}\n\n of points in the domain which converges to c, the corresponding sequence \n\n\n\n\n\n(\n\nf\n(\n\nx\n\nn\n\n\n)\n\n)\n\n\nn\n∈\n\nN\n\n\n\n\n\n{\\displaystyle \\left(f(x_{n})\\right)_{n\\in \\mathbb {N} }}\n\n converges to \n\n\n\nf\n(\nc\n)\n.\n\n\n{\\displaystyle f(c).}\n\n  In mathematical notation, \n\n\n\n∀\n(\n\nx\n\nn\n\n\n\n)\n\nn\n∈\n\nN\n\n\n\n⊂\nD\n:\n\nlim\n\nn\n→\n∞\n\n\n\nx\n\nn\n\n\n=\nc\n⇒\n\nlim\n\nn\n→\n∞\n\n\nf\n(\n\nx\n\nn\n\n\n)\n=\nf\n(\nc\n)\n\n.\n\n\n{\\displaystyle \\forall (x_{n})_{n\\in \\mathbb {N} }\\subset D:\\lim _{n\\to \\infty }x_{n}=c\\Rightarrow \\lim _{n\\to \\infty }f(x_{n})=f(c)\\,.}\n\n\n\nExplicitly including the definition of the limit of a function, we obtain a self-contained definition: Given a function \n\n\n\nf\n:\nD\n→\n\nR\n\n\n\n{\\displaystyle f:D\\to \\mathbb {R} }\n\n as above and an element \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n of the domain \n\n\n\nD\n\n\n{\\displaystyle D}\n\n, \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is said to be continuous at the point \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n when the following holds: For any positive real number \n\n\n\nε\n>\n0\n,\n\n\n{\\displaystyle \\varepsilon >0,}\n\n however small, there exists some positive real number \n\n\n\nδ\n>\n0\n\n\n{\\displaystyle \\delta >0}\n\n such that for all \n\n\n\nx\n\n\n{\\displaystyle x}\n\n in the domain of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n with \n\n\n\n\nx\n\n0\n\n\n−\nδ\n<\nx\n<\n\nx\n\n0\n\n\n+\nδ\n,\n\n\n{\\displaystyle x_{0}-\\delta <x<x_{0}+\\delta ,}\n\n the value of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n satisfies\n\n\n\n\nf\n\n(\n\nx\n\n0\n\n\n)\n\n−\nε\n<\nf\n(\nx\n)\n<\nf\n(\n\nx\n\n0\n\n\n)\n+\nε\n.\n\n\n{\\displaystyle f\\left(x_{0}\\right)-\\varepsilon <f(x)<f(x_{0})+\\varepsilon .}\n\n\n\nAlternatively written, continuity of \n\n\n\nf\n:\nD\n→\n\nR\n\n\n\n{\\displaystyle f:D\\to \\mathbb {R} }\n\n at \n\n\n\n\nx\n\n0\n\n\n∈\nD\n\n\n{\\displaystyle x_{0}\\in D}\n\n means that for every \n\n\n\nε\n>\n0\n,\n\n\n{\\displaystyle \\varepsilon >0,}\n\n there exists a \n\n\n\nδ\n>\n0\n\n\n{\\displaystyle \\delta >0}\n\n such that for all \n\n\n\nx\n∈\nD\n\n\n{\\displaystyle x\\in D}\n\n:\n\n\n\n\n\n|\n\nx\n−\n\nx\n\n0\n\n\n\n|\n\n<\nδ\n \n \n\n implies \n\n \n \n\n|\n\nf\n(\nx\n)\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n<\nε\n.\n\n\n{\\displaystyle \\left|x-x_{0}\\right|<\\delta ~~{\\text{ implies }}~~|f(x)-f(x_{0})|<\\varepsilon .}\n\n\n\nMore intuitively, we can say that if we want to get all the \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n values to stay in some small neighborhood around \n\n\n\nf\n\n(\n\nx\n\n0\n\n\n)\n\n,\n\n\n{\\displaystyle f\\left(x_{0}\\right),}\n\n we need to choose a small enough neighborhood for the \n\n\n\nx\n\n\n{\\displaystyle x}\n\n values around \n\n\n\n\nx\n\n0\n\n\n.\n\n\n{\\displaystyle x_{0}.}\n\n If we can do that no matter how small the \n\n\n\nf\n(\n\nx\n\n0\n\n\n)\n\n\n{\\displaystyle f(x_{0})}\n\n neighborhood is, then \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is continuous at \n\n\n\n\nx\n\n0\n\n\n.\n\n\n{\\displaystyle x_{0}.}\n\n\n\nIn modern terms, this is generalized by the definition of continuity of a function with respect to a basis for the topology, here the metric topology.\n\nWeierstrass had required that the interval \n\n\n\n\nx\n\n0\n\n\n−\nδ\n<\nx\n<\n\nx\n\n0\n\n\n+\nδ\n\n\n{\\displaystyle x_{0}-\\delta <x<x_{0}+\\delta }\n\n be entirely within the domain \n\n\n\nD\n\n\n{\\displaystyle D}\n\n, but Jordan removed that restriction.\n\nIn proofs and numerical analysis, we often need to know how fast limits are converging, or in other words, control of the remainder. We can formalize this to a definition of continuity. \nA function \n\n\n\nC\n:\n[\n0\n,\n∞\n)\n→\n[\n0\n,\n∞\n]\n\n\n{\\displaystyle C:[0,\\infty )\\to [0,\\infty ]}\n\n is called a control function if\n\nA function \n\n\n\nf\n:\nD\n→\nR\n\n\n{\\displaystyle f:D\\to R}\n\n is C-continuous at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n if there exists such a neighbourhood \n\n\n\nN\n(\n\nx\n\n0\n\n\n)\n\n\n{\\textstyle N(x_{0})}\n\n that \n\n\n\n\n\n|\n\nf\n(\nx\n)\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n≤\nC\n\n(\n\n|\n\nx\n−\n\nx\n\n0\n\n\n\n|\n\n)\n\n\n for all \n\nx\n∈\nD\n∩\nN\n(\n\nx\n\n0\n\n\n)\n\n\n{\\displaystyle |f(x)-f(x_{0})|\\leq C\\left(\\left|x-x_{0}\\right|\\right){\\text{ for all }}x\\in D\\cap N(x_{0})}\n\n\n\nA function is continuous in \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n if it is C-continuous for some control function C.\n\nThis approach leads naturally to refining the notion of continuity by restricting the set of admissible control functions. For a given set of control functions \n\n\n\n\n\nC\n\n\n\n\n{\\displaystyle {\\mathcal {C}}}\n\n a function is \n\n\n\n\n\nC\n\n\n\n\n{\\displaystyle {\\mathcal {C}}}\n\n-continuous if it is \n\n\n\nC\n\n\n{\\displaystyle C}\n\n-continuous for some \n\n\n\nC\n∈\n\n\nC\n\n\n.\n\n\n{\\displaystyle C\\in {\\mathcal {C}}.}\n\n For example, the Lipschitz and Hölder continuous functions of exponent α below are defined by the set of control functions \n\n\n\n\n\n\n\nC\n\n\n\n\nL\ni\np\ns\nc\nh\ni\nt\nz\n\n\n\n=\n{\nC\n:\nC\n(\nδ\n)\n=\nK\n\n|\n\nδ\n\n|\n\n,\n \nK\n>\n0\n}\n\n\n{\\displaystyle {\\mathcal {C}}_{\\mathrm {Lipschitz} }=\\{C:C(\\delta )=K|\\delta |,\\ K>0\\}}\n\n \nrespectively  \n\n\n\n\n\n\n\nC\n\n\n\n\nHölder\n\n−\nα\n\n\n=\n{\nC\n:\nC\n(\nδ\n)\n=\nK\n\n|\n\nδ\n\n\n|\n\n\nα\n\n\n,\n \nK\n>\n0\n}\n.\n\n\n{\\displaystyle {\\mathcal {C}}_{{\\text{Hölder}}-\\alpha }=\\{C:C(\\delta )=K|\\delta |^{\\alpha },\\ K>0\\}.}\n\n\n\nContinuity can also be defined in terms of oscillation: a function f is continuous at a point \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n if and only if its oscillation at that point is zero;[10] in symbols, \n\n\n\n\nω\n\nf\n\n\n(\n\nx\n\n0\n\n\n)\n=\n0.\n\n\n{\\displaystyle \\omega _{f}(x_{0})=0.}\n\n A benefit of this definition is that it quantifies discontinuity: the oscillation gives how much the function is discontinuous at a point.\n\nThis definition is helpful in descriptive set theory to study the set of discontinuities and continuous points – the continuous points are the intersection of the sets where the oscillation is less than \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n (hence a \n\n\n\n\nG\n\nδ\n\n\n\n\n{\\displaystyle G_{\\delta }}\n\n set) – and gives a rapid proof of one direction of the Lebesgue integrability condition.[11]\n\nThe oscillation is equivalent to the \n\n\n\nε\n−\nδ\n\n\n{\\displaystyle \\varepsilon -\\delta }\n\n definition by a simple re-arrangement and by using a limit (lim sup, lim inf) to define oscillation: if (at a given point) for a given \n\n\n\n\nε\n\n0\n\n\n\n\n{\\displaystyle \\varepsilon _{0}}\n\n there is no \n\n\n\nδ\n\n\n{\\displaystyle \\delta }\n\n that satisfies the \n\n\n\nε\n−\nδ\n\n\n{\\displaystyle \\varepsilon -\\delta }\n\n definition, then the oscillation is at least \n\n\n\n\nε\n\n0\n\n\n,\n\n\n{\\displaystyle \\varepsilon _{0},}\n\n and conversely if for every \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n there is a desired \n\n\n\nδ\n,\n\n\n{\\displaystyle \\delta ,}\n\n the oscillation is 0. The oscillation definition can be naturally generalized to maps from a topological space to a metric space.\n\nCauchy defined the continuity of a function in the following intuitive terms: an infinitesimal change in the independent variable corresponds to an infinitesimal change of the dependent variable (see Cours d'analyse, page 34). Non-standard analysis is a way of making this mathematically rigorous. The real line is augmented by adding infinite and infinitesimal numbers to form the hyperreal numbers. In nonstandard analysis, continuity can be defined as follows.\n\n(see microcontinuity).  In other words, an infinitesimal increment of the independent variable always produces an infinitesimal change of the dependent variable, giving a modern expression to Augustin-Louis Cauchy's definition of continuity.\n\nChecking the continuity of a given function can be simplified by checking one of the above defining properties for the building blocks of the given function. It is straightforward to show that the sum of two functions, continuous on some domain, is also continuous on this domain. Given\n\n\n\n\nf\n,\ng\n:\nD\n→\n\nR\n\n,\n\n\n{\\displaystyle f,g\\colon D\\to \\mathbb {R} ,}\n\n\nthen the sum of continuous functions\n\n\n\n\ns\n=\nf\n+\ng\n\n\n{\\displaystyle s=f+g}\n\n \n(defined by \n\n\n\ns\n(\nx\n)\n=\nf\n(\nx\n)\n+\ng\n(\nx\n)\n\n\n{\\displaystyle s(x)=f(x)+g(x)}\n\n for all \n\n\n\nx\n∈\nD\n\n\n{\\displaystyle x\\in D}\n\n) is continuous in \n\n\n\nD\n.\n\n\n{\\displaystyle D.}\n\n\n\nThe same holds for the product of continuous functions,\n\n\n\n\np\n=\nf\n⋅\ng\n\n\n{\\displaystyle p=f\\cdot g}\n\n\n(defined by \n\n\n\np\n(\nx\n)\n=\nf\n(\nx\n)\n⋅\ng\n(\nx\n)\n\n\n{\\displaystyle p(x)=f(x)\\cdot g(x)}\n\n for all \n\n\n\nx\n∈\nD\n\n\n{\\displaystyle x\\in D}\n\n)\nis continuous in \n\n\n\nD\n.\n\n\n{\\displaystyle D.}\n\n\n\nCombining the above preservations of continuity and the continuity of constant functions and of the identity function \n\n\n\nI\n(\nx\n)\n=\nx\n\n\n{\\displaystyle I(x)=x}\n\n on \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n, one arrives at the continuity of all polynomial functions on \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n, such as\n\n\n\n\nf\n(\nx\n)\n=\n\nx\n\n3\n\n\n+\n\nx\n\n2\n\n\n−\n5\nx\n+\n3\n\n\n{\\displaystyle f(x)=x^{3}+x^{2}-5x+3}\n\n\n(pictured on the right).\n\nIn the same way, it can be shown that the reciprocal of a continuous function\n\n\n\n\nr\n=\n1\n\n/\n\nf\n\n\n{\\displaystyle r=1/f}\n\n\n(defined by \n\n\n\nr\n(\nx\n)\n=\n1\n\n/\n\nf\n(\nx\n)\n\n\n{\\displaystyle r(x)=1/f(x)}\n\n for all \n\n\n\nx\n∈\nD\n\n\n{\\displaystyle x\\in D}\n\n such that \n\n\n\nf\n(\nx\n)\n≠\n0\n\n\n{\\displaystyle f(x)\\neq 0}\n\n)\nis continuous in \n\n\n\nD\n∖\n{\nx\n:\nf\n(\nx\n)\n=\n0\n}\n.\n\n\n{\\displaystyle D\\setminus \\{x:f(x)=0\\}.}\n\n\n\nThis implies that, excluding the roots of \n\n\n\ng\n,\n\n\n{\\displaystyle g,}\n\n the quotient of continuous functions\n\n\n\n\nq\n=\nf\n\n/\n\ng\n\n\n{\\displaystyle q=f/g}\n\n\n(defined by \n\n\n\nq\n(\nx\n)\n=\nf\n(\nx\n)\n\n/\n\ng\n(\nx\n)\n\n\n{\\displaystyle q(x)=f(x)/g(x)}\n\n for all \n\n\n\nx\n∈\nD\n\n\n{\\displaystyle x\\in D}\n\n, such that \n\n\n\ng\n(\nx\n)\n≠\n0\n\n\n{\\displaystyle g(x)\\neq 0}\n\n)\nis also continuous on \n\n\n\nD\n∖\n{\nx\n:\ng\n(\nx\n)\n=\n0\n}\n\n\n{\\displaystyle D\\setminus \\{x:g(x)=0\\}}\n\n.\n\nFor example, the function (pictured)\n\n\n\n\ny\n(\nx\n)\n=\n\n\n\n2\nx\n−\n1\n\n\nx\n+\n2\n\n\n\n\n\n{\\displaystyle y(x)={\\frac {2x-1}{x+2}}}\n\n\nis defined for all real numbers \n\n\n\nx\n≠\n−\n2\n\n\n{\\displaystyle x\\neq -2}\n\n and is continuous at every such point. Thus, it is a continuous function. The question of continuity at \n\n\n\nx\n=\n−\n2\n\n\n{\\displaystyle x=-2}\n\n does not arise since \n\n\n\nx\n=\n−\n2\n\n\n{\\displaystyle x=-2}\n\n is not in the domain of \n\n\n\ny\n.\n\n\n{\\displaystyle y.}\n\n There is no continuous function \n\n\n\nF\n:\n\nR\n\n→\n\nR\n\n\n\n{\\displaystyle F:\\mathbb {R} \\to \\mathbb {R} }\n\n that agrees with \n\n\n\ny\n(\nx\n)\n\n\n{\\displaystyle y(x)}\n\n for all \n\n\n\nx\n≠\n−\n2.\n\n\n{\\displaystyle x\\neq -2.}\n\n\n\nSince the function sine is continuous on all reals, the sinc function \n\n\n\nG\n(\nx\n)\n=\nsin\n⁡\n(\nx\n)\n\n/\n\nx\n,\n\n\n{\\displaystyle G(x)=\\sin(x)/x,}\n\n is defined and continuous for all real \n\n\n\nx\n≠\n0.\n\n\n{\\displaystyle x\\neq 0.}\n\n However, unlike the previous example, G can be extended to a continuous function on all real numbers, by defining the value \n\n\n\nG\n(\n0\n)\n\n\n{\\displaystyle G(0)}\n\n to be 1, which is the limit of \n\n\n\nG\n(\nx\n)\n,\n\n\n{\\displaystyle G(x),}\n\n when x approaches 0, i.e.,\n\n\n\n\nG\n(\n0\n)\n=\n\nlim\n\nx\n→\n0\n\n\n\n\n\nsin\n⁡\nx\n\nx\n\n\n=\n1.\n\n\n{\\displaystyle G(0)=\\lim _{x\\to 0}{\\frac {\\sin x}{x}}=1.}\n\n\n\nThus, by setting\n\nthe sinc-function becomes a continuous function on all real numbers. The term removable singularity is used in such cases when (re)defining values of a function to coincide with the appropriate limits make a function continuous at specific points.\n\nA more involved construction of continuous functions is the function composition. Given two continuous functions\n\n\n\n\ng\n:\n\nD\n\ng\n\n\n⊆\n\nR\n\n→\n\nR\n\ng\n\n\n⊆\n\nR\n\n\n\n and \n\n\nf\n:\n\nD\n\nf\n\n\n⊆\n\nR\n\n→\n\nR\n\nf\n\n\n⊆\n\nD\n\ng\n\n\n,\n\n\n{\\displaystyle g:D_{g}\\subseteq \\mathbb {R} \\to R_{g}\\subseteq \\mathbb {R} \\quad {\\text{ and }}\\quad f:D_{f}\\subseteq \\mathbb {R} \\to R_{f}\\subseteq D_{g},}\n\n \ntheir composition, denoted as\n\n\n\n\nc\n=\ng\n∘\nf\n:\n\nD\n\nf\n\n\n→\n\nR\n\n,\n\n\n{\\displaystyle c=g\\circ f:D_{f}\\to \\mathbb {R} ,}\n\n and defined by \n\n\n\nc\n(\nx\n)\n=\ng\n(\nf\n(\nx\n)\n)\n,\n\n\n{\\displaystyle c(x)=g(f(x)),}\n\n is continuous.\n\nThis construction allows stating, for example, that\n\n\n\n\n\ne\n\nsin\n⁡\n(\nln\n⁡\nx\n)\n\n\n\n\n{\\displaystyle e^{\\sin(\\ln x)}}\n\n \nis continuous for all \n\n\n\nx\n>\n0.\n\n\n{\\displaystyle x>0.}\n\n\n\nAn example of a discontinuous function is the Heaviside step function \n\n\n\nH\n\n\n{\\displaystyle H}\n\n, defined by\n\n\n\n\nH\n(\nx\n)\n=\n\n\n{\n\n\n\n1\n\n\n\n if \n\nx\n≥\n0\n\n\n\n\n0\n\n\n\n if \n\nx\n<\n0\n\n\n\n\n\n\n\n\n{\\displaystyle H(x)={\\begin{cases}1&{\\text{ if }}x\\geq 0\\\\0&{\\text{ if }}x<0\\end{cases}}}\n\n\n\nPick for instance \n\n\n\nε\n=\n1\n\n/\n\n2\n\n\n{\\displaystyle \\varepsilon =1/2}\n\n. Then there is no \n\n\n\nδ\n\n\n{\\displaystyle \\delta }\n\n-neighborhood around \n\n\n\nx\n=\n0\n\n\n{\\displaystyle x=0}\n\n, i.e. no open interval \n\n\n\n(\n−\nδ\n,\n\nδ\n)\n\n\n{\\displaystyle (-\\delta ,\\;\\delta )}\n\n with \n\n\n\nδ\n>\n0\n,\n\n\n{\\displaystyle \\delta >0,}\n\n that will force all the \n\n\n\nH\n(\nx\n)\n\n\n{\\displaystyle H(x)}\n\n values to be within the \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n-neighborhood of \n\n\n\nH\n(\n0\n)\n\n\n{\\displaystyle H(0)}\n\n, i.e. within \n\n\n\n(\n1\n\n/\n\n2\n,\n\n3\n\n/\n\n2\n)\n\n\n{\\displaystyle (1/2,\\;3/2)}\n\n. Intuitively, we can think of this type of discontinuity as a sudden jump in function values.\n\nSimilarly, the signum or sign function\n\n\n\n\nsgn\n⁡\n(\nx\n)\n=\n\n\n{\n\n\n\n\n\n \n1\n\n\n\n if \n\nx\n>\n0\n\n\n\n\n\n\n \n0\n\n\n\n if \n\nx\n=\n0\n\n\n\n\n−\n1\n\n\n\n if \n\nx\n<\n0\n\n\n\n\n\n\n\n\n{\\displaystyle \\operatorname {sgn}(x)={\\begin{cases}\\;\\;\\ 1&{\\text{ if }}x>0\\\\\\;\\;\\ 0&{\\text{ if }}x=0\\\\-1&{\\text{ if }}x<0\\end{cases}}}\n\n\nis discontinuous at \n\n\n\nx\n=\n0\n\n\n{\\displaystyle x=0}\n\n but continuous everywhere else. Yet another example: the function\n\n\n\n\nf\n(\nx\n)\n=\n\n\n{\n\n\n\nsin\n⁡\n\n(\n\nx\n\n−\n2\n\n\n)\n\n\n\n\n if \n\nx\n≠\n0\n\n\n\n\n0\n\n\n\n if \n\nx\n=\n0\n\n\n\n\n\n\n\n\n{\\displaystyle f(x)={\\begin{cases}\\sin \\left(x^{-2}\\right)&{\\text{ if }}x\\neq 0\\\\0&{\\text{ if }}x=0\\end{cases}}}\n\n\nis continuous everywhere apart from \n\n\n\nx\n=\n0\n\n\n{\\displaystyle x=0}\n\n.\n\nBesides plausible continuities and discontinuities like above, there are also functions with a behavior, often coined pathological, for example, Thomae's function,\n\n\n\n\nf\n(\nx\n)\n=\n\n\n{\n\n\n\n1\n\n\n\n if \n\nx\n=\n0\n\n\n\n\n\n\n1\nq\n\n\n\n\n\n if \n\nx\n=\n\n\np\nq\n\n\n\n(in lowest terms) is a rational number\n\n\n\n\n\n0\n\n\n\n if \n\nx\n\n is irrational\n\n.\n\n\n\n\n\n\n\n\n{\\displaystyle f(x)={\\begin{cases}1&{\\text{ if }}x=0\\\\{\\frac {1}{q}}&{\\text{ if }}x={\\frac {p}{q}}{\\text{(in lowest terms) is a rational number}}\\\\0&{\\text{ if }}x{\\text{ is irrational}}.\\end{cases}}}\n\n\nis continuous at all irrational numbers and discontinuous at all rational numbers. In a similar vein, Dirichlet's function, the indicator function for the set of rational numbers,\n\n\n\n\nD\n(\nx\n)\n=\n\n\n{\n\n\n\n0\n\n\n\n if \n\nx\n\n is irrational \n\n(\n∈\n\nR\n\n∖\n\nQ\n\n)\n\n\n\n\n1\n\n\n\n if \n\nx\n\n is rational \n\n(\n∈\n\nQ\n\n)\n\n\n\n\n\n\n\n\n{\\displaystyle D(x)={\\begin{cases}0&{\\text{ if }}x{\\text{  is irrational }}(\\in \\mathbb {R} \\setminus \\mathbb {Q} )\\\\1&{\\text{ if }}x{\\text{ is rational }}(\\in \\mathbb {Q} )\\end{cases}}}\n\n\nis nowhere continuous.\n\nLet \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n be a function that is continuous at a point \n\n\n\n\nx\n\n0\n\n\n,\n\n\n{\\displaystyle x_{0},}\n\n and \n\n\n\n\ny\n\n0\n\n\n\n\n{\\displaystyle y_{0}}\n\n be a value such \n\n\n\nf\n\n(\n\nx\n\n0\n\n\n)\n\n≠\n\ny\n\n0\n\n\n.\n\n\n{\\displaystyle f\\left(x_{0}\\right)\\neq y_{0}.}\n\n Then \n\n\n\nf\n(\nx\n)\n≠\n\ny\n\n0\n\n\n\n\n{\\displaystyle f(x)\\neq y_{0}}\n\n throughout some neighbourhood of \n\n\n\n\nx\n\n0\n\n\n.\n\n\n{\\displaystyle x_{0}.}\n\n[13]\n\nProof: By the definition of continuity, take \n\n\n\nε\n=\n\n\n\n\n|\n\n\ny\n\n0\n\n\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n\n2\n\n\n>\n0\n\n\n{\\displaystyle \\varepsilon ={\\frac {|y_{0}-f(x_{0})|}{2}}>0}\n\n , then there exists \n\n\n\nδ\n>\n0\n\n\n{\\displaystyle \\delta >0}\n\n such that \n\n\n\n\n\n|\n\nf\n(\nx\n)\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n<\n\n\n\n|\n\n\ny\n\n0\n\n\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n2\n\n\n\n\n whenever \n\n\n\n|\n\nx\n−\n\nx\n\n0\n\n\n\n|\n\n<\nδ\n\n\n{\\displaystyle \\left|f(x)-f(x_{0})\\right|<{\\frac {\\left|y_{0}-f(x_{0})\\right|}{2}}\\quad {\\text{ whenever }}\\quad |x-x_{0}|<\\delta }\n\n\nSuppose there is a point in the neighbourhood \n\n\n\n\n|\n\nx\n−\n\nx\n\n0\n\n\n\n|\n\n<\nδ\n\n\n{\\displaystyle |x-x_{0}|<\\delta }\n\n for which \n\n\n\nf\n(\nx\n)\n=\n\ny\n\n0\n\n\n;\n\n\n{\\displaystyle f(x)=y_{0};}\n\n then we have the contradiction\n\n\n\n\n\n|\n\nf\n(\n\nx\n\n0\n\n\n)\n−\n\ny\n\n0\n\n\n\n|\n\n<\n\n\n\n|\n\nf\n(\n\nx\n\n0\n\n\n)\n−\n\ny\n\n0\n\n\n\n|\n\n2\n\n\n.\n\n\n{\\displaystyle \\left|f(x_{0})-y_{0}\\right|<{\\frac {\\left|f(x_{0})-y_{0}\\right|}{2}}.}\n\n\n\nThe intermediate value theorem is an existence theorem, based on the real number property of completeness, and states:\n\nFor example, if a child grows from 1 m to 1.5 m between the ages of two and six years, then, at some time between two and six years of age, the child's height must have been 1.25 m.\n\nAs a consequence, if f is continuous on \n\n\n\n[\na\n,\nb\n]\n\n\n{\\displaystyle [a,b]}\n\n and \n\n\n\nf\n(\na\n)\n\n\n{\\displaystyle f(a)}\n\n and \n\n\n\nf\n(\nb\n)\n\n\n{\\displaystyle f(b)}\n\n differ in sign, then, at some point \n\n\n\nc\n∈\n[\na\n,\nb\n]\n,\n\n\n{\\displaystyle c\\in [a,b],}\n\n \n\n\n\nf\n(\nc\n)\n\n\n{\\displaystyle f(c)}\n\n must equal zero.\n\nThe extreme value theorem states that if a function f is defined on a closed interval \n\n\n\n[\na\n,\nb\n]\n\n\n{\\displaystyle [a,b]}\n\n (or any closed and bounded set) and is continuous there, then the function attains its maximum, i.e. there exists \n\n\n\nc\n∈\n[\na\n,\nb\n]\n\n\n{\\displaystyle c\\in [a,b]}\n\n with \n\n\n\nf\n(\nc\n)\n≥\nf\n(\nx\n)\n\n\n{\\displaystyle f(c)\\geq f(x)}\n\n for all \n\n\n\nx\n∈\n[\na\n,\nb\n]\n.\n\n\n{\\displaystyle x\\in [a,b].}\n\n The same is true of the minimum of f. These statements are not, in general, true if the function is defined on an open interval \n\n\n\n(\na\n,\nb\n)\n\n\n{\\displaystyle (a,b)}\n\n (or any set that is not both closed and bounded), as, for example, the continuous function \n\n\n\nf\n(\nx\n)\n=\n\n\n1\nx\n\n\n,\n\n\n{\\displaystyle f(x)={\\frac {1}{x}},}\n\n defined on the open interval (0,1), does not attain a maximum, being unbounded above.\n\nEvery differentiable function\n\n\n\n\nf\n:\n(\na\n,\nb\n)\n→\n\nR\n\n\n\n{\\displaystyle f:(a,b)\\to \\mathbb {R} }\n\n\nis continuous, as can be shown. The converse  does not hold: for example, the absolute value function\n\nis everywhere continuous. However, it is not differentiable at \n\n\n\nx\n=\n0\n\n\n{\\displaystyle x=0}\n\n (but is so everywhere else). Weierstrass's function is also everywhere continuous but nowhere differentiable.\n\nThe derivative f′(x) of a differentiable function f(x) need not be continuous. If f′(x) is continuous, f(x) is said to be continuously differentiable. The set of such functions is denoted \n\n\n\n\nC\n\n1\n\n\n(\n(\na\n,\nb\n)\n)\n.\n\n\n{\\displaystyle C^{1}((a,b)).}\n\n More generally, the set of functions\n\n\n\n\nf\n:\nΩ\n→\n\nR\n\n\n\n{\\displaystyle f:\\Omega \\to \\mathbb {R} }\n\n\n(from an open interval (or open subset of \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n) \n\n\n\nΩ\n\n\n{\\displaystyle \\Omega }\n\n to the reals) such that f is \n\n\n\nn\n\n\n{\\displaystyle n}\n\n times differentiable and such that the \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-th derivative of f is continuous is denoted \n\n\n\n\nC\n\nn\n\n\n(\nΩ\n)\n.\n\n\n{\\displaystyle C^{n}(\\Omega ).}\n\n See differentiability class. In the field of computer graphics, properties related (but not identical) to \n\n\n\n\nC\n\n0\n\n\n,\n\nC\n\n1\n\n\n,\n\nC\n\n2\n\n\n\n\n{\\displaystyle C^{0},C^{1},C^{2}}\n\n are sometimes called \n\n\n\n\nG\n\n0\n\n\n\n\n{\\displaystyle G^{0}}\n\n (continuity of position), \n\n\n\n\nG\n\n1\n\n\n\n\n{\\displaystyle G^{1}}\n\n (continuity of tangency), and \n\n\n\n\nG\n\n2\n\n\n\n\n{\\displaystyle G^{2}}\n\n (continuity of curvature); see Smoothness of curves and surfaces.\n\nEvery continuous function\n\n\n\n\nf\n:\n[\na\n,\nb\n]\n→\n\nR\n\n\n\n{\\displaystyle f:[a,b]\\to \\mathbb {R} }\n\n\nis integrable (for example in the sense of the Riemann integral). The converse does not hold, as the (integrable but discontinuous) sign function shows.\n\nGiven a sequence\n\n\n\n\n\nf\n\n1\n\n\n,\n\nf\n\n2\n\n\n,\n…\n:\nI\n→\n\nR\n\n\n\n{\\displaystyle f_{1},f_{2},\\dotsc :I\\to \\mathbb {R} }\n\n\nof functions such that the limit\n\n\n\n\nf\n(\nx\n)\n:=\n\nlim\n\nn\n→\n∞\n\n\n\nf\n\nn\n\n\n(\nx\n)\n\n\n{\\displaystyle f(x):=\\lim _{n\\to \\infty }f_{n}(x)}\n\n\nexists for all \n\n\n\nx\n∈\nD\n,\n\n\n{\\displaystyle x\\in D,}\n\n, the resulting function \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is referred to as the pointwise limit of the sequence of functions  \n\n\n\n\n\n(\n\nf\n\nn\n\n\n)\n\n\nn\n∈\nN\n\n\n.\n\n\n{\\displaystyle \\left(f_{n}\\right)_{n\\in N}.}\n\n The pointwise limit function need not be continuous, even if all functions \n\n\n\n\nf\n\nn\n\n\n\n\n{\\displaystyle f_{n}}\n\n are continuous, as the animation at the right shows. However, f is continuous if all functions \n\n\n\n\nf\n\nn\n\n\n\n\n{\\displaystyle f_{n}}\n\n are continuous and the sequence converges uniformly, by the uniform convergence theorem. This theorem can be used to show that the exponential functions, logarithms, square root function, and trigonometric functions are continuous.\n\nDiscontinuous functions may be discontinuous in a restricted way, giving rise to the concept of directional continuity (or right and left continuous functions) and semi-continuity. Roughly speaking, a function is right-continuous if no jump occurs when the limit point is approached from the right. Formally, f is said to be right-continuous at the point c if the following holds: For any number \n\n\n\nε\n>\n0\n\n\n{\\displaystyle \\varepsilon >0}\n\n however small, there exists some number \n\n\n\nδ\n>\n0\n\n\n{\\displaystyle \\delta >0}\n\n such that for all x in the domain with \n\n\n\nc\n<\nx\n<\nc\n+\nδ\n,\n\n\n{\\displaystyle c<x<c+\\delta ,}\n\n the value of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n will satisfy\n\n\n\n\n\n|\n\nf\n(\nx\n)\n−\nf\n(\nc\n)\n\n|\n\n<\nε\n.\n\n\n{\\displaystyle |f(x)-f(c)|<\\varepsilon .}\n\n\n\nThis is the same condition as continuous functions, except it is required to hold for x strictly larger than c only. Requiring it instead for all x with \n\n\n\nc\n−\nδ\n<\nx\n<\nc\n\n\n{\\displaystyle c-\\delta <x<c}\n\n yields the notion of left-continuous functions. A function is continuous if and only if it is both right-continuous and left-continuous.\n\nA function f is lower semi-continuous if, roughly, any jumps that might occur only go down, but not up. That is, for any \n\n\n\nε\n>\n0\n,\n\n\n{\\displaystyle \\varepsilon >0,}\n\n there exists some number \n\n\n\nδ\n>\n0\n\n\n{\\displaystyle \\delta >0}\n\n such that for all x in the domain with \n\n\n\n\n|\n\nx\n−\nc\n\n|\n\n<\nδ\n,\n\n\n{\\displaystyle |x-c|<\\delta ,}\n\n the value of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n satisfies\n\n\n\n\nf\n(\nx\n)\n≥\nf\n(\nc\n)\n−\nϵ\n.\n\n\n{\\displaystyle f(x)\\geq f(c)-\\epsilon .}\n\n\nThe reverse condition is upper semi-continuity.\n\n\n\nThe concept of continuous real-valued functions can be generalized to functions between metric spaces. A metric space is a set \n\n\n\nX\n\n\n{\\displaystyle X}\n\n equipped with a function (called metric) \n\n\n\n\nd\n\nX\n\n\n,\n\n\n{\\displaystyle d_{X},}\n\n that can be thought of as a measurement of the distance of any two elements in X. Formally, the metric is a function\n\n\n\n\n\nd\n\nX\n\n\n:\nX\n×\nX\n→\n\nR\n\n\n\n{\\displaystyle d_{X}:X\\times X\\to \\mathbb {R} }\n\n\nthat satisfies a number of requirements, notably the triangle inequality. Given two metric spaces \n\n\n\n\n(\n\nX\n,\n\nd\n\nX\n\n\n\n)\n\n\n\n{\\displaystyle \\left(X,d_{X}\\right)}\n\n and \n\n\n\n\n(\n\nY\n,\n\nd\n\nY\n\n\n\n)\n\n\n\n{\\displaystyle \\left(Y,d_{Y}\\right)}\n\n and a function\n\n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n\nthen \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is continuous at the point \n\n\n\nc\n∈\nX\n\n\n{\\displaystyle c\\in X}\n\n (with respect to the given metrics) if for any positive real number \n\n\n\nε\n>\n0\n,\n\n\n{\\displaystyle \\varepsilon >0,}\n\n there exists a positive real number \n\n\n\nδ\n>\n0\n\n\n{\\displaystyle \\delta >0}\n\n such that all \n\n\n\nx\n∈\nX\n\n\n{\\displaystyle x\\in X}\n\n satisfying \n\n\n\n\nd\n\nX\n\n\n(\nx\n,\nc\n)\n<\nδ\n\n\n{\\displaystyle d_{X}(x,c)<\\delta }\n\n will also satisfy \n\n\n\n\nd\n\nY\n\n\n(\nf\n(\nx\n)\n,\nf\n(\nc\n)\n)\n<\nε\n.\n\n\n{\\displaystyle d_{Y}(f(x),f(c))<\\varepsilon .}\n\n As in the case of real functions above, this is equivalent to the condition that for every sequence \n\n\n\n\n(\n\nx\n\nn\n\n\n)\n\n\n\n{\\displaystyle \\left(x_{n}\\right)}\n\n in \n\n\n\nX\n\n\n{\\displaystyle X}\n\n with limit \n\n\n\nlim\n\nx\n\nn\n\n\n=\nc\n,\n\n\n{\\displaystyle \\lim x_{n}=c,}\n\n we have \n\n\n\nlim\nf\n\n(\n\nx\n\nn\n\n\n)\n\n=\nf\n(\nc\n)\n.\n\n\n{\\displaystyle \\lim f\\left(x_{n}\\right)=f(c).}\n\n The latter condition can be weakened as follows: \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is continuous at the point \n\n\n\nc\n\n\n{\\displaystyle c}\n\n if and only if for every convergent sequence \n\n\n\n\n(\n\nx\n\nn\n\n\n)\n\n\n\n{\\displaystyle \\left(x_{n}\\right)}\n\n in \n\n\n\nX\n\n\n{\\displaystyle X}\n\n with limit \n\n\n\nc\n\n\n{\\displaystyle c}\n\n, the sequence \n\n\n\n\n(\n\nf\n\n(\n\nx\n\nn\n\n\n)\n\n\n)\n\n\n\n{\\displaystyle \\left(f\\left(x_{n}\\right)\\right)}\n\n is a Cauchy sequence, and \n\n\n\nc\n\n\n{\\displaystyle c}\n\n is in the domain of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n.\n\nThe set of points at which a function between metric spaces is continuous is a \n\n\n\n\nG\n\nδ\n\n\n\n\n{\\displaystyle G_{\\delta }}\n\n set – this follows from the \n\n\n\nε\n−\nδ\n\n\n{\\displaystyle \\varepsilon -\\delta }\n\n definition of continuity.\n\nThis notion of continuity is applied, for example, in functional analysis. A key statement in this area says that a linear operator\n\n\n\n\nT\n:\nV\n→\nW\n\n\n{\\displaystyle T:V\\to W}\n\n\nbetween normed vector spaces \n\n\n\nV\n\n\n{\\displaystyle V}\n\n and \n\n\n\nW\n\n\n{\\displaystyle W}\n\n (which are vector spaces equipped with a compatible norm, denoted \n\n\n\n‖\nx\n‖\n\n\n{\\displaystyle \\|x\\|}\n\n) is continuous if and only if it is bounded, that is, there is a constant \n\n\n\nK\n\n\n{\\displaystyle K}\n\n such that\n\n\n\n\n‖\nT\n(\nx\n)\n‖\n≤\nK\n‖\nx\n‖\n\n\n{\\displaystyle \\|T(x)\\|\\leq K\\|x\\|}\n\n\nfor all \n\n\n\nx\n∈\nV\n.\n\n\n{\\displaystyle x\\in V.}\n\n\n\nThe concept of continuity for functions between metric spaces can be strengthened in various ways by limiting the way \n\n\n\nδ\n\n\n{\\displaystyle \\delta }\n\n depends on \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n and c in the definition above.  Intuitively, a function f as above is uniformly continuous if the \n\n\n\nδ\n\n\n{\\displaystyle \\delta }\n\n does\nnot depend on the point c. More precisely, it is required that for every real number \n\n\n\nε\n>\n0\n\n\n{\\displaystyle \\varepsilon >0}\n\n there exists \n\n\n\nδ\n>\n0\n\n\n{\\displaystyle \\delta >0}\n\n such that for every \n\n\n\nc\n,\nb\n∈\nX\n\n\n{\\displaystyle c,b\\in X}\n\n with \n\n\n\n\nd\n\nX\n\n\n(\nb\n,\nc\n)\n<\nδ\n,\n\n\n{\\displaystyle d_{X}(b,c)<\\delta ,}\n\n we have that \n\n\n\n\nd\n\nY\n\n\n(\nf\n(\nb\n)\n,\nf\n(\nc\n)\n)\n<\nε\n.\n\n\n{\\displaystyle d_{Y}(f(b),f(c))<\\varepsilon .}\n\n Thus, any uniformly continuous function is continuous. The converse does not generally hold but holds when the domain space X is compact. Uniformly continuous maps can be defined in the more general situation of uniform spaces.[14]\n\nA function is Hölder continuous with exponent α (a real number) if there is a constant K such that for all \n\n\n\nb\n,\nc\n∈\nX\n,\n\n\n{\\displaystyle b,c\\in X,}\n\n the inequality\n\n\n\n\n\nd\n\nY\n\n\n(\nf\n(\nb\n)\n,\nf\n(\nc\n)\n)\n≤\nK\n⋅\n(\n\nd\n\nX\n\n\n(\nb\n,\nc\n)\n\n)\n\nα\n\n\n\n\n{\\displaystyle d_{Y}(f(b),f(c))\\leq K\\cdot (d_{X}(b,c))^{\\alpha }}\n\n\nholds. Any Hölder continuous function is uniformly continuous. The particular case \n\n\n\nα\n=\n1\n\n\n{\\displaystyle \\alpha =1}\n\n is referred to as Lipschitz continuity. That is, a function is Lipschitz continuous if there is a constant K such that the inequality\n\n\n\n\n\nd\n\nY\n\n\n(\nf\n(\nb\n)\n,\nf\n(\nc\n)\n)\n≤\nK\n⋅\n\nd\n\nX\n\n\n(\nb\n,\nc\n)\n\n\n{\\displaystyle d_{Y}(f(b),f(c))\\leq K\\cdot d_{X}(b,c)}\n\n\nholds for any \n\n\n\nb\n,\nc\n∈\nX\n.\n\n\n{\\displaystyle b,c\\in X.}\n\n[15] The Lipschitz condition occurs, for example, in the Picard–Lindelöf theorem concerning the solutions of ordinary differential equations.\n\nAnother, more abstract, notion of continuity is the continuity of functions between topological spaces in which there generally is no formal notion of distance, as there is in the case of metric spaces. A topological space is a set X together with a topology on X, which is a set of subsets of X satisfying a few requirements with respect to their unions and intersections that generalize the properties of the open balls in metric spaces while still allowing one to talk about the  neighborhoods of a given point. The elements of a topology are called open subsets of X (with respect to the topology).\n\nA function\n\n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n\nbetween two topological spaces X and Y is continuous if for every open set \n\n\n\nV\n⊆\nY\n,\n\n\n{\\displaystyle V\\subseteq Y,}\n\n the inverse image\n\n\n\n\n\nf\n\n−\n1\n\n\n(\nV\n)\n=\n{\nx\n∈\nX\n\n\n|\n\n\nf\n(\nx\n)\n∈\nV\n}\n\n\n{\\displaystyle f^{-1}(V)=\\{x\\in X\\;|\\;f(x)\\in V\\}}\n\n\nis an open subset of X. That is, f is a function between the sets X and Y (not on the elements of the topology \n\n\n\n\nT\n\nX\n\n\n\n\n{\\displaystyle T_{X}}\n\n), but the continuity of f depends on the topologies used on X and Y.\n\nThis is equivalent to the condition that the preimages of the closed sets (which are the complements of the open subsets) in Y are closed in X.\n\nAn extreme example: if a set X is given the discrete topology (in which every subset is open), all functions\n\n\n\n\nf\n:\nX\n→\nT\n\n\n{\\displaystyle f:X\\to T}\n\n\nto any topological space T are continuous. On the other hand, if X is equipped with the indiscrete topology (in which the only open subsets are the empty set and X) and the space T set is at least T0, then the only continuous functions are the constant functions. Conversely, any function whose codomain is indiscrete is continuous.\n\nThe translation in the language of neighborhoods of the \n\n\n\n(\nε\n,\nδ\n)\n\n\n{\\displaystyle (\\varepsilon ,\\delta )}\n\n-definition of continuity leads to the following definition of the continuity at a point:\n\nThis definition is equivalent to the same statement with neighborhoods restricted to open neighborhoods and can be restated in several ways by using preimages rather than images.\n\nAlso, as every set that contains a neighborhood is also a neighborhood, and \n\n\n\n\nf\n\n−\n1\n\n\n(\nV\n)\n\n\n{\\displaystyle f^{-1}(V)}\n\n is the largest subset U of X such that \n\n\n\nf\n(\nU\n)\n⊆\nV\n,\n\n\n{\\displaystyle f(U)\\subseteq V,}\n\n this definition may be simplified into:\n\nAs an open set is a set that is a neighborhood of all its points, a function \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous at every point of X if and only if it is a continuous function.\n\nIf X and Y are metric spaces, it is equivalent to consider the neighborhood system of open balls centered at x and f(x) instead of all neighborhoods. This gives back the above \n\n\n\nε\n−\nδ\n\n\n{\\displaystyle \\varepsilon -\\delta }\n\n definition of continuity in the context of metric spaces.  In general topological spaces, there is no notion of nearness or distance. If, however, the target space is a Hausdorff space, it is still true that f is continuous at a if and only if the limit of f as x approaches a is f(a). At an isolated point, every function is continuous.\n\nGiven \n\n\n\nx\n∈\nX\n,\n\n\n{\\displaystyle x\\in X,}\n\n a map \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous at \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if and only if whenever \n\n\n\n\n\nB\n\n\n\n\n{\\displaystyle {\\mathcal {B}}}\n\n is a filter on \n\n\n\nX\n\n\n{\\displaystyle X}\n\n that converges to \n\n\n\nx\n\n\n{\\displaystyle x}\n\n in \n\n\n\nX\n,\n\n\n{\\displaystyle X,}\n\n which is expressed by writing \n\n\n\n\n\nB\n\n\n→\nx\n,\n\n\n{\\displaystyle {\\mathcal {B}}\\to x,}\n\n then necessarily \n\n\n\nf\n(\n\n\nB\n\n\n)\n→\nf\n(\nx\n)\n\n\n{\\displaystyle f({\\mathcal {B}})\\to f(x)}\n\n in \n\n\n\nY\n.\n\n\n{\\displaystyle Y.}\n\n \nIf \n\n\n\n\n\nN\n\n\n(\nx\n)\n\n\n{\\displaystyle {\\mathcal {N}}(x)}\n\n denotes the neighborhood filter at \n\n\n\nx\n\n\n{\\displaystyle x}\n\n then \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous at \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if and only if \n\n\n\nf\n(\n\n\nN\n\n\n(\nx\n)\n)\n→\nf\n(\nx\n)\n\n\n{\\displaystyle f({\\mathcal {N}}(x))\\to f(x)}\n\n in \n\n\n\nY\n.\n\n\n{\\displaystyle Y.}\n\n[16] Moreover, this happens if and only if the prefilter \n\n\n\nf\n(\n\n\nN\n\n\n(\nx\n)\n)\n\n\n{\\displaystyle f({\\mathcal {N}}(x))}\n\n is a filter base for the neighborhood filter of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n in \n\n\n\nY\n.\n\n\n{\\displaystyle Y.}\n\n[16]\n\nSeveral equivalent definitions for a topological structure exist; thus, several equivalent ways exist to define a continuous function.\n\nIn several contexts, the topology of a space is conveniently specified in terms of limit points. This is often accomplished by specifying when a point is the limit of a sequence. Still, for some spaces that are too large in some sense, one specifies also when a point is the limit of more general sets of points indexed by a directed set, known as nets. A function is (Heine-)continuous only if it takes limits of sequences to limits of sequences. In the former case, preservation of limits is also sufficient; in the latter, a function may preserve all limits of sequences yet still fail to be continuous, and preservation of nets is a necessary and sufficient condition.\n\nIn detail, a function \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is sequentially continuous if whenever a sequence \n\n\n\n\n(\n\nx\n\nn\n\n\n)\n\n\n\n{\\displaystyle \\left(x_{n}\\right)}\n\n in \n\n\n\nX\n\n\n{\\displaystyle X}\n\n converges to a limit \n\n\n\nx\n,\n\n\n{\\displaystyle x,}\n\n the sequence \n\n\n\n\n(\n\nf\n\n(\n\nx\n\nn\n\n\n)\n\n\n)\n\n\n\n{\\displaystyle \\left(f\\left(x_{n}\\right)\\right)}\n\n converges to \n\n\n\nf\n(\nx\n)\n.\n\n\n{\\displaystyle f(x).}\n\n  Thus, sequentially continuous functions \"preserve sequential limits.\"  Every continuous function is sequentially continuous. If \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is a first-countable space and countable choice holds, then the converse also holds: any function preserving sequential limits is continuous. In particular, if \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is a metric space, sequential continuity and continuity are equivalent. For non-first-countable spaces, sequential continuity might be strictly weaker than continuity. (The spaces for which the two properties are equivalent are called sequential spaces.) This motivates the consideration of nets instead of sequences in general topological spaces. Continuous functions preserve the limits of nets, and this property characterizes continuous functions.\n\nFor instance, consider the case of real-valued functions of one real variable:[17]\n\nTheorem — A function \n\n\n\nf\n:\nA\n⊆\n\nR\n\n→\n\nR\n\n\n\n{\\displaystyle f:A\\subseteq \\mathbb {R} \\to \\mathbb {R} }\n\n is continuous at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n if and only if it is sequentially continuous at that point.\n\nProof. Assume that \n\n\n\nf\n:\nA\n⊆\n\nR\n\n→\n\nR\n\n\n\n{\\displaystyle f:A\\subseteq \\mathbb {R} \\to \\mathbb {R} }\n\n is continuous at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n (in the sense of \n\n\n\nϵ\n−\nδ\n\n\n{\\displaystyle \\epsilon -\\delta }\n\n continuity). Let \n\n\n\n\n\n(\n\nx\n\nn\n\n\n)\n\n\nn\n≥\n1\n\n\n\n\n{\\displaystyle \\left(x_{n}\\right)_{n\\geq 1}}\n\n be a sequence converging at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n (such a sequence always exists, for example, \n\n\n\n\nx\n\nn\n\n\n=\nx\n,\n\n for all \n\nn\n\n\n{\\displaystyle x_{n}=x,{\\text{ for all }}n}\n\n); since \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is continuous at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n\n\n\n\n\n∀\nϵ\n>\n0\n\n∃\n\nδ\n\nϵ\n\n\n>\n0\n:\n0\n<\n\n|\n\nx\n−\n\nx\n\n0\n\n\n\n|\n\n<\n\nδ\n\nϵ\n\n\n\n⟹\n\n\n|\n\nf\n(\nx\n)\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n<\nϵ\n.\n\n(\n∗\n)\n\n\n{\\displaystyle \\forall \\epsilon >0\\,\\exists \\delta _{\\epsilon }>0:0<|x-x_{0}|<\\delta _{\\epsilon }\\implies |f(x)-f(x_{0})|<\\epsilon .\\quad (*)}\n\n\nFor any such \n\n\n\n\nδ\n\nϵ\n\n\n\n\n{\\displaystyle \\delta _{\\epsilon }}\n\n we can find a natural number \n\n\n\n\nν\n\nϵ\n\n\n>\n0\n\n\n{\\displaystyle \\nu _{\\epsilon }>0}\n\n such that for all \n\n\n\nn\n>\n\nν\n\nϵ\n\n\n,\n\n\n{\\displaystyle n>\\nu _{\\epsilon },}\n\n\n\n\n\n\n\n|\n\n\nx\n\nn\n\n\n−\n\nx\n\n0\n\n\n\n|\n\n<\n\nδ\n\nϵ\n\n\n,\n\n\n{\\displaystyle |x_{n}-x_{0}|<\\delta _{\\epsilon },}\n\n\nsince \n\n\n\n\n(\n\nx\n\nn\n\n\n)\n\n\n\n{\\displaystyle \\left(x_{n}\\right)}\n\n converges at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n; combining this with \n\n\n\n(\n∗\n)\n\n\n{\\displaystyle (*)}\n\n we obtain\n\n\n\n\n∀\nϵ\n>\n0\n\n∃\n\nν\n\nϵ\n\n\n>\n0\n:\n∀\nn\n>\n\nν\n\nϵ\n\n\n\n\n|\n\nf\n(\n\nx\n\nn\n\n\n)\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n<\nϵ\n.\n\n\n{\\displaystyle \\forall \\epsilon >0\\,\\exists \\nu _{\\epsilon }>0:\\forall n>\\nu _{\\epsilon }\\quad |f(x_{n})-f(x_{0})|<\\epsilon .}\n\n\nAssume on the contrary that \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is sequentially continuous and proceed by contradiction: suppose \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is not continuous at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n\n\n\n\n\n∃\nϵ\n>\n0\n:\n∀\n\nδ\n\nϵ\n\n\n>\n0\n,\n\n∃\n\nx\n\n\nδ\n\nϵ\n\n\n\n\n:\n0\n<\n\n|\n\n\nx\n\n\nδ\n\nϵ\n\n\n\n\n−\n\nx\n\n0\n\n\n\n|\n\n<\n\nδ\n\nϵ\n\n\n\n⟹\n\n\n|\n\nf\n(\n\nx\n\n\nδ\n\nϵ\n\n\n\n\n)\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n>\nϵ\n\n\n{\\displaystyle \\exists \\epsilon >0:\\forall \\delta _{\\epsilon }>0,\\,\\exists x_{\\delta _{\\epsilon }}:0<|x_{\\delta _{\\epsilon }}-x_{0}|<\\delta _{\\epsilon }\\implies |f(x_{\\delta _{\\epsilon }})-f(x_{0})|>\\epsilon }\n\n\nthen we can take \n\n\n\n\nδ\n\nϵ\n\n\n=\n1\n\n/\n\nn\n,\n\n∀\nn\n>\n0\n\n\n{\\displaystyle \\delta _{\\epsilon }=1/n,\\,\\forall n>0}\n\n and call the corresponding point \n\n\n\n\nx\n\n\nδ\n\nϵ\n\n\n\n\n=:\n\nx\n\nn\n\n\n\n\n{\\displaystyle x_{\\delta _{\\epsilon }}=:x_{n}}\n\n: in this way we have defined a sequence \n\n\n\n(\n\nx\n\nn\n\n\n\n)\n\nn\n≥\n1\n\n\n\n\n{\\displaystyle (x_{n})_{n\\geq 1}}\n\n such that\n\n\n\n\n∀\nn\n>\n0\n\n\n|\n\n\nx\n\nn\n\n\n−\n\nx\n\n0\n\n\n\n|\n\n<\n\n\n1\nn\n\n\n,\n\n\n|\n\nf\n(\n\nx\n\nn\n\n\n)\n−\nf\n(\n\nx\n\n0\n\n\n)\n\n|\n\n>\nϵ\n\n\n{\\displaystyle \\forall n>0\\quad |x_{n}-x_{0}|<{\\frac {1}{n}},\\quad |f(x_{n})-f(x_{0})|>\\epsilon }\n\n\nby construction \n\n\n\n\nx\n\nn\n\n\n→\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{n}\\to x_{0}}\n\n but \n\n\n\nf\n(\n\nx\n\nn\n\n\n)\n↛\nf\n(\n\nx\n\n0\n\n\n)\n\n\n{\\displaystyle f(x_{n})\\not \\to f(x_{0})}\n\n, which contradicts the hypothesis of sequential continuity. \n\n\n\n◼\n\n\n{\\displaystyle \\blacksquare }\n\n\n\nIn terms of the interior operator, a function \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n between topological spaces is continuous if and only if for every subset \n\n\n\nB\n⊆\nY\n,\n\n\n{\\displaystyle B\\subseteq Y,}\n\n\n\n\n\n\n\nf\n\n−\n1\n\n\n\n(\n\n\nint\n\nY\n\n\n⁡\nB\n\n)\n\n \n⊆\n \n\nint\n\nX\n\n\n⁡\n\n(\n\n\nf\n\n−\n1\n\n\n(\nB\n)\n\n)\n\n.\n\n\n{\\displaystyle f^{-1}\\left(\\operatorname {int} _{Y}B\\right)~\\subseteq ~\\operatorname {int} _{X}\\left(f^{-1}(B)\\right).}\n\n\n\nIn terms of the closure operator, \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous if and only if for every subset \n\n\n\nA\n⊆\nX\n,\n\n\n{\\displaystyle A\\subseteq X,}\n\n\n\n\n\n\nf\n\n(\n\n\ncl\n\nX\n\n\n⁡\nA\n\n)\n\n \n⊆\n \n\ncl\n\nY\n\n\n⁡\n(\nf\n(\nA\n)\n)\n.\n\n\n{\\displaystyle f\\left(\\operatorname {cl} _{X}A\\right)~\\subseteq ~\\operatorname {cl} _{Y}(f(A)).}\n\n\nThat is to say, given any element \n\n\n\nx\n∈\nX\n\n\n{\\displaystyle x\\in X}\n\n that belongs to the closure of a subset \n\n\n\nA\n⊆\nX\n,\n\n\n{\\displaystyle A\\subseteq X,}\n\n \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n necessarily belongs to the closure of \n\n\n\nf\n(\nA\n)\n\n\n{\\displaystyle f(A)}\n\n in \n\n\n\nY\n.\n\n\n{\\displaystyle Y.}\n\n If we declare that a point \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is close to a subset \n\n\n\nA\n⊆\nX\n\n\n{\\displaystyle A\\subseteq X}\n\n if \n\n\n\nx\n∈\n\ncl\n\nX\n\n\n⁡\nA\n,\n\n\n{\\displaystyle x\\in \\operatorname {cl} _{X}A,}\n\n then this terminology allows for a plain English description of continuity: \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is continuous if and only if for every subset \n\n\n\nA\n⊆\nX\n,\n\n\n{\\displaystyle A\\subseteq X,}\n\n \n\n\n\nf\n\n\n{\\displaystyle f}\n\n maps points that are close to \n\n\n\nA\n\n\n{\\displaystyle A}\n\n to points that are close to \n\n\n\nf\n(\nA\n)\n.\n\n\n{\\displaystyle f(A).}\n\n Similarly, \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is continuous at a fixed given point \n\n\n\nx\n∈\nX\n\n\n{\\displaystyle x\\in X}\n\n if and only if whenever \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is close to a subset \n\n\n\nA\n⊆\nX\n,\n\n\n{\\displaystyle A\\subseteq X,}\n\n then \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is close to \n\n\n\nf\n(\nA\n)\n.\n\n\n{\\displaystyle f(A).}\n\n\n\nInstead of specifying topological spaces by their open subsets, any topology on \n\n\n\nX\n\n\n{\\displaystyle X}\n\n can alternatively be determined by a closure operator or by an interior operator.  \nSpecifically, the map that sends a subset \n\n\n\nA\n\n\n{\\displaystyle A}\n\n of a topological space \n\n\n\nX\n\n\n{\\displaystyle X}\n\n to its topological closure \n\n\n\n\ncl\n\nX\n\n\n⁡\nA\n\n\n{\\displaystyle \\operatorname {cl} _{X}A}\n\n satisfies the Kuratowski closure axioms. Conversely, for any closure operator \n\n\n\nA\n↦\ncl\n⁡\nA\n\n\n{\\displaystyle A\\mapsto \\operatorname {cl} A}\n\n there exists a unique topology \n\n\n\nτ\n\n\n{\\displaystyle \\tau }\n\n on \n\n\n\nX\n\n\n{\\displaystyle X}\n\n (specifically, \n\n\n\nτ\n:=\n{\nX\n∖\ncl\n⁡\nA\n:\nA\n⊆\nX\n}\n\n\n{\\displaystyle \\tau :=\\{X\\setminus \\operatorname {cl} A:A\\subseteq X\\}}\n\n) such that for every subset \n\n\n\nA\n⊆\nX\n,\n\n\n{\\displaystyle A\\subseteq X,}\n\n \n\n\n\ncl\n⁡\nA\n\n\n{\\displaystyle \\operatorname {cl} A}\n\n is equal to the topological closure \n\n\n\n\ncl\n\n(\nX\n,\nτ\n)\n\n\n⁡\nA\n\n\n{\\displaystyle \\operatorname {cl} _{(X,\\tau )}A}\n\n of \n\n\n\nA\n\n\n{\\displaystyle A}\n\n in \n\n\n\n(\nX\n,\nτ\n)\n.\n\n\n{\\displaystyle (X,\\tau ).}\n\n If the sets \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are each associated with closure operators (both denoted by \n\n\n\ncl\n\n\n{\\displaystyle \\operatorname {cl} }\n\n) then a map \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous if and only if \n\n\n\nf\n(\ncl\n⁡\nA\n)\n⊆\ncl\n⁡\n(\nf\n(\nA\n)\n)\n\n\n{\\displaystyle f(\\operatorname {cl} A)\\subseteq \\operatorname {cl} (f(A))}\n\n for every subset \n\n\n\nA\n⊆\nX\n.\n\n\n{\\displaystyle A\\subseteq X.}\n\n\n\nSimilarly, the map that sends a subset \n\n\n\nA\n\n\n{\\displaystyle A}\n\n of \n\n\n\nX\n\n\n{\\displaystyle X}\n\n to its topological interior \n\n\n\n\nint\n\nX\n\n\n⁡\nA\n\n\n{\\displaystyle \\operatorname {int} _{X}A}\n\n defines an interior operator. Conversely, any interior operator \n\n\n\nA\n↦\nint\n⁡\nA\n\n\n{\\displaystyle A\\mapsto \\operatorname {int} A}\n\n induces a unique topology \n\n\n\nτ\n\n\n{\\displaystyle \\tau }\n\n on \n\n\n\nX\n\n\n{\\displaystyle X}\n\n (specifically, \n\n\n\nτ\n:=\n{\nint\n⁡\nA\n:\nA\n⊆\nX\n}\n\n\n{\\displaystyle \\tau :=\\{\\operatorname {int} A:A\\subseteq X\\}}\n\n) such that for every \n\n\n\nA\n⊆\nX\n,\n\n\n{\\displaystyle A\\subseteq X,}\n\n \n\n\n\nint\n⁡\nA\n\n\n{\\displaystyle \\operatorname {int} A}\n\n is equal to the topological interior \n\n\n\n\nint\n\n(\nX\n,\nτ\n)\n\n\n⁡\nA\n\n\n{\\displaystyle \\operatorname {int} _{(X,\\tau )}A}\n\n of \n\n\n\nA\n\n\n{\\displaystyle A}\n\n in \n\n\n\n(\nX\n,\nτ\n)\n.\n\n\n{\\displaystyle (X,\\tau ).}\n\n If the sets \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are each associated with interior operators (both denoted by \n\n\n\nint\n\n\n{\\displaystyle \\operatorname {int} }\n\n) then a map \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous if and only if \n\n\n\n\nf\n\n−\n1\n\n\n(\nint\n⁡\nB\n)\n⊆\nint\n⁡\n\n(\n\n\nf\n\n−\n1\n\n\n(\nB\n)\n\n)\n\n\n\n{\\displaystyle f^{-1}(\\operatorname {int} B)\\subseteq \\operatorname {int} \\left(f^{-1}(B)\\right)}\n\n for every subset \n\n\n\nB\n⊆\nY\n.\n\n\n{\\displaystyle B\\subseteq Y.}\n\n[18]\n\nContinuity can also be characterized in terms of filters. A function \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous if and only if whenever a filter \n\n\n\n\n\nB\n\n\n\n\n{\\displaystyle {\\mathcal {B}}}\n\n on \n\n\n\nX\n\n\n{\\displaystyle X}\n\n converges in \n\n\n\nX\n\n\n{\\displaystyle X}\n\n to a point \n\n\n\nx\n∈\nX\n,\n\n\n{\\displaystyle x\\in X,}\n\n then the prefilter \n\n\n\nf\n(\n\n\nB\n\n\n)\n\n\n{\\displaystyle f({\\mathcal {B}})}\n\n converges in \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n to \n\n\n\nf\n(\nx\n)\n.\n\n\n{\\displaystyle f(x).}\n\n This characterization remains true if the word \"filter\" is replaced by \"prefilter.\"[16]\n\nIf \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n and \n\n\n\ng\n:\nY\n→\nZ\n\n\n{\\displaystyle g:Y\\to Z}\n\n are continuous, then so is the composition \n\n\n\ng\n∘\nf\n:\nX\n→\nZ\n.\n\n\n{\\displaystyle g\\circ f:X\\to Z.}\n\n If \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n is continuous and\n\nThe possible topologies on a fixed set X are partially ordered: a topology \n\n\n\n\nτ\n\n1\n\n\n\n\n{\\displaystyle \\tau _{1}}\n\n is said to be coarser than another topology \n\n\n\n\nτ\n\n2\n\n\n\n\n{\\displaystyle \\tau _{2}}\n\n (notation: \n\n\n\n\nτ\n\n1\n\n\n⊆\n\nτ\n\n2\n\n\n\n\n{\\displaystyle \\tau _{1}\\subseteq \\tau _{2}}\n\n) if every open subset with respect to \n\n\n\n\nτ\n\n1\n\n\n\n\n{\\displaystyle \\tau _{1}}\n\n is also open with respect to \n\n\n\n\nτ\n\n2\n\n\n.\n\n\n{\\displaystyle \\tau _{2}.}\n\n Then, the identity map\n\n\n\n\n\nid\n\nX\n\n\n:\n\n(\n\nX\n,\n\nτ\n\n2\n\n\n\n)\n\n→\n\n(\n\nX\n,\n\nτ\n\n1\n\n\n\n)\n\n\n\n{\\displaystyle \\operatorname {id} _{X}:\\left(X,\\tau _{2}\\right)\\to \\left(X,\\tau _{1}\\right)}\n\n\nis continuous if and only if \n\n\n\n\nτ\n\n1\n\n\n⊆\n\nτ\n\n2\n\n\n\n\n{\\displaystyle \\tau _{1}\\subseteq \\tau _{2}}\n\n (see also comparison of topologies). More generally, a continuous function\n\n\n\n\n\n(\n\nX\n,\n\nτ\n\nX\n\n\n\n)\n\n→\n\n(\n\nY\n,\n\nτ\n\nY\n\n\n\n)\n\n\n\n{\\displaystyle \\left(X,\\tau _{X}\\right)\\to \\left(Y,\\tau _{Y}\\right)}\n\n\nstays continuous if the topology \n\n\n\n\nτ\n\nY\n\n\n\n\n{\\displaystyle \\tau _{Y}}\n\n is replaced by a coarser topology and/or \n\n\n\n\nτ\n\nX\n\n\n\n\n{\\displaystyle \\tau _{X}}\n\n is replaced by a finer topology.\n\nSymmetric to the concept of a continuous map is an open map, for which images of open sets are open. If an open map f has an inverse function, that inverse is continuous, and if a continuous map g has an inverse, that inverse is open. Given a bijective function f between two topological spaces, the inverse function \n\n\n\n\nf\n\n−\n1\n\n\n\n\n{\\displaystyle f^{-1}}\n\n need not be continuous. A bijective continuous function with a continuous inverse function is called a homeomorphism.\n\nIf a continuous bijection has as its domain a compact space and its codomain is Hausdorff, then it is a homeomorphism.\n\nGiven a function\n\n\n\n\nf\n:\nX\n→\nS\n,\n\n\n{\\displaystyle f:X\\to S,}\n\n\nwhere X is a topological space and S is a set (without a specified topology), the final topology on S is defined by letting the open sets of S be those subsets A of S for which \n\n\n\n\nf\n\n−\n1\n\n\n(\nA\n)\n\n\n{\\displaystyle f^{-1}(A)}\n\n is open in X. If S has an existing topology, f is continuous with respect to this topology if and only if the existing topology is coarser than the final topology on S. Thus, the final topology is the finest topology on S that makes f continuous. If f is surjective, this topology is canonically identified with the quotient topology under the equivalence relation defined by f.\n\nDually, for a function f from a set S to a topological space X, the initial topology on S is defined by designating as an open set every subset A of S such that \n\n\n\nA\n=\n\nf\n\n−\n1\n\n\n(\nU\n)\n\n\n{\\displaystyle A=f^{-1}(U)}\n\n for some open subset U of X. If S has an existing topology, f is continuous with respect to this topology if and only if the existing topology is finer than the initial topology on S. Thus, the initial topology is the coarsest topology on S that makes f continuous. If f is injective, this topology is canonically identified with the subspace topology of S, viewed as a subset of X.\n\nA topology on a set S is uniquely determined by the class of all continuous functions \n\n\n\nS\n→\nX\n\n\n{\\displaystyle S\\to X}\n\n into all topological spaces X. Dually, a similar idea can be applied to maps \n\n\n\nX\n→\nS\n.\n\n\n{\\displaystyle X\\to S.}\n\n\n\nIf \n\n\n\nf\n:\nS\n→\nY\n\n\n{\\displaystyle f:S\\to Y}\n\n is a continuous function from some subset \n\n\n\nS\n\n\n{\\displaystyle S}\n\n of a topological space \n\n\n\nX\n\n\n{\\displaystyle X}\n\n then a continuous extension of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n to \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is any continuous function \n\n\n\nF\n:\nX\n→\nY\n\n\n{\\displaystyle F:X\\to Y}\n\n such that \n\n\n\nF\n(\ns\n)\n=\nf\n(\ns\n)\n\n\n{\\displaystyle F(s)=f(s)}\n\n for every \n\n\n\ns\n∈\nS\n,\n\n\n{\\displaystyle s\\in S,}\n\n which is a condition that often written as \n\n\n\nf\n=\nF\n\n\n\n|\n\n\n\nS\n\n\n.\n\n\n{\\displaystyle f=F{\\big \\vert }_{S}.}\n\n In words, it is any continuous function \n\n\n\nF\n:\nX\n→\nY\n\n\n{\\displaystyle F:X\\to Y}\n\n that restricts to \n\n\n\nf\n\n\n{\\displaystyle f}\n\n on \n\n\n\nS\n.\n\n\n{\\displaystyle S.}\n\n This notion is used, for example, in the Tietze extension theorem and the Hahn–Banach theorem. If \n\n\n\nf\n:\nS\n→\nY\n\n\n{\\displaystyle f:S\\to Y}\n\n is not continuous, then it could not possibly have a continuous extension. If \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is a Hausdorff space and \n\n\n\nS\n\n\n{\\displaystyle S}\n\n is a dense subset of \n\n\n\nX\n\n\n{\\displaystyle X}\n\n then a continuous extension of \n\n\n\nf\n:\nS\n→\nY\n\n\n{\\displaystyle f:S\\to Y}\n\n to \n\n\n\nX\n,\n\n\n{\\displaystyle X,}\n\n if one exists, will be unique. The Blumberg theorem states that if \n\n\n\nf\n:\n\nR\n\n→\n\nR\n\n\n\n{\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} }\n\n is an arbitrary function then there exists a dense subset \n\n\n\nD\n\n\n{\\displaystyle D}\n\n of \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n such that the restriction \n\n\n\nf\n\n\n\n|\n\n\n\nD\n\n\n:\nD\n→\n\nR\n\n\n\n{\\displaystyle f{\\big \\vert }_{D}:D\\to \\mathbb {R} }\n\n is continuous; in other words, every function \n\n\n\n\nR\n\n→\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} \\to \\mathbb {R} }\n\n can be restricted to some dense subset on which it is continuous. \n\nVarious other mathematical domains use the concept of continuity in different but related meanings. For example, in order theory, an order-preserving function \n\n\n\nf\n:\nX\n→\nY\n\n\n{\\displaystyle f:X\\to Y}\n\n between particular types of partially ordered sets \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is continuous if for each directed subset \n\n\n\nA\n\n\n{\\displaystyle A}\n\n of \n\n\n\nX\n,\n\n\n{\\displaystyle X,}\n\n we have \n\n\n\nsup\nf\n(\nA\n)\n=\nf\n(\nsup\nA\n)\n.\n\n\n{\\displaystyle \\sup f(A)=f(\\sup A).}\n\n Here \n\n\n\n\nsup\n\n\n\n{\\displaystyle \\,\\sup \\,}\n\n is the supremum with respect to the orderings in \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n,\n\n\n{\\displaystyle Y,}\n\n respectively. This notion of continuity is the same as topological continuity when the partially ordered sets are given the Scott topology.[19][20]\n\nIn category theory, a functor\n\n\n\n\nF\n:\n\n\nC\n\n\n→\n\n\nD\n\n\n\n\n{\\displaystyle F:{\\mathcal {C}}\\to {\\mathcal {D}}}\n\n\nbetween two categories is called continuous if it commutes with small limits. That is to say,\n\n\n\n\n\n\n\nlim\n←\n\n\n\ni\n∈\nI\n\n\n⁡\nF\n(\n\nC\n\ni\n\n\n)\n≅\nF\n\n(\n\n\n\n\nlim\n←\n\n\n\ni\n∈\nI\n\n\n⁡\n\nC\n\ni\n\n\n\n)\n\n\n\n{\\displaystyle \\varprojlim _{i\\in I}F(C_{i})\\cong F\\left(\\varprojlim _{i\\in I}C_{i}\\right)}\n\n\nfor any small (that is, indexed by a set \n\n\n\nI\n,\n\n\n{\\displaystyle I,}\n\n as opposed to a class) diagram of objects in \n\n\n\n\n\nC\n\n\n\n\n{\\displaystyle {\\mathcal {C}}}\n\n.\n\nA continuity space is a generalization of metric spaces and posets,[21][22] which uses the concept of quantales, and that can be used to unify the notions of metric spaces and domains.[23]\n\nIn measure theory, a function \n\n\n\nf\n:\nE\n→\n\n\nR\n\n\nk\n\n\n\n\n{\\displaystyle f:E\\to \\mathbb {R} ^{k}}\n\n defined on a Lebesgue measurable set \n\n\n\nE\n⊆\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle E\\subseteq \\mathbb {R} ^{n}}\n\n is called approximately continuous at a point \n\n\n\n\nx\n\n0\n\n\n∈\nE\n\n\n{\\displaystyle x_{0}\\in E}\n\n if the approximate limit of \n\n\n\nf\n\n\n{\\displaystyle f}\n\n at \n\n\n\n\nx\n\n0\n\n\n\n\n{\\displaystyle x_{0}}\n\n exists and equals \n\n\n\nf\n(\n\nx\n\n0\n\n\n)\n\n\n{\\displaystyle f(x_{0})}\n\n. This generalizes the notion of continuity by replacing the ordinary limit with the approximate limit. A fundamental result known as the Stepanov-Denjoy theorem states that a function is measurable if and only if it is approximately continuous almost everywhere.[24]\n"
    },
    {
        "title": "Nonlinear system",
        "content": "Collective intelligence\nCollective action\nSelf-organized criticality\nHerd mentality\nPhase transition\nAgent-based modelling\nSynchronization\nAnt colony optimization\nParticle swarm optimization\nSwarm behaviour\n\nSocial network analysis\nSmall-world networks\nCentrality\nMotifs\nGraph theory\nScaling\nRobustness\nSystems biology\nDynamic networks\n\nEvolutionary computation\nGenetic algorithms\nGenetic programming\nArtificial life\nMachine learning\nEvolutionary developmental biology\nArtificial intelligence\nEvolutionary robotics\n\nReaction–diffusion systems\nPartial differential equations\nDissipative structures\nPercolation\nCellular automata\nSpatial ecology\nSelf-replication\n\nConversation theory\nEntropy\nFeedback \nGoal-oriented\nHomeostasis \nInformation theory\nOperationalization\nSecond-order cybernetics\nSelf-reference\nSystem dynamics\nSystems science\nSystems thinking\nSensemaking\nVariety\n\nOrdinary differential equations\nPhase space\nAttractors\nPopulation dynamics\nChaos\nMultistability\nBifurcation\n\nRational choice theory\nBounded rationality\n\nIn mathematics and science, a nonlinear system (or a non-linear system) is a system in which the change of the output is not proportional to the change of the input.[1][2] Nonlinear problems are of interest to engineers, biologists,[3][4][5] physicists,[6][7] mathematicians, and many other scientists since most systems are inherently nonlinear in nature.[8] Nonlinear dynamical systems, describing changes in variables over time, may appear chaotic, unpredictable, or counterintuitive, contrasting with much simpler linear systems.\n\nTypically, the behavior of a nonlinear system is described in mathematics by a nonlinear system of equations, which is a set of simultaneous equations in which the unknowns (or the unknown functions in the case of differential equations) appear as variables of a polynomial of degree higher than one or in the argument of a function which is not a polynomial of degree one.\nIn other words, in a nonlinear system of equations, the equation(s) to be solved cannot be written as a linear combination of the unknown variables or functions that appear in them. Systems can be defined as nonlinear, regardless of whether known linear functions appear in the equations. In particular, a differential equation is linear if it is linear in terms of the unknown function and its derivatives, even if nonlinear in terms of the other variables appearing in it.\n\nAs nonlinear dynamical equations are difficult to solve, nonlinear systems are commonly approximated by linear equations (linearization). This works well up to some accuracy and some range for the input values, but some interesting phenomena such as solitons, chaos,[9] and singularities are hidden by linearization. It follows that some aspects of the dynamic behavior of a nonlinear system can appear to be counterintuitive, unpredictable or even chaotic. Although such chaotic behavior may resemble random behavior, it is in fact not random. For example, some aspects of the weather are seen to be chaotic, where simple changes in one part of the system produce complex effects throughout. This nonlinearity is one of the reasons why accurate long-term forecasts are impossible with current technology.\n\nSome authors use the term nonlinear science for the study of nonlinear systems. This term is disputed by others:\n\nUsing a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals.\nIn mathematics, a linear map (or linear function) \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is one which satisfies both of the following properties:\n\nAdditivity implies homogeneity for any rational α, and, for continuous functions, for any real α. For a complex α, homogeneity does not follow from additivity. For example, an antilinear map is additive but not homogeneous. The conditions of additivity and homogeneity are often combined in the superposition principle\n\nAn equation written as\n\nis called linear if \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is a linear map (as defined above) and nonlinear otherwise. The equation is called homogeneous if \n\n\n\nC\n=\n0\n\n\n{\\displaystyle C=0}\n\n and \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is a homogeneous function.\n\nThe definition \n\n\n\nf\n(\nx\n)\n=\nC\n\n\n{\\displaystyle f(x)=C}\n\n is very general in that \n\n\n\nx\n\n\n{\\displaystyle x}\n\n can be any sensible mathematical object (number, vector, function, etc.), and the function \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n can literally be any mapping, including integration or differentiation with associated constraints (such as boundary values). If \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n contains differentiation with respect to \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, the result will be a differential equation.\n\nA nonlinear system of equations consists of a set of equations in several variables such that at least one of them is not a linear equation.\n\nFor a single equation of the form \n\n\n\nf\n(\nx\n)\n=\n0\n,\n\n\n{\\displaystyle f(x)=0,}\n\n many methods have been designed; see Root-finding algorithm. In the case where f is a polynomial, one has a polynomial equation such as\n\n\n\n\n\nx\n\n2\n\n\n+\nx\n−\n1\n=\n0.\n\n\n{\\displaystyle x^{2}+x-1=0.}\n\n The general root-finding algorithms apply to polynomial roots, but, generally they do not find all the roots, and when they fail to find a root, this does not imply that there is no roots. Specific methods for polynomials allow finding all roots or the real roots; see real-root isolation.\n\nSolving systems of polynomial equations, that is finding the common zeros of a set of several polynomials in several variables is a difficult problem for which elaborated algorithms have been designed, such as Gröbner base algorithms.[11]\n\nFor the general case of system of equations formed by equating to zero several differentiable functions, the main method is Newton's method and its variants. Generally they may provide a solution, but do not provide any information on the number of solutions.\n\nA nonlinear recurrence relation defines successive terms of a sequence as a nonlinear function of preceding terms. Examples of nonlinear recurrence relations are the logistic map and the relations that define the various Hofstadter sequences. Nonlinear discrete models that represent a wide class of nonlinear recurrence relationships include the NARMAX (Nonlinear Autoregressive Moving Average with eXogenous inputs) model and the related nonlinear system identification and analysis procedures.[12] These approaches can be used to study a wide class of complex nonlinear behaviors in the time, frequency, and spatio-temporal domains.\n\nA system of differential equations is said to be nonlinear if it is not a system of linear equations. Problems involving nonlinear differential equations are extremely diverse, and methods of solution or analysis are problem dependent. Examples of nonlinear differential equations are the Navier–Stokes equations in fluid dynamics and the Lotka–Volterra equations in biology.\n\nOne of the greatest difficulties of nonlinear problems is that it is not generally possible to combine known solutions into new solutions. In linear problems, for example, a family of linearly independent solutions can be used to construct general solutions through the superposition principle. A good example of this is one-dimensional heat transport with Dirichlet boundary conditions, the solution of which can be written as a time-dependent linear combination of sinusoids of differing frequencies; this makes solutions very flexible. It is often possible to find several very specific solutions to nonlinear equations, however the lack of a superposition principle prevents the construction of new solutions.\n\nFirst order ordinary differential equations are often exactly solvable by separation of variables, especially for autonomous equations. For example, the nonlinear equation\n\nhas \n\n\n\nu\n=\n\n\n1\n\nx\n+\nC\n\n\n\n\n\n{\\displaystyle u={\\frac {1}{x+C}}}\n\n as a general solution (and also the special solution \n\n\n\nu\n=\n0\n,\n\n\n{\\displaystyle u=0,}\n\n corresponding to the limit of the general solution when C tends to infinity). The equation is nonlinear because it may be written as\n\nand the left-hand side of the equation is not a linear function of \n\n\n\nu\n\n\n{\\displaystyle u}\n\n and its derivatives. Note that if the \n\n\n\n\nu\n\n2\n\n\n\n\n{\\displaystyle u^{2}}\n\n term were replaced with \n\n\n\nu\n\n\n{\\displaystyle u}\n\n, the problem would be linear (the exponential decay problem).\n\nSecond and higher order ordinary differential equations (more generally, systems of nonlinear equations) rarely yield closed-form solutions, though implicit solutions and solutions involving nonelementary integrals are encountered.\n\nCommon methods for the qualitative analysis of nonlinear ordinary differential equations include:\n\nThe most common basic approach to studying nonlinear partial differential equations is to change the variables (or otherwise transform the problem) so that the resulting problem is simpler (possibly linear). Sometimes, the equation may be transformed into one or more ordinary differential equations, as seen in separation of variables, which is always useful whether or not the resulting ordinary differential equation(s) is solvable.\n\nAnother common (though less mathematical) tactic, often exploited in fluid and heat mechanics, is to use scale analysis to simplify a general, natural equation in a certain specific boundary value problem. For example, the (very) nonlinear Navier-Stokes equations can be simplified into one linear partial differential equation in the case of transient, laminar, one dimensional flow in a circular pipe; the scale analysis provides conditions under which the flow is laminar and one dimensional and also yields the simplified equation.\n\nOther methods include examining the characteristics and using the methods outlined above for ordinary differential equations.\n\nA classic, extensively studied nonlinear problem is the dynamics of a frictionless pendulum under the influence of gravity. Using Lagrangian mechanics, it may be shown[14] that the motion of a pendulum can be described by the dimensionless nonlinear equation\n\nwhere gravity points \"downwards\" and \n\n\n\nθ\n\n\n{\\displaystyle \\theta }\n\n is the angle the pendulum forms with its rest position, as shown in the figure at right. One approach to \"solving\" this equation is to use \n\n\n\nd\nθ\n\n/\n\nd\nt\n\n\n{\\displaystyle d\\theta /dt}\n\n as an integrating factor, which would eventually yield\n\nwhich is an implicit solution involving an elliptic integral. This \"solution\" generally does not have many uses because most of the nature of the solution is hidden in the nonelementary integral (nonelementary unless \n\n\n\n\nC\n\n0\n\n\n=\n2\n\n\n{\\displaystyle C_{0}=2}\n\n).\n\nAnother way to approach the problem is to linearize any nonlinearity (the sine function term in this case) at the various points of interest through Taylor expansions. For example, the linearization at \n\n\n\nθ\n=\n0\n\n\n{\\displaystyle \\theta =0}\n\n, called the small angle approximation, is\n\nsince \n\n\n\nsin\n⁡\n(\nθ\n)\n≈\nθ\n\n\n{\\displaystyle \\sin(\\theta )\\approx \\theta }\n\n for \n\n\n\nθ\n≈\n0\n\n\n{\\displaystyle \\theta \\approx 0}\n\n. This is a simple harmonic oscillator corresponding to oscillations of the pendulum near the bottom of its path. Another linearization would be at \n\n\n\nθ\n=\nπ\n\n\n{\\displaystyle \\theta =\\pi }\n\n, corresponding to the pendulum being straight up:\n\nsince \n\n\n\nsin\n⁡\n(\nθ\n)\n≈\nπ\n−\nθ\n\n\n{\\displaystyle \\sin(\\theta )\\approx \\pi -\\theta }\n\n for \n\n\n\nθ\n≈\nπ\n\n\n{\\displaystyle \\theta \\approx \\pi }\n\n. The solution to this problem involves hyperbolic sinusoids, and note that unlike the small angle approximation, this approximation is unstable, meaning that \n\n\n\n\n|\n\nθ\n\n|\n\n\n\n{\\displaystyle |\\theta |}\n\n will usually grow without limit, though bounded solutions are possible. This corresponds to the difficulty of balancing a pendulum upright, it is literally an unstable state.\n\nOne more interesting linearization is possible around \n\n\n\nθ\n=\nπ\n\n/\n\n2\n\n\n{\\displaystyle \\theta =\\pi /2}\n\n, around which \n\n\n\nsin\n⁡\n(\nθ\n)\n≈\n1\n\n\n{\\displaystyle \\sin(\\theta )\\approx 1}\n\n:\n\nThis corresponds to a free fall problem. A very useful qualitative picture of the pendulum's dynamics may be obtained by piecing together such linearizations, as seen in the figure at right. Other techniques may be used to find (exact) phase portraits and approximate periods.\n"
    },
    {
        "title": "Variable (mathematics)",
        "content": "\nIn mathematics, a variable (from Latin variabilis, \"changeable\") is a symbol, typically a letter, that refers to an unspecified mathematical object.[1][2][3] One says colloquially that the variable represents or denotes the object, and that any valid candidate for the object is the value of the variable. The values a variable can take are usually of the same kind, often numbers. More specifically, the values involved may form a set, such as the set of real numbers.\n\nThe object may not always exist, or it might be uncertain whether any valid candidate exists or not. For example, one could represent two integers by the variables p and q and require that the value of the square of p is twice the square of q, which in algebraic notation can be written p2 = 2 q2. A definitive proof that this relationship is impossible to satisfy when p and q are restricted to integer numbers isn't obvious, but it has been known since ancient times and has had a big influence on mathematics ever since.\n\nOriginally, the term \"variable\" was used primarily for the argument of a function, in which case its value can vary in the domain of the function. This is the motivation for the choice of the term. Also, variables are used for denoting values of functions, such as ⁠\n\n\n\ny\n\n\n{\\displaystyle y}\n\n⁠ in \n\n\n\ny\n=\nf\n(\nx\n)\n.\n\n\n{\\displaystyle y=f(x).}\n\n\n\nA variable may represent an unspecified number that remains fixed during the resolution of a problem; in which case, it is often called a parameter. A variable may denote an unknown number that has to be determined; in which case, it is called an unknown; for example, in the quadratic equation \n\n\n\na\n\nx\n\n2\n\n\n+\nb\nx\n+\nc\n=\n0\n,\n\n\n{\\displaystyle ax^{2}+bx+c=0,}\n\n the variables \n\n\n\na\n,\nb\n,\nc\n\n\n{\\displaystyle a,b,c}\n\n are parameters, and \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is the unknown.\n\nSometimes the same symbol can be used to denote both a variable and a constant, that is a well defined mathematical object. For example, the Greek letter π generally represents the number π, but has also been used to denote a projection. Similarly the letter e often denotes Euler's number, but has been used to denote an unassigned coefficient for quartic function and higher degree polynomials. Even the symbol ⁠\n\n\n\n1\n\n\n{\\displaystyle 1}\n\n⁠ has been used to denote an identity element of an arbitrary field. These two notions are used almost identically, therefore one usually must be told whether a given symbol denotes a variable or a constant.[4]\n\nVariables are often used for representing matrices, functions, their arguments, sets and their elements, vectors, spaces, etc.[5]\n\nIn mathematical logic, a variable is a symbol that either represents an unspecified constant of the theory, or is being quantified over.[6][7][8]\n\nThe earliest uses of an \"unknown quantity\" date back to at least the Ancient Egyptians with the Moscow Mathematical Papyrus (c. 1500 BC) which described problems with unknowns rhetorically, called the \"Aha problems\". The \"Aha problems\" involve finding unknown quantities (referred to as aha, \"stack\") if the sum of the quantity and part(s) of it are given (The Rhind Mathematical Papyrus also contains four of these type of problems). For example, problem 19 asks one to calculate a quantity taken 1+1⁄2 times and added to 4 to make 10.[9] In modern mathematical notation: \n\n\n\n\n\n3\n2\n\n\nx\n+\n4\n=\n10\n\n\n{\\textstyle {\\frac {3}{2}}x+4=10}\n\n. Around the same time in Mesopotamia, mathematics of the Old Babylonian period (c. 2000 BC - 1500 BC) was more advanced, also studying quadratic and cubic equations.[10]\n\nIn works of ancient greece such as Euclid's Elements (c. 300 BC), mathematics was described gemoetrically. For example, The Elements, proposition 1 of Book II, Euclid includes the proposition:\n\n\"If there be two straight lines, and one of them be cut into any number of segments whatever, the rectangle contained by the two straight lines is equal to the rectangles contained by the uncut straight line and each of the segments.\"\n\nThis corresponds to the algebraic identity a(b+c)=ab+ac (distributivity), but is described entirely geometrically. Euclid, and other greek geometers, also used single letters refer to geometric points and shapes. This kind of algebra is now sometimes called Greek geometric algebra.[10]\n\nDiophantus of Alexandria,[11] pioneered a form of syncopated algebra in his Arithmetica (c. 200 AD), which introduced symbolic manipulation of expressions with unknowns and powers, but without modern symbols for relations (such as equality or inequality) or exponents.[12] An unknown number was called \n\n\n\nζ\n\n\n{\\displaystyle \\zeta }\n\n.[13] The square of \n\n\n\nζ\n\n\n{\\displaystyle \\zeta }\n\n was \n\n\n\n\nΔ\n\nv\n\n\n\n\n{\\displaystyle \\Delta ^{v}}\n\n; the cube was \n\n\n\n\nK\n\nv\n\n\n\n\n{\\displaystyle K^{v}}\n\n; the fourth power was \n\n\n\n\nΔ\n\nv\n\n\nΔ\n\n\n{\\displaystyle \\Delta ^{v}\\Delta }\n\n; and the fifth power was \n\n\n\nΔ\n\nK\n\nv\n\n\n\n\n{\\displaystyle \\Delta K^{v}}\n\n.[14] So for example, what would be written in modern notation as:\n\n\n\n\nx\n\n3\n\n\n−\n2\n\nx\n\n2\n\n\n+\n10\nx\n−\n1\n,\n\n\n{\\displaystyle x^{3}-2x^{2}+10x-1,}\n\nWould be written in Diophantus's syncopated notation as:\n\nIn the 7th century BC, Brahmagupta used different colours to represent the unknowns in algebraic equations in the Brāhmasphuṭasiddhānta. One section of this book is called \"Equations of Several Colours\".[15] Greek and other ancient mathematical advances, were often trapped in long periods of stagnation, and so there were few revolutions in notation, but this began to change by the early modern period.\n\nAt the end of the 16th century, François Viète introduced the idea of representing known and unknown numbers by letters, nowadays called variables, and the idea of computing with them as if they were numbers—in order to obtain the result by a simple replacement. Viète's convention was to use consonants for known values, and vowels for unknowns.[16]\n\nIn 1637, René Descartes \"invented the convention of representing unknowns in equations by x, y, and z, and knowns by a, b, and c\".[17] Contrarily to Viète's convention, Descartes' is still commonly in use. The history of the letter x in math was discussed in an 1887 Scientific American article.[18]\n\nStarting in the 1660s, Isaac Newton and Gottfried Wilhelm Leibniz independently developed the infinitesimal calculus, which essentially consists of studying how an infinitesimal variation of a time-varying quantity, called a Fluent, induces a corresponding variation of another quantity which is a function of the first variable. Almost a century later, Leonhard Euler fixed the terminology of infinitesimal calculus, and introduced the notation y = f(x) for a function f, its variable x and its value y. Until the end of the 19th century, the word variable referred almost exclusively to the arguments and the values of functions.\n\nIn the second half of the 19th century, it appeared that the foundation of infinitesimal calculus was not formalized enough to deal with apparent paradoxes such as a nowhere differentiable continuous function. To solve this problem, Karl Weierstrass introduced a new formalism consisting of replacing the intuitive notion of limit by a formal definition. The older notion of limit was \"when the variable x varies and tends toward a, then f(x) tends toward L\", without any accurate definition of \"tends\". Weierstrass replaced this sentence by the formula\n\nin which none of the five variables is considered as varying.\n\nThis static formulation led to the modern notion of variable, which is simply a symbol representing a mathematical object that either is unknown, or may be replaced by any element of a given set (e.g., the set of real numbers).\n\nVariables are generally denoted by a single letter, most often from the Latin alphabet and less often from the Greek, which may be lowercase or capitalized. The letter may be followed by a subscript: a number (as in x2), another variable (xi), a word or abbreviation of a word (xtotal) or a mathematical expression (x2i + 1). Under the influence of computer science, some variable names in pure mathematics consist of several letters and digits. Following René Descartes (1596–1650), letters at the beginning of the alphabet such as a, b, c are commonly used for known values and parameters, and letters at the end of the alphabet such as (x, y, z) are commonly used for unknowns and variables of functions.[19] In printed mathematics, the norm is to set variables and constants in an italic typeface.[20]\n\nFor example, a general quadratic function is conventionally written as \n\n\n\na\n\nx\n\n2\n\n\n+\nb\nx\n+\nc\n\n\n\n{\\textstyle ax^{2}+bx+c\\,}\n\n, where a, b and c are parameters (also called constants, because they are constant functions), while x is the variable of the function. A more explicit way to denote this function is \n\n\n\nx\n↦\na\n\nx\n\n2\n\n\n+\nb\nx\n+\nc\n\n\n\n{\\textstyle x\\mapsto ax^{2}+bx+c\\,}\n\n, which clarifies the function-argument status of x and the constant status of a, b and c. Since c occurs in a term that is a constant function of x, it is called the constant term.[21]\n\nSpecific branches and applications of mathematics have specific naming conventions for variables. Variables with similar roles or meanings are often assigned consecutive letters or the same letter with different subscripts. For example, the three axes in 3D coordinate space are conventionally called x, y, and z. In physics, the names of variables are largely determined by the physical quantity they describe, but various naming conventions exist. A convention often followed in probability and statistics is to use X, Y, Z for the names of random variables, keeping x, y, z for variables representing corresponding better-defined values.\n\nIt is common for variables to play different roles in the same mathematical formula, and names or qualifiers have been introduced to distinguish them. For example, the general cubic equation\n\nis interpreted as having five variables: four, a, b, c, d, which are taken to be given numbers and the fifth variable, x, is understood to be an unknown number. To distinguish them, the variable x is called an unknown, and the other variables are called parameters or coefficients, or sometimes constants, although this last terminology is incorrect for an equation, and should be reserved for the function defined by the left-hand side of this equation.\n\nIn the context of functions, the term variable refers commonly to the arguments of the functions. This is typically the case in sentences like \"function of a real variable\", \"x is the variable of the function f: x ↦ f(x)\", \"f is a function of the variable x\" (meaning that the argument of the function is referred to by the variable x).\n\nIn the same context, variables that are independent of x define constant functions and are therefore called constant. For example, a constant of integration is an arbitrary constant function that is added to a particular antiderivative to obtain the other antiderivatives. Because of the strong relationship between polynomials and polynomial functions, the term \"constant\" is often used to denote the coefficients of a polynomial, which are constant functions of the indeterminates.\n\nOther specific names for variables are:\n\nAll these denominations of variables are of semantic nature, and the way of computing with them (syntax) is the same for all.\n\nIn calculus and its application to physics and other sciences, it is rather common to consider a variable, say y, whose possible values depend on the value of another variable, say x. In mathematical terms, the dependent variable y represents the value of a function of x. To simplify formulas, it is often useful to use the same symbol for the dependent variable y and the function mapping x onto y. For example, the state of a physical system depends on measurable quantities such as the pressure, the temperature, the spatial position, ..., and all these quantities vary when the system evolves, that is, they are function of the time. In the formulas describing the system, these quantities are represented by variables which are dependent on the time, and thus considered implicitly as functions of the time.\n\nTherefore, in a formula, a dependent variable is a variable that is implicitly a function of another (or several other) variables. An independent variable is a variable that is not dependent.[23]\n\nThe property of a variable to be dependent or independent depends often of the point of view and is not intrinsic. For example, in the notation f(x, y, z), the three variables may be all independent and the notation represents a function of three variables. On the other hand, if y and z depend on x (are dependent variables) then the notation represents a function of the single independent variable x.[24]\n\nIf one defines a function f from the real numbers to the real numbers by\n\nthen x is a variable standing for the argument of the function being defined, which can be any real number.\n\nIn the identity\n\nthe variable i is a summation variable which designates in turn each of the integers 1, 2, ...,  n (it is also called index because its variation is over a discrete set of values) while n is a parameter (it does not vary within the formula).\n\nIn the theory of polynomials, a polynomial of degree 2 is generally denoted as ax2 + bx + c, where a, b and c are called coefficients (they are assumed to be fixed, i.e., parameters of the problem considered) while x is called a variable. When studying this polynomial for its polynomial function this x stands for the function argument. When studying the polynomial as an object in itself, x is taken to be an indeterminate, and would often be written with a capital letter instead to indicate this status.\n\nConsider the equation describing the ideal gas law, \n\n\n\n\nP\nV\n=\nN\n\nk\n\nB\n\n\nT\n.\n\n\n{\\displaystyle PV=Nk_{B}T.}\n\n\nThis equation would generally be interpreted to have four variables, and one constant. The constant is \n\n\n\n\nk\n\nB\n\n\n\n\n{\\displaystyle k_{B}}\n\n, the Boltzmann constant. One of the variables, \n\n\n\nN\n\n\n{\\displaystyle N}\n\n, the number of particles, is a positive integer (and therefore a discrete variable), while the other three, \n\n\n\nP\n,\nV\n\n\n{\\displaystyle P,V}\n\n and \n\n\n\nT\n\n\n{\\displaystyle T}\n\n, for pressure, volume and temperature, are continuous variables.\n\nOne could rearrange this equation to obtain \n\n\n\nP\n\n\n{\\displaystyle P}\n\n as a function of the other variables,\n\n\n\n\nP\n(\nV\n,\nN\n,\nT\n)\n=\n\n\n\nN\n\nk\n\nB\n\n\nT\n\nV\n\n\n.\n\n\n{\\displaystyle P(V,N,T)={\\frac {Nk_{B}T}{V}}.}\n\n\nThen \n\n\n\nP\n\n\n{\\displaystyle P}\n\n, as a function of the other variables, is the dependent variable, while its arguments, \n\n\n\nV\n,\nN\n\n\n{\\displaystyle V,N}\n\n and \n\n\n\nT\n\n\n{\\displaystyle T}\n\n, are independent variables. One could approach this function more formally and think about its domain and range: in function notation, here \n\n\n\nP\n\n\n{\\displaystyle P}\n\n is a function \n\n\n\nP\n:\n\n\nR\n\n\n>\n0\n\n\n×\n\nN\n\n×\n\n\nR\n\n\n>\n0\n\n\n→\n\nR\n\n\n\n{\\displaystyle P:\\mathbb {R} _{>0}\\times \\mathbb {N} \\times \\mathbb {R} _{>0}\\rightarrow \\mathbb {R} }\n\n.\n\nHowever, in an experiment, in order to determine the dependence of pressure on a single one of the independent variables, it is necessary to fix all but one of the variables, say \n\n\n\nT\n\n\n{\\displaystyle T}\n\n. This gives a function\n\n\n\n\nP\n(\nT\n)\n=\n\n\n\nN\n\nk\n\nB\n\n\nT\n\nV\n\n\n,\n\n\n{\\displaystyle P(T)={\\frac {Nk_{B}T}{V}},}\n\n\nwhere now \n\n\n\nN\n\n\n{\\displaystyle N}\n\n and \n\n\n\nV\n\n\n{\\displaystyle V}\n\n are also regarded as constants. Mathematically, this constitutes a partial application of the earlier function \n\n\n\nP\n\n\n{\\displaystyle P}\n\n.\n\nThis illustrates how independent variables and constants are largely dependent on the point of view taken. One could even regard \n\n\n\n\nk\n\nB\n\n\n\n\n{\\displaystyle k_{B}}\n\n as a variable to obtain a function \n\n\n\n\nP\n(\nV\n,\nN\n,\nT\n,\n\nk\n\nB\n\n\n)\n=\n\n\n\nN\n\nk\n\nB\n\n\nT\n\nV\n\n\n.\n\n\n{\\displaystyle P(V,N,T,k_{B})={\\frac {Nk_{B}T}{V}}.}\n\n\n\nConsidering constants and variables can lead to the concept of moduli spaces. For illustration, consider the equation for a parabola, \n\n\n\n\ny\n=\na\n\nx\n\n2\n\n\n+\nb\nx\n+\nc\n,\n\n\n{\\displaystyle y=ax^{2}+bx+c,}\n\n\nwhere \n\n\n\na\n,\nb\n,\nc\n,\nx\n\n\n{\\displaystyle a,b,c,x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n are all considered to be real. The set of points \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n in the 2D plane satisfying this equation trace out the graph of a parabola. Here, \n\n\n\na\n,\nb\n\n\n{\\displaystyle a,b}\n\n and \n\n\n\nc\n\n\n{\\displaystyle c}\n\n are regarded as constants, which specify the parabola, while \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n are variables.\n\nThen instead regarding \n\n\n\na\n,\nb\n\n\n{\\displaystyle a,b}\n\n and \n\n\n\nc\n\n\n{\\displaystyle c}\n\n as variables, we observe that each set of 3-tuples \n\n\n\n(\na\n,\nb\n,\nc\n)\n\n\n{\\displaystyle (a,b,c)}\n\n corresponds to a different parabola. That is, they specify coordinates on the 'space of parabolas': this is known as a moduli space of parabolas.\n"
    },
    {
        "title": "Celestial mechanics",
        "content": "Celestial mechanics is the branch of astronomy that deals with the motions of objects in outer space. Historically, celestial mechanics applies principles of physics (classical mechanics) to astronomical objects, such as stars and planets, to produce ephemeris data.\n\nModern analytic celestial mechanics started with Isaac Newton's Principia (1687). The name celestial mechanics is more recent than that. Newton wrote that the field should be called \"rational mechanics\". The term \"dynamics\" came in a little later with Gottfried Leibniz, and over a century after Newton, Pierre-Simon Laplace introduced the term celestial mechanics. Prior to Kepler, there was little connection between exact, quantitative prediction of planetary positions, using geometrical or numerical techniques, and contemporary discussions of the physical causes of the planets' motion.\n\nJohannes Kepler as the first to closely integrate the predictive geometrical astronomy, which had been dominant from Ptolemy in the 2nd century to Copernicus, with physical concepts to produce a New Astronomy, Based upon Causes, or Celestial Physics in 1609. His work led to the laws of planetary orbits, which he developed using his physical principles and the planetary observations made by Tycho Brahe. Kepler's elliptical model greatly improved the accuracy of predictions of planetary motion, years before Newton developed his law of gravitation in 1686.\n\nIsaac Newton is credited with introducing the idea that the motion of objects in the heavens, such as planets, the Sun, and the Moon, and the motion of objects on the ground, like cannon balls and falling apples, could be described by the same set of physical laws. In this sense he unified celestial and terrestrial dynamics. Using his law of gravity, Newton confirmed Kepler's laws for elliptical orbits by deriving them from the gravitational two-body problem, which Newton included in his epochal Philosophiæ Naturalis Principia Mathematica in 1687.\n\nAfter Newton, Joseph-Louis Lagrange attempted to solve the three-body problem in 1772, analyzed the stability of planetary orbits, and discovered the existence of the Lagrange points. Lagrange also reformulated the principles of classical mechanics, emphasizing energy more than force, and developing a method to use a single polar coordinate equation to describe any orbit, even those that are parabolic and hyperbolic. This is useful for calculating the behaviour of planets and comets and such (parabolic and hyperbolic orbits are conic section extensions of Kepler's elliptical orbits). More recently, it has also become useful to calculate spacecraft trajectories.\n\nHenri Poincaré published two now classical monographs, \"New Methods of Celestial Mechanics\" (1892–1899) and \"Lectures on Celestial Mechanics\" (1905–1910). In them, he successfully applied the results of their research to the problem of the motion of three bodies and studied in detail the behavior of solutions (frequency, stability, asymptotic, and so on). Poincaré showed that the three-body problem is not integrable. In other words, the general solution of the three-body problem can not be expressed in terms of algebraic and transcendental functions through unambiguous coordinates and velocities of the bodies. His work in this area was the first major achievement in celestial mechanics since Isaac Newton.[1]\n\nThese monographs include an idea of Poincaré, which later became the basis for mathematical \"chaos theory\" (see, in particular, the Poincaré recurrence theorem) and the general theory of dynamical systems. He introduced the important concept of bifurcation points and proved the existence of equilibrium figures such as the non-ellipsoids, including ring-shaped and pear-shaped figures, and their stability. For this discovery, Poincaré received the Gold Medal of the Royal Astronomical Society (1900).[2]\n\nSimon Newcomb was a Canadian-American astronomer who revised Peter Andreas Hansen's table of lunar positions. In 1877, assisted by George William Hill, he recalculated all the major astronomical constants. After 1884 he conceived, with A.M.W. Downing, a plan to resolve much international confusion on the subject. By the time he attended a standardisation conference in Paris, France, in May 1886, the international consensus was that all ephemerides should be based on Newcomb's calculations. A further conference as late as 1950 confirmed Newcomb's constants as the international standard.\n\nAlbert Einstein explained the anomalous precession of Mercury's perihelion in his 1916 paper The Foundation of the General Theory of Relativity. General relativity led astronomers to recognize that Newtonian mechanics did not provide the highest accuracy.\n\nCelestial motion, without additional forces such as drag forces or the thrust of a rocket, is governed by the reciprocal gravitational acceleration between masses. A generalization is the n-body problem,[3] where a number n of masses are mutually interacting via the gravitational force. Although analytically not integrable in the general case,[4] the integration can be well approximated numerically.\n\nIn the \n\n\n\nn\n=\n2\n\n\n{\\displaystyle n=2}\n\n case (two-body problem) the configuration is much simpler than for \n\n\n\nn\n>\n2\n\n\n{\\displaystyle n>2}\n\n. In this case, the system is fully integrable and exact solutions can be found.[5]\n\nA further simplification is based on the \"standard assumptions in astrodynamics\", which include that one body, the orbiting body, is much smaller than the other, the central body. This is also often approximately valid.\n\nPerturbation theory comprises mathematical methods that are used to find an approximate solution to a problem which cannot be solved exactly. (It is closely related to methods used in numerical analysis, which are ancient.) The earliest use of modern perturbation theory was to deal with the otherwise unsolvable mathematical problems of celestial mechanics: Newton's solution for the orbit of the Moon, which moves noticeably differently from a simple Keplerian ellipse because of the competing gravitation of the Earth and the Sun.\n\nPerturbation methods start with a simplified form of the original problem, which is carefully chosen to be exactly solvable. In celestial mechanics, this is usually a Keplerian ellipse, which is correct when there are only two gravitating bodies (say, the Earth and the Moon), or a circular orbit, which is only correct in special cases of two-body motion, but is often close enough for practical use.\n\nThe solved, but simplified problem is then \"perturbed\" to make its time-rate-of-change equations for the object's position closer to the values from the real problem, such as including the gravitational attraction of a third, more distant body (the Sun). The slight changes that result from the terms in the equations – which themselves may have been simplified yet again – are used as corrections to the original solution. Because simplifications are made at every step, the corrections are never perfect, but even one cycle of corrections often provides a remarkably better approximate solution to the real problem.\n\nThere is no requirement to stop at only one cycle of corrections. A partially corrected solution can be re-used as the new starting point for yet another cycle of perturbations and corrections. In principle, for most problems the recycling and refining of prior solutions to obtain a new generation of better solutions could continue indefinitely, to any desired finite degree of accuracy.\n\nThe common difficulty with the method is that the corrections usually progressively make the new solutions very much more complicated, so each cycle is much more difficult to manage than the previous cycle of corrections. Newton is reported to have said, regarding the problem of the Moon's orbit \"It causeth my head to ache.\"[6]\n\nThis general procedure – starting with a simplified problem and gradually adding corrections that make the starting point of the corrected problem closer to the real situation – is a widely used mathematical tool in advanced sciences and engineering. It is the natural extension of the \"guess, check, and fix\" method used anciently with numbers.\n\nProblems in celestial mechanics are often posed in simplifying reference frames, such as the synodic reference frame applied to the three-body problem, where the origin coincides with the barycenter of the two larger celestial bodies. Other reference frames for n-body simulations include those that place the origin to follow the center of mass of a body, such as the heliocentric and the geocentric reference frames.[7] The choice of reference frame gives rise to many phenomena, including the retrograde motion of superior planets while on a geocentric reference frame. \n\nOrbital mechanics or astrodynamics is the application of ballistics and celestial mechanics to the practical problems concerning the motion of rockets, satellites, and other spacecraft. The motion of these objects is usually calculated from Newton's laws of motion and the law of universal gravitation. Orbital mechanics is a core discipline within space-mission design and control.\n\nCelestial mechanics treats more broadly the orbital dynamics of systems under the influence of gravity, including both spacecraft and natural astronomical bodies such as star systems, planets, moons, and comets. Orbital mechanics focuses on spacecraft trajectories, including orbital maneuvers, orbital plane changes, and interplanetary transfers, and is used by mission planners to predict the results of propulsive maneuvers.\n\nResearch\n\nArtwork\n\nCourse notes\n\nAssociations\n\nSimulations\n"
    },
    {
        "title": "Solid mechanics",
        "content": "Solid mechanics (also known as mechanics of solids) is the branch of continuum mechanics that studies the behavior of solid materials, especially their motion and deformation under the action of forces, temperature changes, phase changes, and other external or internal agents.\n\nSolid mechanics is fundamental for civil, aerospace, nuclear, biomedical and mechanical engineering, for geology, and for many branches of physics and chemistry such as materials science.[1] It has specific applications in many other areas, such as understanding the anatomy of living beings, and the design of dental prostheses and surgical implants. One of the most common practical applications of solid mechanics is the Euler–Bernoulli beam equation. Solid mechanics extensively uses tensors to describe stresses, strains, and the relationship between them.\n\nSolid mechanics is a vast subject because of the wide range of solid materials available, such as steel, wood, concrete, biological materials, textiles, geological materials, and plastics.\n\nA solid is a material that can support a substantial amount of shearing force over a given time scale during a natural or industrial process or action. This is what distinguishes solids from fluids, because fluids also support normal forces which are those forces that are directed perpendicular to the material plane across from which they act and normal stress is the normal force per unit area of that material plane. Shearing forces in contrast with normal forces, act parallel rather than perpendicular to the material plane and the shearing force per unit area is called shear stress.\n\nTherefore, solid mechanics examines the shear stress, deformation and the failure of solid materials and structures.\n\nThe most common topics covered in solid mechanics include:\n\nAs shown in the following table, solid mechanics inhabits a central place within continuum mechanics. The field of rheology presents an overlap between solid and fluid mechanics.\n\nA material has a rest shape and its shape departs away from the rest shape due to stress. The amount of departure from rest shape is called deformation, the proportion of deformation to original size is called strain. If the applied stress is sufficiently low (or the imposed strain is small enough), almost all solid materials behave in such a way that the strain is directly proportional to the stress; the coefficient of the proportion is called the modulus of elasticity. This region of deformation is known as the linearly elastic region.\n\nIt is most common for analysts in solid mechanics to use linear material models, due to ease of computation. However, real materials often exhibit non-linear behavior. As new materials are used and old ones are pushed to their limits, non-linear material models are becoming more common.\n\nThese are basic models that describe how a solid responds to an applied stress:\n"
    },
    {
        "title": "Combinatorics",
        "content": "Combinatorics is an area of mathematics primarily concerned with counting, both as a means and as an end to obtaining results, and certain properties of finite structures. It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics and from evolutionary biology to computer science.\n\nCombinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry,[1] as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right.[2] One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.\n\nThe full scope of combinatorics is not universally agreed upon.[3] According to H.J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions.[4] Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with:\n\nLeon Mirsky has said: \"combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained.\"[5] One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella.[6] Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting.\n\nBasic combinatorial concepts and enumerative results appeared throughout the ancient world. Indian physician Sushruta asserts in Sushruta Samhita that 63 combinations can be made out of 6 different tastes, taken one at a time, two at a time, etc., thus computing all 26 − 1 possibilities.  Greek historian Plutarch discusses an argument between Chrysippus (3rd century BCE) and Hipparchus (2nd century BCE) of a rather delicate enumerative problem, which was later shown to be related to Schröder–Hipparchus numbers.[7][8][9] Earlier, in the Ostomachion, Archimedes (3rd century BCE) may have considered the number of configurations of a tiling puzzle,[10] while combinatorial interests possibly were present in lost works by Apollonius.[11][12]\n\nIn the Middle Ages, combinatorics continued to be studied, largely outside of the European civilization. The Indian mathematician Mahāvīra (c. 850) provided formulae for the number of permutations and combinations,[13][14] and these formulas may have been familiar to Indian mathematicians as early as the 6th century CE.[15]  The philosopher and astronomer Rabbi Abraham ibn Ezra (c. 1140) established the symmetry of binomial coefficients, while a closed formula was obtained later by the talmudist and mathematician Levi ben Gerson (better known as Gersonides), in 1321.[16]\nThe arithmetical triangle—a graphical diagram showing relationships among the binomial coefficients—was presented by mathematicians in treatises dating as far back as the 10th century, and would eventually become known as Pascal's triangle. Later, in Medieval England, campanology provided examples of what is now known as Hamiltonian cycles in certain Cayley graphs on permutations.[17][18]\n\nDuring the Renaissance, together with the rest of mathematics and the sciences, combinatorics enjoyed a rebirth. Works of Pascal, Newton, Jacob Bernoulli and Euler became foundational in the emerging field. In modern times, the works of J.J. Sylvester (late 19th century) and Percy MacMahon (early 20th century) helped lay the foundation for enumerative and algebraic combinatorics. Graph theory also enjoyed an increase of interest at the same time, especially in connection with the four color problem.\n\nIn the second half of the 20th century, combinatorics enjoyed a rapid growth, which led to establishment of dozens of new journals and conferences in the subject.[19] In part, the growth was spurred by new connections and applications to other fields, ranging from algebra to probability, from functional analysis to number theory, etc.  These connections shed the boundaries between combinatorics and parts of mathematics and theoretical computer science, but at the same time led to a partial fragmentation of the field.\n\nEnumerative combinatorics is the most classical area of combinatorics and concentrates on counting the number of certain combinatorial objects.  Although counting the number of elements in a set is a rather broad mathematical problem, many of the problems that arise in applications have a relatively simple combinatorial description. Fibonacci numbers is the basic example of a problem in enumerative combinatorics.  The twelvefold way provides a unified framework for counting permutations, combinations and partitions.\n\nAnalytic combinatorics concerns the enumeration of combinatorial structures using tools from complex analysis and probability theory.  In contrast with enumerative combinatorics, which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.\n\nPartition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials.  Originally a part of number theory and analysis, it is now considered a part of combinatorics or an independent field.  It incorporates the bijective approach and various tools in analysis and analytic number theory and has connections with statistical mechanics. Partitions can be graphically visualized with Young diagrams or Ferrers diagrams. They occur in a number of branches of mathematics and physics, including the study of symmetric polynomials and of the symmetric group and in group representation theory in general.\n\nGraphs are fundamental objects  in combinatorics.  Considerations of graph theory range from enumeration (e.g., the number of graphs on n vertices with k edges) to existing structures (e.g., Hamiltonian cycles) to algebraic representations (e.g., given a graph G and two numbers x and y, does the Tutte polynomial TG(x,y) have a combinatorial interpretation?).  Although there are very strong connections between graph theory and combinatorics, they are sometimes thought of as separate subjects.[20]  While combinatorial methods apply to many graph theory problems, the two disciplines are generally used to seek solutions to different types of problems.\n\nDesign theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties. Block designs are combinatorial designs of a special type. This area is one of the oldest parts of combinatorics, such as in Kirkman's schoolgirl problem proposed in 1850. The solution of the problem is a special case of a Steiner system, which play an important role in the classification of finite simple groups. The area has further connections to coding theory and geometric combinatorics.\n\nCombinatorial design theory can be applied to the area of design of experiments.  Some of the basic theory of combinatorial designs originated in the statistician Ronald Fisher's work on the design of biological experiments. Modern applications are also found in a wide gamut of areas including finite geometry, tournament scheduling, lotteries, mathematical chemistry, mathematical biology, algorithm design and analysis, networking, group testing and cryptography.[21]\n\nFinite geometry is the study of geometric systems having only a finite number of points. Structures analogous to those found in continuous geometries (Euclidean plane, real projective space, etc.) but defined combinatorially are the main items studied. This area provides a rich source of examples for design theory. It should not be confused with discrete geometry (combinatorial geometry).\n\nOrder theory is the study of partially ordered sets, both finite and infinite. It provides a formal framework for describing statements such as \"this is less than that\" or \"this precedes that\". Various examples of partial orders appear in algebra, geometry, number theory and throughout combinatorics and graph theory.  Notable classes and examples of partial orders include lattices and Boolean algebras.\n\nMatroid theory abstracts part of geometry. It studies the properties of sets (usually, finite sets) of vectors in a vector space that do not depend on the particular coefficients in a linear dependence relation.  Not only the structure but also enumerative properties belong to matroid theory.  Matroid theory was introduced by Hassler Whitney and studied as a part of order theory.  It is now an independent field of study with a number of connections with other parts of combinatorics.\n\nExtremal combinatorics studies how large or how small a collection of finite objects (numbers, graphs, vectors, sets, etc.) can be, if it has to satisfy certain restrictions. Much of extremal combinatorics concerns classes of set systems; this is called extremal set theory. For instance, in an n-element set, what is the largest number of k-element subsets that can pairwise intersect one another?  What is the largest number of subsets of which none contains any other?  The latter question is answered by Sperner's theorem, which gave rise to much of extremal set theory.\n\nThe types of questions addressed in this case are about the largest possible graph which satisfies certain properties.  For example, the largest triangle-free graph on 2n vertices is a complete bipartite graph Kn,n. Often it is too hard even to find the extremal answer f(n) exactly and one can only give an asymptotic estimate.\n\nRamsey theory is another part of extremal combinatorics.  It states that any sufficiently large configuration will contain some sort of order. It is an advanced generalization of the pigeonhole principle.\n\nIn probabilistic combinatorics, the questions are of the following type: what is the probability of a certain property for a random discrete object, such as a random graph? For instance, what is the average number of triangles in a random graph? Probabilistic methods are also used to determine the existence of combinatorial objects with certain prescribed properties (for which explicit examples might be difficult to find) by observing that the probability of randomly selecting an object with those properties is greater than 0. This approach (often referred to as the probabilistic method) proved highly effective in applications to extremal combinatorics and graph theory. A closely related area is the study of finite Markov chains, especially on combinatorial objects. Here again probabilistic tools are used to estimate the mixing time.[clarification needed]\n\nOften associated with Paul Erdős, who did the pioneering work on the subject, probabilistic combinatorics was traditionally viewed as a set of tools to study problems in other parts of combinatorics. The area recently grew to become an independent field of combinatorics.\n\nAlgebraic combinatorics is an area of mathematics that employs methods of abstract algebra, notably group theory and representation theory, in various combinatorial contexts and, conversely, applies combinatorial techniques to problems in algebra. Algebraic combinatorics has come to be seen more expansively as an area of mathematics where the interaction of combinatorial and algebraic methods is particularly strong and significant. Thus the combinatorial topics may be enumerative in nature or involve matroids, polytopes, partially ordered sets, or finite geometries. On the algebraic side, besides group and representation theory, lattice theory and commutative algebra are common.\n\nCombinatorics on words deals with formal languages.  It  arose independently within several branches of mathematics, including number theory, group theory and probability. It has applications to enumerative combinatorics, fractal analysis, theoretical computer science, automata theory, and linguistics. While many applications are new, the classical Chomsky–Schützenberger hierarchy of classes of formal grammars is perhaps the best-known result in the field.\n\nGeometric combinatorics is related to convex and discrete geometry. It asks, for example, how many faces of each dimension a convex polytope can have.  Metric properties of polytopes play an important role as well, e.g. the Cauchy theorem on the rigidity of convex polytopes.  Special polytopes are also considered, such as permutohedra, associahedra and Birkhoff polytopes. Combinatorial geometry is a historical name for discrete geometry.\n\nIt includes a number of subareas such as polyhedral combinatorics (the study of faces of convex polyhedra), convex geometry (the study of convex sets, in particular combinatorics of their intersections), and discrete geometry, which in turn has many applications to computational geometry. The study of regular polytopes, Archimedean solids, and kissing numbers is also a part of geometric combinatorics. Special polytopes are also considered, such as the permutohedron, associahedron and Birkhoff polytope.\n\nCombinatorial analogs of concepts and methods in topology are used to study graph coloring, fair division, partitions, partially ordered sets, decision trees, necklace problems and discrete Morse theory.  It should not be confused with combinatorial topology which is an older name for algebraic topology.\n\nArithmetic combinatorics arose out of the interplay between number theory, combinatorics, ergodic theory, and harmonic analysis. It is about combinatorial estimates associated with arithmetic operations (addition, subtraction, multiplication, and division). Additive number theory (sometimes also called additive combinatorics) refers to the special case when only the operations of addition and subtraction are involved.  One important technique in arithmetic combinatorics is the ergodic theory of dynamical systems.\n\nInfinitary combinatorics, or combinatorial set theory, is an extension of ideas in combinatorics to infinite sets.  It is a part of set theory, an area of mathematical logic, but uses tools and ideas from both set theory and extremal combinatorics. Some of the things studied include continuous graphs and trees, extensions of Ramsey's theorem, and Martin's axiom. Recent developments concern combinatorics of the continuum[22] and combinatorics on successors of singular cardinals.[23]\n\nGian-Carlo Rota used the name continuous combinatorics[24] to describe geometric probability, since there are many analogies between counting and measure.\n\nCombinatorial optimization is the study of optimization on discrete and combinatorial objects. It started as a part of combinatorics and graph theory, but is now viewed as a branch of applied mathematics and computer science, related to operations research, algorithm theory and computational complexity theory.\n\nCoding theory started as a part of design theory with early combinatorial constructions of error-correcting codes. The main idea of the subject is to design efficient and reliable methods of data transmission. It is now a large field of study, part of information theory.\n\nDiscrete geometry (also called combinatorial geometry) also began as a part of combinatorics, with early results on convex polytopes and kissing numbers.  With the emergence of applications of discrete geometry to computational geometry, these two fields partially merged and became a separate field of study.  There remain many connections with geometric and topological combinatorics, which themselves can be viewed as outgrowths of the early discrete geometry.\n\nCombinatorial aspects of dynamical systems is another emerging field.  Here dynamical systems can be defined on combinatorial objects.  See for example\ngraph dynamical system.\n\nThere are increasing interactions between combinatorics and physics, particularly statistical physics. Examples include an exact solution of the Ising model, and a connection between the Potts model on one hand, and the chromatic and Tutte polynomials on the other hand.\n"
    },
    {
        "title": "Ulam spiral",
        "content": "The Ulam spiral or prime spiral is a graphical depiction of the set of prime numbers, devised by mathematician Stanisław Ulam in 1963 and popularized in Martin Gardner's Mathematical Games column in Scientific American a short time later.[1] It is constructed by writing the positive integers in a square spiral and specially marking the prime numbers.\n\nUlam and Gardner emphasized the striking appearance in the spiral of prominent diagonal, horizontal, and vertical lines containing large numbers of primes. Both Ulam and Gardner noted that the existence of such prominent lines is not unexpected, as lines in the spiral correspond to quadratic polynomials, and certain such polynomials, such as Euler's prime-generating polynomial x2 − x + 41, are believed to produce a high density of prime numbers.[2][3]  Nevertheless, the Ulam spiral is connected with major unsolved problems in number theory such as Landau's problems.  In particular, no quadratic polynomial has ever been proved to generate infinitely many primes, much less to have a high asymptotic density of them, although there is a well-supported conjecture as to what that asymptotic density should be.\n\nIn 1932, 31 years prior to Ulam's discovery, the herpetologist Laurence Klauber constructed a triangular, non-spiral array containing vertical and diagonal lines exhibiting a similar concentration of prime numbers.  Like Ulam, Klauber noted the connection with prime-generating polynomials, such as Euler's.[4]\n\nThe Ulam spiral is constructed by writing the positive integers in a spiral arrangement on a square lattice:\n\nand then marking the prime numbers:\n\nIn the figure, primes appear to concentrate along certain diagonal lines.  In the 201×201 Ulam spiral shown above, diagonal lines are clearly visible, confirming the pattern to that point. Horizontal and vertical lines with a high density of primes, while less prominent, are also evident.  Most often, the number spiral is started with the number 1 at the center, but it is possible to start with any number, and the same concentration of primes along diagonal, horizontal, and vertical lines is observed. Starting with 41 at the center gives a diagonal containing an unbroken string of 40 primes (starting from 1523 southwest of the origin, decreasing to 41 at the origin, and increasing to 1601 northeast of the origin), the longest example of its kind.[5]\n\nAccording to Gardner, Ulam discovered the spiral in 1963 while doodling during the presentation of \"a long and very boring paper\" at a scientific meeting.[1]  These hand calculations amounted to \"a few hundred points\".  Shortly afterwards, Ulam, with collaborators Myron Stein and Mark Wells, used MANIAC II at Los Alamos Scientific Laboratory to extend the calculation to about 100,000 points.  The group also computed the density of primes among numbers up to 10,000,000 along some of the prime-rich lines as well as along some of the prime-poor lines.  Images of the spiral up to 65,000 points were displayed on \"a scope attached to the machine\" and then photographed.[6]  The Ulam spiral was described in Martin Gardner's March 1964 Mathematical Games column in Scientific American  and featured on the front cover of that issue.  Some of the photographs of Stein, Ulam, and Wells were reproduced in the column.\n\nIn an addendum to the Scientific American column, Gardner mentioned the earlier paper of Klauber.[7][8]\nKlauber describes his construction as follows, \"The integers are arranged in triangular order with 1 at the apex, the second line containing numbers 2 to 4, the third 5 to 9, and so forth.  When the primes have been indicated, it is found that there are concentrations in certain vertical and diagonal lines, and amongst these the so-called Euler sequences with high concentrations of primes are discovered.\"[4]\n\nDiagonal, horizontal, and vertical lines in the number spiral correspond to polynomials of the form\n\nwhere b and c are integer constants.  When b is even, the lines are diagonal, and either all numbers are odd, or all are even, depending on the value of c.  It is therefore no surprise that all primes other than 2 lie in alternate diagonals of the Ulam spiral.  Some  polynomials, such as \n\n\n\n4\n\nn\n\n2\n\n\n+\n8\nn\n+\n3\n\n\n{\\displaystyle 4n^{2}+8n+3}\n\n, while producing only odd values, factorize over the integers \n\n\n\n(\n4\n\nn\n\n2\n\n\n+\n8\nn\n+\n3\n)\n=\n(\n2\nn\n+\n1\n)\n(\n2\nn\n+\n3\n)\n\n\n{\\displaystyle (4n^{2}+8n+3)=(2n+1)(2n+3)}\n\n and are therefore never prime except possibly when one of the factors equals 1. Such examples correspond to diagonals that are devoid of primes or nearly so.\n\nTo gain insight into why some of the remaining odd diagonals may have a higher concentration of primes than others, consider \n\n\n\n4\n\nn\n\n2\n\n\n+\n6\nn\n+\n1\n\n\n{\\displaystyle 4n^{2}+6n+1}\n\n and \n\n\n\n4\n\nn\n\n2\n\n\n+\n6\nn\n+\n5\n\n\n{\\displaystyle 4n^{2}+6n+5}\n\n. Compute remainders upon division by 3 as n takes successive values 0, 1, 2, ....  For the first of these polynomials, the sequence of remainders is 1, 2, 2, 1, 2, 2, ..., while for the second, it is 2, 0, 0, 2, 0, 0, .... This implies that in the sequence of values taken by the second polynomial, two out of every three are divisible by 3, and hence certainly not prime, while in the sequence of values taken by the first polynomial, none are divisible by 3. Thus it seems plausible that the first polynomial will produce values with a higher density of primes than will the second. At the very least, this observation gives little reason to believe that the corresponding diagonals will be equally dense with primes. One should, of course, consider divisibility by primes other than 3. Examining divisibility by 5 as well, remainders upon division by 15 repeat with pattern 1, 11, 14, 10, 14, 11, 1, 14, 5, 4, 11, 11, 4, 5, 14 for the first polynomial, and with pattern 5, 0, 3, 14, 3, 0, 5, 3, 9, 8, 0, 0, 8, 9, 3 for the second, implying that only three out of 15 values in the second sequence are potentially prime (being divisible by neither 3 nor 5), while 12 out of 15 values in the first sequence are potentially prime (since only three are divisible by 5 and none are divisible by 3).\n\nWhile rigorously-proved results about primes in quadratic sequences are scarce, considerations like those above give rise to a plausible conjecture on the asymptotic density of primes in such sequences, which is described in the next section.\n\nIn their 1923 paper on the Goldbach Conjecture, Hardy and Littlewood stated a series of conjectures, one of which, if true, would explain some of the striking features of the Ulam spiral. This conjecture, which Hardy and Littlewood called \"Conjecture F\", is a special case of the Bateman–Horn conjecture and asserts an asymptotic formula for the number of primes of the form ax2 + bx + c. Rays emanating from the central region of the Ulam spiral making angles of 45° with the horizontal and vertical correspond to numbers of the form 4x2 + bx + c with b even; horizontal and vertical rays correspond to numbers of the same form with b odd. Conjecture F provides a formula that can be used to estimate the density of primes along such rays. It implies that there will be considerable variability in the density along different rays. In particular, the density is highly sensitive to the discriminant of the polynomial, b2 − 16c.\n\nConjecture F is concerned with polynomials of the form ax2 + bx + c where a, b, and c are integers and a is positive. If the coefficients contain a common factor greater than 1 or if the discriminant Δ = b2 − 4ac is a perfect square, the polynomial factorizes and therefore produces composite numbers as x takes the values 0, 1, 2, ... (except possibly for one or two values of x where one of the factors equals 1). Moreover, if a + b and c are both even, the polynomial produces only even values, and is therefore composite except possibly for the value 2. Hardy and Littlewood assert that, apart from these situations, ax2 + bx + c takes prime values infinitely often as x takes the values 0, 1, 2, ... This statement is a special case of an earlier conjecture of Bunyakovsky and remains open. Hardy and Littlewood further assert that, asymptotically, the number P(n) of primes of the form ax2 + bx + c and less than n is given by\n\nwhere A depends on a, b, and c but not on n. By the prime number theorem, this formula with A set equal to one is the asymptotic number of primes less than n expected in a random set of numbers having the same density as the set of numbers of the form ax2 + bx + c. But since A can take values bigger or smaller than 1, some polynomials, according to the conjecture, will be especially rich in primes, and others especially poor. An unusually rich polynomial is 4x2 − 2x + 41 which forms a visible line in the Ulam spiral. The constant A for this polynomial is approximately 6.6, meaning that the numbers it generates are almost seven times as likely to be prime as random numbers of comparable size, according to the conjecture. This particular polynomial is related to Euler's prime-generating polynomial x2 − x + 41 by replacing x with 2x, or equivalently, by restricting x to the even numbers. The constant A is given by a product running over all prime numbers,\n\nin which \n\n\n\nω\n(\np\n)\n\n\n{\\displaystyle \\omega (p)}\n\n is number of zeros of the quadratic polynomial modulo p and therefore takes one of the values 0, 1, or 2.  Hardy and Littlewood break the product into three factors as\n\nHere the factor ε, corresponding to the prime 2, is 1 if a + b is odd and 2 if a + b is even. The first product index p runs over the finitely-many odd primes dividing both a and b.  For these primes \n\n\n\nω\n(\np\n)\n=\n0\n\n\n{\\displaystyle \\omega (p)=0}\n\n since  p then cannot divide c. The second product index \n\n\n\nϖ\n\n\n{\\displaystyle \\varpi }\n\n runs over the infinitely-many odd primes not dividing a. For these primes \n\n\n\nω\n(\np\n)\n\n\n{\\displaystyle \\omega (p)}\n\n equals 1, 2, or 0 depending on whether the discriminant is 0, a non-zero square, or a non-square modulo p. This is accounted for by the use of the Legendre symbol, \n\n\n\n\n(\n\n\nΔ\nϖ\n\n\n)\n\n\n\n{\\displaystyle \\left({\\frac {\\Delta }{\\varpi }}\\right)}\n\n.  When a prime p divides a but not b there is one root modulo p.  Consequently, such primes do not contribute to the product.\n\nA quadratic polynomial with A ≈ 11.3, currently the highest known value, has been discovered by Jacobson and Williams.[9][10]\n\nKlauber's 1932 paper describes a triangle in which row n contains the numbers (n  −  1)2 + 1 through n2. As in the Ulam spiral, quadratic polynomials generate numbers that lie in straight lines. Vertical lines correspond to numbers of the form k2 − k + M. Vertical and diagonal lines with a high density of prime numbers are evident in the figure.\n\nRobert Sacks devised a variant of the Ulam spiral in 1994. In the Sacks spiral, the non-negative integers are plotted on an Archimedean spiral rather than the square spiral used by Ulam, and are spaced so that one perfect square occurs in each full rotation. (In the Ulam spiral, two squares occur in each rotation.) Euler's prime-generating polynomial, x2 − x + 41, now appears as a single curve as x takes the values 0, 1, 2, ... This curve asymptotically approaches a horizontal line in the left half of the figure. (In the Ulam spiral, Euler's polynomial forms two diagonal lines, one in the top half of the figure, corresponding to even values of x in the sequence, the other in the bottom half of the figure corresponding to odd values of x in the sequence.)\n\nAdditional structure may be seen when composite numbers are also included in the Ulam spiral. The number 1 has only a single factor, itself; each prime number has two factors, itself and 1; composite numbers are divisible by at least three different factors. Using the size of the dot representing an integer to indicate the number of factors and coloring prime numbers red and composite numbers blue produces the figure shown.\n\nSpirals following other tilings of the plane also generate lines rich in prime numbers, for example hexagonal spirals.\n\n\n"
    },
    {
        "title": "Prime number",
        "content": "\n\nA prime number (or a prime) is a natural number greater than 1 that is not a product of two smaller natural numbers. A natural number greater than 1 that is not prime is called a composite number. For example, 5 is prime because the only ways of writing it as a product, 1 × 5 or 5 × 1, involve 5 itself. However, 4 is composite because it is a product (2 × 2) in which both numbers are smaller than 4. Primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than 1 is either a prime itself or can be factorized as a product of primes that is unique up to their order.\n\nThe property of being prime is called primality. A simple but slow method of checking the primality of a given number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, called trial division, tests whether \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is a multiple of any integer between 2 and \n\n\n\n\n\nn\n\n\n\n\n{\\displaystyle {\\sqrt {n}}}\n\n. Faster algorithms include the Miller–Rabin primality test, which is fast but has a small chance of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical. Particularly fast methods are available for numbers of special forms, such as Mersenne numbers. As of October 2024[update] the largest known prime number is a Mersenne prime with 41,024,320 decimal digits.[1][2]\n\nThere are infinitely many primes, as demonstrated by Euclid around 300 BC. No known simple formula separates prime numbers from composite numbers. However, the distribution of primes within the natural numbers in the large can be statistically modelled. The first result in that direction is the prime number theorem, proven at the end of the 19th century, which says roughly that the probability of a randomly chosen large number being prime is inversely proportional to its number of digits, that is, to its logarithm.\n\nSeveral historical questions regarding prime numbers are still unsolved. These include Goldbach's conjecture, that every even integer greater than 2 can be expressed as the sum of two primes, and the twin prime conjecture, that there are infinitely many pairs of primes that differ by two. Such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. Primes are used in several routines in information technology, such as public-key cryptography, which relies on the difficulty of factoring large numbers into their prime factors. In abstract algebra, objects that behave in a generalized way like prime numbers include prime elements and prime ideals.\n\nA natural number (1, 2, 3, 4, 5, 6, etc.) is called a prime number (or a prime) if it is greater than 1 and cannot be written as the product of two smaller natural numbers. The numbers greater than 1 that are not prime are called composite numbers.[3] In other words, \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is prime if \n\n\n\nn\n\n\n{\\displaystyle n}\n\n items cannot be divided up into smaller equal-size groups of more than one item,[4] or if it is not possible to arrange \n\n\n\nn\n\n\n{\\displaystyle n}\n\n dots into a rectangular grid that is more than one dot wide and more than one dot high.[5] For example, among the numbers 1 through 6, the numbers 2, 3, and 5 are the prime numbers,[6] as there are no other numbers that divide them evenly (without a remainder). 1 is not prime, as it is specifically excluded in the definition. 4 = 2 × 2 and 6 = 2 × 3 are both composite.\n\nThe divisors of a natural number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n are the natural numbers that divide \n\n\n\nn\n\n\n{\\displaystyle n}\n\n evenly. Every natural number has both 1 and itself as a divisor. If it has any other divisor, it cannot be prime. This leads to an equivalent definition of prime numbers: they are the numbers with exactly two positive divisors. Those two are 1 and the number itself. As 1 has only one divisor, itself, it is not prime by this definition.[7] Yet another way to express the same thing is that a number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is prime if it is greater than one and if none of the numbers \n\n\n\n2\n,\n3\n,\n…\n,\nn\n−\n1\n\n\n{\\displaystyle 2,3,\\dots ,n-1}\n\n divides \n\n\n\nn\n\n\n{\\displaystyle n}\n\n evenly.[8]\n\nThe first 25 prime numbers (all the prime numbers less than 100) are:[9]\n\nNo even number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n greater than 2 is prime because any such number can be expressed as the product \n\n\n\n2\n×\nn\n\n/\n\n2\n\n\n{\\displaystyle 2\\times n/2}\n\n. Therefore, every prime number other than 2 is an odd number, and is called an odd prime.[10] Similarly, when written in the usual decimal system, all prime numbers larger than 5 end in 1, 3, 7, or 9. The numbers that end with other digits are all composite: decimal numbers that end in 0, 2, 4, 6, or 8 are even, and decimal numbers that end in 0 or 5 are divisible by 5.[11]\n\nThe set of all primes is sometimes denoted by \n\n\n\n\nP\n\n\n\n{\\displaystyle \\mathbf {P} }\n\n (a boldface capital P)[12] or by \n\n\n\n\nP\n\n\n\n{\\displaystyle \\mathbb {P} }\n\n (a blackboard bold capital P).[13]\n\nThe Rhind Mathematical Papyrus, from around 1550 BC, has Egyptian fraction expansions of different forms for prime and composite numbers.[14] However, the earliest surviving records of the study of prime numbers come from the ancient Greek mathematicians, who called them prōtos arithmòs (πρῶτος ἀριθμὸς). Euclid's Elements (c. 300 BC) proves the infinitude of primes and the fundamental theorem of arithmetic, and shows how to construct a perfect number from a Mersenne prime.[15] Another Greek invention, the Sieve of Eratosthenes, is still used to construct lists of primes.[16][17]\n\nAround 1000 AD, the Islamic mathematician Ibn al-Haytham (Alhazen) found Wilson's theorem, characterizing the prime numbers as the numbers \n\n\n\nn\n\n\n{\\displaystyle n}\n\n that evenly divide \n\n\n\n(\nn\n−\n1\n)\n!\n+\n1\n\n\n{\\displaystyle (n-1)!+1}\n\n. He also conjectured that all even perfect numbers come from Euclid's construction using Mersenne primes, but was unable to prove it.[18] Another Islamic mathematician, Ibn al-Banna' al-Marrakushi, observed that the sieve of Eratosthenes can be sped up by considering only the prime divisors up to the square root of the upper limit.[17] Fibonacci took the innovations from Islamic mathematics to Europe. His book Liber Abaci (1202) was the first to describe trial division for testing primality, again using divisors only up to the square root.[17]\n\nIn 1640 Pierre de Fermat stated (without proof) Fermat's little theorem (later proved by Leibniz and Euler).[19] Fermat also investigated the primality of the Fermat numbers \n\n\n\n\n2\n\n\n2\n\nn\n\n\n\n\n+\n1\n\n\n{\\displaystyle 2^{2^{n}}+1}\n\n,[20] and Marin Mersenne studied the Mersenne primes, prime numbers of the form \n\n\n\n\n2\n\np\n\n\n−\n1\n\n\n{\\displaystyle 2^{p}-1}\n\n with \n\n\n\np\n\n\n{\\displaystyle p}\n\n itself a prime.[21] Christian Goldbach formulated Goldbach's conjecture, that every even number is the sum of two primes, in a 1742 letter to Euler.[22] Euler proved Alhazen's conjecture (now the Euclid–Euler theorem) that all even perfect numbers can be constructed from Mersenne primes.[15] He introduced methods from mathematical analysis to this area in his proofs of the infinitude of the primes and the divergence of the sum of the reciprocals of the primes \n\n\n\n\n\n\n1\n2\n\n\n\n+\n\n\n\n1\n3\n\n\n\n+\n\n\n\n1\n5\n\n\n\n+\n\n\n\n1\n7\n\n\n\n+\n\n\n\n1\n11\n\n\n\n+\n⋯\n\n\n{\\displaystyle {\\tfrac {1}{2}}+{\\tfrac {1}{3}}+{\\tfrac {1}{5}}+{\\tfrac {1}{7}}+{\\tfrac {1}{11}}+\\cdots }\n\n.[23] At the start of the 19th century, Legendre and Gauss conjectured that as \n\n\n\nx\n\n\n{\\displaystyle x}\n\n tends to infinity, the number of primes up to \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is asymptotic to \n\n\n\nx\n\n/\n\nlog\n⁡\nx\n\n\n{\\displaystyle x/\\log x}\n\n, where \n\n\n\nlog\n⁡\nx\n\n\n{\\displaystyle \\log x}\n\n is the natural logarithm of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. A weaker consequence of this high density of primes was Bertrand's postulate, that for every \n\n\n\nn\n>\n1\n\n\n{\\displaystyle n>1}\n\n there is a prime between \n\n\n\nn\n\n\n{\\displaystyle n}\n\n and \n\n\n\n2\nn\n\n\n{\\displaystyle 2n}\n\n, proved in 1852 by Pafnuty Chebyshev.[24] Ideas of Bernhard Riemann in his 1859 paper on the zeta-function sketched an outline for proving the conjecture of Legendre and Gauss. Although the closely related Riemann hypothesis remains unproven, Riemann's outline was completed in 1896 by Hadamard and de la Vallée Poussin, and the result is now known as the prime number theorem.[25] Another important 19th century result was Dirichlet's theorem on arithmetic progressions, that certain arithmetic progressions contain infinitely many primes.[26]\n\nMany mathematicians have worked on primality tests for numbers larger than those where trial division is practicably applicable. Methods that are restricted to specific number forms include Pépin's test for Fermat numbers (1877),[27] Proth's theorem (c. 1878),[28] the Lucas–Lehmer primality test (originated 1856), and the generalized Lucas primality test.[17]\n\nSince 1951 all the largest known primes have been found using these tests on computers.[a] The search for ever larger primes has generated interest outside mathematical circles, through the Great Internet Mersenne Prime Search and other distributed computing projects.[9][30] The idea that prime numbers had few applications outside of pure mathematics[b] was shattered in the 1970s when public-key cryptography and the RSA cryptosystem were invented, using prime numbers as their basis.[33]\n\nThe increased practical importance of computerized primality testing and factorization led to the development of improved methods capable of handling large numbers of unrestricted form.[16][34][35] The mathematical theory of prime numbers also moved forward with the Green–Tao theorem (2004) that there are arbitrarily long arithmetic progressions of prime numbers, and Yitang Zhang's 2013 proof that there exist infinitely many prime gaps of bounded size.[36]\n\nMost early Greeks did not even consider 1 to be a number,[37][38] so they could not consider its primality. A few scholars in the Greek and later Roman tradition, including Nicomachus, Iamblichus, Boethius, and Cassiodorus, also considered the prime numbers to be a subdivision of the odd numbers, so they did not consider 2 to be prime either. However, Euclid and a majority of the other Greek mathematicians considered 2 as prime. The medieval Islamic mathematicians largely followed the Greeks in viewing 1 as not being a number.[37] By the Middle Ages and Renaissance, mathematicians began treating 1 as a number, and by the 17th century some of them included it as the first prime number.[39] In the mid-18th century, Christian Goldbach listed 1 as prime in his correspondence with Leonhard Euler;[40] however, Euler himself did not consider 1 to be prime.[41] Many 19th century mathematicians still considered 1 to be prime,[42] and Derrick Norman Lehmer included 1 in his list of primes less than ten million published in 1914.[43] Lists of primes that included 1 continued to be published as recently as 1956.[44][45] However, around this time, by the early 20th century, mathematicians started to agree that 1 should not be classified as a prime number.[42]\n\nIf 1 were to be considered a prime, many statements involving primes would need to be awkwardly reworded. For example, the fundamental theorem of arithmetic would need to be rephrased in terms of factorizations into primes greater than 1, because every number would have multiple factorizations with any number of copies of 1.[42] Similarly, the sieve of Eratosthenes would not work correctly if it handled 1 as a prime, because it would eliminate all multiples of 1 (that is, all other numbers) and output only the single number 1.[45] Some other more technical properties of prime numbers also do not hold for the number 1: for instance, the formulas for Euler's totient function or for the sum of divisors function are different for prime numbers than they are for 1.[46] By the early 20th century, mathematicians began to agree that 1 should not be listed as prime, but rather in its own special category as a \"unit\".[42]\n\nWriting a number as a product of prime numbers is called a prime factorization of the number. For example:\n\nThe terms in the product are called prime factors. The same prime factor may occur more than once; this example has two copies of the prime factor \n\n\n\n5.\n\n\n{\\displaystyle 5.}\n\n When a prime occurs multiple times, exponentiation can be used to group together multiple copies of the same prime number: for example, in the second way of writing the product above, \n\n\n\n\n5\n\n2\n\n\n\n\n{\\displaystyle 5^{2}}\n\n denotes the square or second power of \n\n\n\n5.\n\n\n{\\displaystyle 5.}\n\n\n\nThe central importance of prime numbers to number theory and mathematics in general stems from the fundamental theorem of arithmetic.[47] This theorem states that every integer larger than 1 can be written as a product of one or more primes. More strongly, this product is unique in the sense that any two prime factorizations of the same number will have the same numbers of copies of the same primes, although their ordering may differ.[48] So, although there are many different ways of finding a factorization using an integer factorization algorithm, they all must produce the same result. Primes can thus be considered the \"basic building blocks\" of the natural numbers.[49]\n\nSome proofs of the uniqueness of prime factorizations are based on Euclid's lemma: If \n\n\n\np\n\n\n{\\displaystyle p}\n\n is a prime number and \n\n\n\np\n\n\n{\\displaystyle p}\n\n divides a product \n\n\n\na\nb\n\n\n{\\displaystyle ab}\n\n of integers \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n,\n\n\n{\\displaystyle b,}\n\n then \n\n\n\np\n\n\n{\\displaystyle p}\n\n divides \n\n\n\na\n\n\n{\\displaystyle a}\n\n or \n\n\n\np\n\n\n{\\displaystyle p}\n\n divides \n\n\n\nb\n\n\n{\\displaystyle b}\n\n (or both).[50] Conversely, if a number \n\n\n\np\n\n\n{\\displaystyle p}\n\n has the property that when it divides a product it always divides at least one factor of the product, then \n\n\n\np\n\n\n{\\displaystyle p}\n\n must be prime.[51]\n\nThere are infinitely many prime numbers. Another way of saying this is that the sequence\n\nof prime numbers never ends. This statement is referred to as Euclid's theorem in honor of the ancient Greek mathematician Euclid, since the first known proof for this statement is attributed to him. Many more proofs of the infinitude of primes are known, including an analytical proof by Euler, Goldbach's proof based on Fermat numbers,[52] Furstenberg's proof using general topology,[53] and Kummer's elegant proof.[54]\n\nEuclid's proof[55] shows that every finite list of primes is incomplete. The key idea is to multiply together the primes in any given list and add \n\n\n\n1.\n\n\n{\\displaystyle 1.}\n\n If the list consists of the primes \n\n\n\n\np\n\n1\n\n\n,\n\np\n\n2\n\n\n,\n…\n,\n\np\n\nn\n\n\n,\n\n\n{\\displaystyle p_{1},p_{2},\\ldots ,p_{n},}\n\n this gives the number\n\nBy the fundamental theorem, \n\n\n\nN\n\n\n{\\displaystyle N}\n\n has a prime factorization\n\nwith one or more prime factors. \n\n\n\nN\n\n\n{\\displaystyle N}\n\n is evenly divisible by each of these factors, but \n\n\n\nN\n\n\n{\\displaystyle N}\n\n has a remainder of one when divided by any of the prime numbers in the given list, so none of the prime factors of \n\n\n\nN\n\n\n{\\displaystyle N}\n\n can be in the given list. Because there is no finite list of all the primes, there must be infinitely many primes.\n\nThe numbers formed by adding one to the products of the smallest primes are called Euclid numbers.[56] The first five of them are prime, but the sixth,\n\nis a composite number.\n\nThere is no known efficient formula for primes. For example, there is no non-constant polynomial, even in several variables, that takes only prime values.[57] However, there are numerous expressions that do encode all primes, or only primes. One possible formula is based on Wilson's theorem and generates the number 2 many times and all other primes exactly once.[58] There is also a set of Diophantine equations in nine variables and one parameter with the following property: the parameter is prime if and only if the resulting system of equations has a solution over the natural numbers. This can be used to obtain a single formula with the property that all its positive values are prime.[57]\n\nOther examples of prime-generating formulas come from Mills' theorem and a theorem of Wright. These assert that there are real constants \n\n\n\nA\n>\n1\n\n\n{\\displaystyle A>1}\n\n and \n\n\n\nμ\n\n\n{\\displaystyle \\mu }\n\n such that\n\nare prime for any natural number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n in the first formula, and any number of exponents in the second formula.[59] Here \n\n\n\n⌊\n\n\n⋅\n\n\n⌋\n\n\n{\\displaystyle \\lfloor {}\\cdot {}\\rfloor }\n\n represents the floor function, the largest integer less than or equal to the number in question. However, these are not useful for generating primes, as the primes must be generated first in order to compute the values of \n\n\n\nA\n\n\n{\\displaystyle A}\n\n or \n\n\n\nμ\n.\n\n\n{\\displaystyle \\mu .}\n\n[57]\n\nMany conjectures revolving about primes have been posed. Often having an elementary formulation, many of these conjectures have withstood proof for decades: all four of Landau's problems from 1912 are still unsolved.[60] One of them is Goldbach's conjecture, which asserts that every even integer \n\n\n\nn\n\n\n{\\displaystyle n}\n\n greater than 2 can be written as a sum of two primes.[61] As of 2014[update], this conjecture has been verified for all numbers up to \n\n\n\nn\n=\n4\n⋅\n\n10\n\n18\n\n\n.\n\n\n{\\displaystyle n=4\\cdot 10^{18}.}\n\n[62] Weaker statements than this have been proven; for example, Vinogradov's theorem says that every sufficiently large odd integer can be written as a sum of three primes.[63] Chen's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime (the product of two primes).[64] Also, any even integer greater than 10 can be written as the sum of six primes.[65] The branch of number theory studying such questions is called additive number theory.[66]\n\nAnother type of problem concerns prime gaps, the differences between consecutive primes.\nThe existence of arbitrarily large prime gaps can be seen by noting that the sequence \n\n\n\nn\n!\n+\n2\n,\nn\n!\n+\n3\n,\n…\n,\nn\n!\n+\nn\n\n\n{\\displaystyle n!+2,n!+3,\\dots ,n!+n}\n\n consists of \n\n\n\nn\n−\n1\n\n\n{\\displaystyle n-1}\n\n composite numbers, for any natural number \n\n\n\nn\n.\n\n\n{\\displaystyle n.}\n\n[67] However, large prime gaps occur much earlier than this argument shows.[68] For example, the first prime gap of length 8 is between the primes 89 and 97,[69] much smaller than \n\n\n\n8\n!\n=\n40320.\n\n\n{\\displaystyle 8!=40320.}\n\n It is conjectured that there are infinitely many twin primes, pairs of primes with difference 2; this is the twin prime conjecture. Polignac's conjecture states more generally that for every positive integer \n\n\n\nk\n,\n\n\n{\\displaystyle k,}\n\n there are infinitely many pairs of consecutive primes that differ by \n\n\n\n2\nk\n.\n\n\n{\\displaystyle 2k.}\n\n[70]\nAndrica's conjecture,[70] Brocard's conjecture,[71] Legendre's conjecture,[72] and Oppermann's conjecture[71] all suggest that the largest gaps between primes from \n\n\n\n1\n\n\n{\\displaystyle 1}\n\n to \n\n\n\nn\n\n\n{\\displaystyle n}\n\n should be at most approximately \n\n\n\n\n\nn\n\n\n,\n\n\n{\\displaystyle {\\sqrt {n}},}\n\n a result that is known to follow from the Riemann hypothesis, while the much stronger Cramér conjecture sets the largest gap size at \n\n\n\nO\n(\n(\nlog\n⁡\nn\n\n)\n\n2\n\n\n)\n.\n\n\n{\\displaystyle O((\\log n)^{2}).}\n\n[70] Prime gaps can be generalized to prime \n\n\n\nk\n\n\n{\\displaystyle k}\n\n-tuples, patterns in the differences among more than two prime numbers. Their infinitude and density are the subject of the first Hardy–Littlewood conjecture, which can be motivated by the heuristic that the prime numbers behave similarly to a random sequence of numbers with density given by the prime number theorem.[73]\n\nAnalytic number theory studies number theory through the lens of continuous functions, limits, infinite series, and the related mathematics of the infinite and infinitesimal.\n\nThis area of study began with Leonhard Euler and his first major result, the solution to the Basel problem.\nThe problem asked for the value of the infinite sum \n\n\n\n1\n+\n\n\n\n1\n4\n\n\n\n+\n\n\n\n1\n9\n\n\n\n+\n\n\n\n1\n16\n\n\n\n+\n…\n,\n\n\n{\\displaystyle 1+{\\tfrac {1}{4}}+{\\tfrac {1}{9}}+{\\tfrac {1}{16}}+\\dots ,}\n\n\nwhich today can be recognized as the value \n\n\n\nζ\n(\n2\n)\n\n\n{\\displaystyle \\zeta (2)}\n\n of the Riemann zeta function. This function is closely connected to the prime numbers and to one of the most significant unsolved problems in mathematics, the Riemann hypothesis. Euler showed that \n\n\n\nζ\n(\n2\n)\n=\n\nπ\n\n2\n\n\n\n/\n\n6\n\n\n{\\displaystyle \\zeta (2)=\\pi ^{2}/6}\n\n.[74]\nThe reciprocal of this number, \n\n\n\n6\n\n/\n\n\nπ\n\n2\n\n\n\n\n{\\displaystyle 6/\\pi ^{2}}\n\n, is the limiting probability that two random numbers selected uniformly from a large range are relatively prime (have no factors in common).[75]\n\nThe distribution of primes in the large, such as the question how many primes are smaller than a given, large threshold, is described by the prime number theorem, but no efficient formula for the \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-th prime is known. Dirichlet's theorem on arithmetic progressions, in its basic form, asserts that linear polynomials\n\nwith relatively prime integers \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n take infinitely many prime values. Stronger forms of the theorem state that the sum of the reciprocals of these prime values diverges, and that different linear polynomials with the same \n\n\n\nb\n\n\n{\\displaystyle b}\n\n have approximately the same proportions of primes.\nAlthough conjectures have been formulated about the proportions of primes in higher-degree polynomials, they remain unproven, and it is unknown whether there exists a quadratic polynomial that (for integer arguments) is prime infinitely often.\n\nEuler's proof that there are infinitely many primes considers the sums of reciprocals of primes,\n\nEuler showed that, for any arbitrary real number \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, there exists a prime \n\n\n\np\n\n\n{\\displaystyle p}\n\n for which this sum is bigger than \n\n\n\nx\n\n\n{\\displaystyle x}\n\n.[76] This shows that there are infinitely many primes, because if there were finitely many primes the sum would reach its maximum value at the biggest prime rather than growing past every \n\n\n\nx\n\n\n{\\displaystyle x}\n\n.\nThe growth rate of this sum is described more precisely by Mertens' second theorem.[77] For comparison, the sum\n\ndoes not grow to infinity as \n\n\n\nn\n\n\n{\\displaystyle n}\n\n goes to infinity (see the Basel problem). In this sense, prime numbers occur more often than squares of natural numbers,\nalthough both sets are infinite.[78] Brun's theorem states that the sum of the reciprocals of twin primes,\n\nis finite. Because of Brun's theorem, it is not possible to use Euler's method to solve the twin prime conjecture, that there exist infinitely many twin primes.[78]\n\nThe prime-counting function \n\n\n\nπ\n(\nn\n)\n\n\n{\\displaystyle \\pi (n)}\n\n is defined as the number of primes not greater than \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.[79] For example, \n\n\n\nπ\n(\n11\n)\n=\n5\n\n\n{\\displaystyle \\pi (11)=5}\n\n, since there are five primes less than or equal to 11. Methods such as the Meissel–Lehmer algorithm can compute exact values of \n\n\n\nπ\n(\nn\n)\n\n\n{\\displaystyle \\pi (n)}\n\n faster than it would be possible to list each prime up to \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.[80] The prime number theorem states that \n\n\n\nπ\n(\nn\n)\n\n\n{\\displaystyle \\pi (n)}\n\n is asymptotic to \n\n\n\nn\n\n/\n\nlog\n⁡\nn\n\n\n{\\displaystyle n/\\log n}\n\n, which is denoted as\n\nand means that the ratio of \n\n\n\nπ\n(\nn\n)\n\n\n{\\displaystyle \\pi (n)}\n\n to the right-hand fraction approaches 1 as \n\n\n\nn\n\n\n{\\displaystyle n}\n\n grows to infinity.[81] This implies that the likelihood that a randomly chosen number less than \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is prime is (approximately) inversely proportional to the number of digits in \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.[82]\nIt also implies that the \n\n\n\nn\n\n\n{\\displaystyle n}\n\nth prime number is proportional to \n\n\n\nn\nlog\n⁡\nn\n\n\n{\\displaystyle n\\log n}\n\n[83]\nand therefore that the average size of a prime gap is proportional to \n\n\n\nlog\n⁡\nn\n\n\n{\\displaystyle \\log n}\n\n.[68]\nA more accurate estimate for \n\n\n\nπ\n(\nn\n)\n\n\n{\\displaystyle \\pi (n)}\n\n is given by the offset logarithmic integral[81]\n\nAn arithmetic progression is a finite or infinite sequence of numbers such that consecutive numbers in the sequence all have the same difference.[84] This difference is called the modulus of the progression.[85] For example,\n\nis an infinite arithmetic progression with modulus 9. In an arithmetic progression, all the numbers have the same remainder when divided by the modulus; in this example, the remainder is 3. Because both the modulus 9 and the remainder 3 are multiples of 3, so is every element in the sequence. Therefore, this progression contains only one prime number, 3 itself. In general, the infinite progression\n\ncan have more than one prime only when its remainder \n\n\n\na\n\n\n{\\displaystyle a}\n\n and modulus \n\n\n\nq\n\n\n{\\displaystyle q}\n\n are relatively prime. If they are relatively prime, Dirichlet's theorem on arithmetic progressions asserts that the progression contains infinitely many primes.[86]\n\nThe Green–Tao theorem shows that there are arbitrarily long finite arithmetic progressions consisting only of primes.[36][87]\n\nEuler noted that the function\n\nyields prime numbers for \n\n\n\n1\n≤\nn\n≤\n40\n\n\n{\\displaystyle 1\\leq n\\leq 40}\n\n, although composite numbers appear among its later values.[88][89] The search for an explanation for this phenomenon led to the deep algebraic number theory of Heegner numbers and the class number problem.[90] The Hardy–Littlewood conjecture F predicts the density of primes among the values of quadratic polynomials with integer coefficients in terms of the logarithmic integral and the polynomial coefficients. No quadratic polynomial has been proven to take infinitely many prime values.[91]\n\nThe Ulam spiral[92] arranges the natural numbers in a two-dimensional grid, spiraling in concentric squares surrounding the origin with the prime numbers highlighted. Visually, the primes appear to cluster on certain diagonals and not others, suggesting that some quadratic polynomials take prime values more often than others.[91]\n\nOne of the most famous unsolved questions in mathematics, dating from 1859, and one of the Millennium Prize Problems, is the Riemann hypothesis, which asks where the zeros of the Riemann zeta function \n\n\n\nζ\n(\ns\n)\n\n\n{\\displaystyle \\zeta (s)}\n\n are located.\nThis function is an analytic function on the complex numbers. For complex numbers \n\n\n\ns\n\n\n{\\displaystyle s}\n\n with real part greater than one it equals both an infinite sum over all integers, and an infinite product over the prime numbers,\n\nThis equality between a sum and a product, discovered by Euler, is called an Euler product.[93] The Euler product can be derived from the fundamental theorem of arithmetic, and shows the close connection between the zeta function and the prime numbers.[94]\nIt leads to another proof that there are infinitely many primes: if there were only finitely many,\nthen the sum-product equality would also be valid at \n\n\n\ns\n=\n1\n\n\n{\\displaystyle s=1}\n\n, but the sum would diverge (it is the harmonic series \n\n\n\n1\n+\n\n\n\n1\n2\n\n\n\n+\n\n\n\n1\n3\n\n\n\n+\n…\n\n\n{\\displaystyle 1+{\\tfrac {1}{2}}+{\\tfrac {1}{3}}+\\dots }\n\n) while the product would be finite, a contradiction.[95]\n\nThe Riemann hypothesis states that the zeros of the zeta-function are all either negative even numbers, or complex numbers with real part equal to 1/2.[96] The original proof of the prime number theorem was based on a weak form of this hypothesis, that there are no zeros with real part equal to 1,[97][98] although other more elementary proofs have been found.[99] The prime-counting function can be expressed by Riemann's explicit formula as a sum in which each term comes from one of the zeros of the zeta function; the main term of this sum is the logarithmic integral, and the remaining terms cause the sum to fluctuate above and below the main term.[100] In this sense, the zeros control how regularly the prime numbers are distributed. If the Riemann hypothesis is true, these fluctuations will be small, and the\nasymptotic distribution of primes given by the prime number theorem will also hold over much shorter intervals (of length about the square root of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n for intervals near a number \n\n\n\nx\n\n\n{\\displaystyle x}\n\n).[98]\n\nModular arithmetic modifies usual arithmetic by only using the numbers \n\n\n\n{\n0\n,\n1\n,\n2\n,\n…\n,\nn\n−\n1\n}\n\n\n{\\displaystyle \\{0,1,2,\\dots ,n-1\\}}\n\n, for a natural number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n called the modulus.\nAny other natural number can be mapped into this system by replacing it by its remainder after division by \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.[101] Modular sums, differences and products are calculated by performing the same replacement by the remainder on the result of the usual sum, difference, or product of integers.[102] Equality of integers corresponds to congruence in modular arithmetic: \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n are congruent (written \n\n\n\nx\n≡\ny\n\n\n{\\displaystyle x\\equiv y}\n\n mod \n\n\n\nn\n\n\n{\\displaystyle n}\n\n) when they have the same remainder after division by \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.[103] However, in this system of numbers, division by all nonzero numbers is possible if and only if the modulus is prime. For instance, with the prime number \n\n\n\n7\n\n\n{\\displaystyle 7}\n\n as modulus, division by \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n is possible: \n\n\n\n2\n\n/\n\n3\n≡\n3\n\nmod\n\n7\n\n\n\n\n{\\displaystyle 2/3\\equiv 3{\\bmod {7}}}\n\n, because clearing denominators by multiplying both sides by \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n gives the valid formula \n\n\n\n2\n≡\n9\n\nmod\n\n7\n\n\n\n\n{\\displaystyle 2\\equiv 9{\\bmod {7}}}\n\n. However, with the composite modulus \n\n\n\n6\n\n\n{\\displaystyle 6}\n\n, division by \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n is impossible. There is no valid solution to \n\n\n\n2\n\n/\n\n3\n≡\nx\n\nmod\n\n6\n\n\n\n\n{\\displaystyle 2/3\\equiv x{\\bmod {6}}}\n\n: clearing denominators by multiplying by \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n causes the left-hand side to become \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n while the right-hand side becomes either \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n or \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n. In the terminology of abstract algebra, the ability to perform division means that modular arithmetic modulo a prime number forms a field or, more specifically, a finite field, while other moduli only give a ring but not a field.[104]\n\nSeveral theorems about primes can be formulated using modular arithmetic. For instance, Fermat's little theorem states that if\n\n\n\n\na\n≢\n0\n\n\n{\\displaystyle a\\not \\equiv 0}\n\n (mod \n\n\n\np\n\n\n{\\displaystyle p}\n\n), then \n\n\n\n\na\n\np\n−\n1\n\n\n≡\n1\n\n\n{\\displaystyle a^{p-1}\\equiv 1}\n\n (mod \n\n\n\np\n\n\n{\\displaystyle p}\n\n).[105] Summing this over all choices of \n\n\n\na\n\n\n{\\displaystyle a}\n\n gives the equation\n\nvalid whenever \n\n\n\np\n\n\n{\\displaystyle p}\n\n is prime.\nGiuga's conjecture says that this equation is also a sufficient condition for \n\n\n\np\n\n\n{\\displaystyle p}\n\n to be prime.[106] Wilson's theorem says that an integer \n\n\n\np\n>\n1\n\n\n{\\displaystyle p>1}\n\n is prime if and only if the factorial \n\n\n\n(\np\n−\n1\n)\n!\n\n\n{\\displaystyle (p-1)!}\n\n is congruent to \n\n\n\n−\n1\n\n\n{\\displaystyle -1}\n\n mod \n\n\n\np\n\n\n{\\displaystyle p}\n\n. For a composite number \n\n\n\n\nn\n=\nr\n⋅\ns\n\n\n\n{\\displaystyle \\;n=r\\cdot s\\;}\n\n this cannot hold, since one of its factors divides both n and \n\n\n\n(\nn\n−\n1\n)\n!\n\n\n{\\displaystyle (n-1)!}\n\n, and so \n\n\n\n(\nn\n−\n1\n)\n!\n≡\n−\n1\n\n\n(\nmod\n\nn\n)\n\n\n\n{\\displaystyle (n-1)!\\equiv -1{\\pmod {n}}}\n\n is impossible.[107]\n\nThe \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic order \n\n\n\n\nν\n\np\n\n\n(\nn\n)\n\n\n{\\displaystyle \\nu _{p}(n)}\n\n of an integer \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is the number of copies of \n\n\n\np\n\n\n{\\displaystyle p}\n\n in the prime factorization of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. The same concept can be extended from integers to rational numbers by defining the \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic order of a fraction \n\n\n\nm\n\n/\n\nn\n\n\n{\\displaystyle m/n}\n\n to be \n\n\n\n\nν\n\np\n\n\n(\nm\n)\n−\n\nν\n\np\n\n\n(\nn\n)\n\n\n{\\displaystyle \\nu _{p}(m)-\\nu _{p}(n)}\n\n. The \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic absolute value \n\n\n\n\n|\n\nq\n\n\n|\n\n\np\n\n\n\n\n{\\displaystyle |q|_{p}}\n\n of any rational number \n\n\n\nq\n\n\n{\\displaystyle q}\n\n is then defined as\n\n\n\n\n\n|\n\nq\n\n\n|\n\n\np\n\n\n=\n\np\n\n−\n\nν\n\np\n\n\n(\nq\n)\n\n\n\n\n{\\displaystyle |q|_{p}=p^{-\\nu _{p}(q)}}\n\n. Multiplying an integer by its \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic absolute value cancels out the factors of \n\n\n\np\n\n\n{\\displaystyle p}\n\n in its factorization, leaving only the other primes. Just as the distance between two real numbers can be measured by the absolute value of their distance, the distance between two rational numbers can be measured by their \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic distance, the \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic absolute value of their difference. For this definition of distance, two numbers are close together (they have a small distance) when their difference is divisible by a high power of \n\n\n\np\n\n\n{\\displaystyle p}\n\n. In the same way that the real numbers can be formed from the rational numbers and their distances, by adding extra limiting values to form a complete field, the rational numbers with the \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic distance can be extended to a different complete field, the \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic numbers.[108][109]\n\nThis picture of an order, absolute value, and complete field derived from them can be generalized to algebraic number fields and their valuations (certain mappings from the multiplicative group of the field to a totally ordered additive group, also called orders), absolute values (certain multiplicative mappings from the field to the real numbers, also called norms),[108] and places (extensions to complete fields in which the given field is a dense set, also called completions).[110] The extension from the rational numbers to the real numbers, for instance, is a place in which the distance between numbers is the usual absolute value of their difference. The corresponding mapping to an additive group would be the logarithm of the absolute value, although this does not meet all the requirements of a valuation. According to Ostrowski's theorem, up to a natural notion of equivalence, the real numbers and \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adic numbers, with their orders and absolute values, are the only valuations, absolute values, and places on the rational numbers.[108] The local-global principle allows certain problems over the rational numbers to be solved by piecing together solutions from each of their places, again underlining the importance of primes to number theory.[111]\n\nA commutative ring is an algebraic structure where addition, subtraction and multiplication are defined. The integers are a ring, and the prime numbers in the integers have been generalized to rings in two different ways, prime elements and irreducible elements. An element \n\n\n\np\n\n\n{\\displaystyle p}\n\n of a ring \n\n\n\nR\n\n\n{\\displaystyle R}\n\n is called prime if it is nonzero, has no multiplicative inverse (that is, it is not a unit), and satisfies the following requirement: whenever \n\n\n\np\n\n\n{\\displaystyle p}\n\n divides the product \n\n\n\nx\ny\n\n\n{\\displaystyle xy}\n\n of two elements of \n\n\n\nR\n\n\n{\\displaystyle R}\n\n, it also divides at least one of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n or \n\n\n\ny\n\n\n{\\displaystyle y}\n\n. An element is irreducible if it is neither a unit nor the product of two other non-unit elements. In the ring of integers, the prime and irreducible elements form the same set,\n\nIn an arbitrary ring, all prime elements are irreducible. The converse does not hold in general, but does hold for unique factorization domains.[112]\n\nThe fundamental theorem of arithmetic continues to hold (by definition) in unique factorization domains. An example of such a domain is the Gaussian integers \n\n\n\n\nZ\n\n[\ni\n]\n\n\n{\\displaystyle \\mathbb {Z} [i]}\n\n, the ring of complex numbers of the form \n\n\n\na\n+\nb\ni\n\n\n{\\displaystyle a+bi}\n\n where \n\n\n\ni\n\n\n{\\displaystyle i}\n\n denotes the imaginary unit and \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n are arbitrary integers. Its prime elements are known as Gaussian primes. Not every number that is prime among the integers remains prime in the Gaussian integers; for instance, the number 2 can be written as a product of the two Gaussian primes \n\n\n\n1\n+\ni\n\n\n{\\displaystyle 1+i}\n\n and \n\n\n\n1\n−\ni\n\n\n{\\displaystyle 1-i}\n\n. Rational primes (the prime elements in the integers) congruent to 3 mod 4 are Gaussian primes, but rational primes congruent to 1 mod 4 are not.[113] This is a consequence of Fermat's theorem on sums of two squares,\nwhich states that an odd prime \n\n\n\np\n\n\n{\\displaystyle p}\n\n is expressible as the sum of two squares, \n\n\n\np\n=\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n\n\n{\\displaystyle p=x^{2}+y^{2}}\n\n, and therefore factorable as \n\n\n\np\n=\n(\nx\n+\ni\ny\n)\n(\nx\n−\ni\ny\n)\n\n\n{\\displaystyle p=(x+iy)(x-iy)}\n\n, exactly when \n\n\n\np\n\n\n{\\displaystyle p}\n\n is 1 mod 4.[114]\n\nNot every ring is a unique factorization domain. For instance, in the ring of numbers \n\n\n\na\n+\nb\n\n\n−\n5\n\n\n\n\n{\\displaystyle a+b{\\sqrt {-5}}}\n\n (for integers \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n) the number \n\n\n\n21\n\n\n{\\displaystyle 21}\n\n has two factorizations \n\n\n\n21\n=\n3\n⋅\n7\n=\n(\n1\n+\n2\n\n\n−\n5\n\n\n)\n(\n1\n−\n2\n\n\n−\n5\n\n\n)\n\n\n{\\displaystyle 21=3\\cdot 7=(1+2{\\sqrt {-5}})(1-2{\\sqrt {-5}})}\n\n, where neither of the four factors can be reduced any further, so it does not have a unique factorization. In order to extend unique factorization to a larger class of rings, the notion of a number can be replaced with that of an ideal, a subset of the elements of a ring that contains all sums of pairs of its elements, and all products of its elements with ring elements.\nPrime ideals, which generalize prime elements in the sense that the principal ideal generated by a prime element is a prime ideal, are an important tool and object of study in commutative algebra, algebraic number theory and algebraic geometry. The prime ideals of the ring of integers are the ideals (0), (2), (3), (5), (7), (11), ... The fundamental theorem of arithmetic generalizes to the Lasker–Noether theorem, which expresses every ideal in a Noetherian commutative ring as an intersection of primary ideals, which are the appropriate generalizations of prime powers.[115]\n\nThe spectrum of a ring is a geometric space whose points are the prime ideals of the ring.[116] Arithmetic geometry also benefits from this notion, and many concepts exist in both geometry and number theory. For example, factorization or ramification of prime ideals when lifted to an extension field, a basic problem of algebraic number theory, bears some resemblance with ramification in geometry. These concepts can even assist with in number-theoretic questions solely concerned with integers. For example, prime ideals in the ring of integers of quadratic number fields can be used in proving quadratic reciprocity, a statement that concerns the existence of square roots modulo integer prime numbers.[117] Early attempts to prove Fermat's Last Theorem led to Kummer's introduction of regular primes, integer prime numbers connected with the failure of unique factorization in the cyclotomic integers.[118] The question of how many integer prime numbers factor into a product of multiple prime ideals in an algebraic number field is addressed by Chebotarev's density theorem, which (when applied to the cyclotomic integers) has Dirichlet's theorem on primes in arithmetic progressions as a special case.[119]\n\nIn the theory of finite groups the Sylow theorems imply that, if a power of a prime number \n\n\n\n\np\n\nn\n\n\n\n\n{\\displaystyle p^{n}}\n\n divides the order of a group, then the group has a subgroup of order \n\n\n\n\np\n\nn\n\n\n\n\n{\\displaystyle p^{n}}\n\n. By Lagrange's theorem, any group of prime order is a cyclic group,\nand by Burnside's theorem any group whose order is divisible by only two primes is solvable.[120]\n\nFor a long time, number theory in general, and the study of prime numbers in particular, was seen as the canonical example of pure mathematics, with no applications outside of mathematics[b] other than the use of prime numbered gear teeth to distribute wear evenly.[121] In particular, number theorists such as British mathematician G. H. Hardy prided themselves on doing work that had absolutely no military significance.[122]\n\nThis vision of the purity of number theory was shattered in the 1970s, when it was publicly announced that prime numbers could be used as the basis for the creation of public-key cryptography algorithms.[33]\nThese applications have led to significant study of algorithms for computing with prime numbers, and in particular of primality testing, methods for determining whether a given number is prime. The most basic primality testing routine, trial division, is too slow to be useful for large numbers. One group of modern primality tests is applicable to arbitrary numbers, while more efficient tests are available for numbers of special types. Most primality tests only tell whether their argument is prime or not. Routines that also provide a prime factor of composite arguments (or all of its prime factors) are called factorization algorithms. Prime numbers are also used in computing for checksums, hash tables, and pseudorandom number generators.\n\nThe most basic method of checking the primality of a given integer \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is called trial division. This method divides \n\n\n\nn\n\n\n{\\displaystyle n}\n\n by each integer from 2 up to the square root of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. Any such integer dividing \n\n\n\nn\n\n\n{\\displaystyle n}\n\n evenly establishes \n\n\n\nn\n\n\n{\\displaystyle n}\n\n as composite; otherwise it is prime. Integers larger than the square root do not need to be checked because, whenever \n\n\n\nn\n=\na\n⋅\nb\n\n\n{\\displaystyle n=a\\cdot b}\n\n, one of the two factors \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n is less than or equal to the square root of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. Another optimization is to check only primes as factors in this range.[123] For instance, to check whether 37 is prime, this method divides it by the primes in the range from 2 to \n\n\n\n\n\n37\n\n\n\n\n{\\displaystyle {\\sqrt {37}}}\n\n, which are 2, 3, and 5. Each division produces a nonzero remainder, so 37 is indeed prime.\n\nAlthough this method is simple to describe, it is impractical for testing the primality of large integers, because the number of tests that it performs grows exponentially as a function of the number of digits of these integers.[124] However, trial division is still used, with a smaller limit than the square root on the divisor size, to quickly discover composite numbers with small factors, before using more complicated methods on the numbers that pass this filter.[125]\n\nBefore computers, mathematical tables listing all of the primes or prime factorizations up to a given limit were commonly printed.[126] The oldest known method for generating a list of primes is called the sieve of Eratosthenes.[127] The animation shows an optimized variant of this method.[128] Another more asymptotically efficient sieving method for the same problem is the sieve of Atkin.[129] In advanced mathematics, sieve theory applies similar methods to other problems.[130]\n\nSome of the fastest modern tests for whether an arbitrary given number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is prime are probabilistic (or Monte Carlo) algorithms, meaning that they have a small random chance of producing an incorrect answer.[131] For instance the Solovay–Strassen primality test on a given number \n\n\n\np\n\n\n{\\displaystyle p}\n\n chooses a number \n\n\n\na\n\n\n{\\displaystyle a}\n\n randomly from \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n through \n\n\n\np\n−\n2\n\n\n{\\displaystyle p-2}\n\n and uses modular exponentiation to check\nwhether \n\n\n\n\na\n\n(\np\n−\n1\n)\n\n/\n\n2\n\n\n±\n1\n\n\n{\\displaystyle a^{(p-1)/2}\\pm 1}\n\n is divisible by \n\n\n\np\n\n\n{\\displaystyle p}\n\n.[c] If so, it answers yes and otherwise it answers no. If \n\n\n\np\n\n\n{\\displaystyle p}\n\n really is prime, it will always answer yes, but if \n\n\n\np\n\n\n{\\displaystyle p}\n\n is composite then it answers yes with probability at most 1/2 and no with probability at least 1/2.[132] If this test is repeated \n\n\n\nn\n\n\n{\\displaystyle n}\n\n times on the same number,\nthe probability that a composite number could pass the test every time is at most \n\n\n\n1\n\n/\n\n\n2\n\nn\n\n\n\n\n{\\displaystyle 1/2^{n}}\n\n. Because this decreases exponentially with the number of tests, it provides high confidence (although not certainty) that a number that passes the repeated test is prime. On the other hand, if the test ever fails, then the number is certainly composite.[133]\nA composite number that passes such a test is called a pseudoprime.[132]\n\nIn contrast, some other algorithms guarantee that their answer will always be correct: primes will always be determined to be prime and composites will always be determined to be composite. For instance, this is true of trial division. The algorithms with guaranteed-correct output include both deterministic (non-random) algorithms, such as the AKS primality test,[134]\nand randomized Las Vegas algorithms where the random choices made by the algorithm do not affect its final answer, such as some variations of elliptic curve primality proving.[131]\nWhen the elliptic curve method concludes that a number is prime, it provides primality certificate that can be verified quickly.[135]\nThe elliptic curve primality test is the fastest in practice of the guaranteed-correct primality tests, but its runtime analysis is based on heuristic arguments rather than rigorous proofs. The AKS primality test has mathematically proven time complexity, but is slower than elliptic curve primality proving in practice.[136] These methods can be used to generate large random prime numbers, by generating and testing random numbers until finding one that is prime;\nwhen doing this, a faster probabilistic test can quickly eliminate most composite numbers before a guaranteed-correct algorithm is used to verify that the remaining numbers are prime.[d]\n\nThe following table lists some of these tests. Their running time is given in terms of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, the number to be tested and, for probabilistic algorithms, the number \n\n\n\nk\n\n\n{\\displaystyle k}\n\n of tests performed. Moreover, \n\n\n\nε\n\n\n{\\displaystyle \\varepsilon }\n\n is an arbitrarily small positive number, and log is the logarithm to an unspecified base. The big O notation means that each time bound should be multiplied by a constant factor to convert it from dimensionless units to units of time; this factor depends on implementation details such as the type of computer used to run the algorithm, but not on the input parameters \n\n\n\nn\n\n\n{\\displaystyle n}\n\n and \n\n\n\nk\n\n\n{\\displaystyle k}\n\n.\n\nIn addition to the aforementioned tests that apply to any natural number, some numbers of a special form can be tested for primality more quickly. For example, the Lucas–Lehmer primality test can determine whether a Mersenne number (one less than a power of two) is prime, deterministically, in the same time as a single iteration of the Miller–Rabin test.[141] This is why since 1992 (as of December 2018[update]) the largest known prime has always been a Mersenne prime.[142] It is conjectured that there are infinitely many Mersenne primes.[143]\n\nThe following table gives the largest known primes of various types. Some of these primes have been found using distributed computing. In 2009, the Great Internet Mersenne Prime Search project was awarded a US$100,000 prize for first discovering a prime with at least 10 million digits.[144] The Electronic Frontier Foundation also offers $150,000 and $250,000 for primes with at least 100 million digits and 1 billion digits, respectively.[145]\n\nGiven a composite integer \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, the task of providing one (or all) prime factors is referred to as factorization of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. It is significantly more difficult than primality testing,[152] and although many factorization algorithms are known, they are slower than the fastest primality testing methods. Trial division and Pollard's rho algorithm can be used to find very small factors of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n,[125] and elliptic curve factorization can be effective when \n\n\n\nn\n\n\n{\\displaystyle n}\n\n has factors of moderate size.[153] Methods suitable for arbitrary large numbers that do not depend on the size of its factors include the quadratic sieve and general number field sieve. As with primality testing, there are also factorization algorithms that require their input to have a special form, including the special number field sieve.[154] As of December 2019[update] the largest number known to have been factored by a general-purpose algorithm is RSA-240, which has 240 decimal digits (795 bits) and is the product of two large primes.[155]\n\nShor's algorithm can factor any integer in a polynomial number of steps on a quantum computer.[156] However, current technology can only run this algorithm for very small numbers. As of October 2012[update], the largest number that has been factored by a quantum computer running Shor's algorithm is 21.[157]\n\nSeveral public-key cryptography algorithms, such as RSA and the Diffie–Hellman key exchange, are based on large prime numbers (2048-bit primes are common).[158] RSA relies on the assumption that it is much easier (that is, more efficient) to perform the multiplication of two (large) numbers \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n than to calculate \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n (assumed coprime) if only the product \n\n\n\nx\ny\n\n\n{\\displaystyle xy}\n\n is known.[33] The Diffie–Hellman key exchange relies on the fact that there are efficient algorithms for modular exponentiation (computing \n\n\n\n\na\n\nb\n\n\n\nmod\n\nc\n\n\n\n\n{\\displaystyle a^{b}{\\bmod {c}}}\n\n), while the reverse operation (the discrete logarithm) is thought to be a hard problem.[159]\n\nPrime numbers are frequently used for hash tables. For instance the original method of Carter and Wegman for universal hashing was based on computing hash functions by choosing random linear functions modulo large prime numbers. Carter and Wegman generalized this method to \n\n\n\nk\n\n\n{\\displaystyle k}\n\n-independent hashing by using higher-degree polynomials, again modulo large primes.[160] As well as in the hash function, prime numbers are used for the hash table size in quadratic probing based hash tables to ensure that the probe sequence covers the whole table.[161]\n\nSome checksum methods are based on the mathematics of prime numbers. For instance the checksums used in International Standard Book Numbers are defined by taking the rest of the number modulo 11, a prime number. Because 11 is prime this method can detect both single-digit errors and transpositions of adjacent digits.[162] Another checksum method, Adler-32, uses arithmetic modulo 65521, the largest prime number less than \n\n\n\n\n2\n\n16\n\n\n\n\n{\\displaystyle 2^{16}}\n\n.[163] Prime numbers are also used in pseudorandom number generators including linear congruential generators[164] and the Mersenne Twister.[165]\n\nPrime numbers are of central importance to number theory but also have many applications to other areas within mathematics, including abstract algebra and elementary geometry. For example, it is possible to place prime numbers of points in a two-dimensional grid so that no three are in a line, or so that every triangle formed by three of the points has large area.[166] Another example is Eisenstein's criterion, a test for whether a polynomial is irreducible based on divisibility of its coefficients by a prime number and its square.[167]\n\nThe concept of a prime number is so important that it has been generalized in different ways in various branches of mathematics. Generally, \"prime\" indicates minimality or indecomposability, in an appropriate sense. For example, the prime field of a given field is its smallest subfield that contains both 0 and 1. It is either the field of rational numbers or a finite field with a prime number of elements, whence the name.[168] Often a second, additional meaning is intended by using the word prime, namely that any object can be, essentially uniquely, decomposed into its prime components. For example, in knot theory, a prime knot is a knot that is indecomposable in the sense that it cannot be written as the connected sum of two nontrivial knots. Any knot can be uniquely expressed as a connected sum of prime knots.[169] The prime decomposition of 3-manifolds is another example of this type.[170]\n\nBeyond mathematics and computing, prime numbers have potential connections to quantum mechanics, and have been used metaphorically in the arts and literature. They have also been used in evolutionary biology to explain the life cycles of cicadas.\n\nFermat primes are primes of the form\n\nwith \n\n\n\nk\n\n\n{\\displaystyle k}\n\n a nonnegative integer.[171] They are named after Pierre de Fermat, who conjectured that all such numbers are prime. The first five of these numbers – 3, 5, 17, 257, and 65,537 – are prime,[172] but \n\n\n\n\nF\n\n5\n\n\n\n\n{\\displaystyle F_{5}}\n\n is composite and so are all other Fermat numbers that have been verified as of 2017.[173] A regular \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-gon is constructible using straightedge and compass if and only if the odd prime factors of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n (if any) are distinct Fermat primes.[172] Likewise, a regular \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-gon may be constructed using straightedge, compass, and an angle trisector if and only if the prime factors of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n are any number of copies of 2 or 3 together with a (possibly empty) set of distinct Pierpont primes, primes of the form \n\n\n\n\n2\n\na\n\n\n\n3\n\nb\n\n\n+\n1\n\n\n{\\displaystyle 2^{a}3^{b}+1}\n\n.[174]\n\nIt is possible to partition any convex polygon into \n\n\n\nn\n\n\n{\\displaystyle n}\n\n smaller convex polygons of equal area and equal perimeter, when \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is a power of a prime number, but this is not known for other values of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.[175]\n\nBeginning with the work of Hugh Montgomery and Freeman Dyson in the 1970s, mathematicians and physicists have speculated that the zeros of the Riemann zeta function are connected to the energy levels of quantum systems.[176][177] Prime numbers are also significant in quantum information science, thanks to mathematical structures such as mutually unbiased bases and symmetric informationally complete positive-operator-valued measures.[178][179]\n\nThe evolutionary strategy used by cicadas of the genus Magicicada makes use of prime numbers.[180] These insects spend most of their lives as grubs underground. They only pupate and then emerge from their burrows after 7, 13 or 17 years, at which point they fly about, breed, and then die after a few weeks at most. Biologists theorize that these prime-numbered breeding cycle lengths have evolved in order to prevent predators from synchronizing with these cycles.[181][182] In contrast, the multi-year periods between flowering in bamboo plants are hypothesized to be smooth numbers, having only small prime numbers in their factorizations.[183]\n\nPrime numbers have influenced many artists and writers. The French composer Olivier Messiaen used prime numbers to create ametrical music through \"natural phenomena\". In works such as La Nativité du Seigneur (1935) and Quatre études de rythme (1949–1950), he simultaneously employs motifs with lengths given by different prime numbers to create unpredictable rhythms: the primes 41, 43, 47 and 53 appear in the third étude, \"Neumes rythmiques\". According to Messiaen this way of composing was \"inspired by the movements of nature, movements of free and unequal durations\".[184]\n\nIn his science fiction novel Contact, scientist Carl Sagan suggested that prime factorization could be used as a means of establishing two-dimensional image planes in communications with aliens, an idea that he had first developed informally with American astronomer Frank Drake in 1975.[185] In the novel The Curious Incident of the Dog in the Night-Time by Mark Haddon, the narrator arranges the sections of the story by consecutive prime numbers as a way to convey the mental state of its main character, a mathematically gifted teen with Asperger syndrome.[186] Prime numbers are used as a metaphor for loneliness and isolation in the Paolo Giordano novel The Solitude of Prime Numbers, in which they are portrayed as \"outsiders\" among integers.[187]\n"
    },
    {
        "title": "Independence (probability theory)",
        "content": "Independence is a fundamental notion in probability theory, as in statistics and the theory of stochastic processes. Two events are independent, statistically independent, or stochastically independent[1] if, informally speaking, the occurrence of one does not affect the probability of occurrence of the other or, equivalently, does not affect the odds. Similarly, two random variables are independent if the realization of one does not affect the probability distribution of the other.\n\nWhen dealing with collections of more than two events, two notions of independence need to be distinguished. The events are called pairwise independent if any two events in the collection are independent of each other, while mutual independence (or collective independence) of events means, informally speaking, that each event is independent of any combination of other events in the collection. A similar notion exists for collections of random variables. Mutual independence implies pairwise independence, but not the other way around. In the standard literature of probability theory, statistics, and stochastic processes, independence without further qualification usually refers to mutual independence.\n\nTwo events \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n are independent (often written as \n\n\n\nA\n⊥\nB\n\n\n{\\displaystyle A\\perp B}\n\n or \n\n\n\nA\n⊥\n\n\n\n⊥\nB\n\n\n{\\displaystyle A\\perp \\!\\!\\!\\perp B}\n\n, where the latter symbol often is also used for conditional independence) if and only if their joint probability equals the product of their probabilities:[2]: p. 29 [3]: p. 10 \n\n\n\n\n\nA\n∩\nB\n≠\n∅\n\n\n{\\displaystyle A\\cap B\\neq \\emptyset }\n\n indicates that two independent events \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n have common elements in their sample space so that they are not mutually exclusive (mutually exclusive iff \n\n\n\nA\n∩\nB\n=\n∅\n\n\n{\\displaystyle A\\cap B=\\emptyset }\n\n). Why this defines independence is made clear by rewriting with conditional probabilities \n\n\n\nP\n(\nA\n∣\nB\n)\n=\n\n\n\nP\n(\nA\n∩\nB\n)\n\n\nP\n(\nB\n)\n\n\n\n\n\n{\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}}\n\n as the probability at which the event \n\n\n\nA\n\n\n{\\displaystyle A}\n\n occurs provided that the event \n\n\n\nB\n\n\n{\\displaystyle B}\n\n has or is assumed to have occurred:\n\nand similarly\n\nThus, the occurrence of \n\n\n\nB\n\n\n{\\displaystyle B}\n\n does not affect the probability of \n\n\n\nA\n\n\n{\\displaystyle A}\n\n, and vice versa. In other words, \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n are independent of each other. Although the derived expressions may seem more intuitive, they are not the preferred definition, as the conditional probabilities may be undefined if \n\n\n\n\nP\n\n(\nA\n)\n\n\n{\\displaystyle \\mathrm {P} (A)}\n\n or \n\n\n\n\nP\n\n(\nB\n)\n\n\n{\\displaystyle \\mathrm {P} (B)}\n\n are 0. Furthermore, the preferred definition makes clear by symmetry that when \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is independent of \n\n\n\nB\n\n\n{\\displaystyle B}\n\n, \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is also independent of \n\n\n\nA\n\n\n{\\displaystyle A}\n\n.\n\nStated in terms of odds, two events are independent if and only if the odds ratio of ⁠\n\n\n\nA\n\n\n{\\displaystyle A}\n\n⁠ and ⁠\n\n\n\nB\n\n\n{\\displaystyle B}\n\n⁠ is unity (1). Analogously with probability, this is equivalent to the conditional odds being equal to the unconditional odds:\n\nor to the odds of one event, given the other event, being the same as the odds of the event, given the other event not occurring:\n\nThe odds ratio can be defined as\n\nor symmetrically for odds of ⁠\n\n\n\nB\n\n\n{\\displaystyle B}\n\n⁠ given ⁠\n\n\n\nA\n\n\n{\\displaystyle A}\n\n⁠, and thus is 1 if and only if the events are independent.\n\nA finite set of events \n\n\n\n{\n\nA\n\ni\n\n\n\n}\n\ni\n=\n1\n\n\nn\n\n\n\n\n{\\displaystyle \\{A_{i}\\}_{i=1}^{n}}\n\n is pairwise independent if every pair of events is independent[4]—that is, if and only if for all distinct pairs of indices \n\n\n\nm\n,\nk\n\n\n{\\displaystyle m,k}\n\n,\n\nA finite set of events is mutually independent if every event is independent of any intersection of the other events[4][3]: p. 11 —that is, if and only if for every \n\n\n\nk\n≤\nn\n\n\n{\\displaystyle k\\leq n}\n\n and for every k indices \n\n\n\n1\n≤\n\ni\n\n1\n\n\n<\n⋯\n<\n\ni\n\nk\n\n\n≤\nn\n\n\n{\\displaystyle 1\\leq i_{1}<\\dots <i_{k}\\leq n}\n\n,\n\nThis is called the multiplication rule for independent events. It is not a single condition involving only the product of all the probabilities of all single events; it must hold true for all subsets of events.\n\nFor more than two events, a mutually independent set of events is (by definition) pairwise independent; but the converse is not necessarily true.[2]: p. 30 \n\nStated in terms of log probability, two events are independent if and only if the log probability of the joint event is the sum of the log probability of the individual events:\n\nIn information theory, negative log probability is interpreted as information content, and thus two events are independent if and only if the information content of the combined event equals the sum of information content of the individual events:\n\nSee Information content § Additivity of independent events for details.\n\nTwo random variables \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are independent if and only if (iff) the elements of the π-system generated by them are independent; that is to say, for every \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n, the events \n\n\n\n{\nX\n≤\nx\n}\n\n\n{\\displaystyle \\{X\\leq x\\}}\n\n and \n\n\n\n{\nY\n≤\ny\n}\n\n\n{\\displaystyle \\{Y\\leq y\\}}\n\n are independent events (as defined above in Eq.1). That is, \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n with cumulative distribution functions \n\n\n\n\nF\n\nX\n\n\n(\nx\n)\n\n\n{\\displaystyle F_{X}(x)}\n\n and \n\n\n\n\nF\n\nY\n\n\n(\ny\n)\n\n\n{\\displaystyle F_{Y}(y)}\n\n, are independent iff the combined random variable \n\n\n\n(\nX\n,\nY\n)\n\n\n{\\displaystyle (X,Y)}\n\n has a joint cumulative distribution function[3]: p. 15 \n\nor equivalently, if the probability densities \n\n\n\n\nf\n\nX\n\n\n(\nx\n)\n\n\n{\\displaystyle f_{X}(x)}\n\n and \n\n\n\n\nf\n\nY\n\n\n(\ny\n)\n\n\n{\\displaystyle f_{Y}(y)}\n\n and the joint probability density \n\n\n\n\nf\n\nX\n,\nY\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f_{X,Y}(x,y)}\n\n exist,\n\nA finite set of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n random variables \n\n\n\n{\n\nX\n\n1\n\n\n,\n…\n,\n\nX\n\nn\n\n\n}\n\n\n{\\displaystyle \\{X_{1},\\ldots ,X_{n}\\}}\n\n is pairwise independent if and only if every pair of random variables is independent. Even if the set of random variables is pairwise independent, it is not necessarily mutually independent as defined next.\n\nA finite set of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n random variables \n\n\n\n{\n\nX\n\n1\n\n\n,\n…\n,\n\nX\n\nn\n\n\n}\n\n\n{\\displaystyle \\{X_{1},\\ldots ,X_{n}\\}}\n\n is mutually independent if and only if for any sequence of numbers \n\n\n\n{\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n}\n\n\n{\\displaystyle \\{x_{1},\\ldots ,x_{n}\\}}\n\n, the events \n\n\n\n{\n\nX\n\n1\n\n\n≤\n\nx\n\n1\n\n\n}\n,\n…\n,\n{\n\nX\n\nn\n\n\n≤\n\nx\n\nn\n\n\n}\n\n\n{\\displaystyle \\{X_{1}\\leq x_{1}\\},\\ldots ,\\{X_{n}\\leq x_{n}\\}}\n\n are mutually independent events (as defined above in Eq.3). This is equivalent to the following condition on the joint cumulative distribution function \n\n\n\n\nF\n\n\nX\n\n1\n\n\n,\n…\n,\n\nX\n\nn\n\n\n\n\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle F_{X_{1},\\ldots ,X_{n}}(x_{1},\\ldots ,x_{n})}\n\n. A finite set of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n random variables \n\n\n\n{\n\nX\n\n1\n\n\n,\n…\n,\n\nX\n\nn\n\n\n}\n\n\n{\\displaystyle \\{X_{1},\\ldots ,X_{n}\\}}\n\n is mutually independent if and only if[3]: p. 16 \n\nIt is not necessary here to require that the probability distribution factorizes for all possible \n\n\n\nk\n\n\n{\\displaystyle k}\n\n-element subsets as in the case for \n\n\n\nn\n\n\n{\\displaystyle n}\n\n events. This is not required because e.g. \n\n\n\n\nF\n\n\nX\n\n1\n\n\n,\n\nX\n\n2\n\n\n,\n\nX\n\n3\n\n\n\n\n(\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\nx\n\n3\n\n\n)\n=\n\nF\n\n\nX\n\n1\n\n\n\n\n(\n\nx\n\n1\n\n\n)\n⋅\n\nF\n\n\nX\n\n2\n\n\n\n\n(\n\nx\n\n2\n\n\n)\n⋅\n\nF\n\n\nX\n\n3\n\n\n\n\n(\n\nx\n\n3\n\n\n)\n\n\n{\\displaystyle F_{X_{1},X_{2},X_{3}}(x_{1},x_{2},x_{3})=F_{X_{1}}(x_{1})\\cdot F_{X_{2}}(x_{2})\\cdot F_{X_{3}}(x_{3})}\n\n implies \n\n\n\n\nF\n\n\nX\n\n1\n\n\n,\n\nX\n\n3\n\n\n\n\n(\n\nx\n\n1\n\n\n,\n\nx\n\n3\n\n\n)\n=\n\nF\n\n\nX\n\n1\n\n\n\n\n(\n\nx\n\n1\n\n\n)\n⋅\n\nF\n\n\nX\n\n3\n\n\n\n\n(\n\nx\n\n3\n\n\n)\n\n\n{\\displaystyle F_{X_{1},X_{3}}(x_{1},x_{3})=F_{X_{1}}(x_{1})\\cdot F_{X_{3}}(x_{3})}\n\n.\n\nThe measure-theoretically inclined reader may prefer to substitute events \n\n\n\n{\nX\n∈\nA\n}\n\n\n{\\displaystyle \\{X\\in A\\}}\n\n for events \n\n\n\n{\nX\n≤\nx\n}\n\n\n{\\displaystyle \\{X\\leq x\\}}\n\n in the above definition, where \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is any Borel set. That definition is exactly equivalent to the one above when the values of the random variables are real numbers. It has the advantage of working also for complex-valued random variables or for random variables taking values in any measurable space (which includes topological spaces endowed by appropriate σ-algebras).\n\nTwo random vectors \n\n\n\n\nX\n\n=\n(\n\nX\n\n1\n\n\n,\n…\n,\n\nX\n\nm\n\n\n\n)\n\n\nT\n\n\n\n\n\n{\\displaystyle \\mathbf {X} =(X_{1},\\ldots ,X_{m})^{\\mathrm {T} }}\n\n and \n\n\n\n\nY\n\n=\n(\n\nY\n\n1\n\n\n,\n…\n,\n\nY\n\nn\n\n\n\n)\n\n\nT\n\n\n\n\n\n{\\displaystyle \\mathbf {Y} =(Y_{1},\\ldots ,Y_{n})^{\\mathrm {T} }}\n\n are called independent if[5]: p. 187 \n\nwhere \n\n\n\n\nF\n\n\nX\n\n\n\n(\n\nx\n\n)\n\n\n{\\displaystyle F_{\\mathbf {X} }(\\mathbf {x} )}\n\n and \n\n\n\n\nF\n\n\nY\n\n\n\n(\n\ny\n\n)\n\n\n{\\displaystyle F_{\\mathbf {Y} }(\\mathbf {y} )}\n\n denote the cumulative distribution functions of \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n and \n\n\n\n\nY\n\n\n\n{\\displaystyle \\mathbf {Y} }\n\n and \n\n\n\n\nF\n\n\nX\n,\nY\n\n\n\n(\n\nx\n,\ny\n\n)\n\n\n{\\displaystyle F_{\\mathbf {X,Y} }(\\mathbf {x,y} )}\n\n denotes their joint cumulative distribution function. Independence of \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n and \n\n\n\n\nY\n\n\n\n{\\displaystyle \\mathbf {Y} }\n\n is often denoted by \n\n\n\n\nX\n\n⊥\n\n\n\n⊥\n\nY\n\n\n\n{\\displaystyle \\mathbf {X} \\perp \\!\\!\\!\\perp \\mathbf {Y} }\n\n.\nWritten component-wise, \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n and \n\n\n\n\nY\n\n\n\n{\\displaystyle \\mathbf {Y} }\n\n are called independent if\n\nThe definition of independence may be extended from random vectors to a stochastic process. Therefore, it is required for an independent stochastic process that the random variables obtained by sampling the process at any \n\n\n\nn\n\n\n{\\displaystyle n}\n\n times \n\n\n\n\nt\n\n1\n\n\n,\n…\n,\n\nt\n\nn\n\n\n\n\n{\\displaystyle t_{1},\\ldots ,t_{n}}\n\n are independent random variables for any \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.[6]: p. 163 \n\nFormally, a stochastic process \n\n\n\n\n\n{\n\nX\n\nt\n\n\n}\n\n\nt\n∈\n\n\nT\n\n\n\n\n\n\n{\\displaystyle \\left\\{X_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n\n is called independent, if and only if for all \n\n\n\nn\n∈\n\nN\n\n\n\n{\\displaystyle n\\in \\mathbb {N} }\n\n and for all \n\n\n\n\nt\n\n1\n\n\n,\n…\n,\n\nt\n\nn\n\n\n∈\n\n\nT\n\n\n\n\n{\\displaystyle t_{1},\\ldots ,t_{n}\\in {\\mathcal {T}}}\n\n\n\nwhere \n\n\n\n\nF\n\n\nX\n\n\nt\n\n1\n\n\n\n\n,\n…\n,\n\nX\n\n\nt\n\nn\n\n\n\n\n\n\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n=\n\nP\n\n(\nX\n(\n\nt\n\n1\n\n\n)\n≤\n\nx\n\n1\n\n\n,\n…\n,\nX\n(\n\nt\n\nn\n\n\n)\n≤\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle F_{X_{t_{1}},\\ldots ,X_{t_{n}}}(x_{1},\\ldots ,x_{n})=\\mathrm {P} (X(t_{1})\\leq x_{1},\\ldots ,X(t_{n})\\leq x_{n})}\n\n. Independence of a stochastic process is a property within a stochastic process, not between two stochastic processes.\n\nIndependence of two stochastic processes is a property between two stochastic processes \n\n\n\n\n\n{\n\nX\n\nt\n\n\n}\n\n\nt\n∈\n\n\nT\n\n\n\n\n\n\n{\\displaystyle \\left\\{X_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n\n and \n\n\n\n\n\n{\n\nY\n\nt\n\n\n}\n\n\nt\n∈\n\n\nT\n\n\n\n\n\n\n{\\displaystyle \\left\\{Y_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n\n that are defined on the same probability space \n\n\n\n(\nΩ\n,\n\n\nF\n\n\n,\nP\n)\n\n\n{\\displaystyle (\\Omega ,{\\mathcal {F}},P)}\n\n. Formally, two stochastic processes \n\n\n\n\n\n{\n\nX\n\nt\n\n\n}\n\n\nt\n∈\n\n\nT\n\n\n\n\n\n\n{\\displaystyle \\left\\{X_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n\n and \n\n\n\n\n\n{\n\nY\n\nt\n\n\n}\n\n\nt\n∈\n\n\nT\n\n\n\n\n\n\n{\\displaystyle \\left\\{Y_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n\n are said to be independent if for all \n\n\n\nn\n∈\n\nN\n\n\n\n{\\displaystyle n\\in \\mathbb {N} }\n\n and for all \n\n\n\n\nt\n\n1\n\n\n,\n…\n,\n\nt\n\nn\n\n\n∈\n\n\nT\n\n\n\n\n{\\displaystyle t_{1},\\ldots ,t_{n}\\in {\\mathcal {T}}}\n\n, the random vectors \n\n\n\n(\nX\n(\n\nt\n\n1\n\n\n)\n,\n…\n,\nX\n(\n\nt\n\nn\n\n\n)\n)\n\n\n{\\displaystyle (X(t_{1}),\\ldots ,X(t_{n}))}\n\n and \n\n\n\n(\nY\n(\n\nt\n\n1\n\n\n)\n,\n…\n,\nY\n(\n\nt\n\nn\n\n\n)\n)\n\n\n{\\displaystyle (Y(t_{1}),\\ldots ,Y(t_{n}))}\n\n are independent,[7]: p. 515  i.e. if\n\nThe definitions above (Eq.1 and Eq.2) are both generalized by the following definition of independence for σ-algebras. Let \n\n\n\n(\nΩ\n,\nΣ\n,\n\nP\n\n)\n\n\n{\\displaystyle (\\Omega ,\\Sigma ,\\mathrm {P} )}\n\n be a probability space and let \n\n\n\n\n\nA\n\n\n\n\n{\\displaystyle {\\mathcal {A}}}\n\n and \n\n\n\n\n\nB\n\n\n\n\n{\\displaystyle {\\mathcal {B}}}\n\n be two sub-σ-algebras of \n\n\n\nΣ\n\n\n{\\displaystyle \\Sigma }\n\n. \n\n\n\n\n\nA\n\n\n\n\n{\\displaystyle {\\mathcal {A}}}\n\n and \n\n\n\n\n\nB\n\n\n\n\n{\\displaystyle {\\mathcal {B}}}\n\n are said to be independent if, whenever \n\n\n\nA\n∈\n\n\nA\n\n\n\n\n{\\displaystyle A\\in {\\mathcal {A}}}\n\n and \n\n\n\nB\n∈\n\n\nB\n\n\n\n\n{\\displaystyle B\\in {\\mathcal {B}}}\n\n,\n\nLikewise, a finite family of σ-algebras \n\n\n\n(\n\nτ\n\ni\n\n\n\n)\n\ni\n∈\nI\n\n\n\n\n{\\displaystyle (\\tau _{i})_{i\\in I}}\n\n, where \n\n\n\nI\n\n\n{\\displaystyle I}\n\n is an index set, is said to be independent if and only if\n\nand an infinite family of σ-algebras is said to be independent if all its finite subfamilies are independent.\n\nThe new definition relates to the previous ones very directly:\n\nUsing this definition, it is easy to show that if \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are random variables and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is constant, then \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are independent, since the σ-algebra generated by a constant random variable is the trivial σ-algebra \n\n\n\n{\n∅\n,\nΩ\n}\n\n\n{\\displaystyle \\{\\varnothing ,\\Omega \\}}\n\n. Probability zero events cannot affect independence so independence also holds if \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is only Pr-almost surely constant.\n\nNote that an event is independent of itself if and only if\n\nThus an event is independent of itself if and only if it almost surely occurs or its complement almost surely occurs; this fact is useful when proving zero–one laws.[8]\n\nIf \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are statistically independent random variables, then the expectation operator \n\n\n\nE\n\n\n{\\displaystyle \\operatorname {E} }\n\n has the property\n\nand the covariance \n\n\n\ncov\n⁡\n[\nX\n,\nY\n]\n\n\n{\\displaystyle \\operatorname {cov} [X,Y]}\n\n is zero, as follows from\n\n\nThe converse does not hold: if two random variables have a covariance of 0 they still may be not independent. \nSimilarly for two stochastic processes \n\n\n\n\n\n{\n\nX\n\nt\n\n\n}\n\n\nt\n∈\n\n\nT\n\n\n\n\n\n\n{\\displaystyle \\left\\{X_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n\n and \n\n\n\n\n\n{\n\nY\n\nt\n\n\n}\n\n\nt\n∈\n\n\nT\n\n\n\n\n\n\n{\\displaystyle \\left\\{Y_{t}\\right\\}_{t\\in {\\mathcal {T}}}}\n\n: If they are independent, then they are uncorrelated.[10]: p. 151 \n\nTwo random variables \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are independent if and only if the characteristic function of the random vector \n\n\n\n(\nX\n,\nY\n)\n\n\n{\\displaystyle (X,Y)}\n\n satisfies\n\nIn particular the characteristic function of their sum is the product of their marginal characteristic functions:\n\nthough the reverse implication is not true. Random variables that satisfy the latter condition are called subindependent.\n\nThe event of getting a 6 the first time a die is rolled and the event of getting a 6 the second time are independent. By contrast, the event of getting a 6 the first time a die is rolled and the event that the sum of the numbers seen on the first and second trial is 8 are not independent.\n\nIf two cards are drawn with replacement from a deck of cards, the event of drawing a red card on the first trial and that of drawing a red card on the second trial are independent. By contrast, if two cards are drawn without replacement from a deck of cards, the event of drawing a red card on the first trial and that of drawing a red card on the second trial are not independent, because a deck that has had a red card removed has proportionately fewer red cards.\n\nConsider the two probability spaces shown. In both cases, \n\n\n\n\nP\n\n(\nA\n)\n=\n\nP\n\n(\nB\n)\n=\n1\n\n/\n\n2\n\n\n{\\displaystyle \\mathrm {P} (A)=\\mathrm {P} (B)=1/2}\n\n and \n\n\n\n\nP\n\n(\nC\n)\n=\n1\n\n/\n\n4\n\n\n{\\displaystyle \\mathrm {P} (C)=1/4}\n\n. The events in the first space are pairwise independent because \n\n\n\n\nP\n\n(\nA\n\n|\n\nB\n)\n=\n\nP\n\n(\nA\n\n|\n\nC\n)\n=\n1\n\n/\n\n2\n=\n\nP\n\n(\nA\n)\n\n\n{\\displaystyle \\mathrm {P} (A|B)=\\mathrm {P} (A|C)=1/2=\\mathrm {P} (A)}\n\n, \n\n\n\n\nP\n\n(\nB\n\n|\n\nA\n)\n=\n\nP\n\n(\nB\n\n|\n\nC\n)\n=\n1\n\n/\n\n2\n=\n\nP\n\n(\nB\n)\n\n\n{\\displaystyle \\mathrm {P} (B|A)=\\mathrm {P} (B|C)=1/2=\\mathrm {P} (B)}\n\n, and \n\n\n\n\nP\n\n(\nC\n\n|\n\nA\n)\n=\n\nP\n\n(\nC\n\n|\n\nB\n)\n=\n1\n\n/\n\n4\n=\n\nP\n\n(\nC\n)\n\n\n{\\displaystyle \\mathrm {P} (C|A)=\\mathrm {P} (C|B)=1/4=\\mathrm {P} (C)}\n\n; but the three events are not mutually independent. The events in the second space are both pairwise independent and mutually independent. To illustrate the difference, consider conditioning on two events. In the pairwise independent case, although any one event is independent of each of the other two individually, it is not independent of the intersection of the other two:\n\nIn the mutually independent case, however,\n\nIt is possible to create a three-event example in which\n\nand yet no two of the three events are pairwise independent (and hence the set of events are not mutually independent).[11] This example shows that mutual independence involves requirements on the products of probabilities of all combinations of events, not just the single events as in this example.\n\nThe events \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n are conditionally independent given an event \n\n\n\nC\n\n\n{\\displaystyle C}\n\n when\n\n\n\n\n\n\nP\n\n(\nA\n∩\nB\n∣\nC\n)\n=\n\nP\n\n(\nA\n∣\nC\n)\n⋅\n\nP\n\n(\nB\n∣\nC\n)\n\n\n{\\displaystyle \\mathrm {P} (A\\cap B\\mid C)=\\mathrm {P} (A\\mid C)\\cdot \\mathrm {P} (B\\mid C)}\n\n.\n\nIntuitively, two random variables \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are conditionally independent given \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n if, once \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n is known, the value of \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n does not add any additional information about \n\n\n\nX\n\n\n{\\displaystyle X}\n\n. For instance, two measurements \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n of the same underlying quantity \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n are not independent, but they are conditionally independent given \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n (unless the errors in the two measurements are somehow connected).\n\nThe formal definition of conditional independence is based on the idea of conditional distributions. If \n\n\n\nX\n\n\n{\\displaystyle X}\n\n, \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n, and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n are discrete random variables, then we define \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n to be conditionally independent given \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n if\n\nfor all \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, \n\n\n\ny\n\n\n{\\displaystyle y}\n\n and \n\n\n\nz\n\n\n{\\displaystyle z}\n\n such that \n\n\n\n\nP\n\n(\nZ\n=\nz\n)\n>\n0\n\n\n{\\displaystyle \\mathrm {P} (Z=z)>0}\n\n. On the other hand, if the random variables are continuous and have a joint probability density function \n\n\n\n\nf\n\nX\nY\nZ\n\n\n(\nx\n,\ny\n,\nz\n)\n\n\n{\\displaystyle f_{XYZ}(x,y,z)}\n\n, then \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are conditionally independent given \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n if\n\nfor all real numbers \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, \n\n\n\ny\n\n\n{\\displaystyle y}\n\n and \n\n\n\nz\n\n\n{\\displaystyle z}\n\n such that \n\n\n\n\nf\n\nZ\n\n\n(\nz\n)\n>\n0\n\n\n{\\displaystyle f_{Z}(z)>0}\n\n.\n\nIf discrete \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n are conditionally independent given \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n, then\n\nfor any \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, \n\n\n\ny\n\n\n{\\displaystyle y}\n\n and \n\n\n\nz\n\n\n{\\displaystyle z}\n\n with \n\n\n\n\nP\n\n(\nZ\n=\nz\n)\n>\n0\n\n\n{\\displaystyle \\mathrm {P} (Z=z)>0}\n\n. That is, the conditional distribution for \n\n\n\nX\n\n\n{\\displaystyle X}\n\n given \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n and \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n is the same as that given \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n alone. A similar equation holds for the conditional probability density functions in the continuous case.\n\nIndependence can be seen as a special kind of conditional independence, since probability can be seen as a kind of conditional probability given no events.\n\nBefore 1933, independence, in probability theory, was defined in a verbal manner. For example, de Moivre gave the following definition: “Two events are independent, when they have no connexion one with the other, and that the happening of one neither forwards nor obstructs the happening of the other”.[12] If there are n independent events, the probability of the event, that all of them happen was computed as the product of the probabilities of these n events. Apparently, there was the conviction, that  this formula was a consequence of the above definition. (Sometimes this was called the Multiplication Theorem.), Of course, a proof of his assertion cannot work without further more formal tacit assumptions.\n\nThe definition of independence, given in this article, became the standard definition (now used in all books) after it appeared in 1933 as part of Kolmogorov's axiomatization of probability.[13] Kolmogorov credited it to S.N. Bernstein, and quoted a publication which had appeared in Russian in 1927.[14]\n\nUnfortunately, both Bernstein and Kolmogorov had not been aware of the work of the Georg Bohlmann. Bohlmann had given the same definition for two events in 1901[15] and for n events in 1908[16] In the latter paper, he studied his notion in detail. For example, he gave the first example showing that pairwise  independence does not imply mutual independence.\nEven today, Bohlmann is rarely quoted. More about his work can be found in On the contributions of Georg Bohlmann to probability theory from de:Ulrich Krengel.[17]\n"
    },
    {
        "title": "Number",
        "content": "\n\nA number is a mathematical object used to count, measure, and label. The most basic examples are the natural numbers 1, 2, 3, 4, and so forth.[1] Numbers can be represented in language with number words. More universally, individual numbers can be represented by symbols, called numerals; for example, \"5\" is a numeral that represents the number five. As only a relatively small number of symbols can be memorized, basic numerals are commonly organized in a numeral system, which is an organized way to represent any number. The most common numeral system is the Hindu–Arabic numeral system, which allows for the representation of any non-negative integer using a combination of ten fundamental numeric symbols, called digits.[2][a] In addition to their use in counting and measuring, numerals are often used for labels (as with telephone numbers), for ordering (as with serial numbers), and for codes (as with ISBNs). In common usage, a numeral is not clearly distinguished from the number that it represents.\n\nIn mathematics, the notion of number has been extended over the centuries to include zero (0),[3] negative numbers,[4] rational numbers such as one half \n\n\n\n\n(\n\n\n\n1\n2\n\n\n\n)\n\n\n\n{\\displaystyle \\left({\\tfrac {1}{2}}\\right)}\n\n, real numbers such as the square root of 2 \n\n\n\n\n(\n\n\n2\n\n\n)\n\n\n\n{\\displaystyle \\left({\\sqrt {2}}\\right)}\n\n and π,[5] and complex numbers[6] which extend the real numbers with a square root of −1 (and its combinations with real numbers by adding or subtracting its multiples).[4] Calculations with numbers are done with arithmetical operations, the most familiar being addition, subtraction, multiplication, division, and exponentiation. Their study or usage is called arithmetic, a term which may also refer to number theory, the study of the properties of numbers.\n\nBesides their practical uses, numbers have cultural significance throughout the world.[7][8] For example, in Western society, the number 13 is often regarded as unlucky, and \"a million\" may signify \"a lot\" rather than an exact quantity.[7] Though it is now regarded as pseudoscience, belief in a mystical significance of numbers, known as numerology, permeated ancient and medieval thought.[9] Numerology heavily influenced the development of Greek mathematics, stimulating the investigation of many problems in number theory which are still of interest today.[9]\n\nDuring the 19th century, mathematicians began to develop many different abstractions which share certain properties of numbers, and may be seen as extending the concept. Among the first were the hypercomplex numbers, which consist of various extensions or modifications of the complex number system. In modern mathematics, number systems are considered important special examples of more general algebraic structures such as rings and fields, and the application of the term \"number\" is a matter of convention, without fundamental significance.[10]\n\nBones and other artifacts have been discovered with marks cut into them that many believe are tally marks.[11] These tally marks may have been used for counting elapsed time, such as numbers of days, lunar cycles or keeping records of quantities, such as of animals.\n\nA tallying system has no concept of place value (as in modern decimal notation), which limits its representation of large numbers. Nonetheless, tallying systems are considered the first kind of abstract numeral system.\n\nThe first known system with place value was the Mesopotamian base 60 system (c. 3400 BC) and the earliest known base 10 system dates to 3100 BC in Egypt.[12]\n\nNumbers should be distinguished from numerals, the symbols used to represent numbers. The Egyptians invented the first ciphered numeral system, and the Greeks followed by mapping their counting numbers onto Ionian and Doric alphabets.[13] Roman numerals, a system that used combinations of letters from the Roman alphabet, remained dominant in Europe until the spread of the superior Hindu–Arabic numeral system around the late 14th century, and the Hindu–Arabic numeral system remains the most common system for representing numbers in the world today.[14][better source needed] The key to the effectiveness of the system was the symbol for zero, which was developed by ancient Indian mathematicians around 500 AD.[14]\n\nThe first known recorded use of zero dates to AD 628, and appeared in the Brāhmasphuṭasiddhānta, the main work of the Indian mathematician Brahmagupta. He treated 0 as a number and discussed operations involving it, including division by zero. By this time (the 7th century), the concept had clearly reached Cambodia in the form of Khmer numerals, and documentation shows the idea later spreading to China and the Islamic world.\n\nBrahmagupta's Brāhmasphuṭasiddhānta is the first book that mentions zero as a number, hence Brahmagupta is usually considered the first to formulate the concept of zero. He gave rules of using zero with negative and positive numbers, such as \"zero plus a positive number is a positive number, and a negative number plus zero is the negative number\". The Brāhmasphuṭasiddhānta is the earliest known text to treat zero as a number in its own right, rather than as simply a placeholder digit in representing another number as was done by the Babylonians or as a symbol for a lack of quantity as was done by Ptolemy and the Romans.\n\nThe use of 0 as a number should be distinguished from its use as a placeholder numeral in place-value systems. Many ancient texts used 0. Babylonian and Egyptian texts used it. Egyptians used the word nfr to denote zero balance in double entry accounting. Indian texts used a Sanskrit word Shunye or shunya to refer to the concept of void. In mathematics texts this word often refers to the number zero.[15] In a similar vein, Pāṇini (5th century BC) used the null (zero) operator in the Ashtadhyayi, an early example of an algebraic grammar for the Sanskrit language (also see Pingala).\n\nThere are other uses of zero before Brahmagupta, though the documentation is not as complete as it is in the Brāhmasphuṭasiddhānta.\n\nRecords show that the Ancient Greeks seemed unsure about the status of 0 as a number: they asked themselves \"How can 'nothing' be something?\" leading to interesting philosophical and, by the Medieval period, religious arguments about the nature and existence of 0 and the vacuum. The paradoxes of Zeno of Elea depend in part on the uncertain interpretation of 0. (The ancient Greeks even questioned whether 1 was a number.)\n\nThe late Olmec people of south-central Mexico began to use a symbol for zero, a shell glyph, in the New World, possibly by the 4th century BC but certainly by 40 BC, which became an integral part of Maya numerals and the Maya calendar. Maya arithmetic used base 4 and base 5 written as base 20. George I. Sánchez in 1961 reported a base 4, base 5 \"finger\" abacus.[16][better source needed]\n\nBy 130 AD, Ptolemy, influenced by Hipparchus and the Babylonians, was using a symbol for 0 (a small circle with a long overbar) within a sexagesimal numeral system otherwise using alphabetic Greek numerals. Because it was used alone, not as just a placeholder, this Hellenistic zero was the first documented use of a true zero in the Old World. In later Byzantine manuscripts of his Syntaxis Mathematica (Almagest), the Hellenistic zero had morphed into the Greek letter Omicron (otherwise meaning 70).\n\nAnother true zero was used in tables alongside Roman numerals by 525 (first known use by Dionysius Exiguus), but as a word, nulla meaning nothing, not as a symbol. When division produced 0 as a remainder, nihil, also meaning nothing, was used. These medieval zeros were used by all future medieval computists (calculators of Easter). An isolated use of their initial, N, was used in a table of Roman numerals by Bede or a colleague about 725, a true zero symbol.\n\nThe abstract concept of negative numbers was recognized as early as 100–50 BC in China. The Nine Chapters on the Mathematical Art contains methods for finding the areas of figures; red rods were used to denote positive coefficients, black for negative.[17] The first reference in a Western work was in the 3rd century AD in Greece. Diophantus referred to the equation equivalent to 4x + 20 = 0 (the solution is negative) in Arithmetica, saying that the equation gave an absurd result.\n\nDuring the 600s, negative numbers were in use in India to represent debts. Diophantus' previous reference was discussed more explicitly by Indian mathematician Brahmagupta, in Brāhmasphuṭasiddhānta in 628, who used negative numbers to produce the general form quadratic formula that remains in use today. However, in the 12th century in India, Bhaskara gives negative roots for quadratic equations but says the negative value \"is in this case not to be taken, for it is inadequate; people do not approve of negative roots\".\n\nEuropean mathematicians, for the most part, resisted the concept of negative numbers until the 17th century, although Fibonacci allowed negative solutions in financial problems where they could be interpreted as debts (chapter 13 of Liber Abaci, 1202) and later as losses (in Flos). René Descartes called them false roots as they cropped up in algebraic polynomials yet he found a way to swap true roots and false roots as well. At the same time, the Chinese were indicating negative numbers by drawing a diagonal stroke through the right-most non-zero digit of the corresponding positive number's numeral.[18] The first use of negative numbers in a European work was by Nicolas Chuquet during the 15th century. He used them as exponents, but referred to them as \"absurd numbers\".\n\nAs recently as the 18th century, it was common practice to ignore any negative results returned by equations on the assumption that they were meaningless.\n\nIt is likely that the concept of fractional numbers dates to prehistoric times. The Ancient Egyptians used their Egyptian fraction notation for rational numbers in mathematical texts such as the Rhind Mathematical Papyrus and the Kahun Papyrus. Classical Greek and Indian mathematicians made studies of the theory of rational numbers, as part of the general study of number theory.[19] The best known of these is Euclid's Elements, dating to roughly 300 BC. Of the Indian texts, the most relevant is the Sthananga Sutra, which also covers number theory as part of a general study of mathematics.\n\nThe concept of decimal fractions is closely linked with decimal place-value notation; the two seem to have developed in tandem. For example, it is common for the Jain math sutra to include calculations of decimal-fraction approximations to pi or the square root of 2.[citation needed] Similarly, Babylonian math texts used sexagesimal (base 60) fractions with great frequency.\n\nThe earliest known use of irrational numbers was in the Indian Sulba Sutras composed between 800 and 500 BC.[20][better source needed] The first existence proofs of irrational numbers is usually attributed to Pythagoras, more specifically to the Pythagorean Hippasus of Metapontum, who produced a (most likely geometrical) proof of the irrationality of the square root of 2. The story goes that Hippasus discovered irrational numbers when trying to represent the square root of 2 as a fraction. However, Pythagoras believed in the absoluteness of numbers, and could not accept the existence of irrational numbers. He could not disprove their existence through logic, but he could not accept irrational numbers, and so, allegedly and frequently reported, he sentenced Hippasus to death by drowning, to impede spreading of this disconcerting news.[21][better source needed]\n\nThe 16th century brought final European acceptance of negative integral and fractional numbers. By the 17th century, mathematicians generally used decimal fractions with modern notation. It was not, however, until the 19th century that mathematicians separated irrationals into algebraic and transcendental parts, and once more undertook the scientific study of irrationals. It had remained almost dormant since Euclid. In 1872, the publication of the theories of Karl Weierstrass (by his pupil E. Kossak), Eduard Heine,[22] Georg Cantor,[23] and Richard Dedekind[24] was brought about. In 1869, Charles Méray had taken the same point of departure as Heine, but the theory is generally referred to the year 1872. Weierstrass's method was completely set forth by Salvatore Pincherle (1880), and Dedekind's has received additional prominence through the author's later work (1888) and endorsement by Paul Tannery (1894). Weierstrass, Cantor, and Heine base their theories on infinite series, while Dedekind founds his on the idea of a cut (Schnitt) in the system of real numbers, separating all rational numbers into two groups having certain characteristic properties. The subject has received later contributions at the hands of Weierstrass, Kronecker,[25] and Méray.\n\nThe search for roots of quintic and higher degree equations was an important development, the Abel–Ruffini theorem (Ruffini 1799, Abel 1824) showed that they could not be solved by radicals (formulas involving only arithmetical operations and roots). Hence it was necessary to consider the wider set of algebraic numbers (all solutions to polynomial equations). Galois (1832) linked polynomial equations to group theory giving rise to the field of Galois theory.\n\nSimple continued fractions, closely related to irrational numbers (and due to Cataldi, 1613), received attention at the hands of Euler,[26] and at the opening of the 19th century were brought into prominence through the writings of Joseph Louis Lagrange. Other noteworthy contributions have been made by Druckenmüller (1837), Kunze (1857), Lemke (1870), and Günther (1872). Ramus[27] first connected the subject with determinants, resulting, with the subsequent contributions of Heine,[28] Möbius, and Günther,[29] in the theory of Kettenbruchdeterminanten.\n\nThe existence of transcendental numbers[30] was first established by Liouville (1844, 1851). Hermite proved in 1873 that e is transcendental and Lindemann proved in 1882 that π is transcendental. Finally, Cantor showed that the set of all real numbers is uncountably infinite but the set of all algebraic numbers is countably infinite, so there is an uncountably infinite number of transcendental numbers.\n\nThe earliest known conception of mathematical infinity appears in the Yajur Veda, an ancient Indian script, which at one point states, \"If you remove a part from infinity or add a part to infinity, still what remains is infinity.\" Infinity was a popular topic of philosophical study among the Jain mathematicians c. 400 BC. They distinguished between five types of infinity: infinite in one and two directions, infinite in area, infinite everywhere, and infinite perpetually. The symbol \n\n\n\n\n∞\n\n\n\n{\\displaystyle {\\text{∞}}}\n\n is often used to represent an infinite quantity.\n\nAristotle defined the traditional Western notion of mathematical infinity. He distinguished between actual infinity and potential infinity—the general consensus being that only the latter had true value. Galileo Galilei's Two New Sciences discussed the idea of one-to-one correspondences between infinite sets. But the next major advance in the theory was made by Georg Cantor; in 1895 he published a book about his new set theory, introducing, among other things, transfinite numbers and formulating the continuum hypothesis.\n\nIn the 1960s, Abraham Robinson showed how infinitely large and infinitesimal numbers can be rigorously defined and used to develop the field of nonstandard analysis. The system of hyperreal numbers represents a rigorous method of treating the ideas about infinite and infinitesimal numbers that had been used casually by mathematicians, scientists, and engineers ever since the invention of infinitesimal calculus by Newton and Leibniz.\n\nA modern geometrical version of infinity is given by projective geometry, which introduces \"ideal points at infinity\", one for each spatial direction. Each family of parallel lines in a given direction is postulated to converge to the corresponding ideal point. This is closely related to the idea of vanishing points in perspective drawing.\n\nThe earliest fleeting reference to square roots of negative numbers occurred in the work of the mathematician and inventor Heron of Alexandria in the 1st century AD, when he considered the volume of an impossible frustum of a pyramid. They became more prominent when in the 16th century closed formulas for the roots of third and fourth degree polynomials were discovered by Italian mathematicians such as Niccolò Fontana Tartaglia and Gerolamo Cardano. It was soon realized that these formulas, even if one was only interested in real solutions, sometimes required the manipulation of square roots of negative numbers.\n\nThis was doubly unsettling since they did not even consider negative numbers to be on firm ground at the time. When René Descartes coined the term \"imaginary\" for these quantities in 1637, he intended it as derogatory. (See imaginary number for a discussion of the \"reality\" of complex numbers.) A further source of confusion was that the equation\n\nseemed capriciously inconsistent with the algebraic identity\n\nwhich is valid for positive real numbers a and b, and was also used in complex number calculations with one of a, b positive and the other negative. The incorrect use of this identity, and the related identity\n\nin the case when both a and b are negative even bedeviled Euler.[31] This difficulty eventually led him to the convention of using the special symbol i in place of \n\n\n\n\n\n−\n1\n\n\n\n\n{\\displaystyle {\\sqrt {-1}}}\n\n to guard against this mistake.\n\nThe 18th century saw the work of Abraham de Moivre and Leonhard Euler. De Moivre's formula (1730) states:\n\nwhile Euler's formula of complex analysis (1748) gave us:\n\nThe existence of complex numbers was not completely accepted until Caspar Wessel described the geometrical interpretation in 1799. Carl Friedrich Gauss rediscovered and popularized it several years later, and as a result the theory of complex numbers received a notable expansion. The idea of the graphic representation of complex numbers had appeared, however, as early as 1685, in Wallis's De algebra tractatus.\n\nIn the same year, Gauss provided the first generally accepted proof of the fundamental theorem of algebra, showing that every polynomial over the complex numbers has a full set of solutions in that realm. Gauss studied complex numbers of the form a + bi, where a and b are integers (now called Gaussian integers) or rational numbers. His student, Gotthold Eisenstein, studied the type a + bω, where ω is a complex root of x3 − 1 = 0 (now called Eisenstein integers). Other such classes (called cyclotomic fields) of complex numbers derive from the roots of unity xk − 1 = 0 for higher values of k. This generalization is largely due to Ernst Kummer, who also invented ideal numbers, which were expressed as geometrical entities by Felix Klein in 1893.\n\nIn 1850 Victor Alexandre Puiseux took the key step of distinguishing between poles and branch points, and introduced the concept of essential singular points.[clarification needed] This eventually led to the concept of the extended complex plane.\n\nPrime numbers have been studied throughout recorded history.[citation needed] They are positive integers that are divisible only by 1 and themselves. Euclid devoted one book of the Elements to the theory of primes; in it he proved the infinitude of the primes and the fundamental theorem of arithmetic, and presented the Euclidean algorithm for finding the greatest common divisor of two numbers.\n\nIn 240 BC, Eratosthenes used the Sieve of Eratosthenes to quickly isolate prime numbers. But most further development of the theory of primes in Europe dates to the Renaissance and later eras.[citation needed]\n\nIn 1796, Adrien-Marie Legendre conjectured the prime number theorem, describing the asymptotic distribution of primes. Other results concerning the distribution of the primes include Euler's proof that the sum of the reciprocals of the primes diverges, and the Goldbach conjecture, which claims that any sufficiently large even number is the sum of two primes. Yet another conjecture related to the distribution of prime numbers is the Riemann hypothesis, formulated by Bernhard Riemann in 1859. The prime number theorem was finally proved by Jacques Hadamard and Charles de la Vallée-Poussin in 1896. Goldbach and Riemann's conjectures remain unproven and unrefuted.\n\nNumbers can be classified into sets, called number sets or number systems, such as the natural numbers and the real numbers. The main number systems are as follows:\n\n\n\n\n\n\n\nN\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbb {N} _{0}}\n\n or \n\n\n\n\n\nN\n\n\n1\n\n\n\n\n{\\displaystyle \\mathbb {N} _{1}}\n\n are sometimes used.\n\nEach of these number systems is a subset of the next one. So, for example, a rational number is also a real number, and every real number is also a complex number. This can be expressed symbolically as\n\nA more complete list of number sets appears in the following diagram.\n\nThe most familiar numbers are the natural numbers (sometimes called whole numbers or counting numbers): 1, 2, 3, and so on. Traditionally, the sequence of natural numbers started with 1 (0 was not even considered a number for the Ancient Greeks.) However, in the 19th century, set theorists and other mathematicians started including 0 (cardinality of the empty set, i.e. 0 elements, where 0 is thus the smallest cardinal number) in the set of natural numbers.[32][33] Today, different mathematicians use the term to describe both sets, including 0 or not. The mathematical symbol for the set of all natural numbers is N, also written \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n, and sometimes \n\n\n\n\n\nN\n\n\n0\n\n\n\n\n{\\displaystyle \\mathbb {N} _{0}}\n\n or \n\n\n\n\n\nN\n\n\n1\n\n\n\n\n{\\displaystyle \\mathbb {N} _{1}}\n\n when it is necessary to indicate whether the set should start with 0 or 1, respectively.\n\nIn the base 10 numeral system, in almost universal use today for mathematical operations, the symbols for natural numbers are written using ten digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. The radix or base is the number of unique numerical digits, including zero, that a numeral system uses to represent numbers (for the decimal system, the radix is 10). In this base 10 system, the rightmost digit of a natural number has a place value of 1, and every other digit has a place value ten times that of the place value of the digit to its right.\n\nIn set theory, which is capable of acting as an axiomatic foundation for modern mathematics,[34] natural numbers can be represented by classes of equivalent sets. For instance, the number 3 can be represented as the class of all sets that have exactly three elements. Alternatively, in Peano Arithmetic, the number 3 is represented as sss0, where s is the \"successor\" function (i.e., 3 is the third successor of 0). Many different representations are possible; all that is needed to formally represent 3 is to inscribe a certain symbol or pattern of symbols three times.\n\nThe negative of a positive integer is defined as a number that produces 0 when it is added to the corresponding positive integer. Negative numbers are usually written with a negative sign (a minus sign). As an example, the negative of 7 is written −7, and 7 + (−7) = 0. When the set of negative numbers is combined with the set of natural numbers (including 0), the result is defined as the set of integers, Z also written \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n. Here the letter Z comes from German  Zahl 'number'. The set of integers forms a ring with the operations addition and multiplication.[35]\n\nThe natural numbers form a subset of the integers. As there is no common standard for the inclusion or not of zero in the natural numbers, the natural numbers without zero are commonly referred to as positive integers, and the natural numbers with zero are referred to as non-negative integers.\n\nA rational number is a number that can be expressed as a fraction with an integer numerator and a positive integer denominator. Negative denominators are allowed, but are commonly avoided, as every rational number is equal to a fraction with positive denominator. Fractions are written as two integers, the numerator and the denominator, with a dividing bar between them. The fraction ⁠m/n⁠ represents m parts of a whole divided into n equal parts. Two different fractions may correspond to the same rational number; for example ⁠1/2⁠ and ⁠2/4⁠ are equal, that is:\n\nIn general,\n\nIf the absolute value of m is greater than n (supposed to be positive), then the absolute value of the fraction is greater than 1. Fractions can be greater than, less than, or equal to 1 and can also be positive, negative, or 0. The set of all rational numbers includes the integers since every integer can be written as a fraction with denominator 1. For example −7 can be written ⁠−7/1⁠. The symbol for the rational numbers is Q (for quotient), also written \n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n.\n\nThe symbol for the real numbers is R, also written as \n\n\n\n\nR\n\n.\n\n\n{\\displaystyle \\mathbb {R} .}\n\n They include all the measuring numbers. Every real number corresponds to a point on the number line. The following paragraph will focus primarily on positive real numbers. The treatment of negative real numbers is according to the general rules of arithmetic and their denotation is simply prefixing the corresponding positive numeral by a minus sign, e.g. −123.456.\n\nMost real numbers can only be approximated by decimal numerals, in which a decimal point is placed to the right of the digit with place value 1. Each digit to the right of the decimal point has a place value one-tenth of the place value of the digit to its left. For example, 123.456 represents ⁠123456/1000⁠, or, in words, one hundred, two tens, three ones, four tenths, five hundredths, and six thousandths. A real number can be expressed by a finite number of decimal digits only if it is rational and its fractional part has a denominator whose prime factors are 2 or 5 or both, because these are the prime factors of 10, the base of the decimal system. Thus, for example, one half is 0.5, one fifth is 0.2, one-tenth is 0.1, and one fiftieth is 0.02. Representing other real numbers as decimals would require an infinite sequence of digits to the right of the decimal point. If this infinite sequence of digits follows a pattern, it can be written with an ellipsis or another notation that indicates the repeating pattern. Such a decimal is called a repeating decimal. Thus ⁠1/3⁠ can be written as 0.333..., with an ellipsis to indicate that the pattern continues. Forever repeating 3s are also written as 0.3.[36]\n\nIt turns out that these repeating decimals (including the repetition of zeroes) denote exactly the rational numbers, i.e., all rational numbers are also real numbers, but it is not the case that every real number is rational. A real number that is not rational is called irrational. A famous irrational real number is the π, the ratio of the circumference of any circle to its diameter. When pi is written as\n\nas it sometimes is, the ellipsis does not mean that the decimals repeat (they do not), but rather that there is no end to them. It has been proved that π is irrational. Another well-known number, proven to be an irrational real number, is\n\nthe square root of 2, that is, the unique positive real number whose square is 2. Both these numbers have been approximated (by computer) to trillions ( 1 trillion = 1012 = 1,000,000,000,000 ) of digits.\n\nNot only these prominent examples but almost all real numbers are irrational and therefore have no repeating patterns and hence no corresponding decimal numeral. They can only be approximated by decimal numerals, denoting rounded or truncated real numbers. Any rounded or truncated number is necessarily a rational number, of which there are only countably many. All measurements are, by their nature, approximations, and always have a margin of error. Thus 123.456 is considered an approximation of any real number greater or equal to ⁠1234555/10000⁠ and strictly less than ⁠1234565/10000⁠ (rounding to 3 decimals), or of any real number greater or equal to ⁠123456/1000⁠ and strictly less than ⁠123457/1000⁠ (truncation after the 3. decimal). Digits that suggest a greater accuracy than the measurement itself does, should be removed. The remaining digits are then called significant digits. For example, measurements with a ruler can seldom be made without a margin of error of at least 0.001 m. If the sides of a rectangle are measured as 1.23 m and 4.56 m, then multiplication gives an area for the rectangle between 5.614591 m2 and 5.603011 m2. Since not even the second digit after the decimal place is preserved, the following digits are not significant. Therefore, the result is usually rounded to 5.61.\n\nJust as the same fraction can be written in more than one way, the same real number may have more than one decimal representation. For example, 0.999..., 1.0, 1.00, 1.000, ..., all represent the natural number 1. A given real number has only the following decimal representations: an approximation to some finite number of decimal places, an approximation in which a pattern is established that continues for an unlimited number of decimal places or an exact value with only finitely many decimal places. In this last case, the last non-zero digit may be replaced by the digit one smaller followed by an unlimited number of 9s, or the last non-zero digit may be followed by an unlimited number of zeros. Thus the exact real number 3.74 can also be written 3.7399999999... and 3.74000000000.... Similarly, a decimal numeral with an unlimited number of 0s can be rewritten by dropping the 0s to the right of the rightmost nonzero digit, and a decimal numeral with an unlimited number of 9s can be rewritten by increasing by one the rightmost digit less than 9, and changing all the 9s to the right of that digit to 0s. Finally, an unlimited sequence of 0s to the right of a decimal place can be dropped. For example, 6.849999999999... = 6.85 and 6.850000000000... = 6.85. Finally, if all of the digits in a numeral are 0, the number is 0, and if all of the digits in a numeral are an unending string of 9s, you can drop the nines to the right of the decimal place, and add one to the string of 9s to the left of the decimal place. For example, 99.999... = 100.\n\nThe real numbers also have an important but highly technical property called the least upper bound property.\n\nIt can be shown that any ordered field, which is also complete, is isomorphic to the real numbers. The real numbers are not, however, an algebraically closed field, because they do not include a solution (often called a square root of minus one) to the algebraic equation \n\n\n\n\nx\n\n2\n\n\n+\n1\n=\n0\n\n\n{\\displaystyle x^{2}+1=0}\n\n.\n\nMoving to a greater level of abstraction, the real numbers can be extended to the complex numbers. This set of numbers arose historically from trying to find closed formulas for the roots of cubic and quadratic polynomials. This led to expressions involving the square roots of negative numbers, and eventually to the definition of a new number: a square root of −1, denoted by i, a symbol assigned by Leonhard Euler, and called the imaginary unit. The complex numbers consist of all numbers of the form\n\nwhere a and b are real numbers. Because of this, complex numbers correspond to points on the complex plane, a vector space of two real dimensions. In the expression a + bi, the real number a is called the real part and b is called the imaginary part. If the real part of a complex number is 0, then the number is called an imaginary number or is referred to as purely imaginary; if the imaginary part is 0, then the number is a real number. Thus the real numbers are a subset of the complex numbers. If the real and imaginary parts of a complex number are both integers, then the number is called a Gaussian integer. The symbol for the complex numbers is C or \n\n\n\n\nC\n\n\n\n{\\displaystyle \\mathbb {C} }\n\n.\n\nThe fundamental theorem of algebra asserts that the complex numbers form an algebraically closed field, meaning that every polynomial with complex coefficients has a root in the complex numbers. Like the reals, the complex numbers form a field, which is complete, but unlike the real numbers, it is not ordered. That is, there is no consistent meaning assignable to saying that i is greater than 1, nor is there any meaning in saying that i is less than 1. In technical terms, the complex numbers lack a total order that is compatible with field operations.\n\nAn even number is an integer that is \"evenly divisible\" by two, that is divisible by two without remainder; an odd number is an integer that is not even. (The old-fashioned term \"evenly divisible\" is now almost always shortened to \"divisible\".) Any odd number n may be constructed by the formula n = 2k + 1, for a suitable integer k. Starting with k = 0, the first non-negative odd numbers are {1, 3, 5, 7, ...}. Any even number m has the form m = 2k where k is again an integer. Similarly, the first non-negative even numbers are {0, 2, 4, 6, ...}.\n\nA prime number, often shortened to just prime, is an integer greater than 1 that is not the product of two smaller positive integers. The first few prime numbers are 2, 3, 5, 7, and 11. There is no such simple formula as for odd and even numbers to generate the prime numbers. The primes have been widely studied for more than 2000 years and have led to many questions, only some of which have been answered. The study of these questions belongs to number theory. Goldbach's conjecture is an example of a still unanswered question: \"Is every even number the sum of two primes?\"\n\nOne answered question, as to whether every integer greater than one is a product of primes in only one way, except for a rearrangement of the primes, was confirmed; this proven claim is called the fundamental theorem of arithmetic. A proof appears in Euclid's Elements.\n\nMany subsets of the natural numbers have been the subject of specific studies and have been named, often after the first mathematician that has studied them. Example of such sets of integers are Fibonacci numbers and perfect numbers. For more examples, see Integer sequence.\n\nAlgebraic numbers are those that are a solution to a polynomial equation with integer coefficients. Real numbers that are not rational numbers are called irrational numbers. Complex numbers which are not algebraic are called transcendental numbers. The algebraic numbers that are solutions of a monic polynomial equation with integer coefficients are called algebraic integers.\n\nA period is a complex number that can be expressed as an integral of an algebraic function over an algebraic domain. The periods are a class of numbers which includes, alongside the algebraic numbers, many well known mathematical constants such as the number π. The set of periods form a countable ring and bridge the gap between algebraic and transcendental numbers.[37][38]\n\nThe periods can be extended by permitting the integrand to be the product of an algebraic function and the exponential of an algebraic function. This gives another countable ring: the exponential periods. The number e as well as Euler's constant are exponential periods.[37][39]\n\nMotivated by the classical problems of constructions with straightedge and compass, the constructible numbers are those complex numbers whose real and imaginary parts can be constructed using straightedge and compass, starting from a given segment of unit length, in a finite number of steps.\n\nA computable number, also known as recursive number, is a real number such that there exists an algorithm which, given a positive number n as input, produces the first n digits of the computable number's decimal representation. Equivalent definitions can be given using μ-recursive functions, Turing machines or λ-calculus. The computable numbers are stable for all usual arithmetic operations, including the computation of the roots of a polynomial, and thus form a real closed field that contains the real algebraic numbers.\n\nThe computable numbers may be viewed as the real numbers that may be exactly represented in a computer: a computable number is exactly represented by its first digits and a program for computing further digits. However, the computable numbers are rarely used in practice. One reason is that there is no algorithm for testing the equality of two computable numbers. More precisely, there cannot exist any algorithm which takes any computable number as an input, and decides in every case if this number is equal to zero or not.\n\nThe set of computable numbers has the same cardinality as the natural numbers. Therefore, almost all real numbers are non-computable. However, it is very difficult to produce explicitly a real number that is not computable.\n\nThe p-adic numbers may have infinitely long expansions to the left of the decimal point, in the same way that real numbers may have infinitely long expansions to the right. The number system that results depends on what base is used for the digits: any base is possible, but a prime number base provides the best mathematical properties. The set of the p-adic numbers contains the rational numbers, but is not contained in the complex numbers.\n\nThe elements of an algebraic function field over a finite field and algebraic numbers have many similar properties (see Function field analogy). Therefore, they are often regarded as numbers by number theorists. The p-adic numbers play an important role in this analogy.\n\nSome number systems that are not included in the complex numbers may be constructed from the real numbers \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n in a way that generalize the construction of the complex numbers. They are sometimes called hypercomplex numbers. They include the quaternions \n\n\n\n\nH\n\n\n\n{\\displaystyle \\mathbb {H} }\n\n, introduced by Sir William Rowan Hamilton, in which multiplication is not commutative, the octonions \n\n\n\n\nO\n\n\n\n{\\displaystyle \\mathbb {O} }\n\n, in which multiplication is not associative in addition to not being commutative, and the sedenions \n\n\n\n\nS\n\n\n\n{\\displaystyle \\mathbb {S} }\n\n, in which multiplication is not alternative, neither associative nor commutative. The hypercomplex numbers include one real unit together with \n\n\n\n\n2\n\nn\n\n\n−\n1\n\n\n{\\displaystyle 2^{n}-1}\n\n imaginary units, for which n is a non-negative integer. For example, quaternions can generally represented using the form\n\n\n\n\n\na\n+\nb\n\n\ni\n\n+\nc\n\n\nj\n\n+\nd\n\n\nk\n\n,\n\n\n{\\displaystyle a+b\\,\\mathbf {i} +c\\,\\mathbf {j} +d\\,\\mathbf {k} ,}\n\n\n\nwhere the coefficients a, b, c, d are real numbers, and i, j, k are 3 different imaginary units.\n\nEach hypercomplex number system is a subset of the next hypercomplex number system of double dimensions obtained via the Cayley–Dickson construction. For example, the 4-dimensional quaternions \n\n\n\n\nH\n\n\n\n{\\displaystyle \\mathbb {H} }\n\n are a subset of the 8-dimensional quaternions \n\n\n\n\nO\n\n\n\n{\\displaystyle \\mathbb {O} }\n\n, which are in turn a subset of the 16-dimensional sedenions \n\n\n\n\nS\n\n\n\n{\\displaystyle \\mathbb {S} }\n\n, in turn a subset of the 32-dimensional trigintaduonions \n\n\n\n\nT\n\n\n\n{\\displaystyle \\mathbb {T} }\n\n, and ad infinitum with \n\n\n\n\n2\n\nn\n\n\n\n\n{\\displaystyle 2^{n}}\n\n dimensions, with n being any non-negative integer. Including the complex and real numbers and their subsets, this can be expressed symbolically as:\n\nAlternatively, starting from the real numbers \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n, which have zero complex units, this can be expressed as\n\nwith \n\n\n\n\nC\n\nn\n\n\n\n\n{\\displaystyle C_{n}}\n\n containing \n\n\n\n\n2\n\nn\n\n\n\n\n{\\displaystyle 2^{n}}\n\n dimensions.[40]\n\nFor dealing with infinite sets, the natural numbers have been generalized to the ordinal numbers and to the cardinal numbers. The former gives the ordering of the set, while the latter gives its size. For finite sets, both ordinal and cardinal numbers are identified with the natural numbers. In the infinite case, many ordinal numbers correspond to the same cardinal number.\n\nHyperreal numbers are used in non-standard analysis. The hyperreals, or nonstandard reals (usually denoted as *R), denote an ordered field that is a proper extension of the ordered field of real numbers R and satisfies the transfer principle. This principle allows true first-order statements about R to be reinterpreted as true first-order statements about *R.\n\nSuperreal and surreal numbers extend the real numbers by adding infinitesimally small numbers and infinitely large numbers, but still form fields.\n"
    },
    {
        "title": "Integer",
        "content": "\n\nAn integer is the number zero (0), a positive natural number (1, 2, 3, . . .), or the negation of a positive natural number (−1, −2, −3, . . .).[1] The negations or additive inverses of the positive natural numbers are referred to as negative integers.[2] The set of all integers is often denoted by the boldface Z or blackboard bold \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n.[3][4]\n\nThe set of natural numbers \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n is a subset of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n, which in turn is a subset of the set of all rational numbers \n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n, itself a subset of the real numbers \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbb {R} }\n\n.[a] Like the set of natural numbers, the set of integers \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is countably infinite. An integer may be regarded as a real number that can be written without a fractional component. For example, 21, 4, 0, and −2048 are integers, while 9.75, ⁠5+1/2⁠, 5/4, and √2 are not.[8]\n\nThe integers form the smallest group and the smallest ring containing the natural numbers. In algebraic number theory, the integers are sometimes qualified as rational integers to distinguish them from the more general algebraic integers. In fact, (rational) integers are algebraic integers that are also rational numbers.\n\nThe word integer comes from the Latin integer meaning \"whole\" or (literally) \"untouched\", from in (\"not\") plus tangere (\"to touch\"). \"Entire\" derives from the same origin via the French word entier, which means both entire and integer.[9] Historically the term was used for a number that was a multiple of 1,[10][11] or to the whole part of a mixed number.[12][13] Only positive integers were considered, making the term synonymous with the natural numbers. The definition of integer expanded over time to include negative numbers as their usefulness was recognized.[14] For example Leonhard Euler in his 1765 Elements of Algebra defined integers to include both positive and negative numbers.[15]\n\nThe phrase the set of the integers was not used before the end of the 19th century, when Georg Cantor introduced the concept of infinite sets and set theory. The use of the letter Z to denote the set of integers comes from the German word Zahlen (\"numbers\")[3][4] and has been attributed to David Hilbert.[16] The earliest known use of the notation in a textbook occurs in Algèbre written by the collective Nicolas Bourbaki, dating to 1947.[3][17] The notation was not adopted immediately. For example, another textbook used the letter J,[18] and a 1960 paper used Z to denote the non-negative integers.[19] But by 1961, Z was generally used by modern algebra texts to denote the positive and negative integers.[20]\n\nThe symbol \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is often annotated to denote various sets, with varying usage amongst different authors: \n\n\n\n\n\nZ\n\n\n+\n\n\n\n\n{\\displaystyle \\mathbb {Z} ^{+}}\n\n, \n\n\n\n\n\nZ\n\n\n+\n\n\n\n\n{\\displaystyle \\mathbb {Z} _{+}}\n\n, or \n\n\n\n\n\nZ\n\n\n>\n\n\n\n\n{\\displaystyle \\mathbb {Z} ^{>}}\n\n for the positive integers, \n\n\n\n\n\nZ\n\n\n0\n+\n\n\n\n\n{\\displaystyle \\mathbb {Z} ^{0+}}\n\n or \n\n\n\n\n\nZ\n\n\n≥\n\n\n\n\n{\\displaystyle \\mathbb {Z} ^{\\geq }}\n\n for non-negative integers, and \n\n\n\n\n\nZ\n\n\n≠\n\n\n\n\n{\\displaystyle \\mathbb {Z} ^{\\neq }}\n\n for non-zero integers. Some authors use \n\n\n\n\n\nZ\n\n\n∗\n\n\n\n\n{\\displaystyle \\mathbb {Z} ^{*}}\n\n for non-zero integers, while others use it for non-negative integers, or for {–1,1} (the group of units of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n). Additionally, \n\n\n\n\n\nZ\n\n\np\n\n\n\n\n{\\displaystyle \\mathbb {Z} _{p}}\n\n is used to denote either the set of integers modulo p (i.e., the set of congruence classes of integers), or the set of p-adic integers.[21][22]\n\n\nThe whole numbers were synonymous with the integers up until the early 1950s.[23][24][25] In the late 1950s, as part of the New Math movement,[26] American elementary school teachers began teaching that whole numbers referred to the natural numbers, excluding negative numbers, while integer included the negative numbers.[27][28] The whole numbers remain ambiguous to the present day.[29]\n\nRing homomorphisms\n\nAlgebraic structures\n\nRelated structures\n\nAlgebraic number theory\n\nNoncommutative algebraic geometry\n\nFree algebra\n\nClifford algebra\n\nLike the natural numbers, \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is closed under the operations of addition and multiplication, that is, the sum and product of any two integers is an integer. However, with the inclusion of the negative natural numbers (and importantly, 0), \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n, unlike the natural numbers, is also closed under subtraction.[30]\n\nThe integers form a ring which is the most basic one, in the following sense: for any ring, there is a unique ring homomorphism from the integers into this ring. This universal property, namely to be an initial object in the category of rings, characterizes the ring \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n.\n\n\n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is not closed under division, since the quotient of two integers (e.g., 1 divided by 2) need not be an integer. Although the natural numbers are closed under exponentiation, the integers are not (since the result can be a fraction when the exponent is negative).\n\nThe following table lists some of the basic properties of addition and multiplication for any integers a, b, and c:\n\nThe first five properties listed above for addition say that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n, under addition, is an abelian group. It is also a cyclic group, since every non-zero integer can be written as a finite sum 1 + 1 + ... + 1 or (−1) + (−1) + ... + (−1). In fact, \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n under addition is the only infinite cyclic group—in the sense that any infinite cyclic group is isomorphic to \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n.\n\nThe first four properties listed above for multiplication say that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n under multiplication is a commutative monoid. However, not every integer has a multiplicative inverse (as is the case of the number 2), which means that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n under multiplication is not a group.\n\nAll the rules from the above property table (except for the last), when taken together, say that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n together with addition and multiplication is a commutative ring with unity. It is the prototype of all objects of such algebraic structure. Only those equalities of expressions are true in \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n for all values of variables, which are true in any unital commutative ring. Certain non-zero integers map to zero in certain rings.\n\nThe lack of zero divisors in the integers (last property in the table) means that the commutative ring \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is an integral domain.\n\nThe lack of multiplicative inverses, which is equivalent to the fact that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is not closed under division, means that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is not a field. The smallest field containing the integers as a subring is the field of rational numbers. The process of constructing the rationals from the integers can be mimicked to form the field of fractions of any integral domain. And back, starting from an algebraic number field (an extension of rational numbers), its ring of integers can be extracted, which includes \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n as its subring.\n\nAlthough ordinary division is not defined on \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n, the division \"with remainder\" is defined on them. It is called Euclidean division, and possesses the following important property: given two integers a and b with b ≠ 0, there exist unique integers q and r such that a = q × b + r and 0 ≤ r <  |b|, where |b| denotes the absolute value of b. The integer q is called the quotient and r is called the remainder of the division of a by b. The Euclidean algorithm for computing greatest common divisors works by a sequence of Euclidean divisions.\n\nThe above says that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is a Euclidean domain. This implies that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is a principal ideal domain, and any positive integer can be written as the products of primes in an essentially unique way.[31] This is the fundamental theorem of arithmetic.\n\n\n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is a totally ordered set without upper or lower bound. The ordering of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is given by:\n:... −3 < −2 < −1 < 0 < 1 < 2 < 3 < ....\nAn integer is positive if it is greater than zero, and negative if it is less than zero. Zero is defined as neither negative nor positive.\n\nThe ordering of integers is compatible with the algebraic operations in the following way:\n\nThus it follows that \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n together with the above ordering is an ordered ring.\n\nThe integers are the only nontrivial totally ordered abelian group whose positive elements are well-ordered.[32] This is equivalent to the statement that any Noetherian valuation ring is either a field—or a discrete valuation ring.\n\nIn elementary school teaching, integers are often intuitively defined as the union of the (positive) natural numbers, zero, and the negations of the natural numbers. This can be formalized as follows.[33] First construct the set of natural numbers according to the Peano axioms, call this \n\n\n\nP\n\n\n{\\displaystyle P}\n\n. Then construct a set \n\n\n\n\nP\n\n−\n\n\n\n\n{\\displaystyle P^{-}}\n\n which is disjoint from \n\n\n\nP\n\n\n{\\displaystyle P}\n\n and in one-to-one correspondence with \n\n\n\nP\n\n\n{\\displaystyle P}\n\n via a function \n\n\n\nψ\n\n\n{\\displaystyle \\psi }\n\n. For example, take \n\n\n\n\nP\n\n−\n\n\n\n\n{\\displaystyle P^{-}}\n\n to be the ordered pairs \n\n\n\n(\n1\n,\nn\n)\n\n\n{\\displaystyle (1,n)}\n\n with the mapping \n\n\n\nψ\n=\nn\n↦\n(\n1\n,\nn\n)\n\n\n{\\displaystyle \\psi =n\\mapsto (1,n)}\n\n. Finally let 0 be some object not in \n\n\n\nP\n\n\n{\\displaystyle P}\n\n or \n\n\n\n\nP\n\n−\n\n\n\n\n{\\displaystyle P^{-}}\n\n, for example the ordered pair (0,0). Then the integers are defined to be the union \n\n\n\nP\n∪\n\nP\n\n−\n\n\n∪\n{\n0\n}\n\n\n{\\displaystyle P\\cup P^{-}\\cup \\{0\\}}\n\n. \n\nThe traditional arithmetic operations can then be defined on the integers in a piecewise fashion, for each of positive numbers, negative numbers, and zero. For example negation is defined as follows:\n\n\n\n\n\n−\nx\n=\n\n\n{\n\n\n\nψ\n(\nx\n)\n,\n\n\n\nif \n\nx\n∈\nP\n\n\n\n\n\nψ\n\n−\n1\n\n\n(\nx\n)\n,\n\n\n\nif \n\nx\n∈\n\nP\n\n−\n\n\n\n\n\n\n0\n,\n\n\n\nif \n\nx\n=\n0\n\n\n\n\n\n\n\n\n{\\displaystyle -x={\\begin{cases}\\psi (x),&{\\text{if }}x\\in P\\\\\\psi ^{-1}(x),&{\\text{if }}x\\in P^{-}\\\\0,&{\\text{if }}x=0\\end{cases}}}\n\n\n\nThe traditional style of definition leads to many different cases (each arithmetic operation needs to be defined on each combination of types of integer) and makes it tedious to prove that integers obey the various laws of arithmetic.[34]\n\nIn modern set-theoretic mathematics, a more abstract construction[35][36] allowing one to define arithmetical operations without any case distinction is often used instead.[37] The integers can thus be formally constructed as the equivalence classes of ordered pairs of natural numbers (a,b).[38]\n\nThe intuition is that (a,b) stands for the result of subtracting b from a.[38] To confirm our expectation that 1 − 2 and 4 − 5 denote the same number, we define an equivalence relation ~ on these pairs with the following rule:\n\nprecisely when\n\nAddition and multiplication of integers can be defined in terms of the equivalent operations on the natural numbers;[38] by using [(a,b)] to denote the equivalence class having (a,b) as a member, one has:\n\nThe negation (or additive inverse) of an integer is obtained by reversing the order of the pair:\n\nHence subtraction can be defined as the addition of the additive inverse:\n\nThe standard ordering on the integers is given by:\n\nIt is easily verified that these definitions are independent of the choice of representatives of the equivalence classes.\n\nEvery equivalence class has a unique member that is of the form (n,0) or (0,n) (or both at once). The natural number n is identified with the class [(n,0)] (i.e., the natural numbers are embedded into the integers by map sending n to [(n,0)]), and the class [(0,n)] is denoted −n (this covers all remaining classes, and gives the class [(0,0)] a second time since –0 = 0.\n\nThus, [(a,b)] is denoted by\n\nIf the natural numbers are identified with the corresponding integers (using the embedding mentioned above), this convention creates no ambiguity.\n\nThis notation recovers the familiar representation of the integers as {..., −2, −1, 0, 1, 2, ...} .\n\nSome examples are:\n\nIn theoretical computer science, other approaches for the construction of integers are used by automated theorem provers and term rewrite engines. Integers are represented as algebraic terms built using a few basic operations (e.g.,  zero, succ, pred) and using natural numbers, which are assumed to be already constructed (using the Peano approach).\n\nThere exist at least ten such constructions of signed integers.[39] These constructions differ in several ways: the number of basic operations used for the construction, the number (usually, between 0 and 2), and the types of arguments accepted by these operations; the presence or absence of natural numbers as arguments of some of these operations, and the fact that these operations are free constructors or not, i.e., that the same integer can be represented using only one or many algebraic terms.\n\nThe technique for the construction of integers presented in the previous section corresponds to the particular case where there is a single basic operation pair\n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n that takes as arguments two natural numbers \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n, and returns an integer (equal to \n\n\n\nx\n−\ny\n\n\n{\\displaystyle x-y}\n\n). This operation is not free since the integer 0 can be written pair(0,0), or pair(1,1), or pair(2,2), etc.. This technique of construction is used by the proof assistant Isabelle; however, many other tools use alternative construction techniques, notable those based upon free constructors, which are simpler and can be implemented more efficiently in computers.\n\nAn integer is often a primitive data type in computer languages. However, integer data types can only represent a subset of all integers, since practical computers are of finite capacity. Also, in the common two's complement representation, the inherent definition of sign distinguishes between \"negative\" and \"non-negative\" rather than \"negative, positive, and 0\". (It is, however, certainly possible for a computer to determine whether an integer value is truly positive.) Fixed length integer approximation data types (or subsets) are denoted int or Integer in several programming languages (such as Algol68, C, Java, Delphi, etc.).\n\nVariable-length representations of integers, such as bignums, can store any integer that fits in the computer's memory. Other integer data types are implemented with a fixed size, usually a number of bits which is a power of 2 (4, 8, 16, etc.) or a memorable number of decimal digits (e.g., 9 or 10).\n\nThe set of integers is countably infinite, meaning it is possible to pair each integer with a unique natural number. An example of such a pairing is\n\nMore technically, the cardinality of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n is said to equal ℵ0 (aleph-null). The pairing between elements of \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n and \n\n\n\n\nN\n\n\n\n{\\displaystyle \\mathbb {N} }\n\n is called a bijection.\n\nThis article incorporates material from Integer on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.\n"
    },
    {
        "title": "Rational number",
        "content": "In mathematics, a rational number is a number that can be expressed as the quotient or fraction ⁠\n\n\n\n\n\n\np\nq\n\n\n\n\n\n{\\displaystyle {\\tfrac {p}{q}}}\n\n⁠ of two integers, a numerator p and a non-zero denominator q.[1] For example, ⁠\n\n\n\n\n\n\n3\n7\n\n\n\n\n\n{\\displaystyle {\\tfrac {3}{7}}}\n\n⁠ is a rational number, as is every integer (for example, \n\n\n\n−\n5\n=\n\n\n\n\n−\n5\n\n1\n\n\n\n\n\n{\\displaystyle -5={\\tfrac {-5}{1}}}\n\n). The set of all rational numbers, also referred to as \"the rationals\",[2] the field of rationals[3] or the field of rational numbers is usually denoted by boldface Q, or blackboard bold ⁠\n\n\n\n\nQ\n\n.\n\n\n{\\displaystyle \\mathbb {Q} .}\n\n⁠\n\nA rational number is a real number. The real numbers that are rational are those whose decimal expansion either terminates after a finite number of digits (example: 3/4 = 0.75), or eventually begins to repeat the same finite sequence of digits over and over (example: 9/44 = 0.20454545...).[4] This statement is true not only in base 10, but also in every other integer base, such as the binary and hexadecimal ones (see Repeating decimal § Extension to other bases).\n\nA real number that is not rational is called irrational.[5] Irrational numbers include the square root of 2 (⁠\n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n⁠), π, e, and the golden ratio (φ). Since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational.[1]\n\nRational numbers can be formally defined as equivalence classes of pairs of integers (p, q) with q ≠ 0, using the equivalence relation defined as follows:\n\nThe fraction ⁠\n\n\n\n\n\n\np\nq\n\n\n\n\n\n{\\displaystyle {\\tfrac {p}{q}}}\n\n⁠ then denotes the equivalence class of (p, q).[6]\n\nRational numbers together with addition and multiplication form a field which contains the integers, and is contained in any field containing the integers. In other words, the field of rational numbers is a prime field, and a field has characteristic zero if and only if it contains the rational numbers as a subfield. Finite extensions of ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ are called algebraic number fields, and the algebraic closure of ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ is the field of algebraic numbers.[7]\n\nIn mathematical analysis, the rational numbers form a dense subset of the real numbers. The real numbers can be constructed from the rational numbers by completion, using Cauchy sequences, Dedekind cuts, or infinite decimals (see Construction of the real numbers).\n\nIn mathematics, \"rational\" is often used as a noun abbreviating \"rational number\". The adjective rational sometimes means that the coefficients are rational numbers. For example, a rational point is a point with rational coordinates (i.e., a point whose coordinates are rational numbers); a rational matrix is a matrix of rational numbers; a rational polynomial may be a polynomial with rational coefficients, although the term \"polynomial over the rationals\" is generally preferred, to avoid confusion between \"rational expression\" and \"rational function\" (a polynomial is a rational expression and defines a rational function, even if its coefficients are not rational numbers). However, a rational curve is not a curve defined over the rationals, but a curve which can be parameterized by rational functions.\n\nAlthough nowadays rational numbers are defined in terms of ratios, the term rational is not a derivation of ratio. On the contrary, it is ratio that is derived from rational: the first use of ratio with its modern meaning was attested in English about 1660,[8] while the use of rational for qualifying numbers appeared almost a century earlier, in 1570.[9] This meaning of rational came from the mathematical meaning of irrational, which was first used in 1551, and it was used in \"translations of Euclid (following his peculiar use of ἄλογος)\".[10][11]\n\nThis unusual history originated in the fact that ancient Greeks \"avoided heresy by forbidding themselves from thinking of those [irrational] lengths as numbers\".[12] So such lengths were irrational, in the sense of illogical, that is \"not to be spoken about\" (ἄλογος in Greek).[13]\n\nEvery rational number may be expressed in a unique way as an irreducible fraction ⁠\n\n\n\n\n\n\na\nb\n\n\n\n,\n\n\n{\\displaystyle {\\tfrac {a}{b}},}\n\n⁠ where a and b are coprime integers and b > 0. This is often called the canonical form of the rational number.\n\nStarting from a rational number ⁠\n\n\n\n\n\n\na\nb\n\n\n\n,\n\n\n{\\displaystyle {\\tfrac {a}{b}},}\n\n⁠ its canonical form may be obtained by dividing a and b by their greatest common divisor, and, if b < 0, changing the sign of the resulting numerator and denominator.\n\nAny integer n can be expressed as the rational number ⁠\n\n\n\n\n\n\nn\n1\n\n\n\n,\n\n\n{\\displaystyle {\\tfrac {n}{1}},}\n\n⁠ which is its canonical form as a rational number.\n\nIf both fractions are in canonical form, then: \n\nIf both denominators are positive (particularly if both fractions are in canonical form):\n\nOn the other hand, if either denominator is negative, then each fraction with a negative denominator must first be converted into an equivalent form with a positive denominator—by changing the signs of both its numerator and denominator.[6]\n\nTwo fractions are added as follows:\n\nIf both fractions are in canonical form, the result is in canonical form if and only if b, d are coprime integers.[6][14]\n\nIf both fractions are in canonical form, the result is in canonical form if and only if b, d are coprime integers.[14]\n\nThe rule for multiplication is:\n\nwhere the result may be a reducible fraction—even if both original fractions are in canonical form.[6][14]\n\nEvery rational number ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ has an additive inverse, often called its opposite,\n\nIf ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ is in canonical form, the same is true for its opposite.\n\nA nonzero rational number ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ has a multiplicative inverse, also called its reciprocal,\n\nIf ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ is in canonical form, then the canonical form of its reciprocal is either ⁠\n\n\n\n\n\n\nb\na\n\n\n\n\n\n{\\displaystyle {\\tfrac {b}{a}}}\n\n⁠ or ⁠\n\n\n\n\n\n\n\n−\nb\n\n\n−\na\n\n\n\n\n,\n\n\n{\\displaystyle {\\tfrac {-b}{-a}},}\n\n⁠ depending on the sign of a.\n\nIf b, c, d are nonzero, the division rule is \n\nThus, dividing ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ by ⁠\n\n\n\n\n\n\nc\nd\n\n\n\n\n\n{\\displaystyle {\\tfrac {c}{d}}}\n\n⁠ is equivalent to multiplying ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ by the reciprocal of ⁠\n\n\n\n\n\n\nc\nd\n\n\n\n:\n\n\n{\\displaystyle {\\tfrac {c}{d}}:}\n\n⁠[14]\n\nIf n is a non-negative integer, then\n\nThe result is in canonical form if the same is true for ⁠\n\n\n\n\n\n\na\nb\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {a}{b}}.}\n\n⁠ In particular, \n\nIf a ≠ 0, then\n\nIf ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ is in canonical form, the canonical form of the result is ⁠\n\n\n\n\n\n\n\nb\n\nn\n\n\n\na\n\nn\n\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {b^{n}}{a^{n}}}}\n\n⁠ if a > 0 or n is even. Otherwise, the canonical form of the result is ⁠\n\n\n\n\n\n\n\n−\n\nb\n\nn\n\n\n\n\n−\n\na\n\nn\n\n\n\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {-b^{n}}{-a^{n}}}.}\n\n⁠\n\nA finite continued fraction is an expression such as\n\nwhere an are integers. Every rational number ⁠\n\n\n\n\n\n\na\nb\n\n\n\n\n\n{\\displaystyle {\\tfrac {a}{b}}}\n\n⁠ can be represented as a finite continued fraction, whose coefficients an can be determined by applying the Euclidean algorithm to (a, b).\n\nare different ways to represent the same rational value.\n\nThe rational numbers may be built as equivalence classes of ordered pairs of integers.[6][14]\n\nMore precisely, let ⁠\n\n\n\n(\n\nZ\n\n×\n(\n\nZ\n\n∖\n{\n0\n}\n)\n)\n\n\n{\\displaystyle (\\mathbb {Z} \\times (\\mathbb {Z} \\setminus \\{0\\}))}\n\n⁠ be the set of the pairs (m, n) of integers such n ≠ 0. An equivalence relation is defined on this set by \n\nAddition and multiplication can be defined by the following rules:\n\nThis equivalence relation is a congruence relation, which means that it is compatible with the addition and multiplication defined above; the set of rational numbers ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ is the defined as the quotient set by this equivalence relation, ⁠\n\n\n\n(\n\nZ\n\n×\n(\n\nZ\n\n∖\n{\n0\n}\n)\n)\n\n/\n\n∼\n,\n\n\n{\\displaystyle (\\mathbb {Z} \\times (\\mathbb {Z} \\backslash \\{0\\}))/\\sim ,}\n\n⁠ equipped with the addition and the multiplication induced by the above operations. (This construction can be carried out with any integral domain and produces its field of fractions.)[6]\n\nThe equivalence class of a pair (m, n) is denoted ⁠\n\n\n\n\n\n\nm\nn\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {m}{n}}.}\n\n⁠  \nTwo pairs (m1, n1) and (m2, n2) belong to the same equivalence class (that is are equivalent) if and only if \n\nThis means that \n\nif and only if[6][14]\n\nEvery equivalence class ⁠\n\n\n\n\n\n\nm\nn\n\n\n\n\n\n{\\displaystyle {\\tfrac {m}{n}}}\n\n⁠ may be represented by infinitely many pairs, since\n\nEach equivalence class contains a unique canonical representative element. The canonical representative is the unique pair (m, n) in the equivalence class such that m and n are coprime, and n > 0. It is called the representation in lowest terms of the rational number.\n\nThe integers may be considered to be rational numbers identifying the integer n with the rational number ⁠\n\n\n\n\n\n\nn\n1\n\n\n\n.\n\n\n{\\displaystyle {\\tfrac {n}{1}}.}\n\n⁠\n\nA total order may be defined on the rational numbers, that extends the natural order of the integers. One has\n\nIf \n\nThe set ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ of all rational numbers, together with the addition and multiplication operations shown above, forms a field.[6]\n\n⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ has no field automorphism other than the identity. (A field automorphism must fix 0 and 1; as it must fix the sum and the difference  of two fixed elements, it must fix every integer; as it must fix the quotient of two fixed elements, it must fix every rational number, and is thus the identity.)\n\n⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ is a prime field, which is a field that has no subfield other than itself.[15] The rationals are the smallest field with characteristic zero. Every field of characteristic zero contains a unique subfield isomorphic to ⁠\n\n\n\n\nQ\n\n.\n\n\n{\\displaystyle \\mathbb {Q} .}\n\n⁠\n\nWith the order defined above, ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ is an ordered field[14] that has no subfield other than itself, and is the smallest ordered field, in the sense that every ordered field contains a unique subfield isomorphic to ⁠\n\n\n\n\nQ\n\n.\n\n\n{\\displaystyle \\mathbb {Q} .}\n\n⁠\n\n⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ is the field of fractions of the integers ⁠\n\n\n\n\nZ\n\n.\n\n\n{\\displaystyle \\mathbb {Z} .}\n\n⁠[16] The algebraic closure of ⁠\n\n\n\n\nQ\n\n,\n\n\n{\\displaystyle \\mathbb {Q} ,}\n\n⁠ i.e. the field of roots of rational polynomials, is the field of algebraic numbers.\n\nThe rationals are a densely ordered set: between any two rationals, there sits another one, and, therefore, infinitely many other ones.[6] For example, for any two fractions such that \n\n(where \n\n\n\nb\n,\nd\n\n\n{\\displaystyle b,d}\n\n are positive), we have\n\nAny totally ordered set which is countable, dense (in the above sense), and has no least or greatest element is order isomorphic to the rational numbers.[17]\n\nThe set of all rational numbers is countable, as is illustrated in the figure to the right. As a rational number can be expressed as a ratio of two integers, it is possible to assign two integers to any point on a square lattice as in a Cartesian coordinate system, such that any grid point corresponds to a rational number. This method, however, exhibits a form of redundancy, as several different grid points will correspond to the same rational number; these are highlighted in red on the provided graphic. An obvious example can be seen in the line going diagonally towards the bottom right; such ratios will always equal 1, as any non-zero number divided by itself will always equal one.\n\nIt is possible to generate all of the rational numbers without such redundancies: examples include the Calkin–Wilf tree and Stern–Brocot tree.\n\nAs the set of all rational numbers is countable, and the set of all real numbers (as well as the set of irrational numbers) is uncountable, the set of rational numbers is a null set, that is, almost all real numbers are irrational, in the sense of Lebesgue measure.\n\nThe rationals are a dense subset of the real numbers; every real number has rational numbers arbitrarily close to it.[6] A related property is that rational numbers are the only numbers with finite expansions as regular continued fractions.[18]\n\nIn the usual topology of the real numbers, the rationals are neither an open set nor a closed set.[19]\n\nBy virtue of their order, the rationals carry an order topology. The rational numbers, as a subspace of the real numbers, also carry a subspace topology. The rational numbers form a metric space by using the absolute difference metric \n\n\n\nd\n(\nx\n,\ny\n)\n=\n\n|\n\nx\n−\ny\n\n|\n\n,\n\n\n{\\displaystyle d(x,y)=|x-y|,}\n\n and this yields a third topology on ⁠\n\n\n\n\nQ\n\n.\n\n\n{\\displaystyle \\mathbb {Q} .}\n\n⁠ All three topologies coincide and turn the rationals into a topological field. The rational numbers are an important example of a space which is not locally compact. The rationals are characterized topologically as the unique countable metrizable space without isolated points. The space is also totally disconnected. The rational numbers do not form a complete metric space, and the real numbers are the completion of ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ under the metric \n\n\n\nd\n(\nx\n,\ny\n)\n=\n\n|\n\nx\n−\ny\n\n|\n\n\n\n{\\displaystyle d(x,y)=|x-y|}\n\n above.[14]\n\nIn addition to the absolute value metric mentioned above, there are other metrics which turn ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ into a topological field:\n\nLet p be a prime number and for any non-zero integer a, let \n\n\n\n\n|\n\na\n\n\n|\n\n\np\n\n\n=\n\np\n\n−\nn\n\n\n,\n\n\n{\\displaystyle |a|_{p}=p_{-n},}\n\n where pn is the highest power of p dividing a.\n\nIn addition set \n\n\n\n\n|\n\n0\n\n\n|\n\n\np\n\n\n=\n0.\n\n\n{\\displaystyle |0|_{p}=0.}\n\n For any rational number ⁠\n\n\n\n\n\na\nb\n\n\n,\n\n\n{\\displaystyle {\\frac {a}{b}},}\n\n⁠ we set \n\nThen \n\ndefines a metric on ⁠\n\n\n\n\nQ\n\n.\n\n\n{\\displaystyle \\mathbb {Q} .}\n\n⁠[20]\n\nThe metric space ⁠\n\n\n\n(\n\nQ\n\n,\n\nd\n\np\n\n\n)\n\n\n{\\displaystyle (\\mathbb {Q} ,d_{p})}\n\n⁠ is not complete, and its completion is the p-adic number field ⁠\n\n\n\n\n\nQ\n\n\np\n\n\n.\n\n\n{\\displaystyle \\mathbb {Q} _{p}.}\n\n⁠ Ostrowski's theorem states that any non-trivial absolute value on the rational numbers ⁠\n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbb {Q} }\n\n⁠ is equivalent to either the usual real absolute value or a p-adic absolute value.\n"
    },
    {
        "title": "Numerical analysis",
        "content": "\n\nNumerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). It is the study of numerical methods that attempt to find approximate solutions of problems rather than the exact ones. Numerical analysis finds application in all fields of engineering and the physical sciences, and in the 21st century also the life and social sciences like economics, medicine, business and even the arts. Current growth in computing power has enabled the use of more complex numerical analysis, providing detailed and realistic mathematical models in science and engineering. Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis,[2][3][4] and stochastic differential equations and Markov chains for simulating living cells in medicine and biology.\n\nBefore modern computers, numerical methods often relied on hand interpolation formulas, using data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas continue to be used in software algorithms.[5]\n\nThe numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.\n\nNumerical analysis continues this long tradition: rather than giving exact symbolic answers translated into digits and applicable only to real-world measurements, approximate solutions within specified error bounds are used.\n\nThe overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to a wide variety of hard problems, many of which are infeasible to solve symbolically:\n\nThe field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis,[5] as is obvious from the names of important algorithms like Newton's method, Lagrange interpolation polynomial, Gaussian elimination, or Euler's method. The origins of modern numerical analysis are often linked to a 1947 paper by John von Neumann and Herman Goldstine,[7][8]\nbut others consider modern numerical analysis to go back to work by E. T. Whittaker in 1912.[7]\n\nTo facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients. Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions. The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points. The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.\n\nThe mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis,[5] since now longer and more complicated calculations could be done.\n\nThe Leslie Fox Prize for Numerical Analysis was initiated in 1985 by the Institute of Mathematics and its Applications.\n\nDirect methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).\n\nIn contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps, even if infinite precision were possible. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton's method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.[9][10][11][12]\n\nIterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.\n\nAs an example, consider the problem of solving\n\nfor the unknown quantity x.\n\nFor the iterative method, apply the bisection method to f(x) = 3x3 − 24. The initial values are a = 0, b = 3, f(a) = −24, f(b) = 57.\n\nFrom this table it can be concluded that the solution is between 1.875 and 2.0625. The algorithm might return any number in that range with an error less than 0.2.\n\nIll-conditioned problem: Take the function f(x) = 1/(x − 1). Note that f(1.1) = 10 and f(1.001) = 1000: a change in x of less than 0.1 turns into a change in f(x) of nearly 1000. Evaluating f(x) near x = 1 is an ill-conditioned problem.\n\nWell-conditioned problem: By contrast, evaluating the same function f(x) = 1/(x − 1) near x = 10 is a well-conditioned problem. For instance, f(10) = 1/9 ≈ 0.111 and f(11) = 0.1: a modest change in x leads to a modest change in f(x).\n\nFurthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called 'discretization'. For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.\n\nThe study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.\n\nRound-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).\n\nTruncation errors are committed when an iterative method is terminated or a mathematical procedure is approximated and the approximate solution differs from the exact solution. Similarly, discretization induces a discretization error because the solution of the discrete problem does not coincide with the solution of the continuous problem. In the example above to compute the solution of \n\n\n\n3\n\nx\n\n3\n\n\n+\n4\n=\n28\n\n\n{\\displaystyle 3x^{3}+4=28}\n\n, after ten iterations, the calculated root is roughly 1.99. Therefore, the truncation error is roughly 0.01.\n\nOnce an error is generated, it propagates through the calculation. For example, the operation + on a computer is inexact. A calculation of the type ⁠\n\n\n\na\n+\nb\n+\nc\n+\nd\n+\ne\n\n\n{\\displaystyle a+b+c+d+e}\n\n⁠ is even more inexact.\n\nA truncation error is created when a mathematical procedure is approximated. To integrate a function exactly, an infinite sum of regions must be found, but numerically only a finite sum of regions can be found, and hence the approximation of the exact solution. Similarly, to differentiate a function, the differential element approaches zero, but numerically only a nonzero value of the differential element can be chosen.\n\nAn algorithm is called numerically stable if an error, whatever its cause, does not grow to be much larger during the calculation.[13] This happens if the problem is well-conditioned, meaning that the solution changes by only a small amount if the problem data are changed by a small amount.[13] To the contrary, if a problem is 'ill-conditioned', then any small error in the data will grow to be a large error.[13]\nBoth the original problem and the algorithm used to solve that problem can be well-conditioned or ill-conditioned, and any combination is possible.\nSo an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem.\n\nThe field of numerical analysis includes many sub-disciplines. Some of the major ones are:\n\nInterpolation: Observing that the temperature varies from 20 degrees Celsius at 1:00 to 14 degrees at 3:00, a linear interpolation of this data would conclude that it was 17 degrees at 2:00 and 18.5 degrees at 1:30pm.\n\nExtrapolation: If the gross domestic product of a country has been growing an average of 5% per year and was 100 billion last year, it might be extrapolated that it will be 105 billion this year.\n\nRegression: In linear regression, given n points, a line is computed that passes as close as possible to those n points.\n\nOptimization: Suppose lemonade is sold at a lemonade stand, at $1.00 per glass, that 197 glasses of lemonade can be sold per day, and that for each increase of $0.01, one less glass of lemonade will be sold per day. If $1.485 could be charged, profit would be maximized, but due to the constraint of having to charge a whole-cent amount, charging $1.48 or $1.49 per glass will both yield the maximum income of $220.52 per day.\n\nDifferential equation: If 100 fans are set up to blow air from one end of the room to the other and then a feather is dropped into the wind, what happens? The feather will follow the air currents, which may be very complex. One approximation is to measure the speed at which the air is blowing near the feather every second, and advance the simulated feather as if it were moving in a straight line at that same speed for one second, before measuring the wind speed again. This is called the Euler method for solving an ordinary differential equation.\n\nOne of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating-point arithmetic.\n\nInterpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?\n\nExtrapolation is very similar to interpolation, except that now the value of the unknown function at a point which is outside the given points must be found.[14]\n\nRegression is also similar, but it takes into account that the data are imprecise. Given some points, and a measurement of the value of some function at these points (with an error), the unknown function can be found. The least squares-method is one way to achieve this.\n\nAnother fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation \n\n\n\n2\nx\n+\n5\n=\n3\n\n\n{\\displaystyle 2x+5=3}\n\n is linear while \n\n\n\n2\n\nx\n\n2\n\n\n+\n5\n=\n3\n\n\n{\\displaystyle 2x^{2}+5=3}\n\n is not.\n\nMuch effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method[15] are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.\n\nRoot-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton's method is a popular choice.[16][17] Linearization is another technique for solving nonlinear equations.\n\nSeveral important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm[18] is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.\n\nOptimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.\n\nThe field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.\n\nThe method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.\n\nNumerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral.[19] Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson's rule) or Gaussian quadrature.[20] These methods rely on a \"divide and conquer\" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration[21]), or, in modestly large dimensions, the method of sparse grids.\n\nNumerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.[22]\n\nPartial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace.[23] This can be done by a finite element method,[24][25][26] a finite difference method,[27] or (particularly in engineering) a finite volume method.[28] The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.\n\nSince the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free-software alternative is the GNU Scientific Library.\n\nOver the years the Royal Statistical Society published numerous algorithms in its Applied Statistics (code for these \"AS\" functions is here); \nACM similarly, in its Transactions on Mathematical Software (\"TOMS\" code is here).\nThe Naval Surface Warfare Center several times published its Library of Mathematics Subroutines (code here).\n\nThere are several popular numerical computing applications such as MATLAB,[29][30][31] TK Solver, S-PLUS, and IDL[32] as well as free and open-source alternatives such as FreeMat, Scilab,[33][34] GNU Octave (similar to Matlab), and IT++ (a C++ library). There are also programming languages such as R[35] (similar to S-PLUS), Julia,[36] and Python with libraries such as NumPy, SciPy[37][38][39] and SymPy. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.[40][41]\n\nMany computer algebra systems such as Mathematica also benefit from the availability of arbitrary-precision arithmetic which can provide more accurate results.[42][43][44][45]\n\nAlso, any spreadsheet software can be used to solve simple problems relating to numerical analysis. \nExcel, for example, has hundreds of available functions, including for matrices, which may be used in conjunction with its built in \"solver\".\n"
    },
    {
        "title": "Babylonian mathematics",
        "content": "\n\nBabylonian mathematics (also known as Assyro-Babylonian mathematics)[1][2][3][4] is the mathematics developed or practiced by the people of Mesopotamia, as attested by sources mainly surviving from the Old Babylonian period (1830–1531 BC) to the Seleucid from the last three or four centuries BC. With respect to content, there is scarcely any difference between the two groups of texts. Babylonian mathematics remained constant, in character and content, for over a millennium.[5]\n\nIn contrast to the scarcity of sources in Egyptian mathematics, knowledge of Babylonian mathematics is derived from hundreds of clay tablets unearthed since the 1850s. Written in cuneiform, tablets were inscribed while the clay was moist, and baked hard in an oven or by the heat of the sun. The majority of recovered clay tablets date from 1800 to 1600 BC, and cover topics that include fractions, algebra, quadratic and cubic equations and the Pythagorean theorem. The Babylonian tablet YBC 7289 gives an approximation of \n\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\sqrt {2}}}\n\n accurate to three significant sexagesimal digits (about six significant decimal digits).\n\nBabylonian mathematics is a range of numeric and more advanced mathematical practices in the ancient Near East, written in cuneiform script. Study has historically focused on the First Babylonian dynasty old Babylonian period in the early second millennium BC due to the wealth of data available. There has been debate over the earliest appearance of Babylonian mathematics, with historians suggesting a range of dates between the 5th and 3rd millennia BC.[6] Babylonian mathematics was primarily written on clay tablets in cuneiform script in the Akkadian or Sumerian languages.\n\n\"Babylonian mathematics\" is perhaps an unhelpful term since the earliest suggested origins date to the use of accounting devices, such as bullae and tokens, in the 5th millennium BC.[7]\n\nThe Babylonian system of mathematics was a sexagesimal (base 60) numeral system. From this we derive the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 degrees in a circle.[8] The Babylonians were able to make great advances in mathematics for two reasons. Firstly, the number 60 is a superior highly composite number, having factors of 1, 2, 3, 4, 5, 6, 10, 12, 15, 20, 30, 60 (including those that are themselves composite), facilitating calculations with fractions. Additionally, unlike the Egyptians and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values (much as, in our base ten system, 734 = 7×100 + 3×10 + 4×1).[9]\n\nThe Babylonians used pre-calculated tables to assist with arithmetic. For example, two tablets found at Senkerah on the Euphrates in 1854, dating from 2000 BC, give lists of the squares of numbers up to 59 and the cubes of numbers up to 32. The Babylonians used the lists of squares together with the formulae:\n\nto simplify multiplication.\n\nThe Babylonians did not have an algorithm for long division.[10] Instead they based their method on the fact that:\n\ntogether with a table of reciprocals. Numbers whose only prime factors are 2, 3 or 5 (known as 5-smooth or regular numbers) have finite reciprocals in sexagesimal notation, and tables with extensive lists of these reciprocals have been found.\n\nReciprocals such as 1/7, 1/11, 1/13, etc. do not have finite representations in sexagesimal notation. To compute 1/13 or to divide a number by 13 the Babylonians would use an approximation such as:\n\nThe Babylonian clay tablet YBC 7289 (c. 1800–1600 BC) gives an approximation of √2 in four sexagesimal figures, 𒐕 𒌋𒌋𒐼 𒐐𒐕 𒌋 = 1;24,51,10,[11] which is accurate to about six decimal digits,[12] and is the closest possible three-place sexagesimal representation of √2:\n\nAs well as arithmetical calculations, Babylonian mathematicians also developed algebraic methods of solving equations. Once again, these were based on pre-calculated tables.\n\nTo solve a quadratic equation, the Babylonians essentially used the standard quadratic formula. They considered quadratic equations of the form:\n\nwhere b and c were not necessarily integers, but c was always positive. They knew that a solution to this form of equation is:[13]\n\nand they found square roots efficiently using division and averaging.[14] Problems of this type included finding the dimensions of a rectangle given its area and the amount by which the length exceeds the width.\n\nTables of values of n3 + n2 were used to solve certain cubic equations. For example, consider the equation:\n\nMultiplying the equation by a2 and dividing by b3 gives:\n\nSubstituting y = ax/b gives:\n\nwhich could now be solved by looking up the n3 + n2 table to find the value closest to the right-hand side. The Babylonians accomplished this without algebraic notation, showing a remarkable depth of understanding. However, they did not have a method for solving the general cubic equation.\n\nBabylonians modeled exponential growth, constrained growth (via a form of sigmoid functions), and doubling time, the latter in the context of interest on loans.\n\nClay tablets from c. 2000 BC include the exercise \"Given an interest rate of 1/60 per month (no compounding), compute the doubling time.\" This yields an annual interest rate of 12/60 = 20%, and hence a doubling time of 100% growth/20% growth per year = 5 years.[15][16]\n\nThe Plimpton 322 tablet contains a list of \"Pythagorean triples\", i.e., integers \n\n\n\n(\na\n,\nb\n,\nc\n)\n\n\n{\\displaystyle (a,b,c)}\n\n such that \n\n\n\n\na\n\n2\n\n\n+\n\nb\n\n2\n\n\n=\n\nc\n\n2\n\n\n\n\n{\\displaystyle a^{2}+b^{2}=c^{2}}\n\n.\nThe triples are too many and too large to have been obtained by brute force.\n\nMuch has been written on the subject, including some speculation (perhaps anachronistic) as to whether the tablet could have served as an early trigonometrical table. Care must be exercised to see the tablet in terms of methods familiar or accessible to scribes at the time.\n\n[...] the question \"how was the tablet calculated?\" does not have to have the\nsame answer as the question \"what problems does the tablet set?\" The first can be answered\nmost satisfactorily by reciprocal pairs, as first suggested half a century ago, and the second\nby some sort of right-triangle problems.[17]\n\nBabylonians knew the common rules for measuring volumes and areas. They measured the circumference of a circle as three times the diameter and the area as one-twelfth the square of the circumference, which would be correct if π is estimated as 3. They were aware that this was an approximation, and one Old Babylonian mathematical tablet excavated near Susa in 1936 (dated to between the 19th and 17th centuries BC) gives a better approximation of π as 25/8 = 3.125, about 0.5 percent below the exact value.[18]\nThe volume of a cylinder was taken as the product of the base and the height, however, the volume of the frustum of a cone or a square pyramid was incorrectly taken as the product of the height and half the sum of the bases. The Pythagorean rule was also known to the Babylonians.[19][20][21]\n\nThe \"Babylonian mile\" was a measure of distance equal to about 11.3 km (or about seven modern miles).\nThis measurement for distances eventually was converted to a \"time-mile\" used for measuring the travel of the Sun, therefore, representing time.[22]\n\nThe Babylonian astronomers kept detailed records of the rising and setting of stars, the motion of the planets, and the solar and lunar eclipses, all of which required familiarity with angular distances measured on the celestial sphere.[23]\n\nThey also used a form of Fourier analysis to compute an ephemeris (table of astronomical positions), which was discovered in the 1950s by Otto Neugebauer.[24][25][26][27] To make calculations of the movements of celestial bodies, the Babylonians used basic arithmetic and a coordinate system based on the ecliptic, the part of the heavens that the sun and planets travel through.\n\nTablets kept in the British Museum provide evidence that the Babylonians even went so far as to have a concept of objects in an abstract mathematical space. The tablets date from between 350 and 50 B.C.E., revealing that the Babylonians understood and used geometry even earlier than previously thought. The Babylonians used a method for estimating the area under a curve by drawing a trapezoid underneath, a technique previously believed to have originated in 14th century Europe. This method of estimation allowed them to, for example, find the distance Jupiter had traveled in a certain amount of time.[28]\n"
    },
    {
        "title": "History of China",
        "content": "\n\n\n\nThe history of China spans several millennia across a wide geographical area. Each region now considered part of the Chinese world has experienced periods of unity, fracture, prosperity, and strife. Chinese civilization first emerged in the Yellow River valley, which along with the Yangtze basin constitutes the geographic core of the Chinese cultural sphere. China maintains a rich diversity of ethnic and linguistic people groups. The traditional lens for viewing Chinese history is the dynastic cycle: imperial dynasties rise and fall, and are ascribed certain achievements. Throughout pervades the narrative that Chinese civilization can be traced as an unbroken thread many thousands of years into the past, making it one of the cradles of civilization. At various times, states representative of a dominant Chinese culture have directly controlled areas stretching as far west as the Tian Shan, the Tarim Basin, and the Himalayas, as far north as the Sayan Mountains, and as far south as the delta of the Red River.\n\nThe Neolithic period saw increasingly complex polities begin to emerge along the Yellow and Yangtze rivers. The Erlitou culture in the central plains of China is sometimes identified with the Xia dynasty (3rd millennium BC) of traditional Chinese historiography. The earliest surviving written Chinese dates to roughly 1250 BC, consisting of divinations inscribed on oracle bones. Chinese bronze inscriptions, ritual texts dedicated to ancestors, form another large corpus of early Chinese writing. The earliest strata of received literature in Chinese include poetry, divination, and records of official speeches. China is believed to be one of a very few loci of independent invention of writing, and the earliest surviving records display an already-mature written language. The culture remembered by the earliest extant literature is that of the Zhou dynasty (c. 1046 – 256 BC), China's Axial Age, during which the Mandate of Heaven was introduced, and foundations laid for philosophies such as Confucianism, Taoism, Legalism, and Wuxing.\n\nChina was first united under a single imperial state by Qin Shi Huang in 221 BC. Orthography, weights, measures, and law were all standardized. Shortly thereafter, China entered its classical era with the Han dynasty (202 BC – 220 AD), marking a critical period. A term for the Chinese language is still \"Han language\", and the dominant Chinese ethnic group is known as Han Chinese. The Chinese empire reached some of its farthest geographical extents during this period. Confucianism was officially sanctioned and its core texts were edited into their received forms. Wealthy landholding families independent of the ancient aristocracy began to wield significant power. Han technology can be considered on par with that of the contemporaneous Roman Empire: mass production of paper aided the proliferation of written documents, and the written language of this period was employed for millennia afterwards. China became known internationally for its sericulture. When the Han imperial order finally collapsed after four centuries, China entered an equally lengthy period of disunity, during which Buddhism began to have a significant impact on Chinese culture, while Calligraphy, art, historiography, and storytelling flourished. Wealthy families in some cases became more powerful than the central government. The Yangtze River valley was incorporated into the dominant cultural sphere.\n\nA period of unity began in 581 with the Sui dynasty, which soon gave way to the long-lived Tang dynasty (608–907), regarded as another Chinese golden age. The Tang dynasty saw flourishing developments in science, technology, poetry, economics, and geographical influence. China's only officially recognized empress, Wu Zetian, reigned during the dynasty's first century. Buddhism was adopted by Tang emperors. \"Tang people\" is the other common demonym for the Han ethnic group. After the Tang fractured, the Song dynasty (960–1279) saw the maximal extent of imperial Chinese cosmopolitan development. Mechanical printing was introduced, and many of the earliest surviving witnesses of certain texts are wood-block prints from this era. Song scientific advancement led the world, and the imperial examination system gave ideological structure to the political bureaucracy. Confucianism and Taoism were fully knit together in Neo-Confucianism.\n\nEventually, the Mongol Empire conquered all of China, establishing the Yuan dynasty in 1271. Contact with Europe began to increase during this time. Achievements under the subsequent Ming dynasty (1368–1644) include global exploration, fine porcelain, and many extant public works projects, such as those restoring the Grand Canal and Great Wall. Three of the four Classic Chinese Novels were written during the Ming. The Qing dynasty that succeeded the Ming was ruled by ethnic Manchu people. The Qianlong emperor (r.  1735–1796) commissioned a complete encyclopaedia of imperial libraries, totaling nearly a billion words. Imperial China reached its greatest territorial extent of during the Qing, but China came into increasing conflict with European powers, culminating in the Opium Wars and subsequent unequal treaties.\n\nThe 1911 Xinhai Revolution, led by Sun Yat-sen and others, created the Republic of China. From 1927 to 1949, a costly civil war roiled between the Republican government under Chiang Kai-shek and the Communist-aligned Chinese Red Army, interrupted by the industrialized Empire of Japan invading the divided country until its defeat in the Second World War.\n\nAfter the Communist victory, Mao Zedong proclaimed the establishment of the People's Republic of China (PRC) in 1949, with the ROC retreating to Taiwan. Both governments still claim sole legitimacy of the entire mainland area. The PRC has slowly accumulated the majority of diplomatic recognition, and Taiwan's status remains disputed to this day. From 1966 to 1976, the Cultural Revolution in mainland China helped consolidate Mao's power towards the end of his life. After his death, the government began economic reforms under Deng Xiaoping, and became the world's fastest-growing major economy.[when?] China had been the most populous nation in the world for decades since its unification, until it was surpassed by India in 2023.\n\nThe archaic human species of Homo erectus arrived in Eurasia sometime between 1.3 and 1.8 million years ago (Ma) and numerous remains of its subspecies have been found in what is now China.[1] The oldest of these is the southwestern Yuanmou Man (元谋人; in Yunnan), dated to c. 1.7 Ma, which lived in a mixed bushland-forest environment alongside chalicotheres, deer, the elephant Stegodon, rhinos, cattle, pigs, and the giant short-faced hyena.[2] The better-known Peking Man (北京猿人; near Beijing) of 700,000–400,000 BP,[1] was discovered in the Zhoukoudian cave alongside scrapers, choppers, and, dated slightly later, points, burins, and awls.[3] Other Homo erectus fossils have been found widely throughout the region, including the northwestern Lantian Man in Shaanxi, as well minor specimens in northeastern Liaoning and southern Guangdong.[1] The dates of most Paleolithic sites were long debated but have been more reliably established based on modern magnetostratigraphy: Majuangou at 1.66–1.55 Ma, Lanpo at 1.6 Ma, Xiaochangliang at 1.36 Ma, Xiantai at 1.36 Ma, Banshan at 1.32 Ma, Feiliang at 1.2 Ma and Donggutuo at 1.1 Ma.[4] Evidence of fire use by Homo erectus occurred between 1–1.8 million years BP at the archaeological site of Xihoudu, Shanxi Province.[5]\n\nThe circumstances surrounding the evolution of Homo erectus to contemporary H. sapiens is debated; the three main theories include the dominant \"Out of Africa\" theory (OOA), the regional continuity model and the admixture variant of the OOA hypothesis.[1] Regardless, the earliest modern humans have been dated to China at 120,000–80,000 BP based on fossilized teeth discovered in Fuyan Cave of Dao County, Hunan.[6] The larger animals which lived alongside these humans include the extinct Ailuropoda baconi panda, the Crocuta ultima hyena, the Stegodon, and the giant tapir.[6] Evidence of Middle Palaeolithic Levallois technology has been found in the lithic assemblage of Guanyindong Cave site in southwest China, dated to approximately 170,000–80,000 years ago.[7]\n\nThe Neolithic Age in China is considered to have begun about 10,000 years ago.[8] Because the Neolithic is conventionally defined by the presence of agriculture, it follows that the Neolithic began at different times in the various regions of what is now China. Agriculture in China developed gradually, with initial domestication of a few grains and animals gradually expanding with the addition of many others over subsequent millennia.[9] The earliest evidence of cultivated rice, found by the Yangtze River, was carbon-dated to 8,000 years ago.[10] Early evidence for millet agriculture in the Yellow River valley was radiocarbon-dated to about 7000 BC.[11] The Jiahu site is one of the best preserved early agricultural villages (7000 to 5800 BC). At Damaidi in Ningxia, 3,172 cliff carvings dating to 6000–5000 BC have been discovered, \"featuring 8,453 individual characters such as the sun, moon, stars, gods and scenes of hunting or grazing\", according to researcher Li Xiangshi. Written symbols, sometimes called proto-writing, were found at the site of Jiahu, which is dated around 7000 BC,[12] Damaidi around 6000 BC, Dadiwan from 5800 BC to 5400 BC,[13] and Banpo dating from the 5th millennium BC. With agriculture came increased population, the ability to store and redistribute crops, and the potential to support specialist craftsmen and administrators, which may have existed at late Neolithic sites like Taosi and the Liangzhu culture in the Yangtze delta.[10] The cultures of the middle and late Neolithic in the central Yellow River valley are known, respectively, as the Yangshao culture (5000 BC to 3000 BC) and the Longshan culture (3000 BC to 2000 BC). Pigs and dogs were the earliest-domesticated animals in the region, and after about 3000 BC domesticated cattle and sheep arrived from Western Asia. Wheat also arrived at this time but remained a minor crop. Fruit such as peaches, cherries and oranges, as well as chickens and various vegetables, were also domesticated in Neolithic China.[9]\n\nBronze artifacts have been found at the Majiayao culture site (between 3100 and 2700 BC).[14][15] The Bronze Age is also represented at the Lower Xiajiadian culture (2200–1600 BC)[16] site in northeast China. Sanxingdui located in what is now Sichuan is believed to be the site of a major ancient city, of a previously unknown Bronze Age culture (between 2000 and 1200 BC). The site was first discovered in 1929 and then re-discovered in 1986. Chinese archaeologists have identified the Sanxingdui culture to be part of the state of Shu, linking the artifacts found at the site to its early legendary kings.[17][18]\n\n\nFerrous metallurgy begins to appear in the late 6th century in the Yangtze valley.[19] A bronze hatchet with a blade of meteoric iron excavated near the city of Gaocheng in Shijiazhuang (now Hebei) has been dated to the 14th century BC. An Iron Age culture of the Tibetan Plateau has tentatively been associated with the Zhang Zhung culture described in early Tibetan writings.\n\nChinese historians in later periods were accustomed to the notion of one dynasty succeeding another, but the political situation in early China was much more complicated. Hence, as some scholars of China suggest, the Xia and the Shang can refer to political entities that existed concurrently, just as the early Zhou existed at the same time as the Shang.[20] This bears similarities to how China, both contemporaneously and later, has been divided into states that were not one region, legally or culturally.[21]\n\nThe earliest period once considered historical was the legendary era of the sage-emperors Yao, Shun, and Yu. Traditionally, the abdication system was prominent in this period,[22] with Yao yielding his throne to Shun, who abdicated to Yu, who founded the Xia dynasty.\n\nThe Xia dynasty (c. 2070 – c. 1600 BC) is the earliest of the three dynasties described in much later traditional historiography, which includes the Bamboo Annals and Sima Qian's Shiji (c. 91 BC). The Xia is generally considered mythical by Western scholars, but in China it is usually associated with the early Bronze Age site at Erlitou (1900–1500 BC) in Henan that was excavated in 1959. Since no writing was excavated at Erlitou or any other contemporaneous site, there is not enough evidence to prove whether the Xia dynasty ever existed. Some archaeologists claim that the Erlitou site was the capital of the Xia.[23] In any case, the site of Erlitou had a level of political organization that would not be incompatible with the legends of Xia recorded in later texts.[24] More importantly, the Erlitou site has the earliest evidence for an elite who conducted rituals using cast bronze vessels, which would later be adopted by the Shang and Zhou.[25]\n\nBoth archaeological evidence like oracle bones and bronzes, as well as transmitted texts attest the historical existence of the Shang dynasty (c. 1600 – c. 1046 BC). Findings from the earlier Shang period come from excavations at Erligang (modern Zhengzhou). Findings have been found at Yinxu (near modern Anyang, Henan), the site of the final Shang capital during the Late Shang period (c. 1250–1050 BC).[26] The findings at Anyang include the earliest written record of the Chinese so far discovered: inscriptions of divination records in ancient Chinese writing on the bones or shells of animals—the oracle bones, dating from c. 1250 – c. 1046 BC.[27]\n\nA series of at least twenty-nine kings reigned over the Shang dynasty.[28] Throughout their reigns, according to the Shiji, the capital city was moved six times.[29] The final and most important move was to Yin during the reign of Wu Ding c. 1250 BC.[30] The term Yin dynasty has been synonymous with the Shang dynasty in history, although it has lately been used to refer specifically to the latter half of the Shang dynasty.[28]\n\nAlthough written records found at Anyang confirm the existence of the Shang dynasty,[31] Western scholars are often hesitant to associate settlements that are contemporaneous with the Anyang settlement with the Shang dynasty. For example, archaeological findings at Sanxingdui suggest a technologically advanced civilization culturally unlike Anyang. The evidence is inconclusive in proving how far the Shang realm extended from Anyang. The leading hypothesis is that Anyang, ruled by the same Shang in the official history, coexisted and traded with numerous other culturally diverse settlements in the area that is now referred to as China proper.[32]\n\nThe Zhou dynasty (1046 BC to about 256 BC) is the longest-lasting dynasty in Chinese history, though its power declined steadily over the almost eight centuries of its existence. In the late 2nd millennium BC, the Zhou dynasty arose in the Wei River valley of modern western Shaanxi Province, where they were appointed Western Protectors by the Shang. A coalition led by the ruler of the Zhou, King Wu, defeated the Shang at the Battle of Muye. They took over most of the central and lower Yellow River valley and enfeoffed their relatives and allies in semi-independent states across the region.[33] Several of these states eventually became more powerful than the Zhou kings.\n\nThe kings of Zhou invoked the concept of the Mandate of Heaven to legitimize their rule, a concept that was influential for almost every succeeding dynasty.[34] Like Shangdi, Heaven (tian) ruled over all the other gods, and it decided who would rule China.[35] It was believed that a ruler lost the Mandate of Heaven when natural disasters occurred in great number, and when, more realistically, the sovereign had apparently lost his concern for the people. In response, the royal house would be overthrown, and a new house would rule, having been granted the Mandate of Heaven.\n\nThe Zhou established two capitals Zongzhou (near modern Xi'an) and Chengzhou (Luoyang), with the king's court moving between them regularly. The Zhou alliance gradually expanded eastward into Shandong, southeastward into the Huai River valley, and southward into the Yangtze River valley.[33]\n\nIn 771 BC, King You and his forces were defeated in the Battle of Mount Li by rebel states and Quanrong barbarians. The rebel aristocrats established a new ruler, King Ping, in Luoyang,[36]: 4  beginning the second major phase of the Zhou dynasty: the Eastern Zhou period, which is divided into the Spring and Autumn and Warring States periods. The former period is named after the famous Spring and Autumn Annals. The sharply reduced political authority of the royal house left a power vacuum at the center of the Zhou culture sphere. The Zhou kings had delegated local political authority to hundreds of settlement states, some of them only as large as a walled town and surrounding land. These states began to fight against one another and vie for hegemony. The more powerful states tended to conquer and incorporate the weaker ones, so the number of states declined over time.[37] By the 6th century BC most small states had disappeared by being annexed and just a few large and powerful principalities remained. Some southern states, such as Chu and Wu, claimed independence from the Zhou, who undertook wars against some of them (Wu and Yue). Many new cities were established in this period and society gradually became more urbanized and commercialized. Many famous individuals such as Laozi, Confucius and Sun Tzu lived during this chaotic period.\n\nConflict in this period occurred both between and within states. Warfare between states forced the surviving states to develop better administrations to mobilize more soldiers and resources. Within states there was constant jockeying between elite families. For example, the three most powerful families in the Jin state—Zhao, Wei and Han—eventually overthrew the ruling family and partitioned the state between them.\n\nThe Hundred Schools of Thought of classical Chinese philosophy began blossoming during this period and the subsequent Warring States period. Such influential intellectual movements as Confucianism, Taoism, Legalism and Mohism were founded, partly in response to the changing political world. The first two philosophical thoughts would have an enormous influence on Chinese culture.\n\nAfter further political consolidations, seven prominent states remained during the 5th century BC. The years in which these states battled each other is known as the Warring States period. Though the Zhou king nominally remained as such until 256 BC, he was largely a figurehead that held little real power.\n\nNumerous developments were made during this period in the areas of culture and mathematics—including the Zuo Zhuan within the Spring and Autumn Annals (a literary work summarizing the preceding Spring and Autumn period), and the bundle of 21 bamboo slips from the Tsinghua collection, dated to 305 BC—being the world's earliest known example of a two-digit, base-10 multiplication table. The Tsinghua collection indicates that sophisticated commercial arithmetic was already established during this period.[38]\n\nAs neighboring territories of the seven states were annexed (including areas of modern Sichuan and Liaoning), they were now to be governed under an administrative system of commanderies and prefectures. This system had been in use elsewhere since the Spring and Autumn period, and its influence on administration would prove resilient—its terminology can still be seen in the contemporaneous sheng and xian (\"provinces\" and \"counties\") of contemporary China.\n\nThe state of Qin became dominant in the waning decades of the Warring States period, conquering the Shu capital of Jinsha on the Chengdu Plain; and then eventually driving Chu from its place in the Han River valley. Qin imitated the administrative reforms of the other states, thereby becoming a powerhouse.[9] Its final expansion began during the reign of Ying Zheng, ultimately unifying the other six regional powers, and enabling him to proclaim himself as China's first emperor—known to history as Qin Shi Huang.\n\nYing Zheng's establishment of the Qin dynasty (秦朝) in 221 BC effectively formalised the region as a true empire for the first time in Chinese history, rather than a state, and its pivotal status probably led to \"Qin\" (秦) later evolving into the Western term \"China\".[39] To emphasise his sole rule, Zheng proclaimed himself Shi Huangdi (始皇帝; \"First Emperor\"); the Huangdi title, derived from Chinese mythology, became the standard for subsequent rulers.[40][a] Based in Xianyang, the empire was a centralized bureaucratic monarchy, a governing scheme which dominated the future of Imperial China.[42][43] In an effort to improve the Zhou's perceived failures, this system consisted of more than 36 commanderies (郡; jun),[b] made up of counties (县; xian) and progressively smaller divisions, each with a local leader.[46]\n\nMany aspects of society were informed by Legalism, a state ideology promoted by the emperor and his chancellor Li Si that was introduced at an earlier time by Shang Yang.[47] In legal matters this philosophy emphasised mutual responsibility in disputes and severe punishments for crime, while economic practices included the general encouragement of agriculture and repression of trade.[47] Reforms occurred in weights and measures, writing styles (seal script) and metal currency (Ban Liang), all of which were standardized.[48][49] Traditionally, Qin Shi Huang is regarded as ordering a mass burning of books and the live burial of scholars under the guise of Legalism, though contemporary scholars express considerable doubt on the historicity of this event.[47] Despite its importance, Legalism was probably supplemented in non-political matters by Confucianism for social and moral beliefs and the five-element Wuxing (五行) theories for cosmological thought.[50]\n\nThe Qin administration kept exhaustive records on their population, collecting information on their sex, age, social status and residence.[51] Commoners, who made up over 90% of the population,[52] \"suffered harsh treatment\" according to the historian Patricia Buckley Ebrey, as they were often conscripted into forced labor for the empire's construction projects.[53] This included a massive system of imperial highways in 220 BC, which ranged around 4,250 miles (6,840 km) altogether.[54] Other major construction projects were assigned to the general Meng Tian, who concurrently led a successful campaign against the northern Xiongnu peoples (210s BC), reportedly with 300,000 troops.[54][c] Under Qin Shi Huang's orders, Meng supervised the combining of numerous ancient walls into what came to be known as the Great Wall of China and oversaw the building of a 500 miles (800 km) straight highway between northern and southern China.[56] The emperor also oversaw the construction of his monumental mausoleum, which includes the well known Terracotta Army.[57]\n\nAfter Qin Shi Huang's death the Qin government drastically deteriorated and eventually capitulated in 207 BC after the Qin capital was captured and sacked by rebels, which would ultimately lead to the establishment of the Han Empire.[58][59]\n\nThe Han dynasty was founded by Liu Bang, who emerged victorious in the Chu–Han Contention that followed the fall of the Qin dynasty. A golden age in Chinese history, the Han dynasty's long period of stability and prosperity consolidated the foundation of China as a unified state under a central imperial bureaucracy, which was to last intermittently for most of the next two millennia. During the Han dynasty, territory of China was extended to most of the China proper and to areas far west. Confucianism was officially elevated to orthodox status and was to shape the subsequent Chinese civilization. Art, culture and science all advanced to unprecedented heights. With the profound and lasting impacts of this period of Chinese history, the dynasty name \"Han\" had been taken as the name of the Chinese people, now the dominant ethnic group in modern China, and had been commonly used to refer to Chinese language and written characters.\n\nAfter the initial laissez-faire policies of Emperors Wen and Jing, the ambitious Emperor Wu brought the empire to its zenith. To consolidate his power, he disenfranchised the majority of imperial relatives, appointing military governors to control their former lands.[60] As a further step, he extended patronage to Confucianism, which emphasizes stability and order in a well-structured society. Imperial Universities were established to support its study. At the urging of his Legalist advisors, however, he also strengthened the fiscal structure of the dynasty with government monopolies.\n\nMajor military campaigns were launched to weaken the nomadic Xiongnu Empire, limiting their influence north of the Great Wall. Along with the diplomatic efforts led by Zhang Qian, the sphere of influence of the Han Empire extended to the states in the Tarim Basin, opened up the Silk Road that connected China to the west, stimulating bilateral trade and cultural exchange. To the south, various small kingdoms far beyond the Yangtze River Valley were formally incorporated into the empire.\n\nEmperor Wu also dispatched a series of military campaigns against the Baiyue tribes. The Han annexed Minyue in 135 BC and 111 BC, Nanyue in 111 BC, and Dian in 109 BC.[61] Migration and military expeditions led to the cultural assimilation of the south.[62] It also brought the Han into contact with kingdoms in Southeast Asia, introducing diplomacy and trade.[63]\n\nAfter Emperor Wu the empire slipped into gradual stagnation and decline. Economically, the state treasury was strained by excessive campaigns and projects, while land acquisitions by elite families gradually drained the tax base. Various consort clans exerted increasing control over strings of incompetent emperors and eventually the dynasty was briefly interrupted by the usurpation of Wang Mang.\n\nIn AD 9 the usurper Wang Mang claimed that the Mandate of Heaven called for the end of the Han dynasty and the rise of his own, and he founded the short-lived Xin dynasty. Wang Mang started an extensive program of land and other economic reforms, including the outlawing of slavery and land nationalization and redistribution. These programs, however, were never supported by the landholding families, because they favored the peasants. The instability of power brought about chaos, uprisings, and loss of territories. This was compounded by mass flooding of the Yellow River; silt buildup caused it to split into two channels and displaced large numbers of farmers. Wang Mang was eventually killed in Weiyang Palace by an enraged peasant mob in AD 23.\n\nEmperor Guangwu reinstated the Han dynasty with the support of landholding and merchant families at Luoyang, east of the former capital Xi'an. Thus, this new era is termed the Eastern Han dynasty. With the capable administrations of Emperors Ming and Zhang, former glories of the dynasty were reclaimed, with brilliant military and cultural achievements. The Xiongnu Empire was decisively defeated. The diplomat and general Ban Chao further expanded the conquests across the Pamirs to the shores of the Caspian Sea,[64]: 175  thus reopening the Silk Road, and bringing trade, foreign cultures, along with the arrival of Buddhism. With extensive connections with the west, the first of several Roman embassies to China were recorded in Chinese sources, coming from the sea route in AD 166, and a second one in AD 284.\n\nThe Eastern Han dynasty was one of the most prolific eras of science and technology in ancient China, notably the historic invention of papermaking by Cai Lun, and the numerous scientific and mathematical contributions by the famous polymath Zhang Heng.\n\nBy the 2nd century, the empire declined amidst land acquisitions, invasions, and feuding between consort clans and eunuchs. The Yellow Turban Rebellion broke out in AD 184, ushering in an era of warlords. In the ensuing turmoil, three states emerged, trying to gain predominance and reunify the land, giving this historical period its name. The classic historical novel Romance of the Three Kingdoms dramatizes events of this period.\n\nThe warlord Cao Cao reunified the north in 208, and in 220 his son accepted the abdication of Emperor Xian of Han, thus initiating the Wei dynasty. Soon, Wei's rivals Shu and Wu proclaimed their independence. This period was characterized by a gradual decentralization of the state that had existed during the Qin and Han dynasties, and an increase in the power of great families.\n\nIn 266, the Jin dynasty overthrew the Wei and later unified the country in 280, but this union was short-lived.\n\nThe Jin dynasty reunited China proper for the first time since the end of the Han dynasty, ending the Three Kingdoms era. However, the Jin dynasty was severely weakened by the War of the Eight Princes and lost control of northern China after non-Han Chinese settlers rebelled and captured Luoyang and Chang'an. In 317, the Jin prince Sima Rui, based in modern-day Nanjing, became emperor and continued the dynasty, now known as the Eastern Jin, which held southern China for another century. Prior to this move, historians refer to the Jin dynasty as the Western Jin.\n\nNorthern China fragmented into a series of independent states known as the Sixteen Kingdoms, most of which were founded by Xiongnu, Xianbei, Jie, Di and Qiang rulers. These non-Han peoples were ancestors of the Turks, Mongols, and Tibetans. Many had, to some extent, been \"sinicized\" long before their ascent to power. In fact, some of them, notably the Qiang and the Xiongnu, had already been allowed to live in the frontier regions within the Great Wall since late Han times. During this period, warfare ravaged the north and prompted large-scale Han Chinese migration south to the Yangtze River Basin and Delta.\n\nIn the early 5th century China entered a period known as the Northern and Southern dynasties, in which parallel regimes ruled the northern and southern halves of the country. In the south, the Eastern Jin gave way to the Liu Song, Southern Qi, Liang and finally Chen. Each of these Southern dynasties were led by Han Chinese ruling families and used Jiankang (modern Nanjing) as the capital. They held off attacks from the north and preserved many aspects of Chinese civilization, while northern barbarian regimes began to sinify.\n\nIn the north the last of the Sixteen Kingdoms was extinguished in 439 by the Northern Wei, a kingdom founded by the Xianbei, a nomadic people who unified northern China. The Northern Wei eventually split into the Eastern and Western Wei, which then became the Northern Qi and Northern Zhou. These regimes were dominated by Xianbei or Han Chinese who had married into Xianbei families. During this period most Xianbei people adopted Han surnames, eventually leading to complete assimilation into the Han.\n\nDespite the division of the country, Buddhism spread throughout the land. In southern China, fierce debates about whether Buddhism should be allowed were held frequently by the royal court and nobles. By the end of the era, Buddhists and Taoists had become much more tolerant of each other.[65]\n\nThe short-lived Sui dynasty was a pivotal period in Chinese history. Founded by Emperor Wen in 581 in succession of the Northern Zhou, the Sui went on to conquer the Southern Chen in 589 to reunify China, ending three centuries of political division. The Sui pioneered many new institutions, including the government system of Three Departments and Six Ministries, imperial examinations for selecting officials from commoners, while improved on the systems of fubing system of the army conscription and the equal-field system of land distributions. These policies, which were adopted by later dynasties, brought enormous population growth, and amassed excessive wealth to the state. Standardized coinage was enforced throughout the unified empire. Buddhism took root as a prominent religion and was supported officially. Sui China was known for its numerous mega-construction projects. Intended for grains shipment and transporting troops, the Grand Canal was constructed, linking the capitals Daxing (Chang'an) and Luoyang to the wealthy southeast region, and in another route, to the northeast border. The Great Wall was also expanded, while series of military conquests and diplomatic maneuvers further pacified its borders. However, the massive invasions of the Korean Peninsula during the Goguryeo–Sui War failed disastrously, triggering widespread revolts that led to the fall of the dynasty.\n\nThe Tang dynasty was a golden age of Chinese civilization, a prosperous, stable, and creative period with significant developments in culture, art, literature, particularly poetry, and technology. Buddhism became the predominant religion for the common people. Chang'an (modern Xi'an), the national capital, was the largest city in the world during its time.[66]\n\nThe first emperor, Emperor Gaozu, came to the throne on 18 June 618, placed there by his son, Li Shimin, who became the second emperor, Taizong, one of the greatest emperors in Chinese history. Combined military conquests and diplomatic maneuvers reduced threats from Central Asian tribes, extended the border, and brought neighboring states into a tributary system. Military victories in the Tarim Basin kept the Silk Road open, connecting Chang'an to Central Asia and areas far to the west. In the south, lucrative maritime trade routes from port cities such as Guangzhou connected with distant countries, and foreign merchants settled in China, encouraging a cosmopolitan culture. The Tang culture and social systems were observed and adapted by neighboring countries, most notably Japan. Internally the Grand Canal linked the political heartland in Chang'an to the agricultural and economic centers in the eastern and southern parts of the empire. Xuanzang, a Chinese Buddhist monk, scholar, traveller, and translator travelled to India on his own and returned with \"over six hundred Mahayana and Hinayana texts, seven statues of the Buddha and more than a hundred sarira relics.\"\n\nThe prosperity of the early Tang dynasty was abetted by a centralized bureaucracy. The government was organized as \"Three Departments and Six Ministries\" to separately draft, review, and implement policies. These departments were run by royal family members and landed aristocrats, but as the dynasty wore on, were joined or replaced by scholar officials selected by imperial examinations, setting patterns for later dynasties.\n\nUnder the Tang \"equal-field system\" all land was owned by the Emperor and granted to each family according to household size. Men granted land were conscripted for military service for a fixed period each year, a military policy known as the fubing system. These policies stimulated a rapid growth in productivity and a significant army without much burden on the state treasury. By the dynasty's midpoint, however, standing armies had replaced conscription, and land was continuously falling into the hands of private owners and religious institutions granted exemptions.\n\nThe dynasty continued to flourish under the rule of Empress Wu Zetian, the only official empress regnant in Chinese history, and reached its zenith during the long reign of Emperor Xuanzong, who oversaw an empire that stretched from the Pacific to the Aral Sea with at least 50 million people. There were vibrant artistic and cultural creations, including works of the greatest Chinese poets, Li Bai and Du Fu.\n\nAt the zenith of prosperity of the empire, the An Lushan Rebellion from 755 to 763 was a watershed event. War, disease, and economic disruption devastated the population and drastically weakened the central imperial government. Upon suppression of the rebellion, regional military governors, known as jiedushi, gained increasingly autonomous status as the central government lost its ability to control them. With loss of revenue from land tax, the central imperial government came to rely heavily on its salt monopoly. Externally, former submissive states raided the empire and the vast border territories were lost for centuries. Nevertheless, civil society recovered and thrived amidst the weakened imperial bureaucracy.\n\nIn late Tang period the empire was worn out by recurring revolts of the regional military governors, while scholar-officials engaged in fierce factional strife and corrupted eunuchs amassed immense power. Catastrophically, the Huang Chao Rebellion, from 874 to 884, devastated the entire empire for a decade. The sack of the southern port Guangzhou in 879 was followed by the massacre of most of its inhabitants, especially the large foreign merchant enclaves.[68][69] By 881, both capitals, Luoyang and Chang'an, fell successively. The reliance on ethnic Han and Turkic warlords in suppressing the rebellion increased their power and influence. Consequently, the fall of the dynasty following Zhu Wen's usurpation led to an era of division.\n\nIn 808, 30,000 Shatuo under Zhuye Jinzhong defected from the Tibetans to Tang China and the Tibetans punished them by killing Zhuye Jinzhong as they were chasing them.[70] The Uyghurs also fought against an alliance of Shatuo and Tibetans at Beshbalik.[71] The Shatuo Turks under Zhuye Chixin (Li Guochang) served the Tang dynasty in fighting against their fellow Turkic people in the Uyghur Khaganate. In 839, when the Uyghur khaganate (Huigu) general Jueluowu (掘羅勿) rose against the rule of then-reigning Zhangxin Khan, he elicited the help from Zhuye Chixin by giving Zhuye 300 horses, and together, they defeated Zhangxin Khan, who then committed suicide, precipitating the subsequent collapse of the Uyghur Khaganate. In the next few years, when Uyghur Khaganate remnants tried to raid Tang borders, the Shatuo participated extensively in counterattacking the Uyghur Khaganate with other tribes loyal to Tang.[72] In 843, Zhuye Chixin, under the command of the Han Chinese officer Shi Xiong with Tuyuhun, Tangut and Han Chinese troops, participated in a raid against the Uyghur khaganate that led to the slaughter of Uyghur forces at Shahu mountain.[73]\n\nThe period of political disunity between the Tang and the Song, known as the Five Dynasties and Ten Kingdoms period, lasted from 907 to 960. During this half-century, China was in all respects a multi-state system. Five regimes, namely, (Later) Liang, Tang, Jin, Han and Zhou, rapidly succeeded one another in control of the traditional Imperial heartland in northern China. Among the regimes, rulers of (Later) Tang, Jin and Han were sinicized Shatuo Turks, which ruled over an ethnic majority of Han Chinese in the north. More stable and smaller regimes of mostly ethnic Han rulers coexisted in south and western China over the period, cumulatively constituted the \"Ten Kingdoms\".\n\nAmidst political chaos in the north, the strategic Sixteen Prefectures (region along today's Great Wall) were ceded to the emerging Khitan Liao dynasty, which drastically weakened the defense of China proper against northern nomadic empires. To the south, Vietnam gained lasting independence after being a Chinese prefecture for many centuries. With wars dominating in Northern China, there were mass southward migrations of population, which further enhanced the southward shift of cultural and economic centers in China. The era ended with the coup of Later Zhou general Zhao Kuangyin, and the establishment of the Song dynasty in 960, which eventually annihilated the remains of the \"Ten Kingdoms\" and reunified China.\n\nIn 960, the Song dynasty was founded by Emperor Taizu, with its capital established in Kaifeng (then known as Bianjing). In 979, the Song dynasty reunified most of China proper, while large swaths of the outer territories were occupied by sinicized nomadic empires. The Khitan Liao dynasty, which lasted from 907 to 1125, ruled over Manchuria, Mongolia, and parts of Northern China. Meanwhile, in what are now the north-western Chinese provinces of Gansu, Shaanxi, and Ningxia, the Tangut tribes founded the Western Xia dynasty from 1032 to 1227.\n\nAiming to recover the strategic sixteen prefectures lost in the previous dynasty, campaigns were launched against the Liao dynasty in the early Song period, which all ended in failure. Then in 1004, the Liao cavalry swept over the exposed North China Plain and reached the outskirts of Kaifeng, forcing the Song's submission and then agreement to the Chanyuan Treaty, which imposed heavy annual tributes from the Song treasury. The treaty was a significant reversal of Chinese dominance of the traditional tributary system. Yet the annual outflow of Song's silver to the Liao was paid back through the purchase of Chinese goods and products, which expanded the Song economy, and replenished its treasury. This dampened the incentive for the Song to further campaign against the Liao. Meanwhile, this cross-border trade and contact induced further sinicization within the Liao Empire, at the expense of its military might which was derived from its nomadic lifestyle. Similar treaties and social-economical consequences occurred in Song's relations with the Jin dynasty.\n\nWithin the Liao Empire the Jurchen tribes revolted against their overlords to establish the Jin dynasty in 1115. In 1125, the devastating Jin cataphract annihilated the Liao dynasty, while remnants of Liao court members fled to Central Asia to found the Qara Khitai Empire (Western Liao dynasty). Jin's invasion of the Song dynasty followed swiftly. In 1127, Kaifeng was sacked, a massive catastrophe known as the Jingkang Incident, ending the Northern Song dynasty. Later the entire north of China was conquered. The survived members of Song court regrouped in the new capital city of Hangzhou, and initiated the Southern Song dynasty, which ruled territories south of the Huai River. In the ensuing years, the territory and population of China were divided between the Song dynasty, the Jin dynasty and the Western Xia dynasty. The era ended with the Mongol conquest, as Western Xia fell in 1227, the Jin dynasty in 1234, and finally the Southern Song dynasty in 1279.\n\nDespite its military weakness, the Song dynasty is widely considered to be the high point of classical Chinese civilization. The Song economy, facilitated by technological advancement, had reached a level of sophistication probably unseen in world history before its time. The population soared to over 100 million and the living standards of common people improved tremendously due to improvements in rice cultivation and the wide availability of coal for production. The capital cities of Kaifeng and subsequently Hangzhou were both the most populous cities in the world for their time, and encouraged vibrant civil societies unmatched by previous Chinese dynasties. Although land trading routes to the far west were blocked by nomadic empires, there was extensive maritime trade with neighbouring states, such as in South-east Asia, which facilitated the use of Song coinage as the de facto currency of exchange. Giant wooden vessels equipped with compasses traveled throughout the China Seas and northern Indian Ocean. The concept of insurance was practised by merchants to hedge the risks of such long-haul maritime shipments. With prosperous economic activities, the historically first use of paper currency emerged in the western city of Chengdu, as a cheaper supplement to the existing copper coins.\n\nThe Song dynasty was considered to be the golden age of great advancements in science and technology of China, thanks to innovative scholar-officials such as Su Song (1020–1101) and Shen Kuo (1031–1095). Inventions such as the hydro-mechanical astronomical clock, the first continuous and endless power-transmitting chain, woodblock printing and paper money were all invented during the Song dynasty, further cementing its status.\n\nThere was court intrigue between the political reformers and conservatives, led by the chancellors Wang Anshi and Sima Guang, respectively. By the mid-to-late 13th century, the Chinese had adopted the dogma of Neo-Confucian philosophy formulated by Zhu Xi. Enormous literary works were compiled during the Song dynasty, such as the innovative historical narrative Zizhi Tongjian (\"Comprehensive Mirror to Aid in Government\"). The invention of movable-type printing further facilitated the spread of knowledge. Culture and the arts flourished, with grandiose artworks such as Along the River During the Qingming Festival and Eighteen Songs of a Nomad Flute, along with great Buddhist painters such as the prolific Lin Tinggui.\n\nThe Song dynasty was also a period of major innovation in the history of warfare. Gunpowder, while invented in the Tang dynasty, was first put into practical use on the battlefield by the Song army, inspiring a succession of new firearms and siege engines designs. During the Southern Song dynasty, as its survival hinged decisively on guarding the Yangtze and Huai River against the cavalry forces from the north, the first standing navy in China was assembled in 1132, with its admiral's headquarters established at Dinghai. Paddle-wheel warships equipped with trebuchets could launch incendiary bombs made of gunpowder and lime to effect, as recorded in Song's victory over the invading Jin forces at the Battle of Tangdao in the East China Sea, and the Battle of Caishi on the Yangtze River in 1161.\n\nThe advances in civilisation during the Song dynasty came to an abrupt end following the devastating Mongol conquest of the North and subsequently other areas of the empire, during which the population sharply dwindled, with a marked contraction in economy. Despite viciously halting Mongol advances for more than three decades, the Southern Song capital Hangzhou fell in 1276, followed by the final annihilation of the Song standing navy at the Battle of Yamen in 1279.\n\nThe Yuan dynasty was formally proclaimed in 1271, when the Great Khan of Mongol, Kublai Khan, one of the grandsons of Genghis Khan, assumed the additional title of Emperor of China, and considered his inherited part of the Mongol Empire as a Chinese dynasty. In the preceding decades, the Mongols had conquered the Jin dynasty in Northern China, and the Southern Song dynasty fell in 1279 after a protracted and bloody war. The Mongol Yuan dynasty became the first conquest dynasty in Chinese history to rule the entirety of China proper and its population as an ethnic minority. The dynasty also directly controlled the Mongol heartland and other regions, inheriting the largest share of territory of the eastern Mongol empire, which roughly coincided with the modern area of China and nearby regions in East Asia. Further expansion of the empire was halted after defeats in the invasions of Japan and Vietnam. Following the previous Jin dynasty, the capital of Yuan dynasty was established at Khanbaliq (also known as Dadu, modern-day Beijing). The Grand Canal was reconstructed to connect the remote capital city to lively economic hubs in southern part of China, setting the precedence and foundation for Beijing to largely remain as the capital of the successive regimes of the unified Chinese mainland.\n\nA series of Mongol civil wars in the late 13th century led to the division of the Mongol Empire. In 1304 the emperors of the Yuan dynasty were upheld as the nominal Khagan over western khanates (the Chagatai Khanate, the Golden Horde and the Ilkhanate), which nonetheless remained de facto autonomous. The era was known as Pax Mongolica, when much of the Asian continent was ruled by the Mongols. For the first and only time in history, the Silk Road was controlled entirely by a single state, facilitating the flow of people, trade, and cultural exchange. A network of roads and a postal system were established to connect the vast empire. Lucrative maritime trade, developed from the previous Song dynasty, continued to flourish, with Quanzhou and Hangzhou emerging as the largest ports in the world. Adventurous travelers from the far west, most notably the Venetian, Marco Polo, would settle in China for decades. Upon his return, his detail travel record inspired generations of medieval Europeans with the splendors of the far East. The Yuan dynasty was the first ancient economy, where paper currency, known at the time as Jiaochao, was used as the predominant medium of exchange. Its unrestricted issuance in the late Yuan dynasty inflicted hyperinflation, which eventually brought the downfall of the dynasty.\n\nWhile the Mongol rulers of the Yuan dynasty adopted substantially to Chinese culture, their sinicization was of lesser extent compared to earlier conquest dynasties in Chinese history. For preserving racial superiority as the conqueror and ruling class, traditional nomadic customs and heritage from the Mongolian Steppe were held in high regard. On the other hand, the Mongol rulers also adopted flexibly to a variety of cultures from many advanced civilizations within the vast empire. Traditional social structure and culture in China underwent immense transform during the Mongol dominance. Large groups of foreign migrants settled in China, who enjoyed elevated social status over the majority Han Chinese, while enriching Chinese culture with foreign elements. The class of scholar officials and intellectuals, traditional bearers of elite Chinese culture, lost substantial social status. This stimulated the development of culture of the common folks. There were prolific works in zaju variety shows and literary songs (sanqu), which were written in a distinctive poetry style known as qu. Novels of vernacular style gained unprecedented status and popularity.\n\nBefore the Mongol invasion, Chinese dynasties reported approximately 120 million inhabitants; after the conquest had been completed in 1279, the 1300 census reported roughly 60 million people.[74] This major decline is not necessarily due only to Mongol killings. Scholars such as Frederick W. Mote argue that the wide drop in numbers reflects an administrative failure to record rather than an actual decrease; others such as Timothy Brook argue that the Mongols created a system of enserfment among a huge portion of the Chinese populace, causing many to disappear from the census altogether; other historians including William McNeill and David Morgan consider that plague was the main factor behind the demographic decline during this period. In the 14th century China suffered additional depredations from epidemics of plague, estimated to have killed around a quarter of the population of China.[75]: 348–351 \n\nThroughout the Yuan dynasty, there was some general sentiment among the populace against the Mongol dominance. Yet rather than the nationalist cause, it was mainly strings of natural disasters and incompetent, corrupt governance that triggered widespread peasant uprisings since the 1340s. After the massive naval engagement at Lake Poyang, Zhu Yuanzhang prevailed over other rebel forces in the south. He proclaimed himself emperor and founded the Ming dynasty in 1368. The same year his northern expedition army captured the capital Khanbaliq. The Yuan remnants fled back to Mongolia and sustained the regime, but the period of Yuan dominance was effectively over for good. Other Mongol Khanates in Central Asia continued to exist after the fall of Yuan dynasty in China.\n\nThe Ming dynasty was founded by Zhu Yuanzhang in 1368, who proclaimed himself as the Hongwu Emperor. The capital was initially set at Nanjing, and was later moved to Beijing from Yongle Emperor's reign onward.\n\nUrbanization increased as the population grew and as the division of labor grew more complex. Large urban centers, such as Nanjing and Beijing, also contributed to the growth of private industry. In particular, small-scale industries grew up, often specializing in paper, silk, cotton, and porcelain goods. For the most part, however, relatively small urban centers with markets proliferated around the country. Town markets mainly traded food, with some necessary manufactures such as pins or oil.\n\nDespite the xenophobia and intellectual introspection characteristic of the increasingly popular new school of neo-Confucianism, China under the early Ming dynasty was not isolated. Foreign trade and other contacts with the outside world, particularly Japan, increased considerably. Chinese merchants explored all of the Indian Ocean, reaching East Africa with the voyages of Zheng He.\n\nThe Hongwu Emperor, being the only founder of a Chinese dynasty who was also of peasant origin, had laid the foundation of a state that relied fundamentally in agriculture. Commerce and trade, which flourished in the previous Song and Yuan dynasties, were less emphasized. Neo-feudal landholdings of the Song and Mongol periods were expropriated by the Ming rulers. Land estates were confiscated by the government, fragmented, and rented out. Private slavery was forbidden. Consequently, after the death of the Yongle Emperor, independent peasant landholders predominated in Chinese agriculture. These laws might have paved the way to removing the worst of the poverty during the previous regimes. Towards later era of the Ming dynasty, with declining government control, commerce, trade and private industries revived.\n\nThe dynasty had a strong and complex central government that unified and controlled the empire. The emperor's role became more autocratic, although Hongwu Emperor necessarily continued to use what he called the \"Grand Secretariat\" to assist with the immense paperwork of the bureaucracy, including memorials (petitions and recommendations to the throne), imperial edicts in reply, reports of various kinds, and tax records. It was this same bureaucracy that later prevented the Ming government from being able to adapt to changes in society, and eventually led to its decline.\n\nThe Yongle Emperor strenuously tried to extend China's influence beyond its borders by demanding other rulers send ambassadors to China to present tribute. A large navy was built, including four-masted ships displacing 1,500 tons. A standing army of 1 million troops was created. The Chinese armies conquered and occupied Vietnam for around 20 years, while the Chinese fleet sailed the China seas and the Indian Ocean, cruising as far as the east coast of Africa. The Chinese gained influence in eastern Moghulistan. Several maritime Asian nations sent envoys with tribute for the Chinese emperor. Domestically, the Grand Canal was expanded and became a stimulus to domestic trade. Over 100,000 tons of iron per year were produced. Many books were printed using movable type. The imperial palace in Beijing's Forbidden City reached its current splendor. It was also during these centuries that the potential of south China came to be fully exploited. New crops were widely cultivated and industries such as those producing porcelain and textiles flourished.\n\nIn 1449 Esen Tayisi led an Oirat Mongol invasion of northern China which culminated in the capture of the Zhengtong Emperor at Tumu. Since then, the Ming became on the defensive on the northern frontier, which led to the Ming Great Wall being built. Most of what remains of the Great Wall of China today was either built or repaired by the Ming. The brick and granite work was enlarged, the watchtowers were redesigned, and cannons were placed along its length.\n\nAt sea the Ming became increasingly isolationist after the death of the Yongle Emperor. The treasure voyages which sailed the Indian Ocean were discontinued, and the maritime prohibition laws were set in place banning the Chinese from sailing abroad. European traders who reached China in the midst of the Age of Discovery were repeatedly rebuked in their requests for trade, with the Portuguese being repulsed by the Ming navy at Tuen Mun in 1521 and again in 1522. Domestic and foreign demands for overseas trade, deemed illegal by the state, led to widespread wokou piracy attacking the southeastern coastline during the rule of the Jiajing Emperor (1507–1567), which only subsided after the opening of ports in Guangdong and Fujian and much military suppression.[76] In addition to raids from Japan by the wokou, raids from Taiwan and the Philippines by the Pisheye also ravaged the southern coasts.[77] The Portuguese were allowed to settle in Macau in 1557 for trade, which remained in Portuguese hands until 1999. After the Spanish invasion of the Philippines, trade with the Spanish at Manila imported large quantities of Mexican and Peruvian silver from the Spanish Americas to China.[78]: 144–145  The Dutch entry into the Chinese seas was also met with fierce resistance, with the Dutch being chased off the Penghu islands in the Sino-Dutch conflicts of 1622–1624 and were forced to settle in Taiwan instead. The Dutch in Taiwan fought with the Ming in the Battle of Liaoluo Bay in 1633 and lost, and eventually surrendered to the Ming loyalist Koxinga in 1662, after the fall of the Ming dynasty.\n\nIn 1556, during the rule of the Jiajing Emperor, the Shaanxi earthquake killed about 830,000 people, the deadliest earthquake of all time.\n\nThe Ming dynasty intervened deeply in the Japanese invasions of Korea (1592–1598), which ended with the withdrawal of all invading Japanese forces in Korea, and the restoration of the Joseon dynasty, its traditional ally and tributary state. The regional hegemony of the Ming dynasty was preserved at a toll on its resources. Coincidentally, with Ming's control in Manchuria in decline, the Manchu (Jurchen) tribes, under their chieftain Nurhaci, broke away from Ming's rule, and emerged as a powerful, unified state, which was later proclaimed as the Qing dynasty. It went on to subdue the much weakened Korea as its tributary, conquered Mongolia, and expanded its territory to the outskirt of the Great Wall. The most elite army of the Ming dynasty was to station at the Shanhai Pass to guard the last stronghold against the Manchus, which weakened its suppression of internal peasants uprisings.\n\nThe Qing dynasty (1644–1912) was the last imperial dynasty in China. Founded by the Manchus, it was the second conquest dynasty to rule the entirety of China proper, and roughly doubled the territory controlled by the Ming. The Manchus were formerly known as Jurchens, residing in the northeastern part of the Ming territory outside the Great Wall. They emerged as the major threat to the late Ming dynasty after Nurhaci united all Jurchen tribes and his son, Hong Taiji, declared the founding of the Qing dynasty in 1636. The Qing dynasty set up the Eight Banners system that provided the basic framework for the Qing military conquest. Li Zicheng's peasant rebellion captured Beijing in 1644 and the Chongzhen Emperor, the last Ming emperor, committed suicide. The Manchus allied with the Ming general Wu Sangui to seize Beijing, which was made the capital of the Qing dynasty, and then proceeded to subdue the Ming remnants in the south. During the Ming-Qing transition, when the Ming dynasty and later the Southern Ming, the emerging Qing dynasty, and several other factions like the Shun dynasty and Xi dynasty founded by peasant revolt leaders fought against each another, which, along with innumerable natural disasters at that time such as those caused by the Little Ice Age[79] and epidemics like the Great Plague during the last decade of the Ming dynasty,[80] caused enormous loss of lives and significant harm to the economy. In total, these decades saw the loss of as many as 25 million lives, but the Qing appeared to have restored China's imperial power and inaugurate another flowering of the arts.[81] The early Manchu emperors combined traditions of Inner Asian rule with Confucian norms of traditional Chinese government and were considered a Chinese dynasty.\n\nThe Manchus enforced a 'queue order', forcing Han Chinese men to adopt the Manchu queue hairstyle. Officials were required to wear Manchu-style clothing Changshan (bannermen dress and Tangzhuang), but ordinary Han civilians were allowed to wear traditional Han clothing. Bannermen could not undertake trade or manual labor; they had to petition to be removed from banner status. They were considered aristocracy and were given annual pensions, land, and allotments of cloth. The Kangxi Emperor ordered the creation of the Kangxi Dictionary, the most complete dictionary of Chinese characters that had been compiled.\n\nOver the next half-century, all areas previously under the Ming dynasty were consolidated under the Qing. Conquests in Central Asia in the eighteenth century extended territorial control. Between 1673 and 1681, the Kangxi Emperor suppressed the Revolt of the Three Feudatories, an uprising of three generals in Southern China who had been denied hereditary rule of large fiefdoms granted by the previous emperor. In 1683, the Qing staged an amphibious assault on southern Taiwan, bringing down the rebel Kingdom of Tungning, which was founded by the Ming loyalist Koxinga (Zheng Chenggong) in 1662 after the fall of the Southern Ming, and had served as a base for continued Ming resistance in Southern China. The Qing defeated the Russians at Albazin, resulting in the Treaty of Nerchinsk.\n\nBy the end of Qianlong Emperor's long reign in 1796, the Qing Empire was at its zenith. The Qing ruled more than one-third of the world's population, and had the largest economy in the world. By area it was one of the largest empires ever.\n\nIn the 19th century the empire was internally restive and externally threatened by western powers. The defeat by the British Empire in the First Opium War (1840) led to the Treaty of Nanking (1842), under which Hong Kong was ceded to Britain and importation of opium (produced by British Empire territories) was allowed. Opium usage continued to grow in China, adversely affecting societal stability. Subsequent military defeats and unequal treaties with other western powers continued even after the fall of the Qing dynasty.\n\nInternally the Taiping Rebellion (1851–1864), a Christian religious movement led by the \"Heavenly King\" Hong Xiuquan swept from the south to establish the Taiping Heavenly Kingdom and controlled roughly a third of China proper for over a decade. The court in desperation empowered Han Chinese officials such as Zeng Guofan to raise local armies. After initial defeats, Zeng crushed the rebels in the Third Battle of Nanking in 1864.[82] This was one of the largest wars in the 19th century in troop involvement; there was massive loss of life, with a death toll of about 20 million.[83] A string of civil disturbances followed, including the Punti–Hakka Clan Wars, Nian Rebellion, Dungan Revolt, and Panthay Rebellion.[84] All rebellions were ultimately put down, but at enormous cost and with millions dead, seriously weakening the central imperial authority. China never rebuilt a strong central army, and many local officials used their military power to effectively rule independently in their provinces.[82]\n\nYet the dynasty appeared to recover in the Tongzhi Restoration (1860–1872), led by Manchu royal family reformers and Han Chinese officials such as Zeng Guofan and his proteges Li Hongzhang and Zuo Zongtang. Their Self-Strengthening Movement made effective institutional reforms, imported Western factories and communications technology, with prime emphasis on strengthening the military. However, the reform was undermined by official rivalries, cynicism, and quarrels within the imperial family. The defeat of Yuan Shikai's modernized \"Beiyang Fleet\" in the First Sino-Japanese War (1894–1895) led to the formation of the New Army. The Guangxu Emperor, advised by Kang Youwei, then launched a comprehensive reform effort, the Hundred Days' Reform (1898). Empress Dowager Cixi, however, feared that precipitous change would lead to bureaucratic opposition and foreign intervention and quickly suppressed it.\n\nIn the summer of 1900, the Boxer Uprising opposed foreign influence and murdered Chinese Christians and foreign missionaries. When Boxers entered Beijing, the Qing government ordered all foreigners to leave, but they and many Chinese Christians were besieged in the foreign legations quarter. An Eight-Nation Alliance sent the Seymour Expedition of Japanese, Russian, British, Italian, German, French, American, and Austrian troops to relieve the siege, but they were routed and forced to retreat by Boxer and Qing troops at the Battle of Langfang. After the Alliance's attack on the Dagu Forts, the court declared war on the Alliance and authorised the Boxers to join with imperial armies. After fierce fighting at Tianjin, the Alliance formed the second, much larger Gaselee Expedition and finally reached Beijing; the Empress Dowager evacuated to Xi'an. The Boxer Protocol ended the war, exacting a tremendous indemnity.\n\nThe Qing court then instituted administrative and legal reforms known as the late Qing reforms, including abolition of the examination system. But young officials, military officers, and students debated reform, perhaps a constitutional monarchy, or the overthrow of the dynasty and the creation of a republic. They were inspired by an emerging public opinion formed by intellectuals such as Liang Qichao and the revolutionary ideas of Sun Yat-sen. A localised military uprising, the Wuchang uprising, began on 10 October 1911, in Wuchang (today part of Wuhan), and soon spread. The Republic of China was proclaimed on 1 January 1912, ending 2,000 years of dynastic rule.\n\nThe provisional government of the Republic of China was formed in Nanjing on 12 March 1912. Sun Yat-sen became President of the Republic of China, but he turned power over to Yuan Shikai, who commanded the New Army. Over the next few years, Yuan proceeded to abolish the national and provincial assemblies, and declared himself as the emperor of Empire of China in late 1915, in the style of an absolute monarchy. Yuan's imperial ambitions were fiercely opposed by his subordinates; faced with the rapidly growing prospect of violent rebellion, he abdicated in March 1916 and died of natural causes in June.\n\nYuan's death in 1916 left a power vacuum; the republican government (that had been nearly brought to its knees by his policies) was all but shattered. This opened the way for the Warlord Era, during which much of China was ruled by shifting coalitions of competing provincial military leaders and the Beiyang government, ushering in a short-lived period of uncertainty. Intellectuals, disappointed in the failure of the Republic, launched the New Culture Movement.\n\nIn 1919, the May Fourth Movement began as a response to the pro-Japanese terms imposed on China by the Treaty of Versailles following World War I. It quickly became a nationwide protest movement. The protests were a moral success as the cabinet fell and China refused to sign the Treaty of Versailles, which had awarded German holdings of Shandong to Japan. Memory of the mistreatment at Versailles fuels resentment into the 21st century.[85]\n\nPolitical and intellectual ferment waxed strong throughout the 1920s and 1930s. According to Patricia Ebrey:\n\nIn the 1920s Sun Yat-sen established a revolutionary base in Guangzhou and set out to unite the fragmented nation. He welcomed assistance from the Soviet Union (itself fresh from Lenin's Communist takeover) and he entered into an alliance with the fledgling Chinese Communist Party (CCP). After Sun's death from cancer in 1925, one of his protégés, Chiang Kai-shek, seized control of the Nationalist Party (KMT) and succeeded in bringing most of south and central China under its rule in the Northern Expedition (1926–1927). Having defeated the warlords in the south and central China by military force, Chiang was able to secure the nominal allegiance of the warlords in the North and establish the Nationalist government in Nanjing. In 1927, Chiang turned on the CCP and relentlessly purged the Communists elements in his NRA. In 1934, driven from their mountain bases such as the Chinese Soviet Republic, the CCP forces embarked on the Long March across China's most desolate terrain to the northwest, a feat transformed into legend, where they established a guerrilla base at Yan'an in Shaanxi. During the Long March, the communists reorganised under a new leader, Mao Zedong (Mao Tse-tung).\n\nThe bitter Chinese Civil War between the Nationalists and the Communists continued, openly or clandestinely, through the 14-year-long Japanese occupation of various parts of the country (1931–1945). The two Chinese parties nominally formed a United Front to oppose the Japanese in 1937, during the Second Sino-Japanese War (1937–1945), which became a part of World War II, although this alliance was tenuous at best and disagreements, sometimes violent, between the forces were still common. Japanese forces committed numerous war atrocities against the civilian population, including biological warfare (see Unit 731) and the Three Alls Policy (Sankō Sakusen), namely being: \"Kill All, Burn All and Loot All\".[87] During the war, China was recognized as one of the Allied \"Big Four\" in the Declaration by United Nations, as a tribute to its enduring struggle against the invading Japanese.[88] China was one of the four major Allies of World War II, and was later considered one of the primary victors in the war.[89]\n\nFollowing the defeat of Japan in 1945, the war between the Nationalist government forces and the CCP resumed, after failed attempts at reconciliation and a negotiated settlement. By 1949, the CCP had established control over most of the country. Odd Arne Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang, and because in his search for a powerful centralized government, Chiang antagonised too many interest groups in China. Furthermore, his party was weakened in the war against the Japanese. Meanwhile, the Communists told different groups, such as peasants, exactly what they wanted to hear, and cloaked themselves in the cover of Chinese Nationalism.[90] During the civil war both the Nationalists and Communists carried out mass atrocities, with millions of non-combatants killed by both sides.[91] These included deaths from forced conscription and massacres.[92]\n\nThe Nationalists were slowly routed towards the South. When the Nationalist government forces were defeated by CCP forces in mainland China in 1949, the Nationalist government fled to Taiwan with its forces, along with Chiang and a large number of their supporters; the Nationalist government had taken effective control of Taiwan at the end of WWII as part of the overall Japanese surrender, when Japanese troops in Taiwan surrendered to the Republic of China troops there.[93]\n\nUntil the early 1970s the ROC was recognised as the sole legitimate government of China by the United Nations, the United States and most Western nations, refusing to recognise the PRC on account of its status as a communist nation during the Cold War. This changed in 1971 when the PRC was seated in the United Nations, replacing the ROC. The KMT ruled Taiwan under martial law until 1987, with the stated goal of being vigilant against Communist infiltration and preparing to retake mainland China. Therefore, political dissent was not tolerated during that period, and crackdowns against dissidents were common.\n\nIn the 1990s the ROC underwent a major democratic reform, beginning with the 1991 resignation of the members of the Legislative Yuan and National Assembly elected in 1947. These groups were originally created to represent mainland China constituencies. Also lifted were the restrictions on the use of Taiwanese languages in the broadcast media and in schools. In 1996, the ROC held its first direct presidential election, and the incumbent president, KMT candidate Lee Teng-hui, was elected. In 2000, the KMT status as the ruling party ended when the DPP took power, only to regain its status in the 2008 election by Ma Ying-jeou.\n\nDue to the controversial nature of Taiwan's political status, the ROC is currently recognised by merely 12 UN member states and the Holy See as of 2024[update] as the legitimate government of \"China\".\n\nMajor combat in the Chinese Civil War ended in 1949 with the KMT pulling out of the mainland, with the government relocating to Taipei and maintaining control only over a few islands. The CCP was left in control of mainland China. On 1 October 1949, Mao Zedong proclaimed the People's Republic of China.[94] \"Communist China\" and \"Red China\" were two common names for the PRC.[95]\n\nThe PRC was shaped by a series of campaigns and five-year plans. The Great Leap Forward, a radical campaign that encompassed numerous attempted economic and social reforms, resulted in tens of millions of deaths.[96][better source needed] Mao's government carried out mass executions of landowners, instituted collectivisation and implemented the Laogai camp system. Execution, deaths from forced labor and other atrocities resulted in millions of deaths under Mao. In 1966 Mao and his allies launched the Cultural Revolution, which continued until Mao's death a decade later. The Cultural Revolution, motivated by power struggles within the Party and a fear of the Soviet Union, led to a major upheaval in Chinese society.\n\nFollowing the Sino-Soviet split and motivated by concerns of invasion by either the Soviet Union or the United States, China initiated the Third Front campaign to develop national defense and industrial infrastructure in its rugged interior.[97]: 44  Through its distribution of infrastructure, industry, and human capital around the country, the Third Front created favorable conditions for subsequent market development and private enterprise.[97]: 177 \n\nIn 1972, at the peak of the Sino-Soviet split, Mao and Zhou Enlai met U.S. president Richard Nixon in Beijing to establish relations with the US. In the same year, the PRC was admitted to the United Nations in place of the Republic of China, with permanent membership of the Security Council.\n\nA power struggle followed Mao's death in 1976. The Gang of Four were arrested and blamed for the excesses of the Cultural Revolution, marking the end of a turbulent political era in China. Deng Xiaoping outmaneuvered Mao's anointed successor chairman Hua Guofeng, and gradually emerged as the de facto leader over the next few years.\n\nDeng Xiaoping was the Paramount Leader of China from 1978 to 1992, although he never became the head of the party or state, and his influence within the Party led the country to significant economic reforms. The CCP subsequently loosened governmental control over citizens' personal lives and the communes were disbanded with many peasants receiving multiple land leases, which greatly increased incentives and agricultural production. In addition, there were many free market areas opened. The most successful free market area was Shenzhen. It is located in Guangdong and the property tax free area still exists today. This turn of events marked China's transition from a planned economy to a mixed economy with an increasingly open market environment, a system termed by some[98] as market socialism, and officially by the CCP as Socialism with Chinese characteristics. The PRC adopted its current constitution on 4 December 1982.\n\nIn 1989 the death of former general secretary Hu Yaobang helped to spark the Tiananmen Square protests of that year, during which students and others campaigned for several months, speaking out against corruption and in favour of greater political reform, including democratic rights and freedom of speech. However, they were eventually put down on 4 June when Army troops and vehicles entered and forcibly cleared the square, resulting in considerable numbers of fatalities. This event was widely reported, and brought worldwide condemnation and sanctions against the communist government.[99][100]\n\nCCP general secretary and PRC president Jiang Zemin and PRC premier Zhu Rongji, both former mayors of Shanghai, led post-Tiananmen PRC in the 1990s. Under Jiang and Zhu's ten years of administration, the PRC's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%.[101][better source needed] The country formally joined the World Trade Organization in 2001. By 1997 and 1999, former European colonies of British Hong Kong and Portuguese Macau became the Hong Kong and Macau special administrative regions of the People's Republic of China, respectively.\n\nAlthough the PRC needed economic growth to spur its development, the government began to worry that rapid economic growth was degrading the country's natural resources and environment. Another concern was that certain sectors of society were not sufficiently benefiting from the PRC's economic development; one example of this was the wide gap between urban and rural areas in terms of development and prevalence of updated infrastructure. As a result, under former CCP general secretary and President Hu Jintao and Premier Wen Jiabao, the PRC initiated policies to address issues of equitable distribution of resources, but the outcome was not known as of 2014[update].[102] More than 40 million farmers were displaced from their land,[103] usually for economic development, contributing to 87,000 demonstrations and riots across China in 2005.[104] For much of the PRC's population, living standards improved very substantially and freedom increased, but political controls remained tight and rural areas poor.[105]\n\nAccording to the U.S. Department of Defense, as many as 3 million Uyghurs and members of other Muslim minority groups are being held in China's internment camps which are located in the Xinjiang region and which Western news reports often label as \"concentration camps\".[106] The camps were established in late 2010s under Xi Jinping's administration.[107][108] Human Rights Watch says that they have been used to indoctrinate Uyghurs and other Muslims since 2017 as part of a people's war on terror, a policy announced in 2014.[109][110][107] The use of these centers appears to have ended in 2019 following international pressure.[111] Academic Kerry Brown attributes their closures beginning in late 2019 to the expense required to operate them.[112]: 138  China has repeatedly denied this, asserting that the West has never been able to produce reliably-sourced satellite footage of any such detainment or resulting detention of minority groups. Although no comprehensive independent surveys of such centres have been performed as of June 2024, spot checks by journalists have found such sites converted or abandoned.[111] In 2022, a Washington Post reporter checked a dozen sites previously identified as reeducation centres and found \"[m]ost of them appeared to be empty or converted, with several sites labeled as coronavirus quarantine facilities, teachers' schools and vocational schools.\"[111] In 2023, Amnesty International said that they were \"witnessing more and more arbitrary detention\", but that detained individuals were being moved from the camps into the formal prison system.[113]\n\nThe novel coronavirus SARS-CoV-2, which causes the disease COVID-19, was first detected in Wuhan, Hubei in 2019 and led to a global pandemic, causing the majority of the world to enter a period of lockdown for at least a year following.\n"
    },
    {
        "title": "Diophantus",
        "content": "Diophantus of Alexandria[1] (born c. AD 200 – c. 214; died c. AD 284 – c. 298) was a Greek mathematician, who was the author of two main works: On Polygonal Numbers, which survives incomplete, and the Arithmetica in thirteen books, most of it extant, made up of arithmetical problems that are solved through algebraic equations.[2]\n\nHis Arithmetica influenced the development of algebra by Arabs, and his equations influenced modern work in both abstract algebra and computer science.[3] The first five books of his work are purely algebraic.[3] Furthermore, recent studies of Diophantus's work have revealed that the method of solution taught in his Arithmetica matches later medieval Arabic algebra in its concepts and overall procedure.[4]\n\nDiophantus was among the earliest mathematicians who recognized positive rational numbers as numbers, by allowing fractions for coefficients and solutions. He coined the term παρισότης (parisotēs) to refer to an approximate equality.[5] This term was rendered as adaequalitas in Latin, and became the technique of adequality developed by Pierre de Fermat to find maxima for functions and tangent lines to curves. \n\nAlthough not the earliest, the Arithmetica has the best-known use of algebraic notation to solve arithmetical problems coming from Greek antiquity,[6][2] and some of its problems served as inspiration for later mathematicians working in analysis and number theory.[7] In modern use, Diophantine equations are algebraic equations with integer coefficients for which integer solutions are sought. Diophantine geometry and Diophantine approximations are  two other subareas of number theory that are named after him.\n\n\nDiophantus was born into a Greek family and is known to have lived in Alexandria, Egypt, during the Roman era, between AD 200 and 214 to 284 or 298.[6][8][9][a] Much of our knowledge of the life of Diophantus is derived from a 5th-century Greek anthology of number games and puzzles created by Metrodorus. One of the problems (sometimes called his epitaph) states:\nHere lies Diophantus, the wonder behold. Through art algebraic, the stone tells how old: 'God gave him his boyhood one-sixth of his life, One twelfth more as youth while whiskers grew rife; And then yet one-seventh ere marriage begun; In five years there came a bouncing new son. Alas, the dear child of master and sage After attaining half the measure of his father's life chill fate took him. After consoling his fate by the science of numbers for four years, he ended his life.'\nThis puzzle implies that Diophantus' age x can be expressed as\n\nwhich gives x a value of 84 years. However, the accuracy of the information cannot be confirmed.\n\nIn popular culture, this puzzle was the Puzzle No.142 in Professor Layton and Pandora's Box as one of the hardest solving puzzles in the game, which needed to be unlocked by solving other puzzles first.\n\nArithmetica is the major work of Diophantus and the most prominent work on premodern algebra in Greek mathematics. It is a collection of problems giving numerical solutions of both determinate and indeterminate equations. Of the original thirteen books of which Arithmetica consisted only six have survived, though there are some who believe that four Arabic books discovered in 1968 are also by Diophantus.[14] Some Diophantine problems from Arithmetica have been found in Arabic sources.\n\n\nIt should be mentioned here that Diophantus never used general methods in his solutions. Hermann Hankel, renowned German mathematician made the following remark regarding Diophantus:\nOur author (Diophantos) not the slightest trace of a general, comprehensive method is discernible; each problem calls for some special method which refuses to work even for the most closely related problems.  For this reason it is difficult for the modern scholar to solve the 101st problem even after having studied 100 of Diophantos's solutions.[15]\n\nLike many other Greek mathematical treatises, Diophantus was forgotten in Western Europe during the Dark Ages, since the study of ancient Greek, and literacy in general, had greatly declined.  The portion of the Greek Arithmetica that survived, however, was, like all ancient Greek texts transmitted to the early modern world, copied by, and thus known to, medieval Byzantine scholars. Scholia on Diophantus by the Byzantine Greek scholar John Chortasmenos (1370–1437) are preserved together with a comprehensive commentary written by the earlier Greek scholar Maximos Planudes (1260 – 1305), who produced an edition of Diophantus within the library of the Chora Monastery in Byzantine Constantinople.[16] In addition, some portion of the Arithmetica probably survived in the Arab tradition (see above). In 1463 German mathematician Regiomontanus wrote:\nNo one has yet translated from the Greek into Latin the thirteen books of Diophantus, in which the very flower of the whole of arithmetic lies hidden.\nArithmetica was first translated from Greek into Latin by Bombelli in 1570, but the translation was never published. However, Bombelli borrowed many of the problems for his own book Algebra. The editio princeps of Arithmetica was published in 1575 by Xylander. The Latin translation of Arithmetica by Bachet in 1621 became the first Latin edition that was widely available. Pierre de Fermat owned a copy, studied it and made notes in the margins. A later 1895 Latin translation by Paul Tannery was said to be an improvement by Thomas L. Heath, who used it in the 1910 second edition of his English translation.\n\n\nThe 1621 edition of Arithmetica by Bachet gained fame after Pierre de Fermat wrote his famous \"Last Theorem\" in the margins of his copy: \nIf an integer n is greater than 2, then an + bn = cn has no solutions in non-zero integers a, b, and c. I have a truly marvelous proof of this proposition which this margin is too narrow to contain.\nFermat's proof was never found, and the problem of finding a proof for the theorem went unsolved for centuries. A proof was finally found in 1994 by Andrew Wiles after working on it for seven years.  It is believed that Fermat did not actually have the proof he claimed to have. Although the original copy in which Fermat wrote this is lost today, Fermat's son edited the next edition of Diophantus, published in 1670. Even though the text is otherwise inferior to the 1621 edition, Fermat's annotations—including the \"Last Theorem\"—were printed in this version.\n\nFermat was not the first mathematician so moved to write in his own marginal notes to Diophantus; the Byzantine scholar John Chortasmenos (1370–1437) had written \"Thy soul, Diophantus, be with Satan because of the difficulty of your other theorems and particularly of the present theorem\" next to the same problem.[16]\n\nDiophantus wrote several other books besides Arithmetica, but only a few of them have survived.\n\nDiophantus himself refers to a work which consists of a collection of lemmas called The Porisms (or Porismata), but this book is entirely lost.[17]\n\nAlthough The Porisms is lost, we know three lemmas contained there, since Diophantus refers to them in the Arithmetica. One lemma states that the difference of the cubes of two rational numbers is equal to the sum of the cubes of two other rational numbers, i.e. given any a and b, with a > b, there exist c and d, all positive and rational, such that\n\nDiophantus is also known to have written on polygonal numbers, a topic of great interest to Pythagoras and Pythagoreans. Fragments of a book dealing with polygonal numbers are extant.[18]\n\nA book called Preliminaries to the Geometric Elements has been traditionally attributed to Hero of Alexandria. It has been studied recently by Wilbur Knorr, who suggested that the attribution to Hero is incorrect, and that the true author is Diophantus.[19]\n\nDiophantus' work has had a large influence in history. Editions of Arithmetica exerted a profound influence on the development of algebra in Europe in the late sixteenth and through the 17th and 18th centuries.  Diophantus and his works also influenced Arab mathematics and were of great fame among Arab mathematicians. Diophantus' work created a foundation for work on algebra and in fact much of advanced mathematics is based on algebra.[20] How much he affected India is a matter of debate.\n\nDiophantus has been considered \"the father of algebra\" because of his contributions to number theory, mathematical notations and the earliest known use of syncopated notation in his book series Arithmetica.[2] However this is usually debated, because Al-Khwarizmi was also given the title as \"the father of algebra\", nevertheless both mathematicians were responsible for paving the way for algebra today.\n\nToday, Diophantine analysis is the area of study where integer (whole-number) solutions are sought for equations, and Diophantine equations are polynomial equations with integer coefficients to which only integer solutions are sought. It is usually rather difficult to tell whether a given Diophantine equation is solvable. Most of the problems in Arithmetica lead to quadratic equations. Diophantus looked at 3 different types of quadratic equations: ax2 + bx = c, ax2 = bx + c, and ax2 + c = bx. The reason why there were three cases to Diophantus, while today we have only one case, is that he did not have any notion for zero and he avoided negative coefficients by considering the given numbers a, b, c to all be positive in each of the three cases above.  Diophantus was always satisfied with a rational solution and did not require a whole number which means he accepted fractions as solutions to his problems. Diophantus considered negative or irrational square root solutions \"useless\", \"meaningless\", and even \"absurd\". To give one specific example, he calls the equation 4 = 4x + 20 'absurd' because it would lead to a negative value for x. One solution was all he looked for in a quadratic equation. There is no evidence that suggests Diophantus even realized that there could be two solutions to a quadratic equation. He also considered simultaneous quadratic equations.\n\n\nDiophantus made important advances in mathematical notation, becoming the first person known to use algebraic notation and symbolism. Before him everyone wrote out equations completely. Diophantus introduced an algebraic symbolism that used an abridged notation for frequently occurring operations, and an abbreviation for the unknown and for the powers of the unknown. Mathematical historian Kurt Vogel states:[21]\nThe symbolism that Diophantus introduced for the first time, and undoubtedly devised himself, provided a short and readily comprehensible means of expressing an equation... Since an abbreviation is also employed for the word 'equals', Diophantus took a fundamental step from verbal algebra towards symbolic algebra.\nAlthough Diophantus made important advances in symbolism, he still lacked the necessary notation to express more general methods. This caused his work to be more concerned with particular problems rather than general situations. Some of the limitations of Diophantus' notation are that he only had notation for one unknown and, when problems involved more than a single unknown, Diophantus was reduced to expressing \"first unknown\", \"second unknown\", etc. in words. He also lacked a symbol for a general number n. Where we would write ⁠12 + 6n/n2 − 3⁠, Diophantus has to resort to constructions like:  \"... a sixfold number increased by twelve, which is divided by the difference by which the square of the number exceeds three\". Algebra still had a long way to go before very general problems could be written down and solved succinctly.\n\n\"But what we really want to know is to what extent the Alexandrian mathematicians of the period from the first to the fifth centuries C.E. were Greek. Certainly, all of them wrote in Greek and were part of the Greek intellectual community of Alexandria. And most modern studies conclude that the Greek community coexisted [...] So should we assume that Ptolemy and Diophantus, Pappus and Hypatia were ethnically Greek, that their ancestors had come from Greece at some point in the past but had remained effectively isolated from the Egyptians? It is, of course, impossible to answer this question definitively. But research in papyri dating from the early centuries of the common era demonstrates that a significant amount of intermarriage took place between the Greek and Egyptian communities [...] And it is known that Greek marriage contracts increasingly came to resemble Egyptian ones. In addition, even from the founding of Alexandria, small numbers of Egyptians were admitted to the privileged classes in the city to fulfill numerous civic roles. Of course, it was essential in such cases for the Egyptians to become \"Hellenized,\" to adopt Greek habits and the Greek language. Given that the Alexandrian mathematicians mentioned here were active several hundred years after the founding of the city, it would seem at least equally possible that they were ethnically Egyptian as that they remained ethnically Greek. In any case, it is unreasonable to portray them with purely European features when no physical descriptions exist.\"\n\"Diophantos was most likely a Hellenized Babylonian.\""
    },
    {
        "title": "Pierre de Fermat",
        "content": "Pierre de Fermat (French: [pjɛʁ də fɛʁma]; [a]17 August 1601 – 12 January 1665) was a French mathematician who is given credit for early developments that led to infinitesimal calculus, including his technique of adequality. In particular, he is recognized for his discovery of an original method of finding the greatest and the smallest ordinates of curved lines, which is analogous to that of differential calculus, then unknown, and his research into number theory. He made notable contributions to analytic geometry, probability, and optics. He is best known for his Fermat's principle for light propagation and his Fermat's Last Theorem in number theory, which he described in a note at the margin of a copy of Diophantus' Arithmetica. He was also a lawyer[3] at the Parlement of Toulouse, France.\n\nFermat was born in 1601[a] in Beaumont-de-Lomagne, France—the late 15th-century mansion where Fermat was born is now a museum. He was from Gascony, where his father, Dominique Fermat, was a wealthy leather merchant and served three one-year terms as one of the four consuls of Beaumont-de-Lomagne. His mother was Claire de Long.[2] Pierre had one brother and two sisters and was almost certainly brought up in the town of his birth.[citation needed]\n\nHe attended the University of Orléans from 1623 and received a bachelor in civil law in 1626, before moving to Bordeaux. In Bordeaux, he began his first serious mathematical researches, and in 1629 he gave a copy of his restoration of Apollonius's De Locis Planis to one of the mathematicians there. Certainly, in Bordeaux he was in contact with Beaugrand and during this time he produced important work on maxima and minima which he gave to Étienne d'Espagnet who clearly shared mathematical interests with Fermat. There he became much influenced by the work of François Viète.[4]\n\nIn 1630, he bought the office of a councilor at the Parlement de Toulouse, one of the High Courts of Judicature in France, and was sworn in by the Grand Chambre in May 1631. He held this office for the rest of his life. Fermat thereby became entitled to change his name from Pierre Fermat to Pierre de Fermat. On 1 June 1631, Fermat married Louise de Long, a fourth cousin of his mother Claire de Fermat (née de Long). The Fermats had eight children, five of whom survived to adulthood: Clément-Samuel, Jean, Claire, Catherine, and Louise.[5][6][7]\n\nFluent in six languages (French, Latin, Occitan, classical Greek, Italian and Spanish), Fermat was praised for his written verse in several languages and his advice was eagerly sought regarding the emendation of Greek texts. He communicated most of his work in letters to friends, often with little or no proof of his theorems. In some of these letters to his friends, he explored many of the fundamental ideas of calculus before Newton or Leibniz. Fermat was a trained lawyer making mathematics more of a hobby than a profession. Nevertheless, he made important contributions to analytical geometry, probability, number theory and calculus.[8] Secrecy was common in European mathematical circles at the time. This naturally led to priority disputes with contemporaries such as Descartes and Wallis.[9]\n\nAnders Hald writes that, \"The basis of Fermat's mathematics was the classical Greek treatises combined with Vieta's new algebraic methods.\"[10]\n\nFermat's pioneering work in analytic geometry (Methodus ad disquirendam maximam et minimam et de tangentibus linearum curvarum) was circulated in manuscript form in 1636 (based on results achieved in 1629),[11] predating the publication of Descartes' La géométrie (1637), which exploited the work.[12] This manuscript was published posthumously in 1679 in Varia opera mathematica, as Ad Locos Planos et Solidos Isagoge (Introduction to Plane and Solid Loci).[13]\n\nIn Methodus ad disquirendam maximam et minimam et de tangentibus linearum curvarum, Fermat developed a method (adequality) for determining maxima, minima, and tangents to various curves that was equivalent to differential calculus.[14][15] In these works, Fermat obtained a technique for finding the centers of gravity of various plane and solid figures, which led to his further work in quadrature.\n\nFermat was the first person known to have evaluated the integral of general power functions. With his method, he was able to reduce this evaluation to the sum of geometric series.[16] The resulting formula was helpful to Newton, and then Leibniz, when they independently developed the fundamental theorem of calculus.[citation needed]\n\nIn number theory, Fermat studied Pell's equation, perfect numbers, amicable numbers and what would later become Fermat numbers. It was while researching perfect numbers that he discovered Fermat's little theorem. He invented a factorization method—Fermat's factorization method—and popularized the proof by infinite descent, which he used to prove Fermat's right triangle theorem which includes as a corollary Fermat's Last Theorem for the case n = 4. Fermat developed the two-square theorem, and the polygonal number theorem, which states that each number is a sum of three triangular numbers, four square numbers, five pentagonal numbers, and so on.\n\nAlthough Fermat claimed to have proven all his arithmetic theorems, few records of his proofs have survived. Many mathematicians, including Gauss, doubted several of his claims, especially given the difficulty of some of the problems and the limited mathematical methods available to Fermat. His Last Theorem was first discovered by his son in the margin in his father's copy of an edition of Diophantus, and included the statement that the margin was too small to include the proof. It seems that he had not written to Marin Mersenne about it. It was first proven in 1994, by Sir Andrew Wiles, using techniques unavailable to Fermat.[citation needed]\n\nThrough their correspondence in 1654, Fermat and Blaise Pascal helped lay the foundation for the theory of probability. From this brief but productive collaboration on the problem of points, they are now regarded as joint founders of probability theory.[17] Fermat is credited with carrying out the first-ever rigorous probability calculation. In it, he was asked by a professional gambler why if he bet on rolling at least one six in four throws of a die he won in the long term, whereas betting on throwing at least one double-six in 24 throws of two dice resulted in his losing. Fermat showed mathematically why this was the case.[18]\n\nThe first variational principle in physics was articulated by Euclid in his Catoptrica. It says that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection. Hero of Alexandria later showed that this path gave the shortest length and the least time.[19] Fermat refined and generalized this to \"light travels between two given points along the path of shortest time\" now known as the principle of least time.[20] For this, Fermat is recognized as a key figure in the historical development of the fundamental principle of least action in physics. The terms Fermat's principle and Fermat functional were named in recognition of this role.[21]\n\nPierre de Fermat died on January 12, 1665, at Castres, in the present-day department of Tarn.[22] The oldest and most prestigious high school in Toulouse is named after him: the Lycée Pierre-de-Fermat. French sculptor Théophile Barrau made a marble statue named Hommage à Pierre Fermat as a tribute to Fermat, now at the Capitole de Toulouse.\n\nTogether with René Descartes, Fermat was one of the two leading mathematicians of the first half of the 17th century. According to Peter L. Bernstein, in his 1996 book Against the Gods, Fermat \"was a mathematician of rare power. He was an independent inventor of analytic geometry, he contributed to the early development of calculus, he did research on the weight of the earth, and he worked on light refraction and optics. In the course of what turned out to be an extended correspondence with Blaise Pascal, he made a significant contribution to the theory of probability. But Fermat's crowning achievement was in the theory of numbers.\"[23]\n\nRegarding Fermat's work in analysis, Isaac Newton wrote that his own early ideas about calculus came directly from \"Fermat's way of drawing tangents.\"[24]\n\nOf Fermat's number theoretic work, the 20th-century mathematician André Weil wrote that: \"what we possess of his methods for dealing with curves of genus 1 is remarkably coherent; it is still the foundation for the modern theory of such curves. It naturally falls into two parts; the first one ... may conveniently be termed a method of ascent, in contrast with the descent which is rightly regarded as Fermat's own.\"[25] Regarding Fermat's use of ascent, Weil continued: \"The novelty consisted in the vastly extended use which Fermat made of it, giving him at least a partial equivalent of what we would obtain by the systematic use of the group theoretical properties of the rational points on a standard cubic.\"[26] With his gift for number relations and his ability to find proofs for many of his theorems, Fermat essentially created the modern theory of numbers.\n"
    },
    {
        "title": "Leonhard Euler",
        "content": "\n\nLeonhard Euler (/ˈɔɪlər/ OY-lər;[b] German: [ˈleːɔnhaʁt ˈʔɔʏlɐ] ⓘ, Swiss Standard German: [ˈleɔnhard ˈɔʏlər]; 15 April 1707 – 18 September 1783) was a Swiss polymath who was active as a mathematician, physicist, astronomer, logician, geographer, and engineer. He founded the studies of graph theory and topology and made influential discoveries in many other branches of mathematics such as analytic number theory, complex analysis, and infinitesimal calculus. He also introduced much of modern mathematical terminology and notation, including the notion of a mathematical function.[6] He is also known for his work in mechanics, fluid dynamics, optics, astronomy, and music theory.[7] As a result, Euler has been described as a \"universal genius\" who \"was fully equipped with almost unlimited powers of imagination, intellectual gifts and extraordinary memory\".[8]\n\nEuler is regarded as arguably the most prolific contributor in the history of mathematics and science, and the greatest mathematician of the 18th century.[9][10] Several great mathematicians who produced their work after Euler's death have recognised his importance in the field as shown by quotes attributed to many of them: Pierre-Simon Laplace expressed Euler's influence on mathematics by stating, \"Read Euler, read Euler, he is the master of us all.\"[11][c] Carl Friedrich Gauss wrote: \"The study of Euler's works will remain the best school for the different fields of mathematics, and nothing else can replace it.\"[12][d] His 866 publications and his correspondence are being collected in the Opera Omnia Leonhard Euler which, when completed, will consist of 81 quartos.[14][15][16] He spent most of his adult life in Saint Petersburg, Russia, and in Berlin, then the capital of Prussia.\n\nEuler is credited for popularizing the Greek letter \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n\n (lowercase pi) to denote the ratio of a circle's circumference to its diameter, as well as first using the notation \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n for the value of a function, the letter \n\n\n\ni\n\n\n{\\displaystyle i}\n\n to express the imaginary unit \n\n\n\n\n\n−\n1\n\n\n\n\n{\\displaystyle {\\sqrt {-1}}}\n\n, the Greek letter \n\n\n\nΣ\n\n\n{\\displaystyle \\Sigma }\n\n (capital sigma) to express summations, the Greek letter \n\n\n\nΔ\n\n\n{\\displaystyle \\Delta }\n\n (capital delta) for finite differences, and lowercase letters to represent the sides of a triangle while representing the angles as capital letters.[17]  He gave the current definition of the constant \n\n\n\ne\n\n\n{\\displaystyle e}\n\n, the base of the natural logarithm, now known as Euler's number.[18] Euler made contributions to applied mathematics and engineering, such as his study of ships which helped navigation, his three volumes on optics contributed to the design of microscopes and telescopes, and he studied the bending of beams and the critical load of columns.[10]\n\nEuler is credited with being the first to develop graph theory (partly as a solution for the problem of the Seven Bridges of Königsberg, which is also considered the first practical application of topology). He also became famous for, among many other accomplishments, providing a solution to several unsolved problems in number theory and analysis, including the famous Basel problem. Euler has also been credited for discovering that the sum of the numbers of vertices and faces minus the number of edges of a polyhedron equals 2, a number now commonly known as the Euler characteristic. In the field of physics, Euler reformulated Isaac Newton's laws of motion into new laws in his two-volume work Mechanica to better explain the motion of rigid bodies. Euler made contributions to the study of elastic deformations of solid objects. Euler formulated the partial differential equations for the motion of inviscid fluid,[10] and further laid the mathematical foundations of potential theory.[8]\n\nLeonhard Euler was born on 15 April 1707, in Basel to Paul III Euler, a pastor of the Reformed Church, and Marguerite (née Brucker), whose ancestors include a number of well-known scholars in the classics.[19] He was the oldest of four children, having two younger sisters, Anna Maria and Maria Magdalena, and a younger brother, Johann Heinrich.[20][19] Soon after the birth of Leonhard, the Euler family moved from Basel to the town of Riehen, Switzerland, where his father became pastor in the local church and Leonhard spent most of his childhood.[19]\n\nFrom a young age, Euler received schooling in mathematics from his father, who had taken courses from Jacob Bernoulli some years earlier at the University of Basel. Around the age of eight, Euler was sent to live at his maternal grandmother's house and enrolled in the Latin school in Basel. In addition, he received private tutoring from Johannes Burckhardt, a young theologian with a keen interest in mathematics.[19]\n\nIn 1720, at thirteen years of age, Euler enrolled at the University of Basel.[7] Attending university at such a young age was not unusual at the time.[19] The course on elementary mathematics was given by Johann Bernoulli, the younger brother of the deceased Jacob Bernoulli (who had taught Euler's father). Johann Bernoulli and Euler soon got to know each other better. Euler described Bernoulli in his autobiography:[21]\n\nIt was during this time that Euler, backed by Bernoulli, obtained his father's consent to become a mathematician instead of a pastor.[22][23]\n\nIn 1723, Euler received a Master of Philosophy with a dissertation that compared the philosophies of René Descartes and Isaac Newton.[19] Afterwards, he enrolled in the theological faculty of the University of Basel.[23]\n\nIn 1726, Euler completed a dissertation on the propagation of sound with the title De Sono[24][25] with which he unsuccessfully attempted to obtain a position at the University of Basel.[26] In 1727, he entered the Paris Academy prize competition (offered annually and later biennially by the academy beginning in 1720)[27] for the first time. The problem posed that year was to find the best way to place the masts on a ship. Pierre Bouguer, who became known as \"the father of naval architecture\", won and Euler took second place.[28] Over the years, Euler entered this competition 15 times,[27] winning 12 of them.[28]\n\nJohann Bernoulli's two sons, Daniel and Nicolaus, entered into service at the Imperial Russian Academy of Sciences in Saint Petersburg in 1725, leaving Euler with the assurance they would recommend him to a post when one was available.[26] On 31 July 1726, Nicolaus died of appendicitis after spending less than a year in Russia.[29][30] When Daniel assumed his brother's position in the mathematics/physics division, he recommended that the post in physiology that he had vacated be filled by his friend Euler.[26] In November 1726, Euler eagerly accepted the offer, but delayed making the trip to Saint Petersburg while he unsuccessfully applied for a physics professorship at the University of Basel.[26]\n\nEuler arrived in Saint Petersburg in May 1727.[26][23] He was promoted from his junior post in the medical department of the academy to a position in the mathematics department. He lodged with Daniel Bernoulli with whom he worked in close collaboration.[31] Euler mastered Russian, settled into life in Saint Petersburg and took on an additional job as a medic in the Russian Navy.[32]\n\nThe academy at Saint Petersburg, established by Peter the Great, was intended to improve education in Russia and to close the scientific gap with Western Europe. As a result, it was made especially attractive to foreign scholars like Euler.[28] The academy's benefactress, Catherine I, who had continued the progressive policies of her late husband, died before Euler's arrival to Saint Petersburg.[33] The Russian conservative nobility then gained power upon the ascension of the twelve-year-old Peter II.[33] The nobility, suspicious of the academy's foreign scientists, cut funding for Euler and his colleagues and prevented the entrance of foreign and non-aristocratic students into the Gymnasium and universities.[33]\n\nConditions improved slightly after the death of Peter II in 1730 and the German-influenced Anna of Russia assumed power.[34] Euler swiftly rose through the ranks in the academy and was made a professor of physics in 1731.[34] He also left the Russian Navy, refusing a promotion to lieutenant.[34] Two years later, Daniel Bernoulli, fed up with the censorship and hostility he faced at Saint Petersburg, left for Basel. Euler succeeded him as the head of the mathematics department.[35] In January 1734, he married Katharina Gsell (1707–1773), a daughter of Georg Gsell.[36] Frederick II had made an attempt to recruit the services of Euler for his newly established Berlin Academy in 1740, but Euler initially preferred to stay in St Petersburg.[37] But after Empress Anna died and Frederick II agreed to pay 1600 ecus (the same as Euler earned in Russia) he agreed to move to Berlin. In 1741, he requested permission to leave to Berlin, arguing he was in need of a milder climate for his eyesight.[37] The Russian academy gave its consent and would pay him 200 rubles per year as one of its active members.[37]\n\nConcerned about the continuing turmoil in Russia, Euler left St. Petersburg in June 1741 to take up a post at the Berlin Academy, which he had been offered by Frederick the Great of Prussia.[38] He lived for 25 years in Berlin, where he wrote several hundred articles.[23] In 1748 his text on functions called the Introductio in analysin infinitorum was published and in 1755 a text on differential calculus called the Institutiones calculi differentialis was published.[39][40] In 1755, he was elected a foreign member of the Royal Swedish Academy of Sciences[41] and of the French Academy of Sciences.[42] Notable students of Euler in Berlin included Stepan Rumovsky, later considered as the first Russian astronomer.[43][44] In 1748 he declined an offer from the University of Basel to succeed the recently deceased Johann Bernoulli.[23] In 1753 he bought a house in Charlottenburg, in which he lived with his family and widowed mother.[45][46]\n\nEuler became the tutor for Friederike Charlotte of Brandenburg-Schwedt, the Princess of Anhalt-Dessau and Frederick's niece. He wrote over 200 letters to her in the early 1760s, which were later compiled into a volume entitled Letters of Euler on different Subjects in Natural Philosophy Addressed to a German Princess.[47] This work contained Euler's exposition on various subjects pertaining to physics and mathematics and offered valuable insights into Euler's personality and religious beliefs. It was translated into multiple languages, published across Europe and in the United States, and became more widely read than any of his mathematical works. The popularity of the Letters testifies to Euler's ability to communicate scientific matters effectively to a lay audience, a rare ability for a dedicated research scientist.[40]\n\nDespite Euler's immense contribution to the academy's prestige and having been put forward as a candidate for its presidency by Jean le Rond d'Alembert, Frederick II named himself as its president.[46] The Prussian king had a large circle of intellectuals in his court, and he found the mathematician unsophisticated and ill-informed on matters beyond numbers and figures. Euler was a simple, devoutly religious man who never questioned the existing social order or conventional beliefs. He was, in many ways, the polar opposite of Voltaire, who enjoyed a high place of prestige at Frederick's court. Euler was not a skilled debater and often made it a point to argue subjects that he knew little about, making him the frequent target of Voltaire's wit.[40] Frederick also expressed disappointment with Euler's practical engineering abilities, stating:\n\nI wanted to have a water jet in my garden: Euler calculated the force of the wheels necessary to raise the water to a reservoir, from where it should fall back through channels, finally spurting out in Sanssouci. My mill was carried out geometrically and could not raise a mouthful of water closer than fifty paces to the reservoir. Vanity of vanities! Vanity of geometry![48]\nHowever, the disappointment was almost surely unwarranted from a technical perspective. Euler's calculations look likely to be correct, even if Euler's interactions with Frederick and those constructing his fountain may have been dysfunctional.[49]\n\nThroughout his stay in Berlin, Euler maintained a strong connection to the academy in St. Petersburg and also published 109 papers in Russia.[50] He also assisted students from the St. Petersburg academy and at times accommodated Russian students in his house in Berlin.[50] In 1760, with the Seven Years' War raging, Euler's farm in Charlottenburg was sacked by advancing Russian troops.[45] Upon learning of this event, General Ivan Petrovich Saltykov paid compensation for the damage caused to Euler's estate, with Empress Elizabeth of Russia later adding a further payment of 4000 rubles—an exorbitant amount at the time.[51] Euler decided to leave Berlin in 1766 and return to Russia.[52]\n\nDuring his Berlin years (1741–1766), Euler was at the peak of his productivity. He wrote 380 works, 275 of which were published.[53] This included 125 memoirs in the Berlin Academy and over 100 memoirs sent to the St. Petersburg Academy, which had retained him as a member and paid him an annual stipend. Euler's Introductio in Analysin Infinitorum was published in two parts in 1748. In addition to his own research, Euler supervised the library, the observatory, the botanical garden, and the publication of calendars and maps from which the academy derived income.[54] He was even involved in the design of the water fountains at Sanssouci, the King's summer palace.[55]\n\nThe political situation in Russia stabilized after Catherine the Great's accession to the throne, so in 1766 Euler accepted an invitation to return to the St. Petersburg Academy. His conditions were quite exorbitant—a 3000 ruble annual salary, a pension for his wife, and the promise of high-ranking appointments for his sons. At the university he was assisted by his student Anders Johan Lexell.[56] While living in St. Petersburg, a fire in 1771 destroyed his home.[57]\n\nOn 7 January 1734, he married Katharina Gsell (1707–1773), daughter of Georg Gsell, a painter from the Academy Gymnasium in Saint Petersburg.[36] The young couple bought a house by the Neva River.\n\nOf their thirteen children, only five survived childhood,[58] three sons and two daughters.[59] Their first son was Johann Albrecht Euler, whose godfather was Christian Goldbach.[59]\n\nThree years after his wife's death in 1773,[57] Euler married her half-sister, Salome Abigail Gsell (1723–1794).[60] This marriage lasted until his death in 1783.\n\nHis brother Johann Heinrich settled in St. Petersburg in 1735 and was employed as a painter at the academy.[37]\n\nEarly in his life, Euler memorized the entirety of the Aeneid by Virgil, and by old age, could recite the entirety of the poem, along with stating the first and last sentence on each page of the edition from which he had learnt it.[61][62] Euler knew the first hundred prime numbers, and could further state each one of their squares, cubes and powers up to the sixth degree.[63]\n\nEuler was known as a generous and kind person, not neurotic as seen in some geniuses, keeping his good-natured disposition even after becoming entirely blind.[63]\n\nEuler's eyesight worsened throughout his mathematical career. In 1738, three years after nearly expiring from fever,[64] he became almost blind in his right eye. Euler blamed the cartography he performed for the St. Petersburg Academy for his condition,[65] but the cause of his blindness remains the subject of speculation.[66][67] Euler's vision in that eye worsened throughout his stay in Germany, to the extent that Frederick referred to him as \"Cyclops\". Euler remarked on his loss of vision, stating \"Now I will have fewer distractions.\"[65] In 1766 a cataract in his left eye was discovered. Though couching of the cataract temporarily improved his vision, complications ultimately rendered him almost totally blind in  the left eye as well.[42] However, his condition appeared to have little effect on his productivity. With the aid of his scribes, Euler's productivity in many areas of study increased;[68] and, in 1775, he produced, on average, one mathematical paper every week.[42]\n\nIn St. Petersburg on 18 September 1783, after a lunch with his family, Euler was discussing the newly discovered planet Uranus and its orbit with Anders Johan Lexell when he collapsed and died from a brain hemorrhage.[66] Jacob von Staehlin [de] wrote a short obituary for the Russian Academy of Sciences and Russian mathematician Nicolas Fuss, one of Euler's disciples, wrote a more detailed eulogy,[58] which he delivered at a memorial meeting. In his eulogy for the French Academy, French mathematician and philosopher Marquis de Condorcet, wrote:\n\nil cessa de calculer et de vivre— ... he ceased to calculate and to live.[69]\nEuler was buried next to Katharina at the Smolensk Lutheran Cemetery on Vasilievsky Island. In 1837, the Russian Academy of Sciences installed a new monument, replacing his overgrown grave plaque. To commemorate the 250th anniversary of Euler's birth in 1957, his tomb was moved to the Lazarevskoe Cemetery at the Alexander Nevsky Monastery.[70]\n\nEuler worked in almost all areas of mathematics, including geometry, infinitesimal calculus, trigonometry, algebra, and number theory, as well as continuum physics, lunar theory, and other areas of physics. He is a seminal figure in the history of mathematics; if printed, his works, many of which are of fundamental interest, would occupy between 60 and 80 quarto volumes.[42] Euler's name is associated with a large number of topics. Euler's work averages 800 pages a year from 1725 to 1783. He also wrote over 4500 letters and hundreds of manuscripts. It has been estimated that Leonhard Euler was the author of a quarter of the combined output in mathematics, physics, mechanics, astronomy, and navigation in the 18th century, while other researchers credit Euler for a third of the output in mathematics in that century.[17]\n\nEuler introduced and popularized several notational conventions through his numerous and widely circulated textbooks. Most notably, he introduced the concept of a function[6] and was the first to write f(x) to denote the function f applied to the argument x. He also introduced the modern notation for the trigonometric functions, the letter e for the base of the natural logarithm (now also known as Euler's number), the Greek letter Σ for summations and the letter i to denote the imaginary unit.[71] The use of the Greek letter π to denote the ratio of a circle's circumference to its diameter was also popularized by Euler, although it originated with Welsh mathematician William Jones.[72]\n\nThe development of infinitesimal calculus was at the forefront of 18th-century mathematical research, and the Bernoullis—family friends of Euler—were responsible for much of the early progress in the field. Thanks to their influence, studying calculus became the major focus of Euler's work. While some of Euler's proofs are not acceptable by modern standards of mathematical rigour[73] (in particular his reliance on the principle of the generality of algebra), his ideas led to many great advances.\nEuler is well known in analysis for his frequent use and development of power series, the expression of functions as sums of infinitely many terms,[74] such as\n\n\n\n\n\ne\n\nx\n\n\n=\n\n∑\n\nn\n=\n0\n\n\n∞\n\n\n\n\n\nx\n\nn\n\n\n\nn\n!\n\n\n\n=\n\nlim\n\nn\n→\n∞\n\n\n\n(\n\n\n\n1\n\n0\n!\n\n\n\n+\n\n\nx\n\n1\n!\n\n\n\n+\n\n\n\nx\n\n2\n\n\n\n2\n!\n\n\n\n+\n⋯\n+\n\n\n\nx\n\nn\n\n\n\nn\n!\n\n\n\n\n)\n\n.\n\n\n{\\displaystyle e^{x}=\\sum _{n=0}^{\\infty }{x^{n} \\over n!}=\\lim _{n\\to \\infty }\\left({\\frac {1}{0!}}+{\\frac {x}{1!}}+{\\frac {x^{2}}{2!}}+\\cdots +{\\frac {x^{n}}{n!}}\\right).}\n\n\n\nEuler's use of power series enabled him to solve the Basel problem, finding the sum of the reciprocals of squares of every natural number, in 1735 (he provided a more elaborate argument in 1741). The Basel problem was originally posed by Pietro Mengoli in 1644, and by the 1730s was a famous open problem, popularized by Jacob Bernoulli and unsuccessfully attacked by many of the leading mathematicians of the time. Euler found that:[75][76][73]\n\n\n\n\n\n\n∑\n\nn\n=\n1\n\n\n∞\n\n\n\n\n1\n\nn\n\n2\n\n\n\n\n=\n\nlim\n\nn\n→\n∞\n\n\n\n(\n\n\n\n1\n\n1\n\n2\n\n\n\n\n+\n\n\n1\n\n2\n\n2\n\n\n\n\n+\n\n\n1\n\n3\n\n2\n\n\n\n\n+\n⋯\n+\n\n\n1\n\nn\n\n2\n\n\n\n\n\n)\n\n=\n\n\n\nπ\n\n2\n\n\n6\n\n\n.\n\n\n{\\displaystyle \\sum _{n=1}^{\\infty }{1 \\over n^{2}}=\\lim _{n\\to \\infty }\\left({\\frac {1}{1^{2}}}+{\\frac {1}{2^{2}}}+{\\frac {1}{3^{2}}}+\\cdots +{\\frac {1}{n^{2}}}\\right)={\\frac {\\pi ^{2}}{6}}.}\n\n\n\nEuler introduced the constant\n\n\n\n\nγ\n=\n\nlim\n\nn\n→\n∞\n\n\n\n(\n\n1\n+\n\n\n1\n2\n\n\n+\n\n\n1\n3\n\n\n+\n\n\n1\n4\n\n\n+\n⋯\n+\n\n\n1\nn\n\n\n−\nln\n⁡\n(\nn\n)\n\n)\n\n≈\n0.5772\n,\n\n\n{\\displaystyle \\gamma =\\lim _{n\\rightarrow \\infty }\\left(1+{\\frac {1}{2}}+{\\frac {1}{3}}+{\\frac {1}{4}}+\\cdots +{\\frac {1}{n}}-\\ln(n)\\right)\\approx 0.5772,}\n\n\nnow known as Euler's constant or the Euler–Mascheroni constant, and studied its relationship with the harmonic series, the gamma function, and values of the Riemann zeta function.[77]\n\nEuler introduced the use of the exponential function and logarithms in analytic proofs. He discovered ways to express various logarithmic functions using power series, and he successfully defined logarithms for negative and complex numbers, thus greatly expanding the scope of mathematical applications of logarithms.[71] He also defined the exponential function for complex numbers and discovered its relation to the trigonometric functions. For any real number φ (taken to be radians), Euler's formula states that the complex exponential function satisfies\n\n\n\n\n\ne\n\ni\nφ\n\n\n=\ncos\n⁡\nφ\n+\ni\nsin\n⁡\nφ\n\n\n{\\displaystyle e^{i\\varphi }=\\cos \\varphi +i\\sin \\varphi }\n\n\n\nwhich was called \"the most remarkable formula in mathematics\" by Richard Feynman.[78]\n\nA special case of the above formula is known as Euler's identity,\n\n\n\n\n\ne\n\ni\nπ\n\n\n+\n1\n=\n0\n\n\n{\\displaystyle e^{i\\pi }+1=0}\n\n\n\nEuler elaborated the theory of higher transcendental functions by introducing the gamma function[79][80] and introduced a new method for solving quartic equations.[81] He found a way to calculate integrals with complex limits, foreshadowing the development of modern complex analysis. He invented the calculus of variations and formulated the Euler–Lagrange equation for reducing optimization problems in this area to the solution of differential equations.\n\nEuler pioneered the use of analytic methods to solve number theory problems. In doing so, he united two disparate branches of mathematics and introduced a new field of study, analytic number theory. In breaking ground for this new field, Euler created the theory of hypergeometric series, q-series, hyperbolic trigonometric functions, and the analytic theory of continued fractions. For example, he proved the infinitude of primes using the divergence of the harmonic series, and he used analytic methods to gain some understanding of the way prime numbers are distributed. Euler's work in this area led to the development of the prime number theorem.[82]\n\nEuler's interest in number theory can be traced to the influence of Christian Goldbach,[83] his friend in the St. Petersburg Academy.[64] Much of Euler's early work on number theory was based on the work of Pierre de Fermat. Euler developed some of Fermat's ideas and disproved some of his conjectures, such as his conjecture that all numbers of the form \n\n\n\n\n2\n\n\n2\n\nn\n\n\n\n\n+\n1\n\n\n{\\textstyle 2^{2^{n}}+1}\n\n (Fermat numbers) are prime.[84]\n\nEuler linked the nature of prime distribution with ideas in analysis. He proved that the sum of the reciprocals of the primes diverges. In doing so, he discovered the connection between the Riemann zeta function and prime numbers; this is known as the Euler product formula for the Riemann zeta function.[85]\n\nEuler invented the totient function φ(n), the number of positive integers less than or equal to the integer n that are coprime to n. Using properties of this function, he generalized Fermat's little theorem to what is now known as Euler's theorem.[86] He contributed significantly to the theory of perfect numbers, which had fascinated mathematicians since Euclid. He proved that the relationship shown between even perfect numbers and Mersenne primes (which he had earlier proved) was one-to-one, a result otherwise known as the Euclid–Euler theorem.[87] Euler also conjectured the law of quadratic reciprocity. The concept is regarded as a fundamental theorem within number theory, and his ideas paved the way for the work of Carl Friedrich Gauss, particularly Disquisitiones Arithmeticae.[88] By 1772 Euler had proved that 231 − 1 = 2,147,483,647 is a Mersenne prime. It may have remained the largest known prime until 1867.[89]\n\nEuler also contributed major developments to the theory of partitions of an integer.[90]\n\nIn 1735, Euler presented a solution to the problem known as the Seven Bridges of Königsberg.[91] The city of Königsberg, Prussia was set on the Pregel River, and included two large islands that were connected to each other and the mainland by seven bridges. The problem is to decide whether it is possible to follow a path that crosses each bridge exactly once and returns to the starting point. It is not possible: there is no Eulerian circuit. This solution is considered to be the first theorem of graph theory.[91]\n\nEuler also discovered the formula \n\n\n\nV\n−\nE\n+\nF\n=\n2\n\n\n{\\displaystyle V-E+F=2}\n\n relating the number of vertices, edges, and faces of a convex polyhedron,[92] and hence of a planar graph. The constant in this formula is now known as the Euler characteristic for the graph (or other mathematical object), and is related to the genus of the object.[93] The study and generalization of this formula, specifically by Cauchy[94] and L'Huilier,[95] is at the origin of topology.[92]\n\nSome of Euler's greatest successes were in solving real-world problems analytically, and in describing numerous applications of the Bernoulli numbers, Fourier series, Euler numbers, the constants e and π, continued fractions, and integrals. He integrated Leibniz's differential calculus with Newton's Method of Fluxions, and developed tools that made it easier to apply calculus to physical problems. He made great strides in improving the numerical approximation of integrals, inventing what are now known as the Euler approximations. The most notable of these approximations are Euler's method[96] and the Euler–Maclaurin formula.[97][98][99]\n\nEuler helped develop the Euler–Bernoulli beam equation, which became a cornerstone of engineering.[100] Besides successfully applying his analytic tools to problems in classical mechanics, Euler applied these techniques to celestial problems. His work in astronomy was recognized by multiple Paris Academy Prizes over the course of his career. His accomplishments include determining with great accuracy the orbits of comets and other celestial bodies, understanding the nature of comets, and calculating the parallax of the Sun. His calculations contributed to the development of accurate longitude tables.[101]\n\nEuler made important contributions in optics.[102] He disagreed with Newton's corpuscular theory of light,[103] which was the prevailing theory of the time. His 1740s papers on optics helped ensure that the wave theory of light proposed by Christiaan Huygens would become the dominant mode of thought, at least until the development of the quantum theory of light.[104]\n\nIn fluid dynamics, Euler was the first to predict the phenomenon of cavitation, in 1754, long before its first observation in the late 19th century, and the Euler number used in fluid flow calculations comes from his related work on the efficiency of turbines.[105] In 1757 he published an important set of equations for inviscid flow in fluid dynamics, that are now known as the Euler equations.[106]\n\nEuler is well known in structural engineering for his formula giving Euler's critical load, the critical buckling load of an ideal strut, which depends only on its length and flexural stiffness.[107]\n\nEuler is credited with using closed curves to illustrate syllogistic reasoning (1768). These diagrams have become known as Euler diagrams.[108]\n\nAn Euler diagram is a diagrammatic means of representing sets and their relationships. Euler diagrams consist of simple closed curves (usually circles) in the plane that depict sets. Each Euler curve divides the plane into two regions or \"zones\": the interior, which symbolically represents the elements of the set, and the exterior, which represents all elements that are not members of the set. The sizes or shapes of the curves are not important; the significance of the diagram is in how they overlap. The spatial relationships between the regions bounded by each curve (overlap, containment or neither) corresponds to set-theoretic relationships (intersection, subset, and disjointness). Curves whose interior zones do not intersect represent disjoint sets. Two curves whose interior zones intersect represent sets that have common elements; the zone inside both curves represents the set of elements common to both sets (the intersection of the sets). A curve that is contained completely within the interior zone of another represents a subset of it.\n\nEuler diagrams (and their refinement to Venn diagrams) were incorporated as part of instruction in set theory as part of the new math movement in the 1960s.[109] Since then, they have come into wide use as a way of visualizing combinations of characteristics.[110]\n\nOne of Euler's more unusual interests was the application of mathematical ideas in music. In 1739 he wrote the Tentamen novae theoriae musicae (Attempt at a New Theory of Music), hoping to eventually incorporate musical theory as part of mathematics. This part of his work, however, did not receive wide attention and was once described as too mathematical for musicians and too musical for mathematicians.[111] Even when dealing with music, Euler's approach is mainly mathematical,[112] for instance, his introduction of binary logarithms as a way of numerically describing the subdivision of octaves into fractional parts.[113] His writings on music are not particularly numerous (a few hundred pages, in his total production of about thirty thousand pages), but they reflect an early preoccupation and one that remained with him throughout his life.[112]\n\nA first point of Euler's musical theory is the definition of \"genres\", i.e. of possible divisions of the octave using the prime numbers 3 and 5. Euler describes 18 such genres, with the general definition 2mA, where A is the \"exponent\" of the genre (i.e. the sum of the exponents of 3 and 5) and 2m (where \"m is an indefinite number, small or large, so long as the sounds are perceptible\"[114]), expresses that the relation holds independently of the number of octaves concerned. The first genre, with A = 1, is the octave itself (or its duplicates); the second genre, 2m.3, is the octave divided by the fifth (fifth + fourth, C–G–C); the third genre is 2m.5, major third + minor sixth (C–E–C); the fourth is 2m.32, two-fourths and a tone (C–F–B♭–C); the fifth is 2m.3.5 (C–E–G–B–C); etc. Genres 12 (2m.33.5), 13 (2m.32.52) and 14 (2m.3.53) are corrected versions of the diatonic, chromatic and enharmonic, respectively, of the Ancients. Genre 18 (2m.33.52) is the \"diatonico-chromatic\", \"used generally in all compositions\",[115] and which turns out to be identical with the system described by Johann Mattheson.[116] Euler later envisaged the possibility of describing genres including the prime number 7.[117]\n\nEuler devised a specific graph, the Speculum musicum,[118][119] to illustrate the diatonico-chromatic genre, and discussed paths in this graph for specific intervals, recalling his interest in the Seven Bridges of Königsberg (see above). The device drew renewed interest as the Tonnetz in Neo-Riemannian theory (see also Lattice (music)).[120]\n\nEuler further used the principle of the \"exponent\" to propose a derivation of the gradus suavitatis (degree of suavity, of agreeableness) of intervals and chords from their prime factors – one must keep in mind that he considered just intonation, i.e. 1 and the prime numbers 3 and 5 only.[121] Formulas have been proposed extending this system to any number of prime numbers, e.g. in the form\n\n\n\n\nd\ns\n=\n\n∑\n\ni\n\n\n(\n\nk\n\ni\n\n\n\np\n\ni\n\n\n−\n\nk\n\ni\n\n\n)\n+\n1\n,\n\n\n{\\displaystyle ds=\\sum _{i}(k_{i}p_{i}-k_{i})+1,}\n\n\nwhere pi are prime numbers and ki their exponents.[122]\n\nEuler was a religious person throughout his life.[23] Much of what is known of Euler's religious beliefs can be deduced from his Letters to a German Princess and an earlier work, Rettung der Göttlichen Offenbahrung gegen die Einwürfe der Freygeister (Defense of the Divine Revelation against the Objections of the Freethinkers). These works show that Euler was a devout Christian who believed the Bible to be inspired; the Rettung was primarily an argument for the divine inspiration of scripture.[123][124]\n\nEuler opposed the concepts of Leibniz's monadism and the philosophy of Christian Wolff.[125] Euler insisted that knowledge is founded in part on the basis of precise quantitative laws, something that monadism and Wolffian science were unable to provide. Euler also labelled Wolff's ideas as \"heathen and atheistic\".[126]\n\nThere is a famous legend[127] inspired by Euler's arguments with secular philosophers over religion, which is set during Euler's second stint at the St. Petersburg Academy. The French philosopher Denis Diderot was visiting Russia on Catherine the Great's invitation. However, the Empress was alarmed that the philosopher's arguments for atheism were influencing members of her court, and so Euler was asked to confront the Frenchman. Diderot was informed that a learned mathematician had produced a proof of the existence of God: he agreed to view the proof as it was presented in court. Euler appeared, advanced toward Diderot, and in a tone of perfect conviction announced this non-sequitur:\n\n\"Sir, \n\n\n\n\n\n\na\n+\n\nb\n\nn\n\n\n\nn\n\n\n=\nx\n\n\n{\\displaystyle {\\frac {a+b^{n}}{n}}=x}\n\n, hence God exists –reply!\"\n\nDiderot, to whom (says the story) all mathematics was gibberish, stood dumbstruck as peals of laughter erupted from the court. Embarrassed, he asked to leave Russia, a request that was graciously granted by the Empress. However amusing the anecdote may be, it is apocryphal, given that Diderot himself did research in mathematics.[128]\nThe legend was apparently first told by Dieudonné Thiébault with embellishment by Augustus De Morgan.[127]\n\nEuler is widely recognized as one of the greatest mathematicians of all time, and more likely than not the most prolific contributor to mathematics and science.[10] Mathematician and physicist John von Neumann called Euler \"the greatest virtuoso of the period\".[129] Mathematician François Arago noted that \"Euler calculated without any apparent effort, just as men breathe and as eagles sustain themselves in air\".[130] He is generally ranked right below Carl Friedrich Gauss, Isaac Newton and Archimedes as the greatest mathematicians of all time,[130] while some rank him as equal with them.[131] Physicist and mathematician Henri Poincaré described Euler as the \"god of mathematics\".[132]\n\n\nFrench mathematician André Weil noted how he stood above his contemporaries and more than anyone else was able to cement himself as the leading force of his era's mathematics:[129]\nNo mathematician ever attained such a position of undisputed leader- ship in all branches of mathematics, pure and applied, as Euler did for the best part of the eighteenth century.\nSwiss mathematician Nicolas Fuss noted Euler's extraordinary memory and breadth of knowledge, stating that:[8]\n\nKnowledge that we call erudition was not inimical to him. He had read all the best Roman writers, knew perfectly the ancient history of mathematics, held in his memory the historical events of all times and peoples, and could without hesitation adduce by way of examples the most trifling of historical events. He knew more about medicine, botany, and chemistry than might be expected of someone who had not worked especially in those sciences.\nEuler was featured on both the sixth[133] and seventh[134] series of the Swiss 10-franc banknote and on numerous Swiss, German, and Russian postage stamps. In 1782 he was elected a Foreign Honorary Member of the American Academy of Arts and Sciences.[135] The asteroid 2002 Euler was named in his honour.[136]\n\nEuler has an extensive bibliography. His books include:\n\nIt took until 1830 for the bulk of Euler's posthumous works to be individually published,[143] with an additional batch of 61 unpublished works discovered by Paul Heinrich von Fuss (Euler's great-grandson and Nicolas Fuss's son) and published as a collection in 1862.[143][144] A chronological catalog of Euler's works was compiled by Swedish mathematician Gustaf Eneström and published from 1910 to 1913.[145] The catalog, known as the Eneström index, numbers Euler's works from E1 to E866.[146] The Euler Archive was started at Dartmouth College[147] before moving to the Mathematical Association of America[148] and, most recently, to University of the Pacific in 2017.[149]\n\nIn 1907, the Swiss Academy of Sciences created the Euler Commission and charged it with the publication of Euler's complete works. After several delays in the 19th century,[143] the first volume of the Opera Omnia, was published in 1911.[150] However, the discovery of new manuscripts continued to increase the magnitude of this project. Fortunately, the publication of Euler's Opera Omnia has made steady progress, with over 70 volumes (averaging 426 pages each) published by 2006 and 80 volumes published by 2022.[151][15][17] These volumes are organized into four series. The first series compiles the works on analysis, algebra, and number theory; it consists of 29 volumes and numbers over 14,000 pages. The 31 volumes of Series II, amounting to 10,660 pages, contain the works on mechanics, astronomy, and engineering. Series III contains 12 volumes on physics. Series IV, which contains the massive amount of Euler's correspondence, unpublished manuscripts, and notes only began compilation in 1967. After publishing 8 print volumes in Series IV, the project decided in 2022 to publish its remaining projected volumes in Series IV in online format only.[15][150][17]\n"
    },
    {
        "title": "Adrien-Marie Legendre",
        "content": "\n\nAdrien-Marie Legendre (/ləˈʒɑːndər, -ˈʒɑːnd/;[3] French: [adʁiɛ̃ maʁi ləʒɑ̃dʁ]; 18 September 1752 – 9 January 1833) was a French mathematician who made numerous contributions to mathematics. Well-known and important concepts such as the Legendre polynomials and Legendre transformation are named after him. He is also known for his contributions to the method of least squares, and was the first to officially publish on it, though Carl Friedrich Gauss had discovered it before him.[4][5]\n\nAdrien-Marie Legendre was born in Paris on 18 September 1752 to a wealthy family. He received his education at the Collège Mazarin in Paris, and defended his thesis in physics and mathematics in 1770. He taught at the École Militaire in Paris from 1775 to 1780 and at the École Normale from 1795. At the same time, he was associated with the Bureau des Longitudes. In 1782, the Berlin Academy awarded Legendre a prize for his treatise on projectiles in resistant media. This treatise also brought him to the attention of Lagrange.[6]\n\nThe Académie des sciences made Legendre an adjoint member in 1783 and an associate in 1785. In 1789, he was elected a Fellow of the Royal Society.[7]\n\nHe assisted with the Anglo-French Survey (1784–1790) to calculate the precise distance between the Paris Observatory and the Royal Greenwich Observatory by means of trigonometry. To this end in 1787 he visited Dover and London together with Dominique, comte de Cassini and Pierre Méchain. The three also visited William Herschel, the discoverer of the planet Uranus.\n\nLegendre lost his private fortune in 1793 during the French Revolution. That year, he also married Marguerite-Claudine Couhin, who helped him put his affairs in order. In 1795, Legendre became one of six members of the mathematics section of the reconstituted Académie des Sciences, renamed the Institut National des Sciences et des Arts. Later, in 1803, Napoleon reorganized the Institut National, and Legendre became a member of the Geometry section. From 1799 to 1812, Legendre served as mathematics examiner for graduating artillery students at the École Militaire and from 1799 to 1815 he served as permanent mathematics examiner for the École Polytechnique.[8] In 1824, Legendre's pension from the École Militaire was stopped because he refused to vote for the government candidate at the Institut National. In 1831, he was made an officer of the Légion d'Honneur.[6]\n\nLegendre died in Paris on 9 January 1833, after a long and painful illness, and Legendre's widow carefully preserved his belongings to memorialize him. Upon her death in 1856, she was buried next to her husband in the village of Auteuil, where the couple had lived, and left their last country house to the village. Legendre's name is one of the 72 names inscribed on the Eiffel Tower.\n\nAbel's work on elliptic functions was built on Legendre's, and some of Gauss's work in statistics and number theory completed that of Legendre. He developed, and first communicated to his contemporaries before Gauss, the least squares method [9] which has broad application in linear regression, signal processing, statistics, and curve fitting; this was published in 1806 as an appendix to his book on the paths of comets. Today, the term \"least squares method\" is used as a direct translation from the French \"méthode des moindres carrés\".\n\nHis major work is Exercices de Calcul Intégral, published in three volumes in 1811, 1817 and 1819. In the first volume he introduced the basic properties of elliptic integrals, beta functions and gamma functions, introducing the symbol Γ and normalizing it to Γ(n+1) = n!. Further results on the beta and gamma functions along with their applications to mechanics – such as the rotation of the earth, and the attraction of ellipsoids – appeared in the second volume.[10] In 1830, he gave a proof of Fermat's Last Theorem for exponent n = 5, which was also proven by Lejeune Dirichlet in 1828.[10]\n\nIn number theory, he conjectured the quadratic reciprocity law, subsequently proved by Gauss; in connection to this, the Legendre symbol is named after him. He also did pioneering work on the distribution of primes, and on the application of analysis to number theory. His 1798 conjecture of the prime number theorem was rigorously proved by Hadamard and de la Vallée-Poussin in 1896.\n\nLegendre did an impressive amount of work on elliptic functions, including the classification of elliptic integrals, but it took Abel's study of the inverses of Jacobi's functions to solve the problem completely.\n\nHe is known for the Legendre transformation, which is used to go from the Lagrangian to the Hamiltonian formulation of classical mechanics. In thermodynamics it is also used to obtain the enthalpy and the Helmholtz and Gibbs (free) energies from the internal energy. He is also the namesake of the Legendre polynomials, solutions to Legendre's differential equation, which occur frequently in physics and engineering applications, such as electrostatics.\n\nLegendre is best known as the author of Éléments de géométrie, which was published in 1794 and was the leading elementary text on the topic for around 100 years. This text greatly rearranged and simplified many of the propositions from Euclid's Elements to create a more effective textbook.\n\nFor two centuries, until the recent discovery of the error in 2005, books, paintings and articles have incorrectly shown a profile portrait of the obscure French politician Louis Legendre (1752–1797) as a portrait of the mathematician. The error arose from the fact that the sketch was labelled simply \"Legendre\" and appeared in a book along with contemporary mathematicians such as Lagrange. The only known portrait of Legendre, rediscovered in 2008, is found in the 1820 book Album de 73 portraits-charge aquarellés des membres de I'Institut, a book of caricatures of seventy-three members of the Institut de France in Paris by the French artist Julien-Léopold Boilly as shown below:[12][2]\n"
    },
    {
        "title": "Carl Friedrich Gauss",
        "content": "This is an accepted version of this page\n\n\nJohann Carl Friedrich Gauss (German: Gauß [kaʁl ˈfʁiːdʁɪç ˈɡaʊs] ⓘ;[2][3] Latin: Carolus Fridericus Gauss; 30 April 1777 – 23 February 1855) was a German mathematician, astronomer, geodesist, and physicist who contributed to many fields in mathematics and science. He was director of the Göttingen Observatory and professor of astronomy from 1807 until his death in 1855. He is widely considered one of the greatest mathematicians of all time.\n\nWhile studying at the University of Göttingen, he propounded several mathematical theorems. Gauss completed his masterpieces Disquisitiones Arithmeticae and Theoria motus corporum coelestium as a private scholar. He gave the second and third complete proofs of the fundamental theorem of algebra, made contributions to number theory, and developed the theories of binary and ternary quadratic forms.\n\nGauss was instrumental in the identification of Ceres as a dwarf planet. His work on the motion of planetoids disturbed by large planets led to the introduction of the Gaussian gravitational constant and the method of least squares, which he had discovered before Adrien-Marie Legendre published it. Gauss was in charge of the extensive geodetic survey of the Kingdom of Hanover together with an arc measurement project from 1820 to 1844; he was one of the founders of geophysics and formulated the fundamental principles of magnetism. Fruits of his practical work were the inventions of the heliotrope in 1821, a magnetometer in 1833 and – alongside Wilhelm Eduard Weber – the first electromagnetic telegraph in 1833.\n\nGauss was the first to discover and study non-Euclidean geometry, coining the term as well. He further developed a fast Fourier transform some 160 years before John Tukey and James Cooley.\n\nGauss refused to publish incomplete work and left several works to be edited posthumously. He believed that the act of learning, not possession of knowledge, provided the greatest enjoyment. Gauss confessed to disliking teaching, but some of his students became influential mathematicians, such as Richard Dedekind and Bernhard Riemann.\n\nGauss was born on 30 April 1777 in Brunswick in the Duchy of Brunswick-Wolfenbüttel (now in the German state of Lower Saxony). His family was of relatively low social status.[4] His father Gebhard Dietrich Gauss (1744–1808) worked variously as a butcher, bricklayer, gardener, and treasurer of a death-benefit fund. Gauss characterized his father as honourable and respected, but rough and dominating at home. He was experienced in writing and calculating, whereas his second wife Dorothea, Carl Friedrich's mother, was nearly illiterate.[5] He had one elder brother from his father's first marriage.[6]\n\nGauss was a child prodigy in mathematics. When the elementary teachers noticed his intellectual abilities, they brought him to the attention of the Duke of Brunswick who sent him to the local Collegium Carolinum,[a] which he attended from 1792 to 1795 with Eberhard August Wilhelm von Zimmermann as one of his teachers.[8][9][10] Thereafter the Duke granted him the resources for studies of mathematics, sciences, and classical languages at the University of Göttingen until 1798.[11] His professor in mathematics was Abraham Gotthelf Kästner, whom Gauss called \"the leading mathematician among poets, and the leading poet among mathematicians\" because of his epigrams.[12][b] Astronomy was taught by Karl Felix Seyffer, with whom Gauss stayed in correspondence after graduation;[13] Olbers and Gauss mocked him in their correspondence.[14] On the other hand, he thought highly of Georg Christoph Lichtenberg, his teacher of physics, and of Christian Gottlob Heyne, whose lectures in classics Gauss attended with pleasure.[13] Fellow students of this time were Johann Friedrich Benzenberg, Farkas Bolyai, and Heinrich Wilhelm Brandes.[13]\n\nHe was likely a self-taught student in mathematics since he independently rediscovered several theorems.[10] He solved a geometrical problem that had occupied mathematicians since the Ancient Greeks when he determined in 1796 which regular polygons can be constructed by compass and straightedge. This discovery ultimately led Gauss to choose mathematics instead of philology as a career.[15] Gauss's mathematical diary, a collection of short remarks about his results from the years 1796 until 1814, shows that many ideas for his mathematical magnum opus Disquisitiones Arithmeticae (1801) date from this time.[16]\n\nGauss graduated as a Doctor of Philosophy in 1799, not in Göttingen, as is sometimes stated,[c][17] but at the Duke of Brunswick's special request from the University of Helmstedt, the only state university of the duchy. Johann Friedrich Pfaff assessed his doctoral thesis, and Gauss got the degree in absentia without further oral examination.[10] The Duke then granted him the cost of living as a private scholar in Brunswick. Gauss subsequently refused calls from the Russian Academy of Sciences in St. Peterburg and Landshut University.[18][19] Later, the Duke promised him the foundation of an observatory in Brunswick in 1804. Architect Peter Joseph Krahe made preliminary designs, but one of Napoleon's wars cancelled those plans:[20] the Duke was killed in the battle of Jena in 1806. The duchy was abolished in the following year, and Gauss's financial support stopped.\n\nWhen Gauss was calculating asteroid orbits in the first years of the century, he established contact with the astronomical community of Bremen and Lilienthal, especially Wilhelm Olbers, Karl Ludwig Harding, and Friedrich Wilhelm Bessel, as part of the informal group of astronomers known as the Celestial police.[21] One of their aims was the discovery of further planets. They assembled data on asteroids and comets as a basis for Gauss's research on their orbits, which he later published in his astronomical magnum opus Theoria motus corporum coelestium (1809).[22]\n\nIn November 1807, Gauss followed a call to the University of Göttingen, then an institution of the newly founded Kingdom of Westphalia under Jérôme Bonaparte, as full professor and director of the astronomical observatory,[23] and kept the chair until his death in 1855. He was soon confronted with the demand for two thousand francs from the Westphalian government as a war contribution, which he could not afford to pay. Both Olbers and Laplace wanted to help him with the payment, but Gauss refused their assistance. Finally, an anonymous person from Frankfurt, later discovered to be Prince-primate Dalberg,[24] paid the sum.[23]\n\nGauss took on the directorate of the 60-year-old observatory, founded in 1748 by Prince-elector George II and built on a converted fortification tower,[25] with usable, but partly out-of-date instruments.[26] The construction of a new observatory had been approved by Prince-elector George III in principle since 1802, and the Westphalian government continued the planning,[27] but Gauss could not move to his new place of work until September 1816.[19] He got new up-to-date instruments, including two meridian circles from Repsold[28] and Reichenbach,[29] and a heliometer from Fraunhofer.[30]\n\nThe scientific activity of Gauss, besides pure mathematics, can be roughly divided into three periods: astronomy was the main focus in the first two decades of the 19th century, geodesy in the third decade, and physics, mainly magnetism, in the fourth decade.[31]\n\nGauss made no secret of his aversion to giving academic lectures.[18][19] But from the start of his academic career at Göttingen, he continuously gave lectures until 1854.[32] He often complained about the burdens of teaching, feeling that it was a waste of his time. On the other hand, he occasionally described some students as talented.[18] Most of his lectures dealt with astronomy, geodesy, and applied mathematics,[33] and only three lectures on subjects of pure mathematics.[18][d] Some of Gauss's students went on to become renowned mathematicians, physicists, and astronomers: Moritz Cantor, Dedekind, Dirksen, Encke, Gould,[e] Heine, Klinkerfues, Kupffer, Listing, Möbius, Nicolai, Riemann, Ritter, Schering, Scherk, Schumacher, von Staudt, Stern, Ursin; as geoscientists Sartorius von Waltershausen, and Wappäus.[18]\n\nGauss did not write any textbook and disliked the popularization of scientific matters. His only attempts at popularization were his works on the date of Easter (1800/1802) and the essay Erdmagnetismus und Magnetometer of 1836.[35] Gauss published his papers and books exclusively in Latin or German.[f][g] He wrote Latin in a classical style but used some customary modifications set by contemporary mathematicians.[38]\n\nIn his inaugural lecture at Göttingen University from 1808, Gauss claimed reliable observations and results attained only by a strong calculus as the sole tasks of astronomy.[33] At university, he was accompanied by a staff of other lecturers in his disciplines, who completed the educational program; these included the mathematician Thibaut with his lectures,[40] the physicist Mayer, known for his textbooks,[41] his successor Weber since 1831, and in the observatory Harding, who took the main part of lectures in practical astronomy. When the observatory was completed, Gauss took his living accommodation in the western wing of the new observatory and Harding in the eastern one.[19] They had once been on friendly terms, but over time they became alienated, possibly – as some biographers presume – because Gauss had wished the equal-ranked Harding to be no more than his assistant or observer.[19][h] Gauss used the new meridian circles nearly exclusively, and kept them away from Harding, except for some very seldom joint observations.[43]\n\nBrendel subdivides Gauss's astronomic activity chronologically into seven periods, of which the years since 1820 are taken as a \"period of lower astronomical activity\".[44] The new, well-equipped observatory did not work as effectively as other ones; Gauss's astronomical research had the character of a one-man enterprise without a long-time observation program, and the university established a place for an assistant only after Harding died in 1834.[42][43][i]\n\nNevertheless, Gauss twice refused the opportunity to solve the problem by accepting offers from Berlin in 1810 and 1825 to become a full member of the Prussian Academy without burdening lecturing duties, as well as from Leipzig University in 1810 and from Vienna University in 1842, perhaps because of the family's difficult situation.[42] Gauss's salary was raised from 1000 Reichsthaler in 1810 to 2400 Reichsthaler in 1824,[19] and in his later years he was one of the best-paid professors of the university.[45]\n\nWhen Gauss was asked for help by his colleague and friend Friedrich Wilhelm Bessel in 1810, who was in trouble at Königsberg University because of his lack of an academic title, Gauss provided a doctorate honoris causa for Bessel from the Philosophy Faculty of Göttingen in March 1811.[j] Gauss gave another recommendation for an honorary degree for Sophie Germain but only shortly before her death, so she never received it.[48] He also gave successful support to the mathematician Gotthold Eisenstein in Berlin.[49]\n\nGauss was loyal to the House of Hanover. After King William IV died in 1837, the new Hanoverian King Ernest Augustus annulled the 1833 constitution. Seven professors, later known as the \"Göttingen Seven\", protested against this, among them his friend and collaborator Wilhelm Weber and Gauss's son-in-law Heinrich Ewald. All of them were dismissed, and three of them were expelled, but Ewald and Weber could stay in Göttingen. Gauss was deeply affected by this quarrel but saw no possibility to help them.[50]\n\nGauss took part in academic administration: three times he was elected as dean of the Faculty of Philosophy.[51] Being entrusted with the widow's pension fund of the university, he dealt with actuarial science and wrote a report on the strategy for stabilizing the benefits. He was appointed director of the Royal Academy of Sciences in Göttingen for nine years.[51]\n\nGauss remained mentally active into his old age, even while suffering from gout and general unhappiness. On 23 February 1855, he died of a heart attack in Göttingen;[12] and was interred in the Albani Cemetery there. Heinrich Ewald, Gauss's son-in-law, and Wolfgang Sartorius von Waltershausen, Gauss's close friend and biographer, gave eulogies at his funeral.[52]\n\nGauss was a successful investor and accumulated considerable wealth with stocks and securities, finally a value of more than 150 thousand Thaler; after his death, about 18 thousand Thaler were found hidden in his rooms.[53]\n\nThe day after Gauss's death his brain was removed, preserved, and studied by Rudolf Wagner, who found its mass to be slightly above average, at 1,492 grams (3.29 lb).[54][55] Wagner's son Hermann, a geographer, estimated the cerebral area to be 219,588 square millimetres (340.362 sq in) in his doctoral thesis.[56] In 2013, a neurobiologist at the Max Planck Institute for Biophysical Chemistry in Göttingen discovered that Gauss's brain had been mixed up soon after the first investigations, due to mislabelling, with that of the physician Conrad Heinrich Fuchs, who died in Göttingen a few months after Gauss.[57] A further investigation showed no remarkable anomalies in the brains of both persons. Thus, all investigations on Gauss's brain until 1998, except the first ones of Rudolf and Hermann Wagner, actually refer to the brain of Fuchs.[58]\n\nGauss married Johanna Osthoff on 9 October 1805 in St. Catherine's church in Brunswick.[59] They had two sons and one daughter: Joseph (1806–1873), Wilhelmina (1808–1840), and Louis (1809–1810). Johanna died on 11 October 1809, one month after the birth of Louis, who himself died a few months later.[60] Gauss chose the first names of his children in honour of Giuseppe Piazzi, Wilhelm Olbers, and Karl Ludwig Harding, the discoverers of the first asteroids.[61]\n\nOn 4 August 1810, Gauss married Wilhelmine (Minna) Waldeck, a friend of his first wife, with whom he had three more children: Eugen (later Eugene) (1811–1896), Wilhelm (later William) (1813–1879), and Therese (1816–1864). Minna Gauss died on 12 September 1831 after being seriously ill for more than a decade.[62] Therese then took over the household and cared for Gauss for the rest of his life; after her father's death, she married actor Constantin Staufenau.[63] Her sister Wilhelmina married the orientalist Heinrich Ewald.[64] Gauss's mother Dorothea lived in his house from 1817 until she died in 1839.[11]\n\nThe eldest son Joseph, while still a schoolboy, helped his father as an assistant during the survey campaign in the summer of 1821. After a short time at university, in 1824 Joseph joined the Hanoverian army and assisted in surveying again in 1829. In the 1830s he was responsible for the enlargement of the survey network to the western parts of the kingdom. With his geodetical qualifications, he left the service and engaged in the construction of the railway network as director of the Royal Hanoverian State Railways. In 1836 he studied the railroad system in the US for some months.[45][k]\n\nEugen left Göttingen in September 1830 and emigrated to the United States, where he joined the army for five years. He then worked for the American Fur Company in the Midwest. Later, he moved to Missouri and became a successful businessman.[45] Wilhelm married a niece of the astronomer Bessel;[67] he then moved to Missouri, started as a farmer and became wealthy in the shoe business in St. Louis in later years.[68] Eugene and William have numerous descendants in America, but the Gauss descendants left in Germany all derive from Joseph, as the daughters had no children.[45]\n\nIn the first two decades of the 19th century, Gauss was the only important mathematician in Germany, comparable to the leading French ones;[69] his Disquisitiones Arithmeticae was the first mathematical book from Germany to be translated into the French language.[70]\n\nGauss was \"in front of the new development\" with documented research since 1799, his wealth of new ideas, and his rigour of demonstration.[71] Whereas previous mathematicians like Leonhard Euler let the readers take part in their reasoning for new ideas, including certain erroneous deviations from the correct path,[72] Gauss however introduced a new style of direct and complete explanation that did not attempt to show the reader the author's train of thought.[73]\n\nGauss was the first to restore that rigor of demonstration which we admire in the ancients and which had been forced unduly into the background by the exclusive interest of the preceding period in new developments.\nBut for himself, he propagated a quite different ideal, given in a letter to Farkas Bolyai as follows:[74]\n\nIt is not knowledge, but the act of learning, not possession but the act of getting there, which grants the greatest enjoyment. When I have clarified and exhausted a subject, then I turn away from it, in order to go into darkness again.\nThe posthumous papers, his scientific diary,[75] and short glosses in his own textbooks show that he empirically worked to a great extent.[76][77] He was a lifelong busy and enthusiastic calculator, who made his calculations with extraordinary rapidity, mostly without precise controlling, but checked the results by masterly estimation. Nevertheless, his calculations were not always free from mistakes.[78] He coped with the enormous workload by using skillful tools.[79] Gauss used a lot of mathematical tables, examined their exactness, and constructed new tables on various matters for personal use.[80] He developed new tools for effective calculation, for example the Gaussian elimination.[81] It has been taken as a curious feature of his working style that he carried out calculations with a high degree of precision much more than required, and prepared tables with more decimal places than ever requested for practical purposes.[82] Very likely, this method gave him a lot of material which he used in finding theorems in number theory.[79][83]\n\nGauss refused to publish work that he did not consider complete and above criticism. This perfectionism was in keeping with the motto of his personal seal Pauca sed Matura (\"Few, but Ripe\"). Many colleagues encouraged him to publicize new ideas and sometimes rebuked him if he hesitated too long, in their opinion. Gauss defended himself, claiming that the initial discovery of ideas was easy, but preparing a presentable elaboration was a demanding matter for him, for either lack of time or \"serenity of mind\".[35] Nevertheless, he published many short communications of urgent content in various journals, but left a considerable literary estate, too.[84][85] Gauss referred to mathematics as \"the queen of sciences\" and arithmetics as \"the queen of mathematics\",[86] and supposedly once espoused a belief in the necessity of immediately understanding Euler's identity as a benchmark pursuant to becoming a first-class mathematician.[87]\n\nOn certain occasions, Gauss claimed that the ideas of another scholar had already been in his possession previously. Thus his concept of priority as \"the first to discover, not the first to publish\" differed from that of his scientific contemporaries.[88] In contrast to his perfectionism in presenting mathematical ideas, he was criticized for a negligent way of quoting. He justified himself with a very special view of correct quoting: if he gave references, then only in a quite complete way, with respect to the previous authors of importance, which no one should ignore; but quoting in this way needed knowledge of the history of science and more time than he wished to spend.[35]\n\nSoon after Gauss's death, his friend Sartorius published the first biography (1856), written in a rather enthusiastic style. Sartorius saw him as a serene and forward-striving man with childlike modesty,[89] but also of \"iron character\"[90] with an unshakeable strength of mind.[91] Apart from his closer circle, others regarded him as reserved and unapproachable \"like an Olympian sitting enthroned on the summit of science\".[92] His close contemporaries agreed that Gauss was a man of difficult character. He often refused to accept compliments. His visitors were occasionally irritated by his grumpy behaviour, but a short time later his mood could change, and he would become a charming, open-minded host.[35] Gauss abominated polemic natures; together with his colleague Hausmann he opposed to a call for Justus Liebig on a university chair in Göttingen, \"because he was always involved in some polemic.\"[93]\n\nGauss's life was overshadowed by severe problems in his family. When his first wife Johanna suddenly died shortly after the birth of their third child, he revealed the grief in a last letter to his dead wife in the style of an ancient threnody, the most personal surviving document of Gauss.[94][95] The situation worsened when tuberculosis ultimately destroyed the health of his second wife Minna over 13 years;  both his daughters later suffered from the same disease.[96] Gauss himself gave only slight hints of his distress: in a letter to Bessel dated December 1831 he described himself as \"the victim of the worst domestic sufferings\".[35]\n\nBecause of his wife's illness, both younger sons were educated for some years in Celle, far from Göttingen. The military career of his elder son Joseph ended after more than two decades with the rank of a poorly paid first lieutenant, although he had acquired a considerable knowledge of geodesy. He needed financial support from his father even after he was married.[45] The second son Eugen shared a good measure of his father's talent in computation and languages but had a vivacious and sometimes rebellious character. He wanted to study philology, whereas Gauss wanted him to become a lawyer. Having run up debts and caused a scandal in public,[97] Eugen suddenly left Göttingen under dramatic circumstances in September 1830 and emigrated via Bremen to the United States. He wasted the little money he had taken to start, after which his father refused further financial support.[45] The youngest son Wilhelm wanted to qualify for agricultural administration, but had difficulties getting an appropriate education, and eventually emigrated as well. Only Gauss's youngest daughter Therese accompanied him in his last years of life.[63]\n\nCollecting numerical data on very different things, useful or useless, became a habit in his later years, for example, the number of paths from his home to certain places in Göttingen, or the number of living days of persons; he congratulated Humboldt in December 1851 for having reached the same age as Isaac Newton at his death, calculated in days.[98]\n\nSimilar to his excellent knowledge of Latin he was also acquainted with modern languages. At the age of 62, he began to teach himself Russian, very likely to understand scientific writings from Russia, among them those of Lobachevsky on non-Euclidean geometry.[99] Gauss read both classical and modern literature, and English and French works in the original languages.[100][m] His favorite English author was Walter Scott, his favorite German Jean Paul.[102] Gauss liked singing and went to concerts.[103] He was a busy newspaper reader; in his last years, he used to visit an academic press salon of the university every noon.[104] Gauss did not care much for philosophy, and mocked the \"splitting hairs of the so-called metaphysicians\", by which he meant proponents of the contemporary school of Naturphilosophie.[105]\n\nGauss had an \"aristocratic and through and through conservative nature\", with little respect for people's intelligence and morals, following the motto \"mundus vult decipi\".[104] He disliked Napoleon and his system, and all kinds of violence and revolution caused horror to him. Thus he condemned the methods of the Revolutions of 1848, though he agreed with some of their aims, such as the idea of a unified Germany.[90][n] As far as the political system is concerned, he had a low estimation of the constitutional system; he criticized parliamentarians of his time for a lack of knowledge and logical errors.[104]\n\nSome Gauss biographers have speculated on his religious beliefs. He sometimes said \"God arithmetizes\"[106] and \"I succeeded – not on account of my hard efforts, but by the grace of the Lord.\"[107] Gauss was a member of the Lutheran church, like most of the population in northern Germany. It seems that he did not believe all dogmas or understand the Holy Bible quite literally.[108] Sartorius mentioned Gauss's religious tolerance, and estimated his \"insatiable thirst for truth\" and his sense of justice as motivated by religious convictions.[109]\n\nIn his doctoral thesis from 1799, Gauss proved the fundamental theorem of algebra which states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. Mathematicians including Jean le Rond d'Alembert had produced false proofs before him, and Gauss's dissertation contains a critique of d'Alembert's work. He subsequently produced three other proofs, the last one in 1849 being generally rigorous. His attempts clarified the concept of complex numbers considerably along the way.[110]\n\nIn the preface to the Disquisitiones, Gauss dates the beginning of his work on number theory to 1795. By studying the works of previous mathematicians like Fermat, Euler, Lagrange, and Legendre, he realized that these scholars had already found much of what he had discovered by himself.[111] The Disquisitiones Arithmeticae, written in 1798 and published in 1801, consolidated number theory as a discipline and covered both elementary and algebraic number theory. Therein he introduces the triple bar symbol (≡) for congruence and uses it for a clean presentation of modular arithmetic.[112] It deals with the unique factorization theorem and primitive roots modulo n. In the main sections, Gauss presents the first two proofs of the law of quadratic reciprocity[113] and develops the theories of binary[114] and ternary quadratic forms.[115]\n\nThe Disquisitiones include the Gauss composition law for binary quadratic forms, as well as the enumeration of the number of representations of an integer as the sum of three squares. As an almost immediate corollary of his theorem on three squares, he proves the triangular case of the Fermat polygonal number theorem for n = 3.[116] From several analytic results on class numbers that Gauss gives without proof towards the end of the fifth section,[117] it appears that Gauss already knew the class number formula in 1801.[118]\n\nIn the last section, Gauss gives proof for the constructibility of a regular heptadecagon (17-sided polygon) with straightedge and compass by reducing this geometrical problem to an algebraic one.[119] He shows that a regular polygon is constructible if the number of its sides is either a power of 2 or the product of a power of 2 and any number of distinct Fermat primes. In the same section, he gives a result on the number of solutions of certain cubic polynomials with coefficients in finite fields, which amounts to counting integral points on an elliptic curve.[120] An unfinished eighth chapter was found among left papers only after his death, consisting of work done during 1797–1799.[121][122]\n\nOne of Gauss's first results was the empirically found conjecture of 1792 – the later called prime number theorem – giving an estimation of the number of prime numbers by using the integral logarithm.[123][o]\n\nWhen Olbers encouraged Gauss in 1816 to compete for a prize from the French Academy on the proof for Fermat's Last Theorem (FLT), he refused because of his low esteem on this matter. However, among his left works a short undated paper was found with proofs of FLT for the cases n = 3 and n = 5.[125] The particular case of n = 3 was proved much earlier by Leonhard Euler, but Gauss developed a more streamlined proof which made use of Eisenstein integers; though more general, the proof was simpler than in the real integers case.[126]\n\nGauss contributed to solving the Kepler conjecture in 1831 with the proof that a greatest packing density of spheres in the three-dimensional space is given when the centres of the spheres form a cubic face-centred arrangement,[127] when he reviewed a book of Ludwig August Seeber on the theory of reduction of positive ternary quadratic forms.[128] Having noticed some lacks in Seeber's proof, he simplified many of his arguments, proved the central conjecture, and remarked that this theorem is equivalent to the Kepler conjecture for regular arrangements.[129]\n\nIn  two papers on biquadratic residues (1828, 1832) Gauss introduced the ring of Gaussian integers \n\n\n\n\nZ\n\n[\ni\n]\n\n\n{\\displaystyle \\mathbb {Z} [i]}\n\n, showed that it is a unique factorization domain.[130] and generalized some key arithmetic concepts, such as Fermat's little theorem and Gauss's lemma. The main objective of introducing this ring was to formulate the law of biquadratic reciprocity[130] – as Gauss discovered, rings of complex integers are the natural setting for such higher reciprocity laws.[131]\n\nIn the second paper, he stated the general law of biquadratic reciprocity and proved several special cases of it. In an earlier publication from 1818 containing his fifth and sixth proofs of quadratic reciprocity, he claimed the techniques of these proofs (Gauss sums) can be applied to prove higher reciprocity laws.[132]\n\nOne of Gauss's first discoveries was the notion of the arithmetic-geometric mean (AGM) of two positive real numbers.[133] He discovered its relation to elliptic integrals in the years 1798–1799 through the Landen's transformation, and a diary entry recorded the discovery of the connection of Gauss's constant to lemniscatic elliptic functions, a result that Gauss stated that \"will surely open an entirely new field of analysis\".[134] He also made early inroads into the more formal issues of the foundations of complex analysis, and from a letter to Bessel in 1811 it is clear that he knew the \"fundamental theorem of complex analysis\" – Cauchy's integral theorem – and understood the notion of complex residues when integrating around poles.[120][135]\n\nEuler's pentagonal numbers theorem, together with other researches on the AGM and lemniscatic functions, led him to plenty of results on Jacobi theta functions,[120] culminating in the discovery in 1808 of the later called Jacobi triple product identity, which includes Euler's theorem as a special case.[136] His works show that he knew modular transformations of order 3, 5, 7 for elliptic functions since 1808.[137][p][q]\n\nSeveral mathematical fragments in his Nachlass indicate that he knew parts of the modern theory of modular forms.[120] In his work on the multivalued AGM of two complex numbers, he discovered a deep connection between the infinitely many values of the AGM to its two \"simplest values\".[134] In his unpublished writings he recognized and made a sketch of the key concept of fundamental domain for the modular group.[139][140] One of Gauss's sketches of this kind was a drawing of a tessellation of the unit disk by \"equilateral\" hyperbolic triangles with all angles equal to \n\n\n\nπ\n\n/\n\n4\n\n\n{\\displaystyle \\pi /4}\n\n.[141]\n\nAn example of Gauss's insight in the fields of analysis is the cryptic remark that the principles of circle division by compass and straightedge can also be applied to the division of the lemniscate curve, which inspired Abel's theorem on lemniscate division.[r] Another example is his publication \"Summatio quarundam serierum singularium\" (1811) on the determination of the sign of quadratic Gauss sum, in which he solved the main problem by introducing q-analogs of binomial coefficients and manipulating them by several original identities that seem to stem out of his work on elliptic functions theory; however, Gauss cast his argument in a formal way that does not reveal its origin in elliptic functions theory, and only the later work of mathematicians such as Jacobi and Hermite has exposed the crux of his argument.[142]\n\nIn the \"Disquisitiones generales circa series infinitam...\" (1813), he provides the first systematic treatment of the general hypergeometric function \n\n\n\nF\n(\nα\n,\nβ\n,\nγ\n,\nx\n)\n\n\n{\\displaystyle F(\\alpha ,\\beta ,\\gamma ,x)}\n\n, and shows that many of the functions known at the time are special cases of the hypergeometric function.[143] This work is the first one with an exact inquiry of convergence of infinite series in the history of mathematics.[144] Furthermore, it deals with infinite continued fractions arising as ratios of hypergeometric functions which are now called Gauss continued fractions.[145]\n\nIn 1823, Gauss won the prize of the Danish Society with an essay on conformal mappings, which contains several developments that pertain to the field of complex analysis.[146] Gauss stated that angle-preserving mappings in the complex plane must be complex analytic functions, and used the later called Beltrami equation to prove the existence of isothermal coordinates on analytic surfaces. The essay concludes with examples of conformal mappings into a sphere and an ellipsoid of revolution.[147]\n\nGauss often deduced theorems inductively from numerical data he had collected empirically.[77] As such, the use of efficient algorithms to facilitate calculations was vital to his research, and he made many contributions to numeric analysis, as the method of Gaussian quadrature published in 1816.[148]\n\nIn a private letter to Gerling from 1823,[149] he described a solution of a 4X4 system of linear equations by using Gauss-Seidel method – an \"indirect\" iterative method for the solution of linear systems, and recommended it over the usual method of \"direct elimination\" for systems of more than two equations.[150]\n\nGauss invented an algorithm for calculating what is now called discrete Fourier transforms, when calculating the orbits of Pallas and Juno in 1805, 160 years before Cooley and Tukey found their similar Cooley–Tukey FFT algorithm.[151] He developed it as a trigonometric interpolation method, but the paper Theoria Interpolationis Methodo Nova Tractata was published only posthumously in 1876,[152] preceded by the first presentation by Joseph Fourier on the subject in 1807.[153]\n\nThe first publication following the doctoral thesis dealt with the determination of the date of Easter (1800), an elementary matter of mathematics. Gauss aimed to present a most convenient algorithm for people without any knowledge of ecclesiastical or even astronomical chronology, and thus avoided the usually required terms of golden number, epact, solar cycle, domenical letter, and any religious connotations.[154] Biographers speculated on the reason why Gauss dealt with this matter, but it is likely comprehensible by the historical background. The replacement of the Julian calendar by the Gregorian calendar had caused confusion in the Holy Roman Empire since the 16th century and was not finished in Germany until 1700 when the difference of eleven days was deleted, but the difference in calculating the date of Easter remained between Protestant and Catholic territories. A further agreement of 1776 equalized the confessional way of counting; thus in the Protestant states like the Duchy of Brunswick the Easter of 1777, five weeks before Gauss's birth, was the first one calculated in the new manner.[155] The public difficulties of replacement may be the historical background for the confusion on this matter in the Gauss family (see chapter: Anecdotes). For being connected with the Easter regulations, an essay on the date of Pesach followed soon in 1802.[156]\n\nOn 1 January 1801, Italian astronomer Giuseppe Piazzi discovered a new celestial object, presumed it to be the long searched planet between Mars and Jupiter according to the so-called Titius–Bode law, and named it Ceres.[157] He could track it only for a short time until it disappeared behind the glare of the Sun. The mathematical tools of the time were not sufficient to extrapolate a position from the few data for its reappearance. Gauss tackled the problem and predicted a position for possible rediscovery in December 1801. This turned out to be accurate within a half-degree when Franz Xaver von Zach on 7 and 31 December at Gotha, and independently Heinrich Olbers on 1 and 2 January in Bremen, identified the object near the predicted position.[158][s]\n\nGauss's method leads to an equation of the eighth degree, of which one solution, the Earth's orbit, is known. The solution sought is then separated from the remaining six based on physical conditions. In this work, Gauss used comprehensive approximation methods which he created for that purpose.[159]\n\nThe discovery of Ceres led Gauss to the theory of the motion of planetoids disturbed by large planets, eventually published in 1809 as Theoria motus corporum coelestium in sectionibus conicis solem ambientum.[160] It introduced the Gaussian gravitational constant.[33]\n\nSince the new asteroids had been discovered, Gauss occupied himself with the perturbations of their orbital elements. Firstly he examined Ceres with analytical methods similar to those of Laplace, but his favorite object was Pallas, because of its great eccentricity and orbital inclination, whereby Laplace's method did not work. Gauss used his own tools: the arithmetic–geometric mean, the hypergeometric function, and his method of interpolation.[161] He found an orbital resonance with Jupiter in proportion 18:7 in 1812; Gauss gave this result as cipher, and gave the explicit meaning only in letters to Olbers and Bessel.[162][163][t] After long years of work, he finished it in 1816 without a result that seemed sufficient to him. This marked the end of his activities in theoretical astronomy.[165]\n\nOne fruit of Gauss's research on Pallas perturbations was the Determinatio Attractionis... (1818) on a method of theoretical astronomy that later became known as the \"elliptic ring method\". It introduced an averaging conception in which a planet in orbit is replaced by a fictitious ring with mass density proportional to the time taking the planet to follow the corresponding orbital arcs.[166] Gauss presents the method of evaluating the gravitational attraction of such an elliptic ring, which includes several steps; one of them involves a direct application of the arithmetic-geometric mean (AGM) algorithm to calculate an elliptic integral.[167]\n\nWhile Gauss's contributions to theoretical astronomy came to an end, more practical activities in observational astronomy continued and occupied him during his entire career. Even early in 1799, Gauss dealt with the determination of longitude by use of the lunar parallax, for which he developed more convenient formulas than those were in common use.[168] After appointment as director of observatory he attached importance to the fundamental astronomical constants in correspondence with Bessel. Gauss himself provided tables for nutation and aberration, the solar coordinates, and refraction.[169] He made many contributions to spherical geometry, and in this context solved some practical problems about navigation by stars.[170] He published a great number of observations, mainly on minor planets and comets; his last observation was the solar eclipse of 28 July 1851.[171]\n\nGauss likely used the method of least squares for calculating the orbit of Ceres to minimize the impact of measurement error.[88] The method was published first by Adrien-Marie Legendre in 1805, but Gauss claimed in Theoria motus (1809) that he had been using it since 1794 or 1795.[172][173][174] In the history of statistics, this disagreement is called the \"priority dispute over the discovery of the method of least squares\".[88] Gauss proved that the method has the lowest sampling variance within the class of linear unbiased estimators under the assumption of normally distributed errors (Gauss–Markov theorem), in the two-part paper Theoria combinationis observationum erroribus minimis obnoxiae (1823).[175]\n\nIn the first paper he proved Gauss's inequality (a Chebyshev-type inequality) for unimodal distributions, and stated without proof another inequality for moments of the fourth order (a special case of Gauss-Winckler inequality).[176] He derived lower and upper bounds for the variance of sample variance. In the second paper, Gauss described recursive least squares methods. His work on the theory of errors was extended in several directions by the geodesist Friedrich Robert Helmert to the Gauss-Helmert model.[177]\n\nGauss also contributed to problems in probability theory that are not directly concerned with the theory of errors. One example appears as a diary note where he tried to describe the asymptotic distribution of entries in the continued fraction expansion of a random number uniformly distributed in (0,1). He derived this distribution, now known as the Gauss-Kuzmin distribution, as a by-product of the discovery of the ergodicity of the Gauss map for continued fractions. Gauss's solution is the first-ever result in the metrical theory of continued fractions.[178]\n\nGauss was busy with geodetic problems since 1799 when he helped Karl Ludwig von Lecoq with calculations during his survey in Westphalia.[179] Beginning in 1804, he taught himself some geodetic practice with a sextant in Brunswick,[180] and Göttingen.[181]\n\nSince 1816, Gauss's former student Heinrich Christian Schumacher, then professor in Copenhagen, but living in Altona (Holstein) near Hamburg as head of an observatory, carried out a triangulation of the Jutland peninsula from Skagen in the north to Lauenburg in the south.[u] This project was the basis for map production but also aimed at determining the geodetic arc between the terminal sites. Data from geodetic arcs were used to determine the dimensions of the earth geoid, and long arc distances brought more precise results. Schumacher asked Gauss to continue this work further to the south in the Kingdom of Hanover; Gauss agreed after a short time of hesitation. Finally, in May 1820, King George IV gave the order to Gauss.[182]\n\nAn arc measurement needs a precise astronomical determination of at least two points in the network. Gauss and Schumacher used the favourite occasion that both observatories in Göttingen and Altona, in the garden of Schumacher's house, laid nearly in the same longitude. The latitude was measured with both their instruments and a zenith sector of Ramsden that was transported to both observatories.[183][v]\n\nGauss and Schumacher had already determined some angles between Lüneburg, Hamburg, and Lauenburg for the geodetic connection in October 1818.[184] During the summers of 1821 until 1825 Gauss directed the triangulation work personally, from Thuringia in the south to the river Elbe in the north. The triangle between Hoher Hagen, Großer Inselsberg in the Thuringian Forest, and Brocken in the Harz mountains was the largest one Gauss had ever measured with a maximum size of 107 km (66.5 miles). In the thinly populated Lüneburg Heath without significant natural summits or artificial buildings, he had difficulties finding suitable triangulation points; sometimes cutting lanes through the vegetation was necessary.[155][185]\n\nFor pointing signals, Gauss invented a new instrument with movable mirrors and a small telescope that reflects the sunbeams to the triangulation points, and named it heliotrope.[186] Another suitable construction for the same purpose was a sextant with an additional mirror which he named vice heliotrope.[187] Gauss got assistance by soldiers of the Hanoverian army, among them his eldest son Joseph. Gauss took part in the baseline measurement (Braak Base Line) of Schumacher in the village of Braak near Hamburg in 1820, and used the result for the evaluation of the Hanoverian triangulation.[188]\n\nAn additional result was a better value of flattening of the approximative Earth ellipsoid.[189][w] Gauss developed the universal transverse Mercator projection of the ellipsoidal shaped Earth (what he named conform projection)[191] for representing geodetical data in plane charts.\n\nWhen the arc measurement was finished, Gauss began the enlargement of the triangulation to the west to get a survey of the whole Kingdom of Hanover with a Royal decree from 25 March 1828.[192] The practical work was directed by three army officers, among them Lieutenant Joseph Gauss. The complete data evaluation laid in the hands of Gauss, who applied his mathematical inventions such as the method of least squares and the elimination method to it. The project was finished in 1844, and Gauss sent a final report of the project to the government; his method of projection was not edited until 1866.[193][194]\n\nIn 1828, when studying differences in latitude, Gauss first defined a physical approximation for the figure of the Earth as the surface everywhere perpendicular to the direction of gravity;[195] later his doctoral student Johann Benedict Listing called this the geoid.[196]\n\nThe geodetic survey of Hanover fuelled Gauss's interest in differential geometry and topology, fields of mathematics dealing with curves and surfaces. This led him in 1828 to the publication of a memoir that marks the birth of modern differential geometry of surfaces, as it departed from the traditional ways of treating surfaces as cartesian graphs of functions of two variables, and that initiated the exploration of surfaces from the \"inner\" point of view of a two-dimensional being constrained to move on it. As a result, the Theorema Egregium (remarkable theorem), established a property of the notion of Gaussian curvature. Informally, the theorem says that the curvature of a surface can be determined entirely by measuring angles and distances on the surface, regardless of the embedding of the surface in three-dimensional or two-dimensional space.[197]\n\nThe Theorema Egregium leads to the abstraction of surfaces as doubly-extended manifolds; it clarifies the distinction between the intrinsic properties of the manifold (the metric) and its physical realization in ambient space. A consequence is the impossibility of an isometric transformation between surfaces of different Gaussian curvature. This means practically that a sphere or an ellipsoid cannot be transformed to a plane without distortion, which causes a fundamental problem in designing projections for geographical maps.[197] A portion of this essay is dedicated to a profound study of geodesics. In particular, Gauss proves the local Gauss–Bonnet theorem on geodesic triangles, and generalizes Legendre's theorem on spherical triangles to geodesic triangles on arbitrary surfaces with continuous curvature; he found that the angles of a \"sufficiently small\" geodesic triangle deviate from that of a planar triangle of the same sides in a way that depends only on the values of the surface curvature at the vertices of the triangle, regardless of the behaviour of the surface in the triangle interior.[198]\n\nGauss's memoir from 1828 lacks the conception of geodesic curvature. However, in a previously unpublished manuscript, very likely written in 1822–1825, he introduced the term \"side curvature\" (German: \"Seitenkrümmung\") and proved its invariance under isometric transformations, a result that was later obtained by Ferdinand Minding and published by him in 1830. This Gauss paper contains the core of his lemma on total curvature, but also its generalization, found and proved by Pierre Ossian Bonnet in 1848 and known as Gauss–Bonnet theorem.[199]\n\nIn the lifetime of Gauss, a vivid discussion on the Parallel postulate in Euclidean geometry was going on.[200] Numerous efforts were made to prove it in the frame of the Euclidean axioms, whereas some mathematicians discussed the possibility of geometrical systems without it.[201] Gauss thought about the basics of geometry since the 1790s years, but in the 1810s he realized that a non-Euclidean geometry without the parallel postulate could solve the problem.[202][200] In a letter to Franz Taurinus of 1824, he presented a short comprehensible outline of what he named a \"non-Euclidean geometry\",[203] but he strongly forbade Taurinus to make any use of it.[202] Gauss is credited with having been the one to first discover and study non-Euclidean geometry, even coining the term as well.[204][203][205]\n\nThe first publications on non-Euclidean geometry in the history of mathematics were authored by Nikolai Lobachevsky in 1829 and Janos Bolyai in 1832.[201] In the following years, Gauss wrote his ideas on the topic but did not publish them, thus avoiding influencing the contemporary scientific discussion.[202][206] Gauss commended the ideas of Janos Bolyai in a letter to his father and university friend Farkas Bolyai[207] claiming that these were congruent to his own thoughts of some decades.[202][208] However, it is not quite clear to what extent he preceded Lobachevsky and Bolyai, as his letter remarks are only vague and obscure.[201]\n\nSartorius mentioned Gauss's work on non-Euclidean geometry firstly in 1856, but only the edition of left papers in Volume VIII of the Collected Works (1900) showed Gauss's ideas on that matter, at a time when non-Euclidean geometry had yet grown out of controversial discussion.[202]\n\nGauss was also an early pioneer of topology or Geometria Situs, as it was called in his lifetime. The first proof of the fundamental theorem of algebra in 1799 contained an essentially topological argument; fifty years later, he further developed the topological argument in his fourth proof of this theorem.[209]\n\nAnother encounter with topological notions occurred to him in the course of his astronomical work in 1804, when he determined the limits of the region on the celestial sphere in which comets and asteroids might appear, and which he termed \"Zodiacus\". He discovered that if the Earth's and comet's orbits are linked, then by topological reasons the Zodiacus is the entire sphere. In 1848, in the context of the discovery of the asteroid 7 Iris, he  published a further qualitative discussion of the Zodiacus.[210]\n\nIn Gauss's letters of 1820–1830, he thought intensively on topics with close affinity to Geometria Situs, and became gradually conscious of semantic difficulty in this field. Fragments from this period reveal that he tried to classify \"tract figures\", which are closed plane curves with a finite number of transverse self-intersections, that may also be planar projections of knots.[211] To do so he devised a symbolical scheme, the Gauss code, that in a sense captured the characteristic features of tract figures.[212][213]\n\nIn a fragment from 1833, Gauss defined the linking number of two space curves by a certain double integral, and in doing so provided for the first time an analytical formulation of a topological phenomenon. On the same note, he lamented the little progress made in Geometria Situs, and remarked that one of its central problems will be \"to count the intertwinings of two closed or infinite curves\". His notebooks from that period reveal that he was also thinking about other topological objects such as braids and tangles.[210]\n\nGauss's influence in later years to the emerging field of topology, which he held in high esteem, was through occasional remarks and oral communications to Mobius and Listing.[214]\n\nGauss applied the concept of complex numbers to solve well-known problems in a new concise way. For example, in a short note from 1836 on geometric aspects of the ternary forms and their application to crystallography,[215] he stated the fundamental theorem of axonometry, which tells how to represent a 3D cube on a 2D plane with complete accuracy, via complex numbers.[216] He described rotations of this sphere as the action of certain linear fractional transformations on the extended complex plane,[217] and gave a proof for the geometric theorem that the altitudes of a triangle always meet in a single orthocenter.[218]\n\nGauss was concerned with John Napier's \"Pentagramma mirificum\" – a certain spherical pentagram – for several decades;[219] he approached it from various points of view, and gradually gained a full understanding of its geometric, algebraic, and analytic aspects.[220] In particular, in 1843 he stated and proved several theorems connecting elliptic functions, Napier spherical pentagons, and Poncelet pentagons in the plane.[221]\n\nFurthermore, he contributed a solution to the problem of constructing the largest-area ellipse inside a given quadrilateral,[222][223] and discovered a surprising result about the computation of area of pentagons.[224][225]\n\nGauss had been interested in magnetism since 1803.[226] After Alexander von Humboldt visited Göttingen in 1826, both scientists began intensive research on geomagnetism, partly independently, partly in productive cooperation.[227] In 1828, Gauss was Humboldt's guest during the conference of the Society of German Natural Scientists and Physicians in Berlin, where he got acquainted with the physicist Wilhelm Weber.[228]\n\nWhen Weber got the chair for physics in Göttingen as successor of Johann Tobias Mayer by Gauss's recommendation in 1831, both of them started a fruitful collaboration, leading to a new knowledge of magnetism with a representation for the unit of magnetism in terms of mass, charge, and time.[229] They founded the Magnetic Association (German: Magnetischer Verein), an international working group of several observatories, which supported measurements of Earth's magnetic field in many regions of the world with equal methods at arranged dates in the years 1836 to 1841.[230]\n\nIn 1836, Humboldt suggested the establishment of a worldwide net of geomagnetic stations in the British dominions with a letter to the Duke of Sussex, then president of the Royal Society; he proposed that magnetic measures should be taken under standardized conditions using his methods.[231][232] Together with other instigators, this led to a global program known as \"Magnetical crusade\" under the direction of Edward Sabine. The dates, times, and intervals of observations were determined in advance, the Göttingen mean time was used as the standard.[233] 61 stations on all five continents participated in this global program. Gauss and Weber founded a series for publication of the results, six volumes were edited between 1837 and 1843. Weber's departure to Leipzig in 1843 as late effect of the Göttingen Seven affair marked the end of Magnetic Association activity.[230]\n\nFollowing Humboldt's example, Gauss ordered a magnetic observatory to be built in the garden of the observatory, but the scientists differed over instrumental equipment; Gauss preferred stationary instruments, which he thought to give more precise results, whereas Humboldt was accustomed to movable instruments. Gauss was interested in the temporal and spatial variation of magnetic declination, inclination, and intensity, but discriminated Humboldt's concept of magnetic intensity to the terms of \"horizontal\" and \"vertical\" intensity. Together with Weber, he developed methods of measuring the components of the intensity of the magnetic field and constructed a suitable magnetometer to measure absolute values of the strength of the Earth's magnetic field, not more relative ones that depended on the apparatus.[230][234] The precision of the magnetometer was about ten times higher than of previous instruments. With this work, Gauss was the first to derive a non-mechanical quantity by basic mechanical quantities.[233]\n\nGauss carried out a General Theory of Terrestrial Magnetism (1839), in what he believed to describe the nature of magnetic force; according to Felix Klein, this work is a presentation of observations by use of spherical harmonics rather than a physical theory.[235] The theory predicted the existence of exactly two magnetic poles on the Earth, thus Hansteen's idea of four magnetic poles became obsolete,[236] and the data allowed to determine their location with rather good precision.[237]\n\nGauss influenced the beginning of geophysics in Russia, when Adolph Theodor Kupffer, one of his former students, founded a magnetic observatory in St. Petersburg, following the example of the observatory in Göttingen, and similarly, Ivan Simonov in Kazan.[236]\n\nThe discoveries of Hans Christian Ørsted on electromagnetism and Michael Faraday on electromagnetic induction drew Gauss's attention to these matters.[238] Gauss and Weber found rules for branched electric circuits, which were later found independently and firstly published by Gustav Kirchhoff and benamed after him as Kirchhoff's circuit laws,[239] and made inquiries on electromagnetism. They constructed the first electromechanical telegraph in 1833, and Weber himself connected the observatory with the institute for physics in the town centre of Göttingen,[y] but they did not care for any further development of this invention for commercial purposes.[240][241]\n\nGauss's main theoretical interests in electromagnetism were reflected in his attempts to formulate quantitive laws governing electromagnetic induction. In notebooks from these years, he recorded several innovative formulations; he discovered the idea of vector potential function (independently rediscovered by Franz Ernst Neumann in 1845), and in January 1835 he wrote down an \"induction law\" equivalent to Faraday's law, which stated that the electromotive force at a given point in space is equal to the instantaneous rate of change (with respect to time) of this function.[242][243]\n\nGauss tried to find a unifying law for long-distance effects of electrostatics, electrodynamics, electromagnetism, and induction, comparable to Newton's law of gravitation,[244] but his attempt ended in a \"tragic failure\".[233]\n\nSince Isaac Newton had shown theoretically that the Earth and rotating stars assume non-spherical shapes, the problem of attraction of ellipsoids gained importance in mathematical astronomy. In his first publication on potential theory, the \"Theoria attractionis...\" (1813), Gauss provided a closed-form expression to the gravitational attraction of a homogeneous triaxial ellipsoid at every point in space.[245] In contrast to previous research of Maclaurin, Laplace and Lagrange, Gauss's new solution treated the attraction more directly in the form of an elliptic integral. In the process, he also proved and applied some special cases of the so-called Gauss's theorem in vector analysis.[246]\n\nIn the General theorems concerning the attractive and repulsive forces acting in reciprocal proportions of quadratic distances (1840) Gauss gave the baseline of a theory of the magnetic potential, based on Lagrange, Laplace, and Poisson;[235] it seems rather unlikely that he knew the previous works of George Green on this subject.[238] However, Gauss could never give any reasons for magnetism, nor a theory of magnetism similar to Newton's work on gravitation, that enabled scientists to predict geomagnetic effects in the future.[233]\n\nGauss's calculations enabled instrument maker Johann Georg Repsold in Hamburg to construct a new achromatic lens system in 1810. A main problem, among other difficulties, was the nonprecise knowledge of the refractive index and dispersion of the used glass types.[247] In a short article from 1817 Gauss dealt with the problem of removal of chromatic aberration in double lenses, and computed adjustments of the shape and coefficients of refraction required to minimize it. His work was noted by the optician Carl August von Steinheil, who in 1860 introduced the achromatic Steinheil doublet, partly based on Gauss's calculations.[248] Many results in geometrical optics are only scattered in Gauss's correspondences and hand notes.[249]\n\nIn the Dioptrical Investigations (1840), Gauss gave the first systematic analysis on the formation of images under a paraxial approximation (Gaussian optics).[250] He characterized optical systems under a paraxial approximation only by its cardinal points,[251] and he derived the Gaussian lens formula, applicable without restrictions in respect to the thickness of the lenses.[252][253]\n\nGauss's first business in mechanics concerned the earth's rotation. When his university friend Benzenberg carried out experiments to determine the deviation of falling masses from the perpendicular in 1802, what today is known as an effect of the Coriolis force, he asked Gauss for a theory-based calculation of the values for comparison with the experimental ones. Gauss elaborated a system of fundamental equations for the motion, and the results corresponded sufficiently with Benzenberg's data, who added Gauss's considerations as an appendix to his book on falling experiments.[254]\n\nAfter Foucault had demonstrated the earth's rotation by his pendulum experiment in public in 1851, Gerling questioned Gauss for further explanations. This instigated Gauss to design a new apparatus for demonstration with a much shorter length of pendulum than Foucault's one. The oscillations were observed with a reading telescope, with a vertical scale and a mirror fastened at the pendulum. It is described in the Gauss–Gerling correspondence and Weber made some experiments with this apparatus in 1853, but no data were published.[255][256]\n\nGauss's principle of least constraint of 1829 was established as a general concept to overcome the division of mechanics into statics and dynamics, combining D'Alembert's principle with Lagrange's principle of virtual work, and showing analogies to the method of least squares.[257]  \n\nIn 1828, Gauss was appointed to head of a Board for weights and measures of the Kingdom of Hanover. He provided the creation of standards of length and measures. Gauss himself took care of the time-consuming measures and gave detailed orders for the mechanical preparation.[155] In the correspondence with Schumacher, who was also working on this matter, he described new ideas for scales of high precision.[258] He submitted the final reports on the Hanoverian foot and pound to the government in 1841. This work got more than regional importance by the order of a law of 1836 that connected the Hanoverian measures with the English ones.[155]\n\nSeveral stories of his early genius have been reported. Carl Friedrich Gauss's mother had never recorded the date of his birth, remembering only that he had been born on a Wednesday, eight days before the Feast of the Ascension, which occurs 39 days after Easter. Gauss later solved this puzzle about his birthdate in the context of finding the date of Easter, deriving methods to compute the date in both past and future years.[259]\n\nIn his memorial on Gauss, Wolfgang Sartorius von Waltershausen tells a story about the three-year-old Gauss, who corrected a math error his father made. The most popular story, also told by Sartorius, tells of a school exercise: the teacher Büttner and his assistant Martin Bartels ordered students to add an arithmetic series. Out of about a hundred pupils, Gauss was the first to solve the problem correctly by a significant margin.[260][8] Although (or because) Sartorius gave no details, over time many versions of this story have been created, with more and more details regarding the nature of the series – the most frequent being the classical problem of adding together all the integers from 1 to 100 – and the circumstances in the classroom.[261][z]\n\nThe first membership of a scientific society was given to Gauss in 1802 by the Russian Academy of Sciences.[263] Further memberships (corresponding, foreign or full) were awarded from the Academy of Sciences in Göttingen (1802/ 1807),[264] the French Academy of Sciences (1804/ 1820),[265] the Royal Society of London (1804),[266] the Royal Prussian Academy in Berlin (1810),[267] the National Academy of Science in Verona (1810),[268] the Royal Society of Edinburgh (1820),[269] the Bavarian Academy of Sciences of Munich (1820),[270] the Royal Danish Academy in Copenhagen (1821),[271] the Royal Astronomical Society in London (1821),[272] the Royal Swedish Academy of Sciences (1821),[271] the American Academy of Arts and Sciences in Boston (1822),[273]  the Royal Bohemian Society of Sciences in Prague (1833),[274] the Royal Academy of Science, Letters and Fine Arts of Belgium (1841/1845),[275] the Royal Society of Sciences in Uppsala (1843),[274] the Royal Irish Academy in Dublin (1843),[274] the Royal Institute of the Netherlands (1845/ 1851),[276] the Spanish Royal Academy of Sciences in Madrid (1850),[277] the Russian Geographical Society (1851),[278] the Imperial Academy of Sciences in Vienna (1848),[278] the American Philosophical Society (1853),[279] the Cambridge Philosophical Society,[278] and the Royal Hollandish Society of Sciences in Haarlem.[280][281]\n\nBoth the University of Kazan and the Philosophy Faculty of the University of Prague appointed him honorary member in 1848.[280]\n\nGauss received the Lalande Prize from the French Academy of Science in 1809 for the theory of planets and the means of determining their orbits from only three observations,[282] the Danish Academy of Science prize in 1823 for his memoir on conformal projection,[274] and the Copley Medal from the Royal Society in 1838 for \"his inventions and mathematical researches in magnetism\".[281][283][33]\n\nGauss was appointed Knight of the French Legion of Honour in 1837,[284] and was taken as one of the first members of the Prussian Order Pour le Merite (Civil class) when it was established in 1842.[285] He received the Order of the Crown of Westphalia (1810),[281] the Danish Order of the Dannebrog (1817),[281] the Hanoverian Royal Guelphic Order (1815),[281] the Swedish Order of the Polar Star (1844),[286] the Order of Henry the Lion (1849),[286] and the Bavarian Maximilian Order for Science and Art (1853).[278]\n\nThe Kings of Hanover appointed him the honorary titles \"Hofrath\" (1816)[51]  and \"Geheimer Hofrath\"[aa] (1845). In 1949, on the occasion of his golden doctor degree jubilee, he got the honorary citizenship of both towns of Brunswick and Göttingen.[278] Soon after his death a medal was issued by order of King George V of Hanover with the back inscription dedicated \"to the Prince of Mathematicians\".[287]\n\nThe \"Gauss-Gesellschaft Göttingen\" (\"Göttingen Gauss Society\") was founded in 1964 for research on life and work of Carl Friedrich Gauss and related persons and edits the Mitteilungen der Gauss-Gesellschaft (Communications of the Gauss Society).[288]\n\nThe Göttingen Academy of Sciences and Humanities provides a complete collection of the known letters from and to Carl Friedrich Gauss that is accessible online.[34] The literary estate is kept and provided by the Göttingen State and University Library.[289] Written materials from Carl Friedrich Gauss and family members can also be found in the municipal archive of Brunswick.[290]\n"
    },
    {
        "title": "Fermat's Last Theorem",
        "content": "\n\nIn number theory, Fermat's Last Theorem (sometimes called Fermat's conjecture, especially in older texts) states that no three positive integers a, b, and c satisfy the equation an + bn = cn for any integer value of n greater than 2. The cases n = 1 and n = 2 have been known since antiquity to have infinitely many solutions.[1]\n\nThe proposition was first stated as a theorem by Pierre de Fermat around 1637 in the margin of a copy of Arithmetica. Fermat added that he had a proof that was too large to fit in the margin. Although other statements claimed by Fermat without proof were subsequently proven by others and credited as theorems of Fermat (for example, Fermat's theorem on sums of two squares), Fermat's Last Theorem resisted proof, leading to doubt that Fermat ever had a correct proof. Consequently, the proposition became known as a conjecture rather than a theorem. After 358 years of effort by mathematicians, the first successful proof was released in 1994 by Andrew Wiles and formally published in 1995. It was described as a \"stunning advance\" in the citation for Wiles's Abel Prize award in 2016.[2] It also proved much of the Taniyama–Shimura conjecture, subsequently known as the modularity theorem, and opened up entire new approaches to numerous other problems and mathematically powerful modularity lifting techniques.\n\nThe unsolved problem stimulated the development of algebraic number theory in the 19th and 20th centuries. It is among the most notable theorems in the history of mathematics and prior to its proof was in the Guinness Book of World Records as the \"most difficult mathematical problem\", in part because the theorem has the largest number of unsuccessful proofs.[3]\n\nThe Pythagorean equation, x2 + y2 = z2, has an infinite number of positive integer solutions for x, y, and z; these solutions are known as Pythagorean triples (with the simplest example being 3, 4, 5). Around 1637, Fermat wrote in the margin of a book that the more general equation an + bn = cn had no solutions in positive integers if n is an integer greater than 2. Although he claimed to have a general proof of his conjecture, Fermat left no details of his proof, and none has ever been found. His claim was discovered some 30 years later, after his death. This claim, which came to be known as Fermat's Last Theorem, stood unsolved for the next three and a half centuries.[4]\n\nThe claim eventually became one of the most notable unsolved problems of mathematics. Attempts to prove it prompted substantial development in number theory, and over time Fermat's Last Theorem gained prominence as an unsolved problem in mathematics.\n\nThe special case n = 4, proved by Fermat himself, is sufficient to establish that if the theorem is false for some exponent n that is not a prime number, it must also be false for some smaller n, so only prime values of n need further investigation.[note 1] Over the next two centuries (1637–1839), the conjecture was proved for only the primes 3, 5, and 7, although Sophie Germain innovated and proved an approach that was relevant to an entire class of primes. In the mid-19th century, Ernst Kummer extended this and proved the theorem for all regular primes, leaving irregular primes to be analyzed individually. Building on Kummer's work and using sophisticated computer studies, other mathematicians were able to extend the proof to cover all prime exponents up to four million,[5] but a proof for all exponents was inaccessible (meaning that mathematicians generally considered a proof impossible, exceedingly difficult, or unachievable with current knowledge).[6]\n\nSeparately, around 1955, Japanese mathematicians Goro Shimura and Yutaka Taniyama suspected a link might exist between elliptic curves and modular forms, two completely different areas of mathematics. Known at the time as the Taniyama–Shimura conjecture (eventually as the modularity theorem), it stood on its own, with no apparent connection to Fermat's Last Theorem. It was widely seen as significant and important in its own right, but was (like Fermat's theorem) widely considered completely inaccessible to proof.[7]\n\nIn 1984, Gerhard Frey noticed an apparent link between these two previously unrelated and unsolved problems. An outline suggesting this could be proved was given by Frey. The full proof that the two problems were closely linked was accomplished in 1986 by Ken Ribet, building on a partial proof by Jean-Pierre Serre, who proved all but one part known as the \"epsilon conjecture\" (see: Ribet's Theorem and Frey curve).[2] These papers by Frey, Serre and Ribet showed that if the Taniyama–Shimura conjecture could be proven for at least the semi-stable class of elliptic curves, a proof of Fermat's Last Theorem would also follow automatically. The connection is described below: any solution that could contradict Fermat's Last Theorem could also be used to contradict the Taniyama–Shimura conjecture. So if the modularity theorem were found to be true, then by definition, no solution contradicting Fermat's Last Theorem could exist, meaning that Fermat's Last Theorem must also be true.\n\nAlthough both problems were daunting and widely considered to be \"completely inaccessible\" to proof at the time,[2] this was the first suggestion of a route by which Fermat's Last Theorem could be extended and proved for all numbers, not just some numbers. Unlike Fermat's Last Theorem, the Taniyama–Shimura conjecture was a major active research area and viewed as more within reach of contemporary mathematics.[8] However, general opinion was that this simply showed the impracticality of proving the Taniyama–Shimura conjecture.[9] Mathematician John Coates' quoted reaction was a common one:[9]\n\nI myself was very sceptical that the beautiful link between Fermat's Last Theorem and the Taniyama–Shimura conjecture would actually lead to anything, because I must confess I did not think that the Taniyama–Shimura conjecture was accessible to proof. Beautiful though this problem was, it seemed impossible to actually prove. I must confess I thought I probably wouldn't see it proved in my lifetime.\nOn hearing that Ribet had proven Frey's link to be correct, English mathematician Andrew Wiles, who had a childhood fascination with Fermat's Last Theorem and had a background of working with elliptic curves and related fields, decided to try to prove the Taniyama–Shimura conjecture as a way to prove Fermat's Last Theorem. In 1993, after six years of working secretly on the problem, Wiles succeeded in proving enough of the conjecture to prove Fermat's Last Theorem. Wiles's paper was massive in size and scope. A flaw was discovered in one part of his original paper during peer review and required a further year and collaboration with a past student, Richard Taylor, to resolve. As a result, the final proof in 1995 was accompanied by a smaller joint paper showing that the fixed steps were valid. Wiles's achievement was reported widely in the popular press, and was popularized in books and television programs. The remaining parts of the Taniyama–Shimura–Weil conjecture, now proven and known as the modularity theorem, were subsequently proved by other mathematicians, who built on Wiles's work between 1996 and 2001.[10][11][12] For his proof, Wiles was honoured and received numerous awards, including the 2016 Abel Prize.[13][14][15]\n\nThere are several alternative ways to state Fermat's Last Theorem that are mathematically equivalent to the original statement of the problem.\n\nIn order to state them, we use the following notations: let N be the set of natural numbers 1, 2, 3, ..., let Z be the set of integers 0, ±1, ±2, ..., and let Q be the set of rational numbers a/b, where a and b are in Z with b ≠ 0. In what follows we will call a solution to xn + yn = zn where one or more of x, y, or z is zero a trivial solution. A solution where all three are nonzero will be called a non-trivial solution.\n\nFor comparison's sake we start with the original formulation.\n\nMost popular treatments of the subject state it this way. It is also commonly stated over Z:[16]\n\nThe equivalence is clear if n is even. If n is odd and all three of x, y, z are negative, then we can replace x, y, z with −x, −y, −z to obtain a solution in N. If two of them are negative, it must be x and z or y and z. If x, z are negative and y is positive, then we can rearrange to get (−z)n + yn = (−x)n resulting in a solution in N; the other case is dealt with analogously. Now if just one is negative, it must be x or y. If x is negative, and y and z are positive, then it can be rearranged to get (−x)n + zn = yn again resulting in a solution in N; if y is negative, the result follows symmetrically. Thus in all cases a nontrivial solution in Z would also mean a solution exists in N, the original formulation of the problem.\n\nThis is because the exponents of x, y, and z are equal (to n), so if there is a solution in Q, then it can be multiplied through by an appropriate common denominator to get a solution in Z, and hence in N.\n\nA non-trivial solution a, b, c ∈ Z to xn + yn = zn yields the non-trivial solution a/c, b/c ∈ Q for vn + wn = 1. Conversely, a solution a/b, c/d ∈ Q to vn + wn = 1 yields the non-trivial solution ad, cb, bd for xn + yn = zn.\n\nThis last formulation is particularly fruitful, because it reduces the problem from a problem about surfaces in three dimensions to a problem about curves in two dimensions. Furthermore, it allows working over the field Q, rather than over the ring Z; fields exhibit more structure than rings, which allows for deeper analysis of their elements.\n\nExamining this elliptic curve with Ribet's theorem shows that it does not have a modular form. However, the proof by Andrew Wiles proves that any equation of the form y2 = x(x − an)(x + bn) does have a modular form. Any non-trivial solution to xp + yp = zp (with p an odd prime) would therefore create a contradiction, which in turn proves that no non-trivial solutions exist.[18]\n\nIn other words, any solution that could contradict Fermat's Last Theorem could also be used to contradict the modularity theorem. So if the modularity theorem were found to be true, then it would follow that no contradiction to Fermat's Last Theorem could exist either. As described above, the discovery of this equivalent statement was crucial to the eventual solution of Fermat's Last Theorem, as it provided a means by which it could be \"attacked\" for all numbers at once.\n\nIn ancient times it was known that a triangle whose sides were in the ratio 3:4:5 would have a right angle as one of its angles. This was used in construction and later in early geometry. It was also known to be one example of a general rule that any triangle where the length of two sides, each squared and then added together (32 + 42 = 9 + 16 = 25), equals the square of the length of the third side (52 = 25), would also be a right angle triangle. This is now known as the Pythagorean theorem, and a triple of numbers that meets this condition is called a Pythagorean triple; both are named after the ancient Greek Pythagoras. Examples include (3, 4, 5) and (5, 12, 13). There are infinitely many such triples,[19] and methods for generating such triples have been studied in many cultures, beginning with the Babylonians[20] and later ancient Greek, Chinese, and Indian mathematicians.[1] Mathematically, the definition of a Pythagorean triple is a set of three integers (a, b, c) that satisfy the equation[21] a2 + b2 = c2.\n\nFermat's equation, xn + yn = zn with positive integer solutions, is an example of a Diophantine equation,[22] named for the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:\n\nDiophantus's major work is the Arithmetica, of which only a portion has survived.[23] Fermat's conjecture of his Last Theorem was inspired while reading a new edition of the Arithmetica,[24] that was translated into Latin and published in 1621 by Claude Bachet.[25][26]\n\nDiophantine equations have been studied for thousands of years. For example, the solutions to the quadratic Diophantine equation x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC).[27] Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).[28]\nMany Diophantine equations have a form similar to the equation of Fermat's Last Theorem from the point of view of algebra, in that they have no cross terms mixing two letters, without sharing its particular properties. For example, it is known that there are infinitely many positive integers x, y, and z such that xn + yn = zm, where n and m are relatively prime natural numbers.[note 2]\n\nProblem II.8 of the Arithmetica asks how a given square number is split into two other squares; in other words, for a given rational number k, find rational numbers u and v such that k2 = u2 + v2. Diophantus shows how to solve this sum-of-squares problem for k = 4 (the solutions being u = 16/5 and v = 12/5).[29]\n\nAround 1637, Fermat wrote his Last Theorem in the margin of his copy of the Arithmetica next to Diophantus's sum-of-squares problem:[30][31][32]\n\nCubum autem in duos cubos, aut quadratoquadratum in duos quadratoquadratos & generaliter nullam in infinitum ultra quadratum potestatem in duos eiusdem nominis fas est dividere cuius rei demonstrationem mirabilem sane detexi. Hanc marginis exiguitas non caperet.\n\nIt is impossible to separate a cube into two cubes, or a fourth power into two fourth powers, or in general, any power higher than the second, into two like powers. I have discovered a truly marvelous proof of this, which this margin is too narrow to contain.[33][34]\n\nAfter Fermat's death in 1665, his son Clément-Samuel Fermat produced a new edition of the book (1670) augmented with his father's comments.[35] Although not actually a theorem at the time (meaning a mathematical statement for which proof exists), the marginal note became known over time as Fermat's Last Theorem,[30] as it was the last of Fermat's asserted theorems to remain unproved.[36][37]\n\nIt is not known whether Fermat had actually found a valid proof for all exponents n, but it appears unlikely. Only one related proof by him has survived, namely for the case n = 4, as described in the section § Proofs for specific exponents.\n\nWhile Fermat posed the cases of n = 4 and of n = 3 as challenges to his mathematical correspondents, such as Marin Mersenne, Blaise Pascal, and John Wallis,[38] he never posed the general case.[39] Moreover, in the last thirty years of his life, Fermat never again wrote of his \"truly marvelous proof\" of the general case, and never published it. Van der Poorten[39] suggests that while the absence of a proof is insignificant, the lack of challenges means Fermat realised he did not have a proof; he quotes Weil[40] as saying Fermat must have briefly deluded himself with an irretrievable idea. The techniques Fermat might have used in such a \"marvelous proof\" are unknown.\n\nWiles and Taylor's proof relies on 20th-century techniques.[41] Fermat's proof would have had to be elementary by comparison, given the mathematical knowledge of his time.\n\nWhile Harvey Friedman's grand conjecture implies that any provable theorem (including Fermat's last theorem) can be proved using only 'elementary function arithmetic', such a proof need be 'elementary' only in a technical sense and could involve millions of steps, and thus be far too long to have been Fermat's proof.\n\nOnly one relevant proof by Fermat has survived, in which he uses the technique of infinite descent to show that the area of a right triangle with integer sides can never equal the square of an integer.[42][43][44] His proof is equivalent to demonstrating that the equation\n\nhas no primitive solutions in integers (no pairwise coprime solutions). In turn, this proves Fermat's Last Theorem for the case n = 4, since the equation a4 + b4 = c4 can be written as c4 − b4 = (a2)2.\n\nAlternative proofs of the case n = 4 were developed later[45] by Frénicle de Bessy (1676),[46] Leonhard Euler (1738),[47] Kausler (1802),[48] Peter Barlow (1811),[49] Adrien-Marie Legendre (1830),[50] Schopis (1825),[51] Olry Terquem (1846),[52] Joseph Bertrand (1851),[53] Victor Lebesgue (1853, 1859, 1862),[54] Théophile Pépin (1883),[55] Tafelmacher (1893),[56] David Hilbert (1897),[57] Bendz (1901),[58] Gambioli (1901),[59] Leopold Kronecker (1901),[60] Bang (1905),[61] Sommer (1907),[62] Bottari (1908),[63] Karel Rychlík (1910),[64] Nutzhorn (1912),[65] Robert Carmichael (1913),[66] Hancock (1931),[67] Gheorghe Vrănceanu (1966),[68] Grant and Perella (1999),[69] Barbara (2007),[70] and Dolan (2011).[71]\n\nAfter Fermat proved the special case n = 4, the general proof for all n required only that the theorem be established for all odd prime exponents.[72] In other words, it was necessary to prove only that the equation an + bn = cn has no positive integer solutions (a, b, c) when n is an odd prime number. This follows because a solution (a, b, c) for a given n is equivalent to a solution for all the factors of n. For illustration, let n be factored into d and e, n = de. The general equation\n\nimplies that (ad, bd, cd) is a solution for the exponent e\n\nThus, to prove that Fermat's equation has no solutions for n > 2, it would suffice to prove that it has no solutions for at least one prime factor of every n. Each integer n > 2 is divisible by 4 or by an odd prime number (or both). Therefore, Fermat's Last Theorem could be proved for all n if it could be proved for n = 4 and for all odd primes p.\n\nIn the two centuries following its conjecture (1637–1839), Fermat's Last Theorem was proved for three odd prime exponents p = 3, 5 and 7. The case p = 3 was first stated by Abu-Mahmud Khojandi (10th century), but his attempted proof of the theorem was incorrect.[73][74] In 1770, Leonhard Euler gave a proof of p = 3,[75] but his proof by infinite descent[76] contained a major gap.[77][78][79] However, since Euler himself had proved the lemma necessary to complete the proof in other work, he is generally credited with the first proof.[44][80][81] Independent proofs were published[82] by Kausler (1802),[48] Legendre (1823, 1830),[50][83] Calzolari (1855),[84] Gabriel Lamé (1865),[85] Peter Guthrie Tait (1872),[86] Siegmund Günther (1878),[87] Gambioli (1901),[59] Krey (1909),[88] Rychlík (1910),[64] Stockhaus (1910),[89] Carmichael (1915),[90] Johannes van der Corput (1915),[91] Axel Thue (1917),[92] and Duarte (1944).[93]\n\nThe case p = 5 was proved[94] independently by Legendre and Peter Gustav Lejeune Dirichlet around 1825.[95][96][44][97] Alternative proofs were developed[98] by Carl Friedrich Gauss (1875, posthumous),[99] Lebesgue (1843),[100] Lamé (1847),[101] Gambioli (1901),[59][102] Werebrusow (1905),[103][full citation needed] Rychlík (1910),[104][dubious – discuss][full citation needed] van der Corput (1915),[91] and Guy Terjanian (1987).[105]\n\nThe case p = 7 was proved[106][107][44][97] by Lamé in 1839.[108] His rather complicated proof was simplified in 1840 by Lebesgue,[109] and still simpler proofs[110] were published by Angelo Genocchi in 1864, 1874 and 1876.[111] Alternative proofs were developed by Théophile Pépin (1876)[112] and Edmond Maillet (1897).[113]\n\nFermat's Last Theorem was also proved for the exponents n = 6, 10, and 14. Proofs for n = 6 were published by Kausler,[48] Thue,[114] Tafelmacher,[115] Lind,[116] Kapferer,[117] Swift,[118] and Breusch.[119] Similarly, Dirichlet[120] and Terjanian[121] each proved the case n = 14, while Kapferer[117] and Breusch[119] each proved the case n = 10. Strictly speaking, these proofs are unnecessary, since these cases follow from the proofs for n = 3, 5, and 7, respectively. Nevertheless, the reasoning of these even-exponent proofs differs from their odd-exponent counterparts. Dirichlet's proof for n = 14 was published in 1832, before Lamé's 1839 proof for n = 7.[122]\n\nAll proofs for specific exponents used Fermat's technique of infinite descent,[citation needed] either in its original form, or in the form of descent on elliptic curves or abelian varieties. The details and auxiliary arguments, however, were often ad hoc and tied to the individual exponent under consideration.[123] Since they became ever more complicated as p increased, it seemed unlikely that the general case of Fermat's Last Theorem could be proved by building upon the proofs for individual exponents.[123] Although some general results on Fermat's Last Theorem were published in the early 19th century by Niels Henrik Abel and Peter Barlow,[124][125] the first significant work on the general theorem was done by Sophie Germain.[126]\n\nIn the early 19th century, Sophie Germain developed several novel approaches to prove Fermat's Last Theorem for all exponents.[127] First, she defined a set of auxiliary primes θ constructed from the prime exponent p by the equation θ = 2hp + 1, where h is any integer not divisible by three. She showed that, if no integers raised to the pth power were adjacent modulo θ (the non-consecutivity condition), then θ must divide the product xyz. Her goal was to use mathematical induction to prove that, for any given p, infinitely many auxiliary primes θ satisfied the non-consecutivity condition and thus divided xyz; since the product xyz can have at most a finite number of prime factors, such a proof would have established Fermat's Last Theorem. Although she developed many techniques for establishing the non-consecutivity condition, she did not succeed in her strategic goal. She also worked to set lower limits on the size of solutions to Fermat's equation for a given exponent p, a modified version of which was published by Adrien-Marie Legendre. As a byproduct of this latter work, she proved Sophie Germain's theorem, which verified the first case of Fermat's Last Theorem (namely, the case in which p does not divide xyz) for every odd prime exponent less than 270,[127][128] and for all primes p such that at least one of 2p + 1, 4p + 1, 8p + 1, 10p + 1, 14p + 1 and 16p + 1 is prime (specially, the primes p such that 2p + 1 is prime are called Sophie Germain primes). Germain tried unsuccessfully to prove the first case of Fermat's Last Theorem for all even exponents, specifically for n = 2p, which was proved by Guy Terjanian in 1977.[129] In 1985, Leonard Adleman, Roger Heath-Brown and Étienne Fouvry proved that the first case of Fermat's Last Theorem holds for infinitely many odd primes p.[130]\n\nIn 1847, Gabriel Lamé outlined a proof of Fermat's Last Theorem based on factoring the equation xp + yp = zp in complex numbers, specifically the cyclotomic field based on the roots of the number 1. His proof failed, however, because it assumed incorrectly that such complex numbers can be factored uniquely into primes, similar to integers. This gap was pointed out immediately by Joseph Liouville, who later read a paper that demonstrated this failure of unique factorisation, written by Ernst Kummer.\n\nKummer set himself the task of determining whether the cyclotomic field could be generalized to include new prime numbers such that unique factorisation was restored. He succeeded in that task by developing the ideal numbers.\n\n(It is often stated that Kummer was led to his \"ideal complex numbers\" by his interest in Fermat's Last Theorem; there is even a story often told that Kummer, like Lamé, believed he had proven Fermat's Last Theorem until Lejeune Dirichlet told him his argument relied on unique factorization; but the story was first told by Kurt Hensel in 1910 and the evidence indicates it likely derives from a confusion by one of Hensel's sources. Harold Edwards said the belief that Kummer was mainly interested in Fermat's Last Theorem \"is surely mistaken\".[131] See the history of ideal numbers.)\n\nUsing the general approach outlined by Lamé, Kummer proved both cases of Fermat's Last Theorem for all regular prime numbers. However, he could not prove the theorem for the exceptional primes (irregular primes) that conjecturally occur approximately 39% of the time; the only irregular primes below 270 are 37, 59, 67, 101, 103, 131, 149, 157, 233, 257 and 263.\n\nIn the 1920s, Louis Mordell posed a conjecture that implied that Fermat's equation has at most a finite number of nontrivial primitive integer solutions, if the exponent n is greater than two.[132][133] This conjecture was proved in 1983 by Gerd Faltings,[134] and is now known as Faltings's theorem.\n\nIn the latter half of the 20th century, computational methods were used to extend Kummer's approach to the irregular primes. In 1954, Harry Vandiver used a SWAC computer to prove Fermat's Last Theorem for all primes up to 2521.[135] By 1978, Samuel Wagstaff had extended this to all primes less than 125,000.[136] By 1993, Fermat's Last Theorem had been proved for all primes less than four million.[5]\n\nHowever, despite these efforts and their results, no proof existed of Fermat's Last Theorem. Proofs of individual exponents by their nature could never prove the general case: even if all exponents were verified up to an extremely large number X, a higher exponent beyond X might still exist for which the claim was not true. (This had been the case with some other past conjectures, such as with Skewes' number, and it could not be ruled out in this conjecture.)\n\nThe strategy that ultimately led to a successful proof of Fermat's Last Theorem arose from the \"astounding\"[137]: 211  Taniyama–Shimura–Weil conjecture, proposed around 1955—which many mathematicians believed would be near to impossible to prove,[137]: 223  and was linked in the 1980s by Gerhard Frey, Jean-Pierre Serre and Ken Ribet to Fermat's equation. By accomplishing a partial proof of this conjecture in 1994, Andrew Wiles ultimately succeeded in proving Fermat's Last Theorem, as well as leading the way to a full proof by others of what is now known as the modularity theorem.\n\nAround 1955, Japanese mathematicians Goro Shimura and Yutaka Taniyama observed a possible link between two apparently completely distinct branches of mathematics, elliptic curves and modular forms. The resulting modularity theorem (at the time known as the Taniyama–Shimura conjecture) states that every elliptic curve is modular, meaning that it can be associated with a unique modular form.\n\nThe link was initially dismissed as unlikely or highly speculative, but was taken more seriously when number theorist André Weil found evidence supporting it, though not proving it; as a result the conjecture was often known as the Taniyama–Shimura–Weil conjecture.[137]: 211–215 \n\nEven after gaining serious attention, the conjecture was seen by contemporary mathematicians as extraordinarily difficult or perhaps inaccessible to proof.[137]: 203–205, 223, 226  For example, Wiles's doctoral supervisor John Coates states that it seemed \"impossible to actually prove\",[137]: 226  and Ken Ribet considered himself \"one of the vast majority of people who believed [it] was completely inaccessible\", adding that \"Andrew Wiles was probably one of the few people on earth who had the audacity to dream that you can actually go and prove [it].\"[137]: 223 \n\nIn 1984, Gerhard Frey noted a link between Fermat's equation and the modularity theorem, then still a conjecture. If Fermat's equation had any solution (a, b, c) for exponent p > 2, then it could be shown that the semi-stable elliptic curve (now known as a Frey-Hellegouarch[note 3])\n\nwould have such unusual properties that it was unlikely to be modular.[138] This would conflict with the modularity theorem, which asserted that all elliptic curves are modular. As such, Frey observed that a proof of the Taniyama–Shimura–Weil conjecture might also simultaneously prove Fermat's Last Theorem.[139][140] By contraposition, a disproof or refutation of Fermat's Last Theorem would disprove the Taniyama–Shimura–Weil conjecture.\n\nIn plain English, Frey had shown that, if this intuition about his equation was correct, then any set of four numbers (a, b, c, n) capable of disproving Fermat's Last Theorem, could also be used to disprove the Taniyama–Shimura–Weil conjecture. Therefore, if the latter were true, the former could not be disproven, and would also have to be true.\n\nFollowing this strategy, a proof of Fermat's Last Theorem required two steps. First, it was necessary to prove the modularity theorem, or at least to prove it for the types of elliptical curves that included Frey's equation (known as semistable elliptic curves). This was widely believed inaccessible to proof by contemporary mathematicians.[137]: 203–205, 223, 226  Second, it was necessary to show that Frey's intuition was correct: that if an elliptic curve were constructed in this way, using a set of numbers that were a solution of Fermat's equation, the resulting elliptic curve could not be modular. Frey showed that this was plausible but did not go as far as giving a full proof. The missing piece (the so-called \"epsilon conjecture\", now known as Ribet's theorem) was identified by Jean-Pierre Serre who also gave an almost-complete proof and the link suggested by Frey was finally proved in 1986 by Ken Ribet.[141]\n\nFollowing Frey, Serre and Ribet's work, this was where matters stood:\n\nRibet's proof of the epsilon conjecture in 1986 accomplished the first of the two goals proposed by Frey. Upon hearing of Ribet's success, Andrew Wiles, an English mathematician with a childhood fascination with Fermat's Last Theorem, and who had worked on elliptic curves, decided to commit himself to accomplishing the second half: proving a special case of the modularity theorem (then known as the Taniyama–Shimura conjecture) for semistable elliptic curves.[142][143]\n\nWiles worked on that task for six years in near-total secrecy, covering up his efforts by releasing prior work in small segments as separate papers and confiding only in his wife.[137]: 229–230  His initial study suggested proof by induction,[137]: 230–232, 249–252  and he based his initial work and first significant breakthrough on Galois theory[137]: 251–253, 259  before switching to an attempt to extend horizontal Iwasawa theory for the inductive argument around 1990–91 when it seemed that there was no existing approach adequate to the problem.[137]: 258–259  However, by mid-1991, Iwasawa theory also seemed to not be reaching the central issues in the problem.[137]: 259–260 [144][145] In response, he approached colleagues to seek out any hints of cutting-edge research and new techniques, and discovered an Euler system recently developed by Victor Kolyvagin and Matthias Flach that seemed \"tailor made\" for the inductive part of his proof.[137]: 260–261  Wiles studied and extended this approach, which worked. Since his work relied extensively on this approach, which was new to mathematics and to Wiles, in January 1993 he asked his Princeton colleague, Nick Katz, to help him check his reasoning for subtle errors. Their conclusion at the time was that the techniques Wiles used seemed to work correctly.[137]: 261–265 [146][147]\n\nBy mid-May 1993, Wiles was ready to tell his wife he thought he had solved the proof of Fermat's Last Theorem,[137]: 265  and by June he felt sufficiently confident to present his results in three lectures delivered on 21–23 June 1993 at the Isaac Newton Institute for Mathematical Sciences.[148][149] Specifically, Wiles presented his proof of the Taniyama–Shimura conjecture for semistable elliptic curves; together with Ribet's proof of the epsilon conjecture, this implied Fermat's Last Theorem. However, it became apparent during peer review that a critical point in the proof was incorrect. It contained an error in a bound on the order of a particular group. The error was caught by several mathematicians refereeing Wiles's manuscript including Katz (in his role as reviewer),[150] who alerted Wiles on 23 August 1993.[151]\n\nThe error would not have rendered his work worthless: each part of Wiles's work was highly significant and innovative by itself, as were the many developments and techniques he had created in the course of his work, and only one part was affected.[137]: 289, 296–297  However, without this part proved, there was no actual proof of Fermat's Last Theorem. Wiles spent almost a year trying to repair his proof, initially by himself and then in collaboration with his former student Richard Taylor, without success.[152][153][154] By the end of 1993, rumours had spread that under scrutiny, Wiles's proof had failed, but how seriously was not known. Mathematicians were beginning to pressure Wiles to disclose his work whether it was complete or not, so that the wider community could explore and use whatever he had managed to accomplish. But instead of being fixed, the problem, which had originally seemed minor, now seemed very significant, far more serious, and less easy to resolve.[155]\n\nWiles states that on the morning of 19 September 1994, he was on the verge of giving up and was almost resigned to accepting that he had failed, and to publishing his work so that others could build on it and fix the error. He adds that he was having a final look to try and understand the fundamental reasons why his approach could not be made to work, when he had a sudden insight: that the specific reason why the Kolyvagin–Flach approach would not work directly also meant that his original attempts using Iwasawa theory could be made to work, if he strengthened it using his experience gained from the Kolyvagin–Flach approach. Fixing one approach with tools from the other approach would resolve the issue for all the cases that were not already proven by his refereed paper.[152][156] He described later that Iwasawa theory and the Kolyvagin–Flach approach were each inadequate on their own, but together they could be made powerful enough to overcome this final hurdle.[152]\n\nI was sitting at my desk examining the Kolyvagin–Flach method. It wasn't that I believed I could make it work, but I thought that at least I could explain why it didn't work. Suddenly I had this incredible revelation. I realised that, the Kolyvagin–Flach method wasn't working, but it was all I needed to make my original Iwasawa theory work from three years earlier. So out of the ashes of Kolyvagin–Flach seemed to rise the true answer to the problem. It was so indescribably beautiful; it was so simple and so elegant. I couldn't understand how I'd missed it and I just stared at it in disbelief for twenty minutes. Then during the day I walked around the department, and I'd keep coming back to my desk looking to see if it was still there. It was still there. I couldn't contain myself, I was so excited. It was the most important moment of my working life. Nothing I ever do again will mean as much.\nOn 24 October 1994, Wiles submitted two manuscripts, \"Modular elliptic curves and Fermat's Last Theorem\"[158][159] and \"Ring theoretic properties of certain Hecke algebras\",[160] the second of which was co-authored with Taylor and proved that certain conditions were met that were needed to justify the corrected step in the main paper. The two papers were vetted and published as the entirety of the May 1995 issue of the Annals of Mathematics. The proof's method of identification of a deformation ring with a Hecke algebra (now referred to as an R=T theorem) to prove modularity lifting theorems has been an influential development in algebraic number theory.\n\nThese papers established the modularity theorem for semistable elliptic curves, the last step in proving Fermat's Last Theorem, 358 years after it was conjectured.\n\nThe full Taniyama–Shimura–Weil conjecture was finally proved by Diamond (1996),[10] Conrad et al. (1999),[11] and Breuil et al. (2001)[12] who, building on Wiles's work, incrementally chipped away at the remaining cases until the full result was proved. The now fully proved conjecture became known as the modularity theorem.\n\nSeveral other theorems in number theory similar to Fermat's Last Theorem also follow from the same reasoning, using the modularity theorem. For example: no cube can be written as a sum of two coprime nth powers, n ≥ 3. (The case n = 3 was already known by Euler.)\n\nFermat's Last Theorem considers solutions to the Fermat equation: an + bn = cn with positive integers a, b, and c and an integer n greater than 2. There are several generalizations of the Fermat equation to more general equations that allow the exponent n to be a negative integer or rational, or to consider three different exponents.\n\nThe generalized Fermat equation generalizes the statement of Fermat's last theorem by considering positive integer solutions a, b, c, m, n, k satisfying[161]\n\nIn particular, the exponents m, n, k need not be equal, whereas Fermat's last theorem considers the case m = n = k.\n\nThe Beal conjecture, also known as the Mauldin conjecture[162] and the Tijdeman-Zagier conjecture,[163][164][165] states that there are no solutions to the generalized Fermat equation in positive integers a, b, c, m, n, k with a, b, and c being pairwise coprime and all of m, n, k being greater than 2.[166]\n\nThe Fermat–Catalan conjecture generalizes Fermat's last theorem with the ideas of the Catalan conjecture.[167][168] The conjecture states that the generalized Fermat equation has only finitely many solutions (a, b, c, m, n, k) with distinct triplets of values (am, bn, ck), where a, b, c are positive coprime integers and m, n, k are positive integers satisfying\n\nThe statement is about the finiteness of the set of solutions because there are 10 known solutions.[161]\n\nWhen we allow the exponent n to be the reciprocal of an integer, i.e. n = 1/m for some integer m, we have the inverse Fermat equation a1/m + b1/m = c1/m.\nAll solutions of this equation were computed by Hendrik Lenstra in 1992.[169] In the case in which the mth roots are required to be real and positive, all solutions are given by[170]\n\nfor positive integers r, s, t with s and t coprime.\n\nFor the Diophantine equation an/m + bn/m = cn/m with n not equal to 1, Bennett, Glass, and Székely proved in 2004 for n > 2, that if n and m are coprime, then there are integer solutions if and only if 6 divides m, and a1/m, b1/m, and c1/m are different complex 6th roots of the same real number.[171]\n\nAll primitive integer solutions (i.e., those with no prime factor common to all of a, b, and c) to the optic equation a−1 + b−1 = c−1 can be written as[172]\n\nfor positive, coprime integers m, k.\n\nThe case n = −2 also has an infinitude of solutions, and these have a geometric interpretation in terms of right triangles with integer sides and an integer altitude to the hypotenuse.[173][174] All primitive solutions to a−2 + b−2 = d−2 are given by\n\nfor coprime integers u, v with v > u. The geometric interpretation is that a and b are the integer legs of a right triangle and d is the integer altitude to the hypotenuse. Then the hypotenuse itself is the integer\n\nso (a, b, c) is a Pythagorean triple.\n\nThere are no solutions in integers for an + bn = cn for integers n < −2. If there were, the equation could be multiplied through by a|n|b|n|c|n| to obtain (bc)|n| + (ac)|n| = (ab)|n|, which is impossible by Fermat's Last Theorem.\n\nThe abc conjecture roughly states that if three positive integers a, b and c (hence the name) are coprime and satisfy a + b = c, then the radical d of abc is usually not much smaller than c. In particular, the abc conjecture in its most standard formulation implies Fermat's last theorem for n that are sufficiently large.[175][176][177] The modified Szpiro conjecture is equivalent to the abc conjecture and therefore has the same implication.[178][177] An effective version of the abc conjecture, or an effective version of the modified Szpiro conjecture, implies Fermat's Last Theorem outright.[177]\n\nIn 1816, and again in 1850, the French Academy of Sciences offered a prize for a general proof of Fermat's Last Theorem.[179][180] In 1857, the academy awarded 3,000 francs and a gold medal to Kummer for his research on ideal numbers, although he had not submitted an entry for the prize.[179] Another prize was offered in 1883 by the Academy of Brussels.[181]\n\nIn 1908, the German industrialist and amateur mathematician Paul Wolfskehl bequeathed 100,000 gold marks—a large sum at the time—to the Göttingen Academy of Sciences to offer as a prize for a complete proof of Fermat's Last Theorem.[182][183] On 27 June 1908, the academy published nine rules for awarding the prize. Among other things, these rules required that the proof be published in a peer-reviewed journal; the prize would not be awarded until two years after the publication; and that no prize would be given after 13 September 2007, roughly a century after the competition was begun.[184] Wiles collected the Wolfskehl prize money, then worth $50,000, on 27 June 1997.[185] In March 2016, Wiles was awarded the Norwegian government's Abel prize worth €600,000 for \"his stunning proof of Fermat's Last Theorem by way of the modularity conjecture for semistable elliptic curves, opening a new era in number theory\".[186]\n\nPrior to Wiles's proof, thousands of incorrect proofs were submitted to the Wolfskehl committee, amounting to roughly 10 feet (3.0 meters) of correspondence.[187] In the first year alone (1907–1908), 621 attempted proofs were submitted, although by the 1970s, the rate of submission had decreased to roughly 3–4 attempted proofs per month. According to some claims, Edmund Landau tended to use a special preprinted form for such proofs, where the location of the first mistake was left blank to be filled by one of his graduate students.[188] According to F. Schlichting, a Wolfskehl reviewer, most of the proofs were based on elementary methods taught in schools, and often submitted by \"people with a technical education but a failed career\".[189] In the words of mathematical historian Howard Eves, \"Fermat's Last Theorem has the peculiar distinction of being the mathematical problem for which the greatest number of incorrect proofs have been published.\"[181]\n\n\nThe popularity of the theorem outside science has led to it being described as achieving \"that rarest of mathematical accolades: A niche role in pop culture.\"[190]\nArthur Porges' 1954 short story \"The Devil and Simon Flagg\" features a mathematician who bargains with the Devil that the latter cannot produce a proof of Fermat's Last Theorem within twenty-four hours.[191]\n\nIn The Simpsons episode \"The Wizard of Evergreen Terrace\", Homer Simpson writes the equation 398712 + 436512 = 447212 on a blackboard, which appears to be a counterexample to Fermat's Last Theorem. The equation is wrong, but it appears to be correct if entered in a calculator with 10 significant figures.[192]\n\nIn the Star Trek: The Next Generation episode \"The Royale\", Captain Picard states that the theorem is still unproven in the 24th century. The proof was released five years after the episode originally aired.[193]\n"
    },
    {
        "title": "Wiles's proof of Fermat's Last Theorem",
        "content": "\n\nWiles's proof of Fermat's Last Theorem is a proof by British mathematician Sir Andrew Wiles of a special case of the modularity theorem for elliptic curves. Together with Ribet's theorem, it provides a proof for Fermat's Last Theorem. Both Fermat's Last Theorem and the modularity theorem were believed to be impossible to prove using previous knowledge by almost all living mathematicians at the time.[1]: 203–205, 223, 226 \n\nWiles first announced his proof on 23 June 1993 at a lecture in Cambridge entitled \"Modular Forms, Elliptic Curves and Galois Representations\".[2] However, in September 1993 the proof was found to contain an error. One year later on 19 September 1994, in what he would call \"the most important moment of [his] working life\", Wiles stumbled upon a revelation that allowed him to correct the proof to the satisfaction of the mathematical community. The corrected proof was published in 1995.[3]\n\nWiles's proof uses many techniques from algebraic geometry and number theory and has many ramifications in these branches of mathematics. It also uses standard constructions of modern algebraic geometry such as the category of schemes, significant number theoretic ideas from Iwasawa theory, and other 20th-century techniques which were not available to Fermat. The proof's method of identification of a deformation ring with a Hecke algebra (now referred to as an R=T theorem) to prove modularity lifting theorems has been an influential development in algebraic number theory.\n\nTogether, the two papers which contain the proof are 129 pages long[4][5] and consumed over seven years of Wiles's research time. John Coates described the proof as one of the highest achievements of number theory, and John Conway called it \"the proof of the [20th] century.\"[6] Wiles's path to proving Fermat's Last Theorem, by way of proving the modularity theorem for the special case of semistable elliptic curves, established powerful modularity lifting techniques and opened up entire new approaches to numerous other problems. For proving Fermat's Last Theorem, he was knighted, and received other honours such as the 2016 Abel Prize. When announcing that Wiles had won the Abel Prize, the Norwegian Academy of Science and Letters described his achievement as a \"stunning proof\".[3]\n\nFermat's Last Theorem, formulated in 1637, states that no three positive integers a, b, and c can satisfy the equation\n\nif n is an integer greater than two (n > 2).\n\nOver time, this simple assertion became one of the most famous unproved claims in mathematics.  Between its publication and Andrew Wiles's eventual solution over 350 years later, many mathematicians and amateurs attempted to prove this statement, either for all values of n > 2, or for specific cases. It spurred the development of entire new areas within number theory. Proofs were eventually found for all values of n up to around 4 million, first by hand, and later by computer. However, no general proof was found that would be valid for all possible values of n, nor even a hint how such a proof could be undertaken.\n\nSeparately from anything related to Fermat's Last Theorem, in the 1950s and 1960s Japanese mathematician Goro Shimura, drawing on ideas posed by Yutaka Taniyama, conjectured that a connection might exist between elliptic curves and modular forms. These were mathematical objects with no known connection between them. Taniyama and Shimura posed the question whether, unknown to mathematicians, the two kinds of object were actually identical mathematical objects, just seen in different ways.\n\nThey conjectured that every rational elliptic curve is also modular. This became known as the Taniyama–Shimura conjecture. In the West, this conjecture became well known through a 1967 paper by André Weil, who gave conceptual evidence for it; thus, it is sometimes called the Taniyama–Shimura–Weil conjecture.\n\nBy around 1980, much evidence had been accumulated to form conjectures about elliptic curves, and many papers had been written which examined the consequences if the conjecture were true, but the actual conjecture itself was unproven and generally considered inaccessible—meaning that mathematicians believed a proof of the conjecture was probably impossible using current knowledge.\n\nFor decades, the conjecture remained an important but unsolved problem in mathematics. Around 50 years after first being proposed, the conjecture was finally proven and renamed the modularity theorem, largely as a result of Andrew Wiles's work described below.\n\nOn yet another separate branch of development, in the late 1960s, Yves Hellegouarch came up with the idea of associating hypothetical solutions (a, b, c) of Fermat's equation with a completely different mathematical object: an elliptic curve.[7] The curve consists of all points in the plane whose coordinates (x, y) satisfy the relation\n\nSuch an elliptic curve would enjoy very special properties due to the appearance of high powers of integers in its equation and the fact that an + bn = cn would be an nth power as well.\n\nIn 1982–1985, Gerhard Frey called attention to the unusual properties of this same curve, now called a Frey curve. He showed that it was likely that the curve could link Fermat and Taniyama, since any counterexample to Fermat's Last Theorem would probably also imply that an elliptic curve existed that was not modular. Frey showed that there were good reasons to believe that any set of numbers (a, b, c, n) capable of disproving Fermat's Last Theorem could also probably be used to disprove the Taniyama–Shimura–Weil conjecture. Therefore, if the Taniyama–Shimura–Weil conjecture were true, no set of numbers capable of disproving Fermat could exist, so Fermat's Last Theorem would have to be true as well.\n\nThe conjecture says that each elliptic curve with rational coefficients can be constructed in an entirely different way, not by giving its equation but by using modular functions to parametrise coordinates x and y of the points on it. Thus, according to the conjecture, any elliptic curve over Q would have to be a modular elliptic curve, yet if a solution to Fermat's equation with non-zero a, b, c and n greater than 2 existed, the corresponding curve would not be modular, resulting in a contradiction. If the link identified by Frey could be proven, then in turn, it would mean that a disproof of Fermat's Last Theorem would disprove the Taniyama–Shimura–Weil conjecture, or by contraposition, a proof of the latter would prove the former as well.[8]\n\nTo complete this link,  it was necessary to show that Frey's intuition was correct: that a Frey curve, if it existed, could not be modular. In 1985, Jean-Pierre Serre provided a partial proof that a Frey curve could not be modular. Serre did not provide a complete proof of his proposal; the missing part (which Serre had noticed early on[9]: 1 ) became known as the epsilon conjecture (sometimes written ε-conjecture; now known as Ribet's theorem). Serre's main interest was in an even more ambitious conjecture, Serre's conjecture on modular Galois representations, which would imply the Taniyama–Shimura–Weil conjecture. However his partial proof came close to confirming the link between Fermat and Taniyama.\n\nIn the summer of 1986, Ken Ribet succeeded in proving the epsilon conjecture, now known as Ribet's theorem.  His article was published in 1990. In doing so, Ribet finally proved the link between the two theorems by confirming, as Frey had suggested, that a proof of the Taniyama–Shimura–Weil conjecture for the kinds of elliptic curves Frey had identified, together with Ribet's theorem, would also prove Fermat's Last Theorem.\n\nIn mathematical terms, Ribet's theorem showed that if the Galois representation associated with an elliptic curve has certain properties (which Frey's curve has), then that curve cannot be modular, in the sense that there cannot exist a modular form which gives rise to the same Galois representation.[10]\n\nFollowing the developments related to the Frey curve, and its link to both Fermat and Taniyama, a proof of Fermat's Last Theorem would follow from a proof of the Taniyama–Shimura–Weil conjecture—or at least a proof of the conjecture for the kinds of elliptic curves that included Frey's equation (known as semistable elliptic curves).\n\nHowever, despite the progress made by Serre and Ribet, this approach to Fermat was widely considered unusable as well, since almost all mathematicians saw the Taniyama–Shimura–Weil conjecture itself as completely inaccessible to proof with current knowledge.[1]: 203–205, 223, 226  For example, Wiles's ex-supervisor John Coates stated that it seemed \"impossible to actually prove\",[1]: 226  and Ken Ribet considered himself \"one of the vast majority of people who believed [it] was completely inaccessible\".[1]: 223 \n\nHearing of Ribet's 1986 proof of the epsilon conjecture, English mathematician Andrew Wiles, who had studied elliptic curves and had a childhood fascination with Fermat, decided to begin working in secret towards a proof of the Taniyama–Shimura–Weil conjecture, since it was now professionally justifiable,[11] as well as because of the enticing goal of proving such a long-standing problem.\n\nRibet later commented that \"Andrew Wiles was probably one of the few people on earth who had the audacity to dream that you can actually go and prove [it].\"[1]: 223 \n\nWiles initially presented his proof in 1993. It was finally accepted as correct, and published, in 1995, following the correction of a subtle error in one part of his original paper. His work was extended to a full proof of the modularity theorem over the following six years by others, who built on Wiles's work.\n\nDuring 21–23 June 1993, Wiles announced and presented his proof of the Taniyama–Shimura conjecture for semistable elliptic curves, and hence of Fermat's Last Theorem, over the course of three lectures delivered at the Isaac Newton Institute for Mathematical Sciences in Cambridge, England.[2] There was a relatively large amount of press coverage afterwards.[12]\n\nAfter the announcement, Nick Katz was appointed as one of the referees to review Wiles's manuscript. In the course of his review, he asked Wiles a series of clarifying questions that led Wiles to recognise that the proof contained a gap. There was an error in one critical portion of the proof which gave a bound for the order of a particular group: the Euler system used to extend Kolyvagin and Flach's method was incomplete. The error would not have rendered his work worthless—each part of Wiles's work was highly significant and innovative by itself, as were the many developments and techniques he had created in the course of his work, and only one part was affected.[1]: 289, 296–297  Without this part proved, however, there was no actual proof of Fermat's Last Theorem.\n\nWiles spent almost a year trying to repair his proof, initially by himself and then in collaboration with his former student Richard Taylor, without success.[13][14][15] By the end of 1993, rumours had spread that under scrutiny, Wiles's proof had failed, but how seriously was not known. Mathematicians were beginning to pressure Wiles to disclose his work whether or not complete, so that the wider community could explore and use whatever he had managed to accomplish. Instead of being fixed, the problem, which had originally seemed minor, now seemed very significant, far more serious, and less easy to resolve.[16]\n\nWiles states that on the morning of 19 September 1994, he was on the verge of giving up and was almost resigned to accepting that he had failed, and to publishing his work so that others could build on it and find the error. He states that he was having a final look to try to understand the fundamental reasons why his approach could not be made to work, when he had a sudden insight that the specific reason why the Kolyvagin–Flach approach would not work directly also meant that his original attempt using Iwasawa theory could be made to work if he strengthened it using experience gained from the Kolyvagin–Flach approach since then. Each was inadequate by itself, but fixing one approach with tools from the other would resolve the issue and produce a class number formula (CNF) valid for all cases that were not already proven by his refereed paper:[13][17]\n\nI was sitting at my desk examining the Kolyvagin–Flach method. It wasn't that I believed I could make it work, but I thought that at least I could explain why it didn't work. Suddenly I had this incredible revelation. I realised that, the Kolyvagin–Flach method wasn't working, but it was all I needed to make my original Iwasawa theory work from three years earlier. So out of the ashes of Kolyvagin–Flach seemed to rise the true answer to the problem. It was so indescribably beautiful; it was so simple and so elegant. I couldn't understand how I'd missed it and I just stared at it in disbelief for twenty minutes. Then during the day I walked around the department, and I'd keep coming back to my desk looking to see if it was still there. It was still there. I couldn't contain myself, I was so excited. It was the most important moment of my working life. Nothing I ever do again will mean as much.\nOn 6 October Wiles asked three colleagues (including Gerd Faltings) to review his new proof,[19] and on 24 October 1994 Wiles submitted two manuscripts, \"Modular elliptic curves and Fermat's Last Theorem\"[4] and \"Ring theoretic properties of certain Hecke algebras\",[5] the second of which Wiles had written with Taylor and proved that certain conditions were met which were needed to justify the corrected step in the main paper.\n\nThe two papers were vetted and finally published as the entirety of the May 1995 issue of the Annals of Mathematics. The new proof was widely analysed and became accepted as likely correct in its major components.[6][10][11] These papers established the modularity theorem for semistable elliptic curves, the last step in proving Fermat's Last Theorem, 358 years after it was conjectured.\n\nFermat claimed to \"... have discovered a truly marvelous proof of this, which this margin is too narrow to contain\".[20][21] Wiles's proof is very complex, and incorporates the work of so many other specialists that it was suggested in 1994 that only a small number of people were capable of fully understanding at that time all the details of what he had done.[2][22] The complexity of Wiles's proof motivated a 10-day conference at Boston University; the resulting book of conference proceedings aimed to make the full range of required topics accessible to graduate students in number theory.[9]\n\nAs noted above, Wiles proved the Taniyama–Shimura–Weil conjecture for the special case of semistable elliptic curves, rather than for all elliptic curves. Over the following years, Christophe Breuil, Brian Conrad, Fred Diamond, and Richard Taylor (sometimes abbreviated as \"BCDT\")  carried the work further, ultimately proving the Taniyama–Shimura–Weil conjecture for all elliptic curves in a 2001 paper.[23] Now proven, the conjecture became known as the modularity theorem.\n\nIn 2005, Dutch computer scientist Jan Bergstra posed the problem of formalizing Wiles's proof in such a way that it could be verified by computer.[24]\n\n\n\nWiles proved the modularity theorem for semistable elliptic curves, from which Fermat’s last theorem follows using proof by contradiction. In this proof method, one assumes the opposite of what is to be proved, and shows if that were true, it would create a contradiction. The contradiction shows that the assumption (that the conclusion is wrong) must have been incorrect, requiring the conclusion to hold.\n\nThe proof falls roughly in two parts: In the first part, Wiles proves a general result about \"lifts\", known as the \"modularity lifting theorem\". This first part allows him to prove results about elliptic curves by converting them to problems about Galois representations of elliptic curves.  He then uses this result to prove that all semistable curves are modular, by proving that the Galois representations of these curves are modular.\n\nWiles aims first of all to prove a result about these representations, that he will use later: that if a semistable elliptic curve E has a Galois representation ρ(E, p) that is modular, the elliptic curve itself must be modular.\n\nProving this is helpful in two ways: it makes counting and matching easier, and, significantly, to prove the representation is modular, we would only have to prove it for one single prime number p, and we can do this using any prime that makes our work easy – it does not matter which prime we use.\n\nThis is the most difficult part of the problem – technically it means proving that if the Galois representation ρ(E, p) is a modular form, so are all the other related Galois representations ρ(E, p∞) for all powers of p.[3]  This is the so-called \"modular lifting problem\", and Wiles approached it using deformations.\n\nTogether, these allow us to work with representations of curves rather than directly with elliptic curves themselves. Our original goal will have been transformed into proving the modularity of geometric Galois representations of semistable elliptic curves, instead. Wiles described this realization as a \"key breakthrough\".\n\nA Galois representation of an elliptic curve is G → GL(Zp).  To show that a geometric Galois representation of an elliptic curve is a modular form, we need to find a normalized eigenform whose eigenvalues (which are also its Fourier series coefficients) satisfy a congruence relationship for all but a finite number of primes.\n\nThis is Wiles's lifting theorem (or modularity lifting theorem), a major and revolutionary accomplishment at the time. \n\nSo we can try to prove all of our elliptic curves are modular by using one prime number as p - but if we do not succeed in proving this for all elliptic curves, perhaps we can prove the rest by choosing different prime numbers as 'p' for the difficult cases.\n\nThe proof must cover the Galois representations of all semistable elliptic curves E, but for each individual curve, we only need to prove it is modular using one prime number p.)\n\nFrom above, it does not matter which prime is chosen for the representations. We can use any one prime number that is easiest. 3 is the smallest prime number more than 2, and some work has already been done on representations of elliptic curves using ρ(E, 3), so choosing 3 as our prime number is a helpful starting point.\n\nWiles found that it was easier to prove the representation was modular by choosing a prime p = 3 in the cases where the representation ρ(E, 3) is irreducible, but the proof when ρ(E, 3) is reducible was easier to prove by choosing p = 5. So, the proof splits in two at this point. \n\nThe switch between p = 3 and p = 5 has since opened a significant area of study in its own right (see Serre's modularity conjecture).\n\nWiles uses his modularity lifting theorem to make short work of this case:  \n\nThis existing result for p = 3 is crucial to Wiles's approach and is one reason for initially using p = 3.\n\nWiles found that when the representation of an elliptic curve using p = 3 is reducible, it was easier to work with p = 5 and use his new lifting theorem to prove that ρ(E, 5) will always be modular, than to try and prove directly that ρ(E, 3) itself is modular (remembering that we only need to prove it for one prime).\n\nWiles showed that in this case, one could always find another semistable elliptic curve F such that the representation ρ(F, 3) is irreducible and also the representations ρ(E, 5) and ρ(F, 5) are isomorphic (they have identical structures).\n\nThis proves:\n\nWiles opted to attempt to match elliptic curves to a countable set of modular forms. He found that this direct approach was not working, so he transformed the problem by instead matching the Galois representations of the elliptic curves to modular forms. Wiles denotes this matching (or mapping) that, more specifically, is a ring homomorphism:\n\n\n\n\n\nR\n\n\n{\\displaystyle R}\n\n is a deformation ring and \n\n\n\n\nT\n\n\n\n{\\displaystyle \\mathbf {T} }\n\n is a Hecke ring.\n\nWiles had the insight that in many cases this ring homomorphism could be a ring isomorphism (Conjecture 2.16 in Chapter 2, §3 of the 1995 paper[4]). He realised that the map between \n\n\n\nR\n\n\n{\\displaystyle R}\n\n and \n\n\n\n\nT\n\n\n\n{\\displaystyle \\mathbf {T} }\n\n is an isomorphism if and only if two abelian groups occurring in the theory are finite and have the same cardinality. This is sometimes referred to as the \"numerical criterion\". Given this result, Fermat's Last Theorem is reduced to the statement that two groups have the same order. Much of the text of the proof leads into topics and theorems related to ring theory and commutation theory. Wiles's goal was to verify that the map \n\n\n\nR\n→\n\nT\n\n\n\n{\\displaystyle R\\rightarrow \\mathbf {T} }\n\n is an isomorphism and ultimately that \n\n\n\nR\n=\n\nT\n\n\n\n{\\displaystyle R=\\mathbf {T} }\n\n. In treating deformations, Wiles defined four cases, with the flat deformation case requiring more effort to prove and treated in a separate article in the same volume entitled \"Ring-theoretic properties of certain Hecke algebras\".\n\nGerd Faltings, in his bulletin, gives the following commutative diagram (p. 745):\n\nor ultimately that \n\n\n\nR\n=\n\nT\n\n\n\n{\\displaystyle R=\\mathbf {T} }\n\n, indicating a complete intersection. Since Wiles could not show that \n\n\n\nR\n=\n\nT\n\n\n\n{\\displaystyle R=\\mathbf {T} }\n\n directly, he did so through \n\n\n\n\n\nZ\n\n\n3\n\n\n,\n\n\nF\n\n\n3\n\n\n\n\n{\\displaystyle \\mathbf {Z} _{3},\\mathbf {F} _{3}}\n\n and \n\n\n\n\nT\n\n\n/\n\n\n\nm\n\n\n\n\n{\\displaystyle \\mathbf {T} /{\\mathfrak {m}}}\n\n via lifts.\n\nIn order to perform this matching, Wiles had to create a class number formula (CNF). He first attempted to use horizontal Iwasawa theory but that part of his work had an unresolved issue such that he could not create a CNF. At the end of the summer of 1991, he learned about an Euler system recently developed by Victor Kolyvagin and Matthias Flach that seemed \"tailor made\" for the inductive part of his proof, which could be used to create a CNF, and so Wiles set his Iwasawa work aside and began working to extend Kolyvagin and Flach's work instead, in order to create the CNF his proof would require.[25] By the spring of 1993, his work had covered all but a few families of elliptic curves, and in early 1993, Wiles was confident enough of his nearing success to let one trusted colleague into his secret. Since his work relied extensively on using the Kolyvagin–Flach approach, which was new to mathematics and to Wiles, and which he had also extended, in January 1993 he asked his Princeton colleague, Nick Katz, to help him review his work for subtle errors. Their conclusion at the time was that the techniques Wiles used seemed to work correctly.[1]: 261–265 [26]\n\nWiles's use of Kolyvagin–Flach would later be found to be the point of failure in the original proof submission, and he eventually had to revert to Iwasawa theory and a collaboration with Richard Taylor to fix it. In May 1993, while reading a paper by Mazur, Wiles had the insight that the 3/5 switch would resolve the final issues and would then cover all elliptic curves.\n\nGiven an elliptic curve \n\n\n\nE\n\n\n{\\displaystyle E}\n\n over the field \n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbf {Q} }\n\n of rational numbers \n\n\n\nE\n(\n\n\n\n\nQ\n\n¯\n\n\n\n)\n\n\n{\\displaystyle E({\\bar {\\mathbf {Q} }})}\n\n, for every prime power \n\n\n\n\nℓ\n\nn\n\n\n\n\n{\\displaystyle \\ell ^{n}}\n\n, there exists a homomorphism from the absolute Galois group\n\nto\n\nthe group of invertible 2 by 2 matrices whose entries are integers modulo \n\n\n\n\nℓ\n\nn\n\n\n\n\n{\\displaystyle \\ell ^{n}}\n\n.  This is because \n\n\n\nE\n(\n\n\n\n\nQ\n\n¯\n\n\n\n)\n\n\n{\\displaystyle E({\\bar {\\mathbf {Q} }})}\n\n, the points of \n\n\n\nE\n\n\n{\\displaystyle E}\n\n over \n\n\n\n\n\n\n\nQ\n\n¯\n\n\n\n\n\n{\\displaystyle {\\bar {\\mathbf {Q} }}}\n\n, form an abelian group on which \n\n\n\nGal\n⁡\n(\n\n\n\n\nQ\n\n¯\n\n\n\n\n/\n\n\nQ\n\n)\n\n\n{\\displaystyle \\operatorname {Gal} ({\\bar {\\mathbf {Q} }}/\\mathbf {Q} )}\n\n acts; the subgroup of elements \n\n\n\nx\n\n\n{\\displaystyle x}\n\n such that \n\n\n\n\nℓ\n\nn\n\n\nx\n=\n0\n\n\n{\\displaystyle \\ell ^{n}x=0}\n\n is just \n\n\n\n(\n\nZ\n\n\n/\n\n\nℓ\n\nn\n\n\n\nZ\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle (\\mathbf {Z} /\\ell ^{n}\\mathbf {Z} )^{2}}\n\n, and an automorphism of this group is a matrix of the type described.\n\nLess obvious is that given a modular form of a certain special type, a Hecke eigenform with eigenvalues in \n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbf {Q} }\n\n, one also gets a homomorphism\n\nThis goes back to Eichler and Shimura.  The idea is that the Galois group acts first on the modular curve on which the modular form is defined, thence on the Jacobian variety of the curve, and finally on the points of \n\n\n\n\nℓ\n\nn\n\n\n\n\n{\\displaystyle \\ell ^{n}}\n\n power order on that Jacobian.  The resulting representation is not usually 2-dimensional, but the Hecke operators cut out a 2-dimensional piece.  It is easy to demonstrate that these representations come from some elliptic curve but the converse is the difficult part to prove.\n\nInstead of trying to go directly from the elliptic curve to the modular form, one can first pass to the \n\n\n\n\nmod\n\n\nℓ\n\nn\n\n\n\n\n\n\n{\\displaystyle {\\bmod {\\ell ^{n}}}}\n\n representation for some \n\n\n\nℓ\n\n\n{\\displaystyle \\ell }\n\n and \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, and from that to the modular form.  In the case where \n\n\n\nℓ\n=\n3\n\n\n{\\displaystyle \\ell =3}\n\n and \n\n\n\nn\n=\n1\n\n\n{\\displaystyle n=1}\n\n, results of the Langlands–Tunnell theorem show that the \n\n\n\n\nmod\n\n3\n\n\n\n\n{\\displaystyle {\\bmod {3}}}\n\n representation of any elliptic curve over \n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbf {Q} }\n\n comes from a modular form.  The basic strategy is to use induction on \n\n\n\nn\n\n\n{\\displaystyle n}\n\n to show that this is true for \n\n\n\nℓ\n=\n3\n\n\n{\\displaystyle \\ell =3}\n\n and any \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, that ultimately there is a single modular form that works for all n.  To do this, one uses a counting argument, comparing the number of ways in which one can lift a \n\n\n\n\nmod\n\n\nℓ\n\nn\n\n\n\n\n\n\n{\\displaystyle {\\bmod {\\ell ^{n}}}}\n\n Galois representation to one \n\n\n\n\nmod\n\n\nℓ\n\nn\n+\n1\n\n\n\n\n\n\n{\\displaystyle {\\bmod {\\ell ^{n+1}}}}\n\n and the number of ways in which one can lift a \n\n\n\n\nmod\n\n\nℓ\n\nn\n\n\n\n\n\n\n{\\displaystyle {\\bmod {\\ell ^{n}}}}\n\n modular form.  An essential point is to impose a sufficient set of conditions on the Galois representation; otherwise, there will be too many lifts and most will not be modular.  These conditions should be satisfied for the representations coming from modular forms and those coming from elliptic curves.\n\nIf the original \n\n\n\n(\n\nm\no\nd\n\n\n3\n)\n\n\n{\\displaystyle (\\mathrm {mod} \\,3)}\n\n representation has an image which is too small, one runs into trouble with the lifting argument, and in this case, there is a final trick which has since been studied in greater generality in the subsequent work on the Serre modularity conjecture. The idea involves the interplay between the \n\n\n\n(\n\nm\no\nd\n\n\n3\n)\n\n\n{\\displaystyle (\\mathrm {mod} \\,3)}\n\n and \n\n\n\n(\n\nm\no\nd\n\n\n5\n)\n\n\n{\\displaystyle (\\mathrm {mod} \\,5)}\n\n representations. In particular, if the mod-5 Galois representation \n\n\n\n\n\n\nρ\n¯\n\n\n\nE\n,\n5\n\n\n\n\n{\\displaystyle {\\overline {\\rho }}_{E,5}}\n\n associated to an semistable elliptic curve E over Q is irreducible, then there is another semistable elliptic curve E' over Q such that its associated mod-5 Galois representation \n\n\n\n\n\n\nρ\n¯\n\n\n\n\nE\n′\n\n,\n5\n\n\n\n\n{\\displaystyle {\\overline {\\rho }}_{E',5}}\n\n is isomorphic to \n\n\n\n\n\n\nρ\n¯\n\n\n\nE\n,\n5\n\n\n\n\n{\\displaystyle {\\overline {\\rho }}_{E,5}}\n\n and such that its associated mod-3 Galois representation \n\n\n\n\n\n\nρ\n¯\n\n\n\nE\n,\n3\n\n\n\n\n{\\displaystyle {\\overline {\\rho }}_{E,3}}\n\n is irreducible (and therefore modular by Langlands–Tunnell).[27]\n\nIn his 108-page article published in 1995, Wiles divides the subject matter up into the following chapters (preceded here by page numbers):\n\nGerd Faltings subsequently provided some simplifications to the 1995 proof, primarily in switching from geometric constructions to rather simpler algebraic ones.[19][28]  The book of the Cornell conference also contained simplifications to the original proof.[9]\n\nWiles's paper is over 100 pages long and often uses the specialised symbols and notations of group theory, algebraic geometry, commutative algebra, and Galois theory. The mathematicians who helped to lay the groundwork for Wiles often created new specialised concepts and technical jargon.\n\nAmong the introductory presentations are an email which Ribet sent in 1993;[29][30] Hesselink's quick review of top-level issues, which gives just the elementary algebra and avoids abstract algebra;[24] or Daney's web page, which provides a set of his own notes and lists the current books available on the subject. Weston attempts to provide a handy map of some of the relationships between the subjects.[31] F. Q. Gouvêa's 1994 article \"A Marvelous Proof\", which reviews some of the required topics, won a Lester R. Ford award from the Mathematical Association of America.[32][33] Faltings' 5-page technical bulletin on the matter is a quick and technical review of the proof for the non-specialist.[34] For those in search of a commercially available book to guide them, he recommended that those familiar with abstract algebra read Hellegouarch, then read the Cornell book,[9] which is claimed to be accessible to \"a graduate student in number theory\". The Cornell book does not cover the entirety of the Wiles proof.[12]\n"
    },
    {
        "title": "Andrew Wiles",
        "content": "\n\nSir Andrew John Wiles (born 11 April 1953) is an English mathematician and a Royal Society Research Professor at the University of Oxford, specialising in number theory. He is best known for proving Fermat's Last Theorem, for which he was awarded the 2016 Abel Prize and the 2017 Copley Medal and for which he was appointed a Knight Commander of the Order of the British Empire in 2000.[1] In 2018, Wiles was appointed the first Regius Professor of Mathematics at Oxford.[4] Wiles is also a 1997 MacArthur Fellow.\n\nWiles was born in Cambridge to theologian Maurice Frank Wiles and Patricia Wiles. While spending much of his childhood in Nigeria, Wiles developed an interest in mathematics and in Fermat's Last Theorem in particular. After moving to Oxford and graduating from there in 1974, he worked on unifying Galois representations, elliptic curves and modular forms, starting with Barry Mazur's generalizations of Iwasawa theory. In the early 1980s, Wiles spent a few years at the University of Cambridge before moving to Princeton University, where he worked on expanding out and applying Hilbert modular forms. In 1986, upon reading Ken Ribet's seminal work on Fermat's Last Theorem, Wiles set out to prove the modularity theorem for semistable elliptic curves, which implied Fermat's Last Theorem. By 1993, he had been able to convince a knowledgeable colleague that he had a proof of Fermat's Last Theorem, though a flaw was subsequently discovered. After an insight on 19 September 1994, Wiles and his student Richard Taylor were able to circumvent the flaw, and published the results in 1995, to widespread acclaim.\n\nIn proving Fermat's Last Theorem, Wiles developed new tools for mathematicians to begin unifying disparate ideas and theorems. His former student Taylor along with three other mathematicians were able to prove the full modularity theorem by 2000, using Wiles' work. Upon receiving the Abel Prize in 2016, Wiles reflected on his legacy, expressing his belief that he did not just prove Fermat's Last Theorem, but pushed the whole of mathematics as a field towards the Langlands program of unifying number theory.[5]\n\nWiles was born on 11 April 1953 in Cambridge, England, the son of Maurice Frank Wiles (1923–2005) and Patricia Wiles (née Mowll). From 1952 to 1955, his father worked as the chaplain at Ridley Hall, Cambridge, and later became the Regius Professor of Divinity at the University of Oxford.[6]\n\nWiles began his formal schooling in Nigeria, while living there as a very young boy with his parents. However, according to letters written by his parents, for at least the first several months after he was supposed to be attending classes, he refused to go. From that fact, Wiles himself concluded that in his earliest years, he was not enthusiastic about spending time in academic institutions. In an interview with Nadia Hasnaoui in 2021, he said he trusted the letters, yet he could not remember a time when he did not enjoy solving mathematical problems.[7]\n\nWiles attended King's College School, Cambridge,[8] and The Leys School, Cambridge.[9] Wiles told WGBH-TV in 1999 that he came across Fermat's Last Theorem on his way home from school when he was 10 years old. He stopped at his local library where he found a book The Last Problem, by Eric Temple Bell, about the theorem.[10] Fascinated by the existence of a theorem that was so easy to state that he, a ten-year-old, could understand it, but that no one had proven, he decided to be the first person to prove it. However, he soon realised that his knowledge was too limited, so he abandoned his childhood dream until it was brought back to his attention at the age of 33 by Ken Ribet's 1986 proof of the epsilon conjecture, which Gerhard Frey had previously linked to Fermat's equation.[11]\n\nIn 1974, Wiles earned his bachelor's degree in mathematics at Merton College, Oxford.[6] Wiles's graduate research was guided by John Coates, beginning in the summer of 1975. Together they worked on the arithmetic of elliptic curves with complex multiplication by the methods of Iwasawa theory. He further worked with Barry Mazur on the main conjecture of Iwasawa theory over the rational numbers, and soon afterward, he generalised this result to totally real fields.[12][13]\n\nIn 1980, Wiles earned a PhD while at Clare College, Cambridge.[3] After a stay at the Institute for Advanced Study in Princeton, New Jersey, in 1981, Wiles became a Professor of Mathematics at Princeton University.[14]\n\nIn 1985–86, Wiles was a Guggenheim Fellow at the Institut des Hautes Études Scientifiques near Paris and at the École Normale Supérieure.[14]\n\nIn 1989, Wiles was elected to the Royal Society. At that point according to his election certificate, he had been working \"on the construction of ℓ-adic representations attached to Hilbert modular forms, and has applied these to prove the 'main conjecture' for cyclotomic extensions of totally real fields\".[12]\n\nFrom 1988 to 1990, Wiles was a Royal Society Research Professor at the University of Oxford, and then he returned to Princeton.\nFrom 1994 to 2009, Wiles was a Eugene Higgins Professor at Princeton.\n\nStarting in mid-1986, based on successive progress of the previous few years of Gerhard Frey, Jean-Pierre Serre and Ken Ribet, it became clear that Fermat's Last Theorem (the statement that no three positive integers a, b, and c satisfy the equation an + bn = cn for any integer value of n greater than 2) could be proven as a corollary of a limited form of the modularity theorem (unproven at the time and then known as the \"Taniyama–Shimura–Weil conjecture\").[15] The modularity theorem involved elliptic curves, which was also Wiles's own specialist area, and stated that all such curves have a modular form associated with them.[16][17] These curves can be thought of as mathematical objects resembling solutions for a torus' surface, and if Fermat's Last Theorem were false and solutions existed, \"a peculiar curve would result\". A proof of the theorem therefore would involve showing that such a curve would not exist.[18]\n\nThe conjecture was seen by contemporary mathematicians as important, but extraordinarily difficult or perhaps impossible to prove.[19]: 203–205, 223, 226  For example, Wiles's ex-supervisor John Coates stated that it seemed \"impossible to actually prove\",[19]: 226  and Ken Ribet considered himself \"one of the vast majority of people who believed [it] was completely inaccessible\", adding that \"Andrew Wiles was probably one of the few people on earth who had the audacity to dream that you can actually go and prove [it].\"[19]: 223 \n\nDespite this, Wiles, with his from-childhood fascination with Fermat's Last Theorem, decided to undertake the challenge of proving the conjecture, at least to the extent needed for Frey's curve.[19]: 226  He dedicated all of his research time to this problem for over six years in near-total secrecy, covering up his efforts by releasing prior work in small segments as separate papers and confiding only in his wife.[19]: 229–230 \n\nWiles' research involved creating a proof by contradiction of Fermat's Last Theorem, which Ribet in his 1986 work had found to have an elliptic curve and thus an associated modular form if true. Starting by assuming that the theorem was incorrect, Wiles then contradicted the Taniyama–Shimura–Weil conjecture as formulated under that assumption, with Ribet's theorem (which stated that if n were a prime number, no such elliptic curve could have a modular form, so no odd prime counterexample to Fermat's equation could exist). Wiles also proved that the conjecture applied to the special case known as the semistable elliptic curves to which Fermat's equation was tied. In other words, Wiles had found that the Taniyama–Shimura–Weil conjecture was true in the case of Fermat's equation, and Ribet's finding (that the conjecture holding for semistable elliptic curves could mean Fermat's Last Theorem is true) prevailed, thus proving Fermat's Last Theorem.[20][21][15]\n\nIn June 1993, he presented his proof to the public for the first time at a conference in Cambridge. Gina Kolata of The New York Times summed up the presentation as follows:\n\nHe gave a lecture a day on Monday, Tuesday and Wednesday with the title \"Modular Forms, Elliptic Curves and Galois Representations\". There was no hint in the title that Fermat's last theorem would be discussed, Dr. Ribet said. ... Finally, at the end of his third lecture, Dr. Wiles concluded that he had proved a general case of the Taniyama conjecture. Then, seemingly as an afterthought, he noted that that meant that Fermat's last theorem was true. Q.E.D.[18]\nIn August 1993, it was discovered that the proof contained a flaw in several areas, related to properties of the Selmer group and use of a tool called an Euler system.[22][23] Wiles tried and failed for over a year to repair his proof. According to Wiles, the crucial idea for circumventing—rather than closing—this area came to him on 19 September 1994, when he was on the verge of giving up. The circumvention used Galois representations to replace elliptic curves, reduced the problem to a class number formula and solved it, among other matters, all using Victor Kolyvagin's ideas as a basis for fixing Matthias Flach's approach with Iwasawa theory.[23][22] Together with his former student Richard Taylor, Wiles published a second paper which contained the circumvention and thus completed the proof. Both papers were published in May 1995 in a dedicated issue of the Annals of Mathematics.[24][25]\n\nIn 2011, Wiles rejoined the University of Oxford as Royal Society Research Professor.[14]\n\nIn May 2018, Wiles was appointed Regius Professor of Mathematics at Oxford, the first in the university's history.[4]\n\nWiles' work has been used in many fields of mathematics. Notably, in 1999, three of his former students, Richard Taylor, Brian Conrad, and Fred Diamond, working with Christophe Breuil, built upon Wiles' proof to prove the full modularity theorem.[26][15] Wiles's doctoral students have also included Manjul Bhargava (2014 winner of the Fields Medal), Ehud de Shalit, Ritabrata Munshi (winner of the SSB Prize and ICTP Ramanujan Prize), Karl Rubin (son of Vera Rubin), Christopher Skinner, and Vinayak Vatsal (2007 winner of the Coxeter–James Prize).\n\nIn 2016, upon receiving the Abel Prize, Wiles said about his proof of Fermat's Last Theorem, \"The methods that solved it opened up a new way of attacking one of the big webs of conjectures of contemporary mathematics called the Langlands Program, which as a grand vision tries to unify different branches of mathematics. It’s given us a new way to look at that\".[5]\n\nWiles's proof of Fermat's Last Theorem has stood up to the scrutiny of the world's other mathematical experts. Wiles was interviewed for an episode of the BBC documentary series Horizon[27] about Fermat's Last Theorem. This was broadcast as an episode of the PBS science television series Nova with the title \"The Proof\".[10] His work and life are also described in great detail in Simon Singh's popular book Fermat's Last Theorem.\n\nIn 1988, Wiles was awarded the Junior Whitehead Prize of the London Mathematical Society (1988).[6] In 1989, he was elected a Fellow of the Royal Society (FRS)[28][12]\n\nIn 1994, Wiles was elected member of the American Academy of Arts and Sciences.[29] Upon completing his proof of Fermat's Last Theorem in 1995, he was awarded the Schock Prize,[14] Fermat Prize,[30] and Wolf Prize in Mathematics that year.[14] Wiles was elected a Foreign Associate of the National Academy of Sciences[13] and won an NAS Award in Mathematics from the National Academy of Sciences,[31] the Royal Medal, and the Ostrowski Prize in 1996.[32] He won the American Mathematical Society's Cole Prize,[33] a MacArthur Fellowship, and the Wolfskehl Prize in 1997,[34] and was elected member of the American Philosophical Society that year.[35]\n\nIn 1998, Wiles was awarded a silver plaque from the International Mathematical Union recognising his achievements, in place of the Fields Medal, which is restricted to those under the age of 40 (Wiles was 41 when he proved the theorem in 1994).[36] That same year, he was awarded the King Faisal Prize[37] along with the Clay Research Award in 1999,[14] the year the asteroid 9999 Wiles was named after him.[38]\n\nIn 2000, he was awarded Knight Commander of the Order of the British Empire (2000)[39] In 2004 Wiles won the Premio Pitagora. [40] In 2005, he won the Shaw Prize.[30]\n\nThe building at the University of Oxford housing the Mathematical Institute was named after Wiles in 2016.[41] Later that year he won the Abel Prize.[42][43][44][45][46] In 2017, Wiles won the Copley Medal.[1] In 2019, he won the De Morgan Medal.[47]\n"
    },
    {
        "title": "Scheme (mathematics)",
        "content": "In mathematics, specifically algebraic geometry, a scheme is a structure that enlarges the notion of algebraic variety in several ways, such as taking account of multiplicities (the equations x = 0 and x2 = 0 define the same algebraic variety but different schemes) and allowing \"varieties\" defined over any commutative ring (for example, Fermat curves are defined over the integers). \n\nScheme theory was introduced by Alexander Grothendieck in 1960 in his treatise Éléments de géométrie algébrique (EGA); one of its aims was developing the formalism needed to solve deep problems of algebraic geometry, such as the Weil conjectures (the last of which was proved by Pierre Deligne).[1] Strongly based on commutative algebra, scheme theory allows a systematic use of methods of topology and homological algebra. Scheme theory also unifies algebraic geometry with much of number theory, which eventually led to Wiles's proof of Fermat's Last Theorem. \n\nSchemes elaborate the fundamental idea that an algebraic variety is best analyzed through the coordinate ring of regular algebraic functions defined on it (or on its subsets), and each subvariety corresponds to the ideal of functions which vanish on the subvariety. Intuitively, a scheme is a topological space consisting of closed points which correspond to geometric points, together with non-closed points which are generic points of irreducible subvarieties. The space is covered by an atlas of open sets, each endowed with a coordinate ring of regular functions, with specified coordinate changes between the functions over intersecting open sets. Such a structure is called a ringed space or a sheaf of rings. The cases of main interest are the Noetherian schemes, in which the coordinate rings are Noetherian rings.\n\nFormally, a scheme is a ringed space covered by affine schemes. An affine scheme is the spectrum of a commutative ring; its points are the prime ideals of the ring, and its closed points are maximal ideals. The coordinate ring of an affine scheme is the ring itself, and the coordinate rings of open subsets are rings of fractions.\n\nThe relative point of view is that much of algebraic geometry should be developed for a morphism X → Y of schemes (called a scheme X over the base Y ), rather than for an individual scheme. For example, in studying algebraic surfaces, it can be useful to consider families of algebraic surfaces over any scheme Y. In many cases, the family of all varieties of a given type can itself be viewed as a variety or scheme, known as a moduli space.\n\nFor some of the detailed definitions in the theory of schemes, see the glossary of scheme theory.\n\nThe origins of algebraic geometry mostly lie in the study of polynomial equations over the real numbers. By the 19th century, it became clear (notably in the work of Jean-Victor Poncelet and Bernhard Riemann) that algebraic geometry over the real numbers is simplified by working over the field of complex numbers, which has the advantage of being algebraically closed.[2] The early 20th century saw analogies between algebraic geometry and number theory, suggesting the question: can algebraic geometry be developed over other fields, such as those with positive characteristic, and more generally over number rings like the integers, where the tools of topology and complex analysis used to study complex varieties do not seem to apply?\n\nHilbert's Nullstellensatz suggests an approach to algebraic geometry over any algebraically closed field k : the maximal ideals in the polynomial ring k[x1, ... , xn] are in one-to-one correspondence with the set kn of n-tuples of elements of k, and the prime ideals correspond to the irreducible algebraic sets in kn, known as affine varieties. Motivated by these ideas, Emmy Noether and Wolfgang Krull developed commutative algebra in the 1920s and 1930s.[3] Their work generalizes algebraic geometry in a purely algebraic direction, generalizing the study of points (maximal ideals in a polynomial ring) to the study of prime ideals in any commutative ring. For example, Krull defined the dimension of a commutative ring in terms of prime ideals and, at least when the ring is Noetherian, he proved that this definition satisfies many of the intuitive properties of geometric dimension.\n\nNoether and Krull's commutative algebra can be viewed as an algebraic approach to affine algebraic varieties. However, many arguments in algebraic geometry work better for projective varieties, essentially because they are compact. From the 1920s to the 1940s, B. L. van der Waerden, André Weil and Oscar Zariski applied commutative algebra as a new foundation for algebraic geometry in the richer setting of projective (or quasi-projective) varieties.[4] In particular, the Zariski topology is a useful topology on a variety over any algebraically closed field, replacing to some extent the classical topology on a complex variety (based on the metric topology of the complex numbers).\n\nFor applications to number theory, van der Waerden and Weil formulated algebraic geometry over any field, not necessarily algebraically closed. Weil was the first to define an abstract variety (not embedded in projective space), by gluing affine varieties along open subsets, on the model of abstract manifolds in topology. He needed this generality for his construction of the Jacobian variety of a curve over any field. (Later, Jacobians were shown to be projective varieties by Weil, Chow and Matsusaka.)\n\nThe algebraic geometers of the Italian school had often used the somewhat foggy concept of the generic point of an algebraic variety. What is true for the generic point is true for \"most\" points of the variety. In Weil's Foundations of Algebraic Geometry (1946), generic points are constructed by taking points in a very large algebraically closed field, called a universal domain.[4] This worked awkwardly: there were many different generic points for the same variety. (In the later theory of schemes, each algebraic variety has a single generic point.)\n\nIn the 1950s, Claude Chevalley, Masayoshi Nagata and Jean-Pierre Serre, motivated in part by the Weil conjectures relating number theory and algebraic geometry, further extended the objects of algebraic geometry, for example by generalizing the base rings allowed. The word scheme was first used in the 1956 Chevalley Seminar, in which Chevalley pursued Zariski's ideas.[5] According to Pierre Cartier, it was André Martineau who suggested to Serre the possibility of using the spectrum of an arbitrary commutative ring as a foundation for algebraic geometry.[6]\n\nThe theory took its definitive form in Grothendieck's Éléments de géométrie algébrique (EGA) and the later Séminaire de géométrie algébrique (SGA), bringing to a conclusion a generation of experimental suggestions and partial developments.[7] Grothendieck defined the spectrum \n\n\n\nX\n\n\n{\\displaystyle X}\n\n of a commutative ring \n\n\n\nR\n\n\n{\\displaystyle R}\n\n as the space of prime ideals of \n\n\n\nR\n\n\n{\\displaystyle R}\n\n with a natural topology (known as the Zariski topology), but augmented it with a sheaf of rings: to every open subset \n\n\n\nU\n\n\n{\\displaystyle U}\n\n he assigned a commutative ring \n\n\n\n\n\n\nO\n\n\n\nX\n\n\n(\nU\n)\n\n\n{\\displaystyle {\\mathcal {O}}_{X}(U)}\n\n, which may be thought of as the coordinate ring of regular functions on \n\n\n\nU\n\n\n{\\displaystyle U}\n\n. These objects \n\n\n\nSpec\n⁡\n(\nR\n)\n\n\n{\\displaystyle \\operatorname {Spec} (R)}\n\n are the affine schemes; a general scheme is then obtained by \"gluing together\" affine schemes.\n\nMuch of algebraic geometry focuses on projective or quasi-projective varieties over a field \n\n\n\nk\n\n\n{\\displaystyle k}\n\n, most often over the complex numbers. Grothendieck developed a large body of theory for arbitrary schemes extending much of the geometric intuition for varieties. For example, it is common to construct a moduli space first as a scheme, and only later study whether it is a more concrete object such as a projective variety. Applying Grothendieck's theory to schemes over the integers and other number fields led to powerful new perspectives in number theory.\n\nAn affine scheme is a locally ringed space isomorphic to the spectrum \n\n\n\nSpec\n⁡\n(\nR\n)\n\n\n{\\displaystyle \\operatorname {Spec} (R)}\n\n of a commutative ring \n\n\n\nR\n\n\n{\\displaystyle R}\n\n. A scheme is a locally ringed space \n\n\n\nX\n\n\n{\\displaystyle X}\n\n admitting a covering by open sets \n\n\n\n\nU\n\ni\n\n\n\n\n{\\displaystyle U_{i}}\n\n, such that each \n\n\n\n\nU\n\ni\n\n\n\n\n{\\displaystyle U_{i}}\n\n (as a locally ringed space) is an affine scheme.[8] In particular, \n\n\n\nX\n\n\n{\\displaystyle X}\n\n comes with a sheaf \n\n\n\n\n\n\nO\n\n\n\nX\n\n\n\n\n{\\displaystyle {\\mathcal {O}}_{X}}\n\n, which assigns to every open subset \n\n\n\nU\n\n\n{\\displaystyle U}\n\n a commutative ring \n\n\n\n\n\n\nO\n\n\n\nX\n\n\n(\nU\n)\n\n\n{\\displaystyle {\\mathcal {O}}_{X}(U)}\n\n called the ring of regular functions on \n\n\n\nU\n\n\n{\\displaystyle U}\n\n. One can think of a scheme as being covered by \"coordinate charts\" that are affine schemes. The definition means exactly that schemes are obtained by gluing together affine schemes using the Zariski topology. \n\nIn the early days, this was called a prescheme, and a scheme was defined to be a separated prescheme.  The term prescheme has fallen out of use, but can still be found in older books, such as Grothendieck's \"Éléments de géométrie algébrique\" and Mumford's \"Red Book\".[9] The sheaf properties of \n\n\n\n\n\n\nO\n\n\n\nX\n\n\n(\nU\n)\n\n\n{\\displaystyle {\\mathcal {O}}_{X}(U)}\n\n mean that its elements, which are not necessarily functions, can neverthess be patched together from their restrictions in the same way as functions.\n\nA basic example of an affine scheme is affine \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-space over a field \n\n\n\nk\n\n\n{\\displaystyle k}\n\n, for a natural number \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. By definition, \n\n\n\n\nA\n\nk\n\n\nn\n\n\n\n\n{\\displaystyle A_{k}^{n}}\n\n is the spectrum of the polynomial ring \n\n\n\nk\n[\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n]\n\n\n{\\displaystyle k[x_{1},\\dots ,x_{n}]}\n\n. In the spirit of scheme theory, affine \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-space can in fact be defined over any commutative ring \n\n\n\nR\n\n\n{\\displaystyle R}\n\n, meaning \n\n\n\nSpec\n⁡\n(\nR\n[\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n]\n)\n\n\n{\\displaystyle \\operatorname {Spec} (R[x_{1},\\dots ,x_{n}])}\n\n.\n\nSchemes form a category, with morphisms defined as morphisms of locally ringed spaces. (See also: morphism of schemes.) For a scheme Y, a scheme X over Y (or a Y-scheme) means a morphism X → Y of schemes. A scheme X over a commutative ring R means a morphism X → Spec(R).\n\nAn algebraic variety over a field k can be defined as a scheme over k with certain properties. There are different conventions about exactly which schemes should be called varieties. One standard choice is that a variety over k means an integral separated scheme of finite type over k.[10]\n\nA morphism f: X → Y of schemes determines a pullback homomorphism on the rings of regular functions, f*: O(Y) → O(X). In the case of affine schemes, this construction gives a one-to-one correspondence between morphisms Spec(A) → Spec(B) of schemes and ring homomorphisms B → A.[11] In this sense, scheme theory completely subsumes the theory of commutative rings.\n\nSince Z is an initial object in the category of commutative rings, the category of schemes has Spec(Z) as a terminal object.\n\nFor a scheme X over a commutative ring R, an R-point of X means a section of the morphism X → Spec(R). One writes X(R) for the set of R-points of X. In examples, this definition reconstructs the old notion of the set of solutions of the defining equations of X with values in R. When R is a field k, X(k) is also called the set of k-rational points of X.\n\nMore generally, for a scheme X over a commutative ring R and any commutative R-algebra S, an S-point of X means a morphism Spec(S) → X over R. One writes X(S) for the set of S-points of X. (This generalizes the old observation that given some equations over a field k, one can consider the set of solutions of the equations in any field extension E of k.) For a scheme X over R, the assignment S ↦ X(S) is a functor from commutative R-algebras to sets. It is an important observation that a scheme X over R is determined by this functor of points.[12]\n\nThe fiber product of schemes always exists. That is, for any schemes X and Z with morphisms to a scheme Y, the categorical fiber product \n\n\n\nX\n\n×\n\nY\n\n\nZ\n\n\n{\\displaystyle X\\times _{Y}Z}\n\n exists in the category of schemes. If X and Z are schemes over a field k, their fiber product over Spec(k) may be called the product X × Z in the category of k-schemes. For example, the product of affine spaces \n\n\n\n\n\nA\n\n\nm\n\n\n\n\n{\\displaystyle \\mathbb {A} ^{m}}\n\n and \n\n\n\n\n\nA\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbb {A} ^{n}}\n\n over k is affine space \n\n\n\n\n\nA\n\n\nm\n+\nn\n\n\n\n\n{\\displaystyle \\mathbb {A} ^{m+n}}\n\n over k.\n\nSince the category of schemes has fiber products and also a terminal object Spec(Z), it has all finite limits.\n\nHere and below, all the rings considered are commutative.\n\nLet \n\n\n\nk\n\n\n{\\displaystyle k}\n\n be an algebraically closed field. The affine space \n\n\n\n\n\n\nX\n¯\n\n\n\n=\n\n\nA\n\n\nk\n\n\nn\n\n\n\n\n{\\displaystyle {\\bar {X}}=\\mathbb {A} _{k}^{n}}\n\n is the algebraic variety of all points \n\n\n\na\n=\n(\n\na\n\n1\n\n\n,\n…\n,\n\na\n\nn\n\n\n)\n\n\n{\\displaystyle a=(a_{1},\\ldots ,a_{n})}\n\n with coordinates in \n\n\n\nk\n\n\n{\\displaystyle k}\n\n; its coordinate ring is the polynomial ring \n\n\n\nR\n=\nk\n[\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n]\n\n\n{\\displaystyle R=k[x_{1},\\ldots ,x_{n}]}\n\n. The corresponding scheme \n\n\n\nX\n=\n\nS\np\ne\nc\n\n(\nR\n)\n\n\n{\\displaystyle X=\\mathrm {Spec} (R)}\n\n is a topological space with the Zariski topology, whose closed points are the maximal ideals \n\n\n\n\n\n\nm\n\n\n\na\n\n\n=\n(\n\nx\n\n1\n\n\n−\n\na\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n−\n\na\n\nn\n\n\n)\n\n\n{\\displaystyle {\\mathfrak {m}}_{a}=(x_{1}-a_{1},\\ldots ,x_{n}-a_{n})}\n\n, the set of polynomials vanishing at \n\n\n\na\n\n\n{\\displaystyle a}\n\n. The scheme also contains a non-closed point for each non-maximal prime ideal \n\n\n\n\n\np\n\n\n⊂\nR\n\n\n{\\displaystyle {\\mathfrak {p}}\\subset R}\n\n, whose vanishing defines an irreducible subvariety \n\n\n\n\n\n\nV\n¯\n\n\n\n=\n\n\n\nV\n¯\n\n\n\n(\n\n\np\n\n\n)\n⊂\n\n\n\nX\n¯\n\n\n\n\n\n{\\displaystyle {\\bar {V}}={\\bar {V}}({\\mathfrak {p}})\\subset {\\bar {X}}}\n\n; the topological closure of the scheme point \n\n\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {p}}}\n\n is the subscheme \n\n\n\nV\n(\n\n\np\n\n\n)\n=\n{\n\n\nq\n\n\n∈\nX\n \n \n\nwith\n\n \n \n\n\np\n\n\n⊂\n\n\nq\n\n\n}\n\n\n{\\displaystyle V({\\mathfrak {p}})=\\{{\\mathfrak {q}}\\in X\\ \\ {\\text{with}}\\ \\ {\\mathfrak {p}}\\subset {\\mathfrak {q}}\\}}\n\n, including all the closed points of the subvariety, i.e. \n\n\n\n\n\n\nm\n\n\n\na\n\n\n\n\n{\\displaystyle {\\mathfrak {m}}_{a}}\n\n with \n\n\n\na\n∈\n\n\n\nV\n¯\n\n\n\n\n\n{\\displaystyle a\\in {\\bar {V}}}\n\n, or equivalently \n\n\n\n\n\np\n\n\n⊂\n\n\n\nm\n\n\n\na\n\n\n\n\n{\\displaystyle {\\mathfrak {p}}\\subset {\\mathfrak {m}}_{a}}\n\n. \n\n\nThe scheme \n\n\n\nX\n\n\n{\\displaystyle X}\n\n has a basis of open subsets given by the complements of hypersurfaces, \n\n\n\n\n\nU\n\nf\n\n\n=\nX\n∖\nV\n(\nf\n)\n=\n{\n\n\np\n\n\n∈\nX\n \n \n\nwith\n\n \n \nf\n∉\n\n\np\n\n\n}\n\n\n{\\displaystyle U_{f}=X\\smallsetminus V(f)=\\{{\\mathfrak {p}}\\in X\\ \\ {\\text{with}}\\ \\ f\\notin {\\mathfrak {p}}\\}}\n\n \nfor irreducible polynomials \n\n\n\nf\n∈\nR\n\n\n{\\displaystyle f\\in R}\n\n. This set is endowed with its coordinate ring of regular functions \n\n\n\n\n\n\n\nO\n\n\n\nX\n\n\n(\n\nU\n\nf\n\n\n)\n=\nR\n[\n\nf\n\n−\n1\n\n\n]\n=\n{\n\n\n\nr\n\nf\n\nm\n\n\n\n\n\n \n \n\nfor\n\n \n \nr\n∈\nR\n,\n \nm\n∈\n\n\nZ\n\n\n≥\n0\n\n\n}\n\n\n{\\displaystyle {\\mathcal {O}}_{X}(U_{f})=R[f^{-1}]=\\{{\\tfrac {r}{f^{m}}}\\ \\ {\\text{for}}\\ \\ r\\in R,\\ m\\in \\mathbb {Z} _{\\geq 0}\\}}\n\n. \nThis induces a unique sheaf \n\n\n\n\n\n\nO\n\n\n\nX\n\n\n\n\n{\\displaystyle {\\mathcal {O}}_{X}}\n\n which gives the usual ring of rational functions regular on a given open set \n\n\n\nU\n\n\n{\\displaystyle U}\n\n. \n\nEach ring element \n\n\n\nr\n=\nr\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n∈\nR\n\n\n{\\displaystyle r=r(x_{1},\\ldots ,x_{n})\\in R}\n\n, a polynomial function on \n\n\n\n\n\n\nX\n¯\n\n\n\n\n\n{\\displaystyle {\\bar {X}}}\n\n, also defines a function on the points of the scheme \n\n\n\nX\n\n\n{\\displaystyle X}\n\n whose value at \n\n\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {p}}}\n\n lies in the quotient ring \n\n\n\nR\n\n/\n\n\n\np\n\n\n\n\n{\\displaystyle R/{\\mathfrak {p}}}\n\n, the residue ring. We define \n\n\n\nr\n(\n\n\np\n\n\n)\n\n\n{\\displaystyle r({\\mathfrak {p}})}\n\n as the image of \n\n\n\nr\n\n\n{\\displaystyle r}\n\n under the natural map \n\n\n\nR\n→\nR\n\n/\n\n\n\np\n\n\n\n\n{\\displaystyle R\\to R/{\\mathfrak {p}}}\n\n. A maximal ideal \n\n\n\n\n\n\nm\n\n\n\na\n\n\n\n\n{\\displaystyle {\\mathfrak {m}}_{a}}\n\n gives the residue field \n\n\n\nk\n(\n\n\n\nm\n\n\n\na\n\n\n)\n=\nR\n\n/\n\n\n\n\nm\n\n\n\na\n\n\n≅\nk\n\n\n{\\displaystyle k({\\mathfrak {m}}_{a})=R/{\\mathfrak {m}}_{a}\\cong k}\n\n, with the natural isomorphism \n\n\n\n\nx\n\ni\n\n\n↦\n\na\n\ni\n\n\n\n\n{\\displaystyle x_{i}\\mapsto a_{i}}\n\n, so that \n\n\n\nr\n(\n\n\n\nm\n\n\n\na\n\n\n)\n\n\n{\\displaystyle r({\\mathfrak {m}}_{a})}\n\n corresponds to the original value \n\n\n\nr\n(\na\n)\n\n\n{\\displaystyle r(a)}\n\n. \n\nThe vanishing locus of a polynomial \n\n\n\nf\n=\nf\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle f=f(x_{1},\\ldots ,x_{n})}\n\n is a hypersurface subvariety \n\n\n\n\n\n\nV\n¯\n\n\n\n(\nf\n)\n⊂\n\n\nA\n\n\nk\n\n\nn\n\n\n\n\n{\\displaystyle {\\bar {V}}(f)\\subset \\mathbb {A} _{k}^{n}}\n\n, corresponding to the principal ideal \n\n\n\n(\nf\n)\n⊂\nR\n\n\n{\\displaystyle (f)\\subset R}\n\n. The corresponding scheme is \n\n\n\nV\n(\nf\n)\n=\nSpec\n⁡\n(\nR\n\n/\n\n(\nf\n)\n)\n\n\n{\\textstyle V(f)=\\operatorname {Spec} (R/(f))}\n\n, a closed subscheme of affine space. For example, taking \n\n\n\nk\n\n\n{\\displaystyle k}\n\n to be the complex or real numbers, the equation \n\n\n\n\nx\n\n2\n\n\n=\n\ny\n\n2\n\n\n(\ny\n+\n1\n)\n\n\n{\\displaystyle x^{2}=y^{2}(y+1)}\n\n defines a nodal cubic curve in the affine plane \n\n\n\n\n\nA\n\n\nk\n\n\n2\n\n\n\n\n{\\displaystyle \\mathbb {A} _{k}^{2}}\n\n, corresponding to the scheme \n\n\n\nV\n=\nSpec\n⁡\nk\n[\nx\n,\ny\n]\n\n/\n\n(\n\nx\n\n2\n\n\n−\n\ny\n\n2\n\n\n(\ny\n+\n1\n)\n)\n\n\n{\\displaystyle V=\\operatorname {Spec} k[x,y]/(x^{2}-y^{2}(y+1))}\n\n.\n\nThe ring of integers \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n can be considered as the coordinate ring of the scheme \n\n\n\nZ\n=\nSpec\n⁡\n(\n\nZ\n\n)\n\n\n{\\displaystyle Z=\\operatorname {Spec} (\\mathbb {Z} )}\n\n. The Zariski topology has closed points \n\n\n\n\n\n\nm\n\n\n\np\n\n\n=\n(\np\n)\n\n\n{\\displaystyle {\\mathfrak {m}}_{p}=(p)}\n\n, the principal ideals of the prime numbers \n\n\n\np\n∈\n\nZ\n\n\n\n{\\displaystyle p\\in \\mathbb {Z} }\n\n; as well as the generic point \n\n\n\n\n\n\np\n\n\n\n0\n\n\n=\n(\n0\n)\n\n\n{\\displaystyle {\\mathfrak {p}}_{0}=(0)}\n\n, the zero ideal, whose closure is the whole scheme. Closed sets are finite sets, and open sets are their complements, the cofinite sets; any infinite set of points is dense.\n\nThe basis open set corresponding to the irreducible element \n\n\n\np\n∈\n\nZ\n\n\n\n{\\displaystyle p\\in \\mathbb {Z} }\n\n is \n\n\n\n\nU\n\np\n\n\n=\nZ\n∖\n{\n\n\n\nm\n\n\n\np\n\n\n}\n\n\n{\\displaystyle U_{p}=Z\\smallsetminus \\{{\\mathfrak {m}}_{p}\\}}\n\n, with coordinate ring \n\n\n\n\n\n\nO\n\n\n\nZ\n\n\n(\n\nU\n\np\n\n\n)\n=\n\nZ\n\n[\n\np\n\n−\n1\n\n\n]\n=\n{\n\n\n\nn\n\np\n\nm\n\n\n\n\n\n \n\nfor\n\n \nn\n∈\n\nZ\n\n,\n \nm\n≥\n0\n}\n\n\n{\\displaystyle {\\mathcal {O}}_{Z}(U_{p})=\\mathbb {Z} [p^{-1}]=\\{{\\tfrac {n}{p^{m}}}\\ {\\text{for}}\\ n\\in \\mathbb {Z} ,\\ m\\geq 0\\}}\n\n. For the open set \n\n\n\nU\n=\nZ\n∖\n{\n\n\n\nm\n\n\n\n\np\n\n1\n\n\n\n\n,\n…\n,\n\n\n\nm\n\n\n\n\np\n\nℓ\n\n\n\n\n}\n\n\n{\\displaystyle U=Z\\smallsetminus \\{{\\mathfrak {m}}_{p_{1}},\\ldots ,{\\mathfrak {m}}_{p_{\\ell }}\\}}\n\n, this induces \n\n\n\n\n\n\nO\n\n\n\nZ\n\n\n(\nU\n)\n=\n\nZ\n\n[\n\np\n\n1\n\n\n−\n1\n\n\n,\n…\n,\n\np\n\nℓ\n\n\n−\n1\n\n\n]\n\n\n{\\displaystyle {\\mathcal {O}}_{Z}(U)=\\mathbb {Z} [p_{1}^{-1},\\ldots ,p_{\\ell }^{-1}]}\n\n.\n\nA number \n\n\n\nn\n∈\n\nZ\n\n\n\n{\\displaystyle n\\in \\mathbb {Z} }\n\n corresponds to a function on the scheme \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n, a function whose value at \n\n\n\n\n\n\nm\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {m}}_{p}}\n\n lies in the residue field \n\n\n\nk\n(\n\n\n\nm\n\n\n\np\n\n\n)\n=\n\nZ\n\n\n/\n\n(\np\n)\n=\n\n\nF\n\n\np\n\n\n\n\n{\\displaystyle k({\\mathfrak {m}}_{p})=\\mathbb {Z} /(p)=\\mathbb {F} _{p}}\n\n, the finite field of integers modulo \n\n\n\np\n\n\n{\\displaystyle p}\n\n: the function is defined by \n\n\n\nn\n(\n\n\n\nm\n\n\n\np\n\n\n)\n=\nn\n \n\nmod\n\n \np\n\n\n{\\displaystyle n({\\mathfrak {m}}_{p})=n\\ {\\text{mod}}\\ p}\n\n, and also \n\n\n\nn\n(\n\n\n\np\n\n\n\n0\n\n\n)\n=\nn\n\n\n{\\displaystyle n({\\mathfrak {p}}_{0})=n}\n\n in the generic residue ring \n\n\n\n\nZ\n\n\n/\n\n(\n0\n)\n=\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} /(0)=\\mathbb {Z} }\n\n. The function \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is determined by its values at the points \n\n\n\n\n\n\nm\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {m}}_{p}}\n\n only, so we can think of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n as a kind of \"regular function\" on the closed points, a very special type among the arbitrary functions \n\n\n\nf\n\n\n{\\displaystyle f}\n\n with \n\n\n\nf\n(\n\n\n\nm\n\n\n\np\n\n\n)\n∈\n\n\nF\n\n\np\n\n\n\n\n{\\displaystyle f({\\mathfrak {m}}_{p})\\in \\mathbb {F} _{p}}\n\n.  \n\nNote that the point \n\n\n\n\n\n\nm\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {m}}_{p}}\n\n is the vanishing locus of the function \n\n\n\nn\n=\np\n\n\n{\\displaystyle n=p}\n\n, the point where the value of \n\n\n\np\n\n\n{\\displaystyle p}\n\n is equal to zero in the residue field. The field of \"rational functions\" on \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n is the fraction field of the generic residue ring, \n\n\n\nk\n(\n\n\n\np\n\n\n\n0\n\n\n)\n=\nFrac\n⁡\n(\n\nZ\n\n)\n=\n\nQ\n\n\n\n{\\displaystyle k({\\mathfrak {p}}_{0})=\\operatorname {Frac} (\\mathbb {Z} )=\\mathbb {Q} }\n\n. A fraction \n\n\n\na\n\n/\n\nb\n\n\n{\\displaystyle a/b}\n\n has \"poles\" at the points \n\n\n\n\n\n\nm\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {m}}_{p}}\n\n corresponding to prime divisors of the denominator. \n\nThis also gives a geometric interpretaton of Bezout's lemma stating that if the integers \n\n\n\n\nn\n\n1\n\n\n,\n…\n,\n\nn\n\nr\n\n\n\n\n{\\displaystyle n_{1},\\ldots ,n_{r}}\n\n have no common prime factor, then there are integers \n\n\n\n\na\n\n1\n\n\n,\n…\n,\n\na\n\nr\n\n\n\n\n{\\displaystyle a_{1},\\ldots ,a_{r}}\n\n with \n\n\n\n\na\n\n1\n\n\n\nn\n\n1\n\n\n+\n⋯\n+\n\na\n\nr\n\n\n\nn\n\nr\n\n\n=\n1\n\n\n{\\displaystyle a_{1}n_{1}+\\cdots +a_{r}n_{r}=1}\n\n. Geometrically, this is a version of the weak Hilbert Nullstellensatz for the scheme \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n: if the functions \n\n\n\n\nn\n\n1\n\n\n,\n…\n,\n\nn\n\nr\n\n\n\n\n{\\displaystyle n_{1},\\ldots ,n_{r}}\n\n have no common vanishing points \n\n\n\n\n\n\nm\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {m}}_{p}}\n\n in \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n, then they generate the unit ideal \n\n\n\n(\n\nn\n\n1\n\n\n,\n…\n,\n\nn\n\nr\n\n\n)\n=\n(\n1\n)\n\n\n{\\displaystyle (n_{1},\\ldots ,n_{r})=(1)}\n\n in the coordinate ring \n\n\n\n\nZ\n\n\n\n{\\displaystyle \\mathbb {Z} }\n\n. Indeed, we may consider the terms \n\n\n\n\nρ\n\ni\n\n\n=\n\na\n\ni\n\n\n\nn\n\ni\n\n\n\n\n{\\displaystyle \\rho _{i}=a_{i}n_{i}}\n\n as forming a kind of partition of unity subordinate to the covering of \n\n\n\nZ\n\n\n{\\displaystyle Z}\n\n by the open sets \n\n\n\n\nU\n\ni\n\n\n=\nZ\n∖\nV\n(\n\nn\n\ni\n\n\n)\n\n\n{\\displaystyle U_{i}=Z\\smallsetminus V(n_{i})}\n\n. \n\nThe affine space \n\n\n\n\n\nA\n\n\n\nZ\n\n\n\n1\n\n\n=\n{\na\n \n\nfor\n\n \na\n∈\n\nZ\n\n}\n\n\n{\\displaystyle \\mathbb {A} _{\\mathbb {Z} }^{1}=\\{a\\ {\\text{for}}\\ a\\in \\mathbb {Z} \\}}\n\n is a variety with coordinate ring \n\n\n\n\nZ\n\n[\nx\n]\n\n\n{\\displaystyle \\mathbb {Z} [x]}\n\n, the polynomials with integer coefficients. The corresponding scheme is \n\n\n\nY\n=\nSpec\n⁡\n(\n\nZ\n\n[\nx\n]\n)\n\n\n{\\displaystyle Y=\\operatorname {Spec} (\\mathbb {Z} [x])}\n\n, whose points are all of the prime ideals \n\n\n\n\n\np\n\n\n⊂\n\nZ\n\n[\nx\n]\n\n\n{\\displaystyle {\\mathfrak {p}}\\subset \\mathbb {Z} [x]}\n\n. The closed points are maximal ideals of the form \n\n\n\n\n\nm\n\n\n=\n(\np\n,\nf\n(\nx\n)\n)\n\n\n{\\displaystyle {\\mathfrak {m}}=(p,f(x))}\n\n, where \n\n\n\np\n\n\n{\\displaystyle p}\n\n is a prime number, and \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n is a non-constant polynomial with no integer factor and which is irreducible modulo \n\n\n\np\n\n\n{\\displaystyle p}\n\n. Thus, we may picture \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n as two-dimensional, with a \"characteristic direction\" measured by the coordinate \n\n\n\np\n\n\n{\\displaystyle p}\n\n, and a \"spatial direction\" with coordinate \n\n\n\nx\n\n\n{\\displaystyle x}\n\n.\n\nA given prime number \n\n\n\np\n\n\n{\\displaystyle p}\n\n defines a \"vertical line\", the subscheme \n\n\n\nV\n(\np\n)\n\n\n{\\displaystyle V(p)}\n\n of the prime ideal \n\n\n\n\n\np\n\n\n=\n(\np\n)\n\n\n{\\displaystyle {\\mathfrak {p}}=(p)}\n\n: this contains \n\n\n\n\n\nm\n\n\n=\n(\np\n,\nf\n(\nx\n)\n)\n\n\n{\\displaystyle {\\mathfrak {m}}=(p,f(x))}\n\n for all \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n, the \"characteristic \n\n\n\np\n\n\n{\\displaystyle p}\n\n points\" of the scheme. Fixing the \n\n\n\nx\n\n\n{\\displaystyle x}\n\n-coordinate, we have the \"horizontal line\" \n\n\n\nx\n=\na\n\n\n{\\displaystyle x=a}\n\n, the subscheme \n\n\n\nV\n(\nx\n−\na\n)\n\n\n{\\displaystyle V(x-a)}\n\n of the prime ideal \n\n\n\n\n\np\n\n\n=\n(\nx\n−\na\n)\n\n\n{\\displaystyle {\\mathfrak {p}}=(x-a)}\n\n. We also have the line \n\n\n\nV\n(\nb\nx\n−\na\n)\n\n\n{\\displaystyle V(bx-a)}\n\n corresponding to the rational coordinate \n\n\n\nx\n=\na\n\n/\n\nb\n\n\n{\\displaystyle x=a/b}\n\n, which does not intersect \n\n\n\nV\n(\np\n)\n\n\n{\\displaystyle V(p)}\n\n for those \n\n\n\np\n\n\n{\\displaystyle p}\n\n which divide \n\n\n\nb\n\n\n{\\displaystyle b}\n\n. \n\nA higher degree \"horizontal\" subscheme like \n\n\n\nV\n(\n\nx\n\n2\n\n\n+\n1\n)\n\n\n{\\displaystyle V(x^{2}+1)}\n\n corresponds to \n\n\n\nx\n\n\n{\\displaystyle x}\n\n-values which are roots of \n\n\n\n\nx\n\n2\n\n\n+\n1\n\n\n{\\displaystyle x^{2}+1}\n\n, namely \n\n\n\nx\n=\n±\n\n\n−\n1\n\n\n\n\n{\\displaystyle x=\\pm {\\sqrt {-1}}}\n\n. This behaves differently under different \n\n\n\np\n\n\n{\\displaystyle p}\n\n-coordinates. At \n\n\n\np\n=\n5\n\n\n{\\displaystyle p=5}\n\n, we get two points \n\n\n\nx\n=\n±\n2\n \n\nmod\n\n \n5\n\n\n{\\displaystyle x=\\pm 2\\ {\\text{mod}}\\ 5}\n\n, since \n\n\n\n(\n5\n,\n\nx\n\n2\n\n\n+\n1\n)\n=\n(\n5\n,\nx\n−\n2\n)\n∩\n(\n5\n,\nx\n+\n2\n)\n\n\n{\\displaystyle (5,x^{2}+1)=(5,x-2)\\cap (5,x+2)}\n\n. At \n\n\n\np\n=\n2\n\n\n{\\displaystyle p=2}\n\n, we get one ramified double-point \n\n\n\nx\n=\n1\n \n\nmod\n\n \n2\n\n\n{\\displaystyle x=1\\ {\\text{mod}}\\ 2}\n\n, since \n\n\n\n(\n2\n,\n\nx\n\n2\n\n\n+\n1\n)\n=\n(\n2\n,\n(\nx\n−\n1\n\n)\n\n2\n\n\n)\n\n\n{\\displaystyle (2,x^{2}+1)=(2,(x-1)^{2})}\n\n. And at \n\n\n\np\n=\n3\n\n\n{\\displaystyle p=3}\n\n, we get that \n\n\n\n\n\nm\n\n\n=\n(\n3\n,\n\nx\n\n2\n\n\n+\n1\n)\n\n\n{\\displaystyle {\\mathfrak {m}}=(3,x^{2}+1)}\n\n is a prime ideal corresponding to \n\n\n\nx\n=\n±\n\n\n−\n1\n\n\n\n\n{\\displaystyle x=\\pm {\\sqrt {-1}}}\n\n in an extension field of \n\n\n\n\n\nF\n\n\n3\n\n\n\n\n{\\displaystyle \\mathbb {F} _{3}}\n\n; since we cannot distinguish between these values (they are symmetric under the Galois group), we should picture \n\n\n\nV\n(\n3\n,\n\nx\n\n2\n\n\n+\n1\n)\n\n\n{\\displaystyle V(3,x^{2}+1)}\n\n as two fused points. Overall, \n\n\n\nV\n(\n\nx\n\n2\n\n\n+\n1\n)\n\n\n{\\displaystyle V(x^{2}+1)}\n\n is a kind of fusion of two Galois-symmetric horizonal lines, a curve of degree 2. \n\nThe residue field at \n\n\n\n\n\nm\n\n\n=\n(\np\n,\nf\n(\nx\n)\n)\n\n\n{\\displaystyle {\\mathfrak {m}}=(p,f(x))}\n\n is \n\n\n\nk\n(\n\n\nm\n\n\n)\n=\n\nZ\n\n[\nx\n]\n\n/\n\n\n\nm\n\n\n=\n\n\nF\n\n\np\n\n\n[\nx\n]\n\n/\n\n(\nf\n(\nx\n)\n)\n≅\n\n\nF\n\n\np\n\n\n(\nα\n)\n\n\n{\\displaystyle k({\\mathfrak {m}})=\\mathbb {Z} [x]/{\\mathfrak {m}}=\\mathbb {F} _{p}[x]/(f(x))\\cong \\mathbb {F} _{p}(\\alpha )}\n\n, a field extension of \n\n\n\n\n\nF\n\n\np\n\n\n\n\n{\\displaystyle \\mathbb {F} _{p}}\n\n adjoining a root \n\n\n\nx\n=\nα\n\n\n{\\displaystyle x=\\alpha }\n\n of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n; this is a finite field with \n\n\n\n\np\n\nd\n\n\n\n\n{\\displaystyle p^{d}}\n\nelements, \n\n\n\nd\n=\ndeg\n⁡\n(\nf\n)\n\n\n{\\displaystyle d=\\operatorname {deg} (f)}\n\n. A polynomial \n\n\n\nr\n(\nx\n)\n∈\n\nZ\n\n[\nx\n]\n\n\n{\\displaystyle r(x)\\in \\mathbb {Z} [x]}\n\n corresponds to a function on the scheme \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n with values \n\n\n\nr\n(\n\n\nm\n\n\n)\n=\nr\n \n\nm\no\nd\n\n \n\n\nm\n\n\n\n\n{\\displaystyle r({\\mathfrak {m}})=r\\ \\mathrm {mod} \\ {\\mathfrak {m}}}\n\n, that is \n\n\n\nr\n(\n\n\nm\n\n\n)\n=\nr\n(\nα\n)\n∈\n\n\nF\n\n\np\n\n\n(\nα\n)\n\n\n{\\displaystyle r({\\mathfrak {m}})=r(\\alpha )\\in \\mathbb {F} _{p}(\\alpha )}\n\n. Again each \n\n\n\nr\n(\nx\n)\n∈\n\nZ\n\n[\nx\n]\n\n\n{\\displaystyle r(x)\\in \\mathbb {Z} [x]}\n\n is determined by its values \n\n\n\nr\n(\n\n\nm\n\n\n)\n\n\n{\\displaystyle r({\\mathfrak {m}})}\n\n at closed points; \n\n\n\nV\n(\np\n)\n\n\n{\\displaystyle V(p)}\n\n is the vanishing locus of the constant polynomial \n\n\n\nr\n(\nx\n)\n=\np\n\n\n{\\displaystyle r(x)=p}\n\n; and \n\n\n\nV\n(\nf\n(\nx\n)\n)\n\n\n{\\displaystyle V(f(x))}\n\n contains the points in each characteristic \n\n\n\np\n\n\n{\\displaystyle p}\n\n corresponding to Galois orbits of roots of \n\n\n\nf\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n\n in the algebraic closure \n\n\n\n\n\n\n\nF\n\n¯\n\n\n\np\n\n\n\n\n{\\displaystyle {\\overline {\\mathbb {F} }}_{p}}\n\n.\n\nThe scheme \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is not proper, so that pairs of curves may fail to intersect with the expected multiplicity. This is a major obstacle to analyzing Diophantine equations with geometric tools. Arakelov theory overcomes this obstacle by compactifying affine arithmetic schemes, adding points at infinity corresponding to valuations.  \n\nIf we consider a polynomial \n\n\n\nf\n∈\n\nZ\n\n[\nx\n,\ny\n]\n\n\n{\\displaystyle f\\in \\mathbb {Z} [x,y]}\n\n then the affine scheme \n\n\n\nX\n=\nSpec\n⁡\n(\n\nZ\n\n[\nx\n,\ny\n]\n\n/\n\n(\nf\n)\n)\n\n\n{\\displaystyle X=\\operatorname {Spec} (\\mathbb {Z} [x,y]/(f))}\n\n has a canonical morphism to \n\n\n\nSpec\n⁡\n\nZ\n\n\n\n{\\displaystyle \\operatorname {Spec} \\mathbb {Z} }\n\n and is called an arithmetic surface. The fibers \n\n\n\n\nX\n\np\n\n\n=\nX\n\n×\n\nSpec\n⁡\n(\n\nZ\n\n)\n\n\nSpec\n⁡\n(\n\n\nF\n\n\np\n\n\n)\n\n\n{\\displaystyle X_{p}=X\\times _{\\operatorname {Spec} (\\mathbb {Z} )}\\operatorname {Spec} (\\mathbb {F} _{p})}\n\n are then algebraic curves over the finite fields \n\n\n\n\n\nF\n\n\np\n\n\n\n\n{\\displaystyle \\mathbb {F} _{p}}\n\n. If \n\n\n\nf\n(\nx\n,\ny\n)\n=\n\ny\n\n2\n\n\n−\n\nx\n\n3\n\n\n+\na\n\nx\n\n2\n\n\n+\nb\nx\n+\nc\n\n\n{\\displaystyle f(x,y)=y^{2}-x^{3}+ax^{2}+bx+c}\n\n is an elliptic curve, then the fibers over its discriminant locus, where  \n\n\n\n\nΔ\n\nf\n\n\n=\n−\n4\n\na\n\n3\n\n\nc\n+\n\na\n\n2\n\n\n\nb\n\n2\n\n\n+\n18\na\nb\nc\n−\n4\n\nb\n\n3\n\n\n−\n27\n\nc\n\n2\n\n\n=\n0\n \n\nmod\n\n \np\n,\n\n\n{\\displaystyle \\Delta _{f}=-4a^{3}c+a^{2}b^{2}+18abc-4b^{3}-27c^{2}=0\\ {\\text{mod}}\\ p,}\n\nare all singular schemes.[13] For example, if \n\n\n\np\n\n\n{\\displaystyle p}\n\n is a prime number and \n\n\n\nX\n=\nSpec\n⁡\n\n\n\n\nZ\n\n[\nx\n,\ny\n]\n\n\n(\n\ny\n\n2\n\n\n−\n\nx\n\n3\n\n\n−\np\n)\n\n\n\n\n\n{\\displaystyle X=\\operatorname {Spec} {\\frac {\\mathbb {Z} [x,y]}{(y^{2}-x^{3}-p)}}}\n\n then its discriminant is \n\n\n\n−\n27\n\np\n\n2\n\n\n\n\n{\\displaystyle -27p^{2}}\n\n. This curve is singular over the prime numbers \n\n\n\n3\n,\np\n\n\n{\\displaystyle 3,p}\n\n.\n\nIt is also fruitful to consider examples of morphisms as examples of schemes since they demonstrate their technical effectiveness for encapsulating many objects of study in algebraic and arithmetic geometry.\n\nHere are some of the ways in which schemes go beyond older notions of algebraic varieties, and their significance.\n\nA central part of scheme theory is the notion of coherent sheaves, generalizing the notion of (algebraic) vector bundles. For a scheme X, one starts by considering the abelian category of OX-modules, which are sheaves of abelian groups on X that form a module over the sheaf of regular functions OX. In particular, a module M over a commutative ring R determines an associated OX-module ~M on X = Spec(R). A quasi-coherent sheaf on a scheme X means an OX-module that is the sheaf associated to a module on each affine open subset of X. Finally, a coherent sheaf (on a Noetherian scheme X, say) is an OX-module that is the sheaf associated to a finitely generated module on each affine open subset of X.\n\nCoherent sheaves include the important class of vector bundles, which are the sheaves that locally come from finitely generated free modules. An example is the tangent bundle of a smooth variety over a field. However, coherent sheaves are richer; for example, a vector bundle on a closed subscheme Y of X can be viewed as a coherent sheaf on X that is zero outside Y (by the direct image construction). In this way, coherent sheaves on a scheme X include information about all closed subschemes of X. Moreover, sheaf cohomology has good properties for coherent (and quasi-coherent) sheaves. The resulting theory of coherent sheaf cohomology is perhaps the main technical tool in algebraic geometry.[18][19]\n\nConsidered as its functor of points, a scheme is a functor that is a sheaf of sets for the Zariski topology on the category of commutative rings, and that, locally in the Zariski topology, is an affine scheme. This can be generalized in several ways.  One is to use the étale topology. Michael Artin defined an algebraic space as a functor that is a sheaf in the étale topology and that, locally in the étale topology, is an affine scheme. Equivalently, an algebraic space is the quotient of a scheme by an étale equivalence relation. A powerful result, the Artin representability theorem, gives simple conditions for a functor to be represented by an algebraic space.[20]\n\nA further generalization is the idea of a stack. Crudely speaking, algebraic stacks generalize algebraic spaces by having an algebraic group attached to each point, which is viewed as the automorphism group of that point. For example, any action of an algebraic group G on an algebraic variety X determines a quotient stack [X/G], which remembers the stabilizer subgroups for the action of G. More generally, moduli spaces in algebraic geometry are often best viewed as stacks, thereby keeping track of the automorphism groups of the objects being classified.\n\nGrothendieck originally introduced stacks as a tool for the theory of descent. In that formulation, stacks are (informally speaking) sheaves of categories.[21] From this general notion, Artin defined the narrower class of algebraic stacks (or \"Artin stacks\"), which can be considered geometric objects. These include Deligne–Mumford stacks (similar to orbifolds in topology), for which the stabilizer groups are finite, and algebraic spaces, for which the stabilizer groups are trivial. The Keel–Mori theorem says that an algebraic stack with finite stabilizer groups has a coarse moduli space that is an algebraic space.\n\nAnother type of generalization is to enrich the structure sheaf, bringing algebraic geometry closer to homotopy theory. In this setting, known as derived algebraic geometry or \"spectral algebraic geometry\", the structure sheaf is replaced by a homotopical analog of a sheaf of commutative rings (for example, a sheaf of E-infinity ring spectra). These sheaves admit algebraic operations that are associative and commutative only up to an equivalence relation. Taking the quotient by this equivalence relation yields the structure sheaf of an ordinary scheme. Not taking the quotient, however, leads to a theory that can remember higher information, in the same way that derived functors in homological algebra yield higher information about operations such as tensor product and the Hom functor on modules.\n"
    },
    {
        "title": "Algebraic geometry",
        "content": "Algebraic geometry is a branch of mathematics which uses abstract algebraic techniques, mainly from commutative algebra, to solve geometrical problems. Classically, it studies zeros of multivariate polynomials; the modern approach generalizes this in a few different aspects.\n\nThe fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. These are plane algebraic curves. A point of the plane lies on an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of points of special interest like singular points, inflection points and points at infinity. More advanced questions involve the topology of the curve and the relationship between curves defined by different equations.\n\nAlgebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. As a study of systems of polynomial equations in several variables, the subject of algebraic geometry begins with finding specific solutions via equation solving, and then proceeds to understand the intrinsic properties of the totality of solutions of a system of equations. This understanding requires both conceptual theory and computational technique.\n\nIn the 20th century, algebraic geometry split into several subareas.\n\nMuch of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on \"intrinsic\" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles' proof of the longstanding conjecture called Fermat's Last Theorem is an example of the power of this approach.\n\nIn classical algebraic geometry, the main objects of interest are the vanishing sets of collections of polynomials, meaning the set of all points that simultaneously satisfy one or more polynomial equations. For instance, the two-dimensional sphere of radius 1 in three-dimensional Euclidean space R3 could be defined as the set of all points \n\n\n\n(\nx\n,\ny\n,\nz\n)\n\n\n{\\displaystyle (x,y,z)}\n\n with\n\nA \"slanted\" circle in R3 can be defined as the set of all points \n\n\n\n(\nx\n,\ny\n,\nz\n)\n\n\n{\\displaystyle (x,y,z)}\n\n  which satisfy the two polynomial equations\n\nFirst we start with a field k. In classical algebraic geometry, this field was always the complex numbers C, but many of the same results are true if we assume only that k is algebraically closed. We consider the affine space of dimension n over k, denoted An(k) (or more simply An, when k is clear from the context). When one fixes a coordinate system, one may identify An(k) with kn. The purpose of not working with kn is to emphasize that one \"forgets\" the vector space structure that kn carries.\n\nA function f : An → A1 is said to be polynomial (or regular) if it can be written as a polynomial, that is, if there is a polynomial p in k[x1,...,xn] such that f(M) = p(t1,...,tn) for every point M with coordinates (t1,...,tn) in An. The property of a function to be polynomial (or regular) does not depend on the choice of a coordinate system in An.\n\nWhen a coordinate system is chosen, the regular functions on the affine n-space may be identified with the ring of polynomial functions in n variables over k. Therefore, the set of the regular functions on An is a ring, which is denoted k[An].\n\nWe say that a polynomial vanishes at a point if evaluating it at that point gives zero. Let S be a set of polynomials in k[An]. The vanishing set of S (or vanishing locus or zero set) is the set V(S) of all points in An where every polynomial in S vanishes. Symbolically,\n\nA subset of An which is V(S), for some S, is called an algebraic set. The V stands for variety (a specific type of algebraic set to be defined below).\n\nGiven a subset U of An, can one recover the set of polynomials which generate it? If U is any subset of An, define I(U) to be the set of all polynomials whose vanishing set contains U. The I stands for ideal: if two polynomials f and g both vanish on U, then f+g vanishes on U, and if h is any polynomial, then hf vanishes on U, so I(U) is always an ideal of the polynomial ring k[An].\n\nTwo natural questions to ask are:\n\nThe answer to the first question is provided by introducing the Zariski topology, a topology on An whose closed sets are the algebraic sets, and which directly reflects the algebraic structure of k[An]. Then U = V(I(U)) if and only if U is an algebraic set or equivalently a Zariski-closed set. The answer to the second question is given by Hilbert's Nullstellensatz. In one of its forms, it says that I(V(S)) is the radical of the ideal generated by S. In more abstract language, there is a Galois connection, giving rise to two closure operators; they can be identified, and naturally play a basic role in the theory; the example is elaborated at Galois connection.\n\nFor various reasons we may not always want to work with the entire ideal corresponding to an algebraic set U. Hilbert's basis theorem implies that ideals in k[An] are always finitely generated.\n\nAn algebraic set is called irreducible if it cannot be written as the union of two smaller algebraic sets. Any algebraic set is a finite union of irreducible algebraic sets and this decomposition is unique. Thus its elements are called the irreducible components of the algebraic set. An irreducible algebraic set is also called a variety. It turns out that an algebraic set is a variety if and only if it may be defined as the vanishing set of a prime ideal of the polynomial ring.\n\nSome authors do not make a clear distinction between algebraic sets and varieties and use irreducible variety to make the distinction when needed.\n\nJust as continuous functions are the natural maps on topological spaces and smooth functions are the natural maps on differentiable manifolds, there is a natural class of functions on an algebraic set, called regular functions or polynomial functions. A regular function on an algebraic set V contained in An is the restriction to V of a regular function on An. For an algebraic set defined on the field of the complex numbers, the regular functions are smooth and even analytic.\n\nIt may seem unnaturally restrictive to require that a regular function always extend to the ambient space, but it is very similar to the situation in a normal topological space, where the Tietze extension theorem guarantees that a continuous function on a closed subset always extends to the ambient topological space.\n\nJust as with the regular functions on affine space, the regular functions on V form a ring, which we denote by k[V]. This ring is called the coordinate ring of V.\n\nSince regular functions on V come from regular functions on An, there is a relationship between the coordinate rings. Specifically, if a regular function on V is the restriction of two functions f and g in k[An], then f − g is a polynomial function which is null on V and thus belongs to I(V). Thus k[V] may be identified with k[An]/I(V).\n\nUsing regular functions from an affine variety to A1, we can define regular maps from one affine variety to another. First we will define a regular map from a variety into affine space: Let V be a variety contained in An. Choose m regular functions on V, and call them f1, ..., fm. We define a regular map f from V to Am by letting f = (f1, ..., fm). In other words, each fi determines one coordinate of the range of f.\n\nIf V′ is a variety contained in Am, we say that f is a regular map from V to V′ if the range of f is contained in V′.\n\nThe definition of the regular maps apply also to algebraic sets.\nThe regular maps are also called morphisms, as they make the collection of all affine algebraic sets into a category, where the objects are the affine algebraic sets and the morphisms are the regular maps. The affine varieties is a subcategory of the category of the algebraic sets.\n\nGiven a regular map g from V to V′ and a regular function f of k[V′], then f ∘ g ∈ k[V]. The map f → f ∘ g is a ring homomorphism from k[V′] to k[V]. Conversely, every ring homomorphism from k[V′] to k[V] defines a regular map from V to V′. This defines an equivalence of categories between the category of algebraic sets and the opposite category of the finitely generated reduced k-algebras. This equivalence is one of the starting points of scheme theory.\n\nIn contrast to the preceding sections, this section concerns only varieties and not algebraic sets. On the other hand, the definitions extend naturally to projective varieties (next section), as an affine variety and its projective completion have the same field of functions.\n\nIf V is an affine variety, its coordinate ring is an integral domain and has thus a field of fractions which is denoted k(V) and called the field of the rational functions on V or, shortly, the function field of V. Its elements are the restrictions to V of the rational functions over the affine space containing V. The domain of a rational function f is not V but the complement of the subvariety (a hypersurface) where the denominator of f vanishes.\n\nAs with regular maps, one may define a rational map from a variety V to a variety V'. As with the regular maps, the rational maps from V to V' may be identified to the field homomorphisms from k(V') to k(V).\n\nTwo affine varieties are birationally equivalent if there are two rational functions between them which are inverse one to the other in the regions where both are defined. Equivalently, they are birationally equivalent if their function fields are isomorphic.\n\nAn affine variety is a rational variety if it is birationally equivalent to an affine space. This means that the variety admits a rational parameterization, that is a parametrization with rational functions. For example, the circle of equation \n\n\n\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n−\n1\n=\n0\n\n\n{\\displaystyle x^{2}+y^{2}-1=0}\n\n is a rational curve, as it has the parametric equation\n\nwhich may also be viewed as a rational map from the line to the circle.\n\nThe problem of resolution of singularities is to know if every algebraic variety is birationally equivalent to a variety whose projective completion is nonsingular (see also smooth completion). It was solved in the affirmative in characteristic 0 by Heisuke Hironaka in 1964 and is yet unsolved in finite characteristic.\n\nJust as the formulas for the roots of second, third, and fourth degree polynomials suggest extending real numbers to the more algebraically complete setting of the complex numbers, many properties of algebraic varieties suggest extending affine space to a more geometrically complete projective space. Whereas the complex numbers are obtained by adding the number i, a root of the polynomial x2 + 1, projective space is obtained by adding in appropriate points \"at infinity\", points where parallel lines may meet.\n\nTo see how this might come about, consider the variety V(y − x2). If we draw it, we get a parabola. As x goes to positive infinity, the slope of the line from the origin to the point (x, x2) also goes to positive infinity. As x goes to negative infinity, the slope of the same line goes to negative infinity.\n\nCompare this to the variety V(y − x3). This is a cubic curve. As x goes to positive infinity, the slope of the line from the origin to the point (x, x3) goes to positive infinity just as before. But unlike before, as x goes to negative infinity, the slope of the same line goes to positive infinity as well; the exact opposite of the parabola. So the behavior \"at infinity\" of V(y − x3) is different from the behavior \"at infinity\" of V(y − x2).\n\nThe consideration of the projective completion of the two curves, which is their prolongation \"at infinity\" in the projective plane, allows us to quantify this difference: the point at infinity of the parabola is a regular point, whose tangent is the line at infinity, while the point at infinity of the cubic curve is a cusp. Also, both curves are rational, as they are parameterized by x, and the Riemann-Roch theorem implies that the cubic curve must have a singularity, which must be at infinity, as all its points in the affine space are regular.\n\nThus many of the properties of algebraic varieties, including birational equivalence and all the topological properties, depend on the behavior \"at infinity\" and so it is natural to study the varieties in projective space. Furthermore, the introduction of projective techniques made many theorems in algebraic geometry simpler and sharper: For example, Bézout's theorem on the number of intersection points between two varieties can be stated in its sharpest form only in projective space. For these reasons, projective space plays a fundamental role in algebraic geometry.\n\nNowadays, the projective space Pn of dimension n is usually defined as the set of the lines passing through a point, considered as the origin, in the affine space of dimension n + 1, or equivalently to the set of the vector lines in a vector space of dimension n + 1. When a coordinate system has been chosen in the space of dimension n + 1, all the points of a line have the same set of coordinates, up to the multiplication by an element of k. This defines the homogeneous coordinates of a point of Pn as a sequence of n + 1 elements of the base field k, defined up to the multiplication by a nonzero element of k (the same for the whole sequence).\n\nA polynomial in n + 1 variables vanishes at all points of a line passing through the origin if and only if it is homogeneous. In this case, one says that the polynomial vanishes at the corresponding point of Pn. This allows us to define a projective algebraic set in Pn as the set V(f1, ..., fk), where a finite set of homogeneous polynomials {f1, ..., fk}  vanishes. Like for affine algebraic sets, there is a bijection between the projective algebraic sets and the reduced homogeneous ideals which define them. The projective varieties are the projective algebraic sets whose defining ideal is prime. In other words, a projective variety is a projective algebraic set, whose homogeneous coordinate ring is an integral domain, the projective coordinates ring being defined as the quotient of the graded ring or the polynomials in n + 1 variables by the homogeneous (reduced) ideal defining the variety. Every projective algebraic set may be uniquely decomposed into a finite union of projective varieties.\n\nThe only regular functions which may be defined properly on a projective variety are the constant functions. Thus this notion is not used in projective situations. On the other hand, the field of the rational functions or function field  is a useful notion, which, similarly to the affine case, is defined as the set of the quotients of two homogeneous elements of the same degree in the homogeneous coordinate ring.\n\nReal algebraic geometry is the study of real algebraic varieties.\n\nThe fact that the field of the real numbers is an ordered field cannot be ignored in such a study. For example, the curve of equation \n\n\n\n\nx\n\n2\n\n\n+\n\ny\n\n2\n\n\n−\na\n=\n0\n\n\n{\\displaystyle x^{2}+y^{2}-a=0}\n\n is a circle if \n\n\n\na\n>\n0\n\n\n{\\displaystyle a>0}\n\n, but has no real points if \n\n\n\na\n<\n0\n\n\n{\\displaystyle a<0}\n\n. Real algebraic geometry also investigates, more broadly, semi-algebraic sets, which are the solutions of systems of polynomial inequalities. For example, neither branch of the hyperbola of equation \n\n\n\nx\ny\n−\n1\n=\n0\n\n\n{\\displaystyle xy-1=0}\n\n is a real algebraic variety. However, the branch in the first quadrant is a semi-algebraic set defined by \n\n\n\nx\ny\n−\n1\n=\n0\n\n\n{\\displaystyle xy-1=0}\n\n and \n\n\n\nx\n>\n0\n\n\n{\\displaystyle x>0}\n\n.\n\nOne open problem in real algebraic geometry is the following part of Hilbert's sixteenth problem: Decide which respective positions are possible for the ovals of a nonsingular plane curve of degree 8.\n\nOne may date the origin of computational algebraic geometry to meeting EUROSAM'79 (International Symposium on Symbolic and Algebraic Manipulation) held at Marseille, France, in June 1979. At this meeting,\n\nSince then, most results in this area are related to one or several of these items either by using or improving one of these algorithms, or by finding algorithms whose complexity is simply exponential in the number of the variables.\n\nA body of mathematical theory complementary to symbolic methods called numerical algebraic geometry has been developed over the last several decades.  The main computational method is homotopy continuation.  This supports, for example, a model of floating point computation for solving problems of algebraic geometry.\n\nA Gröbner basis is a system of generators of a polynomial ideal whose computation allows the deduction of many properties of the affine algebraic variety defined by the ideal.\n\nGiven an ideal I defining an algebraic set V:\n\nGröbner basis computations do not allow one to compute directly the primary decomposition of I nor the prime ideals defining the irreducible components of V, but most algorithms for this involve Gröbner basis computation. The algorithms which are not based on Gröbner bases use regular chains but may need Gröbner bases in some exceptional situations.\n\nGröbner bases are deemed to be difficult to compute. In fact they may contain, in the worst case, polynomials whose degree is doubly exponential in the number of variables and a number of polynomials which is also doubly exponential. However, this is only a worst case complexity, and the complexity bound of Lazard's algorithm of 1979 may frequently apply. Faugère F5 algorithm realizes this complexity, as it may be viewed as an improvement of Lazard's 1979 algorithm. It follows that the best implementations allow one to compute almost routinely with algebraic sets of degree more than 100. This means that, presently, the difficulty of computing a Gröbner basis is strongly related to the intrinsic difficulty of the problem.\n\nCAD is an algorithm which was introduced in 1973 by G. Collins to implement with an acceptable complexity the Tarski–Seidenberg theorem on quantifier elimination over the real numbers.\n\nThis theorem concerns the formulas of the first-order logic whose atomic formulas are polynomial equalities or inequalities between polynomials with real coefficients. These formulas are thus the formulas which may be constructed from the atomic formulas by the logical operators and (∧), or (∨), not (¬), for all (∀) and exists (∃). Tarski's theorem asserts that, from such a formula, one may compute an equivalent formula without quantifier (∀, ∃).\n\nThe complexity of CAD is doubly exponential in the number of variables. This means that CAD allows, in theory, to solve every problem of real algebraic geometry which may be expressed by such a formula, that is almost every problem concerning explicitly given varieties and semi-algebraic sets.\n\nWhile Gröbner basis computation has doubly exponential complexity only in rare cases, CAD has almost always this high complexity. This implies that, unless if most polynomials appearing in the input are linear, it may not solve problems with more than four variables.\n\nSince 1973, most of the research on this subject is devoted either to improving CAD or finding alternative algorithms in special cases of general interest.\n\nAs an example of the state of art, there are efficient algorithms to find at least a point in every connected component of a semi-algebraic set, and thus to test if a semi-algebraic set is empty. On the other hand, CAD is yet, in practice, the best algorithm to count the number of connected components.\n\nThe basic general algorithms of computational geometry have a double exponential worst case complexity. More precisely, if d is the maximal degree of the input polynomials and n the number of variables, their complexity is at most \n\n\n\n\nd\n\n\n2\n\nc\nn\n\n\n\n\n\n\n{\\displaystyle d^{2^{cn}}}\n\n for some constant c, and, for some inputs, the complexity is at least \n\n\n\n\nd\n\n\n2\n\n\nc\n′\n\nn\n\n\n\n\n\n\n{\\displaystyle d^{2^{c'n}}}\n\n for another constant c′.\n\nDuring the last 20 years of the 20th century, various algorithms have been introduced to solve specific subproblems with a better complexity. Most of these algorithms have a complexity \n\n\n\n\nd\n\nO\n(\n\nn\n\n2\n\n\n)\n\n\n\n\n{\\displaystyle d^{O(n^{2})}}\n\n.[1]\n\nAmong these algorithms which solve a sub problem of the problems solved by Gröbner bases, one may cite testing if an affine variety is empty and solving nonhomogeneous polynomial systems which have a finite number of solutions. Such algorithms are rarely implemented because, on most entries Faugère's F4 and F5 algorithms have a better practical efficiency and probably a similar or better complexity (probably because the evaluation of the complexity of Gröbner basis algorithms on a particular class of entries is a difficult task which has been done only in a few special cases).\n\nThe main algorithms of real algebraic geometry which solve a problem solved by CAD are related to the topology of semi-algebraic sets. One may cite counting the number of connected components, testing if two points are in the same components or computing a Whitney stratification of a real algebraic set. They have a complexity of\n\n\n\n\n\nd\n\nO\n(\n\nn\n\n2\n\n\n)\n\n\n\n\n{\\displaystyle d^{O(n^{2})}}\n\n, but the constant involved by O notation is so high that using them to solve any nontrivial problem effectively solved by CAD, is impossible even if one could use all the existing computing power in the world. Therefore, these algorithms have never been implemented and this is an active research area to search for algorithms with have together a good asymptotic complexity and a good practical efficiency.\n\nThe modern approaches to algebraic geometry redefine and effectively extend the range of basic objects in various levels of generality to schemes, formal schemes, ind-schemes, algebraic spaces, algebraic stacks and so on. The need for this arises already from the useful ideas within theory of varieties, e.g. the formal functions of Zariski can be accommodated by introducing nilpotent elements in structure rings; considering spaces of loops and arcs, constructing quotients by group actions and developing formal grounds for natural intersection theory and deformation theory lead to some of the further extensions.\n\nMost remarkably, in the early 1960s, algebraic varieties were subsumed into Alexander Grothendieck's concept of a scheme. Their local objects are affine schemes or prime spectra which are locally ringed spaces which form a category which is antiequivalent to the category of commutative unital rings, extending the duality between the category of affine algebraic varieties over a field k, and the category of finitely generated reduced k-algebras. The gluing is along Zariski topology; one can glue within the category of locally ringed spaces, but also, using the Yoneda embedding, within the more abstract category of presheaves of sets over the category of affine schemes. The Zariski topology in the set theoretic sense is then replaced by a Grothendieck topology. Grothendieck introduced Grothendieck topologies having in mind more exotic but geometrically finer and more sensitive examples than the crude Zariski topology, namely the étale topology, and the two flat Grothendieck topologies: fppf and fpqc; nowadays some other examples became prominent including Nisnevich topology. Sheaves can be furthermore generalized to stacks in the sense of Grothendieck, usually with some additional representability conditions leading to Artin stacks and, even finer, Deligne–Mumford stacks, both often called algebraic stacks.\n\nSometimes other algebraic sites replace the category of affine schemes. For example, Nikolai Durov has introduced commutative algebraic monads as a generalization of local objects in a generalized algebraic geometry. Versions of a tropical geometry, of an absolute geometry over a field of one element and an algebraic analogue of Arakelov's geometry were realized in this setup.\n\nAnother formal generalization is possible to universal algebraic geometry in which every variety of algebras has its own algebraic geometry. The term variety of algebras should not be confused with algebraic variety.\n\nThe language of schemes, stacks and generalizations has proved to be a valuable way of dealing with geometric concepts and became cornerstones of modern algebraic geometry.\n\nAlgebraic stacks can be further generalized and for many practical questions like deformation theory and intersection theory, this is often the most natural approach. One can extend the Grothendieck site of affine schemes to a higher categorical site of derived affine schemes, by replacing the commutative rings with an infinity category of differential graded commutative algebras, or of simplicial commutative rings or a similar category with an appropriate variant of a Grothendieck topology. One can also replace presheaves of sets by presheaves of simplicial sets (or of infinity groupoids). Then, in presence of an appropriate homotopic machinery one can develop a notion of derived stack as such a presheaf on the infinity category of derived affine schemes, which is satisfying certain infinite categorical version of a sheaf axiom (and to be algebraic, inductively a sequence of representability conditions). Quillen model categories, Segal categories and quasicategories are some of the most often used tools to formalize this yielding the derived algebraic geometry, introduced by the school of Carlos Simpson, including Andre Hirschowitz, Bertrand Toën, Gabrielle Vezzosi, Michel Vaquié and others; and developed further by Jacob Lurie, Bertrand Toën, and Gabriele Vezzosi. Another (noncommutative) version of derived algebraic geometry, using A-infinity categories has been developed from the early 1990s by Maxim Kontsevich and followers.\n\nSome of the roots of algebraic geometry date back to the work of the Hellenistic Greeks from the 5th century BC. The Delian problem, for instance, was to construct a length x so that the cube of side x contained the same volume as the rectangular box a2b for given sides a and b. Menaechmus (c. 350 BC) considered the problem geometrically by intersecting the pair of plane conics ay = x2 and xy = ab.[2] In the 3rd century BC, Archimedes and Apollonius systematically studied additional problems on conic sections using coordinates.[2][3] Apollonius in the Conics further developed a method that is so similar to analytic geometry that his work is sometimes thought to have anticipated the work of Descartes by some 1800 years.[4] His application of reference lines, a diameter and a tangent is essentially no different from our modern use of a coordinate frame, where the distances measured along the diameter from the point of tangency are the abscissas, and the segments parallel to the tangent and intercepted between the axis and the curve are the ordinates. He further developed relations between the abscissas and the corresponding coordinates using geometric methods like using parabolas and curves.[5][6][7] Medieval  mathematicians, including Omar Khayyam, Leonardo of Pisa, Gersonides and Nicole Oresme in the Medieval Period,[8] solved certain cubic and quadratic equations by purely algebraic means and then interpreted the results geometrically. The Persian mathematician Omar Khayyám (born 1048 AD) believed that there was a relationship between arithmetic, algebra and geometry.[9][10][11] This was criticized by Jeffrey Oaks, who claims that the study of curves by means of equations originated with Descartes in the seventeenth century.[12]\n\nSuch techniques of applying geometrical constructions to algebraic problems were also adopted by a number of Renaissance mathematicians such as Gerolamo Cardano and Niccolò Fontana \"Tartaglia\" on their studies of the cubic equation. The geometrical approach to construction problems, rather than the algebraic one, was favored by most 16th and 17th century mathematicians, notably Blaise Pascal who argued against the use of algebraic and analytical methods in geometry.[13] The French mathematicians Franciscus Vieta and later René Descartes and Pierre de Fermat revolutionized the conventional way of thinking about construction problems through the introduction of coordinate geometry. They were interested primarily in the properties of algebraic curves, such as those defined by Diophantine equations (in the case of Fermat), and the algebraic reformulation of the classical Greek works on conics and cubics (in the case of Descartes).\n\nDuring the same period, Blaise Pascal and Gérard Desargues approached geometry from a different perspective, developing the synthetic notions of projective geometry. Pascal and Desargues also studied curves, but from the purely geometrical point of view: the analog of the Greek ruler and compass construction. Ultimately, the analytic geometry of Descartes and Fermat won out, for it supplied the 18th century mathematicians with concrete quantitative tools needed to study physical problems using the new calculus of Newton and Leibniz. However, by the end of the 18th century, most of the algebraic character of coordinate geometry was subsumed by the calculus of infinitesimals of Lagrange and Euler.\n\nIt took the simultaneous 19th century developments of non-Euclidean geometry and Abelian integrals in order to bring the old algebraic ideas back into the geometrical fold. The first of these new developments was seized up by Edmond Laguerre and Arthur Cayley, who attempted to ascertain the generalized metric properties of projective space. Cayley introduced the idea of homogeneous polynomial forms, and more specifically quadratic forms, on projective space. Subsequently, Felix Klein studied projective geometry (along with other types of geometry) from the viewpoint that the geometry on a space is encoded in a certain class of transformations on the space. By the end of the 19th century, projective geometers were studying more general kinds of transformations on figures in projective space. Rather than the projective linear transformations which were normally regarded as giving the fundamental Kleinian geometry on projective space, they concerned themselves also with the higher degree birational transformations. This weaker notion of congruence would later lead members of the 20th century Italian school of algebraic geometry to classify algebraic surfaces up to birational isomorphism.\n\nThe second early 19th century development, that of Abelian integrals, would lead Bernhard Riemann to the development of Riemann surfaces.\n\nIn the same period began the algebraization of the algebraic geometry through commutative algebra. The prominent results in this direction are Hilbert's basis theorem and Hilbert's Nullstellensatz, which are the basis of the connection between algebraic geometry and commutative algebra, and Macaulay's multivariate resultant, which is the basis of elimination theory. Probably because of the size of the computation which is implied by multivariate resultants, elimination theory was forgotten during the middle of the 20th century until it was renewed by singularity theory and computational algebraic geometry.[a]\n\nB. L. van der Waerden, Oscar Zariski and André Weil developed a foundation for algebraic geometry based on contemporary commutative algebra, including valuation theory and the theory of ideals. One of the goals was to give a rigorous framework for proving the results of the Italian school of algebraic geometry. In particular, this school used systematically the notion of generic point without any precise definition, which was first given by these authors during the 1930s.\n\nIn the 1950s and 1960s, Jean-Pierre Serre and Alexander Grothendieck recast the foundations making use of sheaf theory. Later, from about 1960, and largely led by Grothendieck, the idea of schemes was worked out, in conjunction with a very refined apparatus of homological techniques. After a decade of rapid development the field stabilized in the 1970s, and new applications were made, both to number theory and to more classical geometric questions on algebraic varieties, singularities, moduli, and formal moduli.\n\nAn important class of varieties, not easily understood directly from their defining equations, are the abelian varieties, which are the projective varieties whose points form an abelian group. The prototypical examples are the elliptic curves, which have a rich theory. They were instrumental in the proof of Fermat's Last Theorem and are also used in elliptic-curve cryptography.\n\nIn parallel with the abstract trend of the algebraic geometry, which is concerned with general statements about varieties, methods for effective computation with concretely-given varieties have also been developed, which lead to the new area of computational algebraic geometry. One of the founding methods of this area is the theory of Gröbner bases, introduced by Bruno Buchberger in 1965. Another founding method, more specially devoted to real algebraic geometry, is the cylindrical algebraic decomposition, introduced by George E. Collins in 1973.\n\nSee also: derived algebraic geometry.\n\nAn analytic variety over the field of real or complex numbers is defined locally as the set of common solutions of several equations involving analytic functions. It is analogous to the concept of algebraic variety in that it carries a structure sheaf of analytic functions instead of regular functions. Any complex manifold is a complex analytic variety. Since analytic varieties may have singular points, not all complex analytic varieties are manifolds. Over a non-archimedean field analytic geometry is studied via rigid analytic spaces.\n\nModern analytic geometry over the field of complex numbers is closely related to complex algebraic geometry, as has been shown by Jean-Pierre Serre in his paper GAGA,[14] the name of which is French for Algebraic geometry and analytic geometry. The GAGA results over the field of complex numbers may be extended to rigid analytic spaces over non-archimedean fields.[15]\n\nAlgebraic geometry now finds applications in statistics,[16] control theory,[17][18] robotics,[19] error-correcting codes,[20] phylogenetics[21] and geometric modelling.[22] There are also connections to string theory,[23] game theory,[24] graph matchings,[25] solitons[26] and integer programming.[27]\n\n\n"
    },
    {
        "title": "Category theory",
        "content": "Category theory is a general theory of mathematical structures and their relations. It was introduced by Samuel Eilenberg and Saunders Mac Lane in the middle of the 20th century in their foundational work on algebraic topology.[1] Category theory is used in almost all areas of mathematics. In particular, many constructions of new mathematical objects from previous ones that appear similarly in several contexts are conveniently expressed and unified in terms of categories. Examples include quotient spaces, direct products, completion, and duality.\n\nMany areas of computer science also rely on category theory, such as functional programming and semantics.\n\nA category is formed by two sorts of objects: the objects of the category, and the morphisms, which relate two objects called the source and the target of the morphism. Metaphorically, a morphism is an arrow that maps its source to its target. Morphisms can be composed if the target of the first morphism equals the source of the second one. Morphism composition has similar properties as function composition (associativity and existence of an identity morphism for each object). Morphisms are often some sort of functions, but this is not always the case. For example, a monoid may be viewed as a category with a single object, whose morphisms are the elements of the monoid.\n\nThe second fundamental concept of category theory is the concept of a functor, which plays the role of a morphism between two categories \n\n\n\n\n\n\nC\n\n\n\n1\n\n\n\n\n{\\displaystyle {\\mathcal {C}}_{1}}\n\n and \n\n\n\n\n\n\nC\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\mathcal {C}}_{2}}\n\n: it maps objects of \n\n\n\n\n\n\nC\n\n\n\n1\n\n\n\n\n{\\displaystyle {\\mathcal {C}}_{1}}\n\n to objects of \n\n\n\n\n\n\nC\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\mathcal {C}}_{2}}\n\n and morphisms of \n\n\n\n\n\n\nC\n\n\n\n1\n\n\n\n\n{\\displaystyle {\\mathcal {C}}_{1}}\n\n to morphisms of \n\n\n\n\n\n\nC\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\mathcal {C}}_{2}}\n\n in such a way that sources are mapped to sources, and targets are mapped to targets (or, in the case of a contravariant functor, sources are mapped to targets and vice-versa). A third fundamental concept is a natural transformation that may be viewed as a morphism of functors.\n\nA category \n\n\n\n\n\nC\n\n\n\n\n{\\displaystyle {\\mathcal {C}}}\n\n consists of the following three mathematical entities:\n\nEach morphism \n\n\n\nf\n\n\n{\\displaystyle f}\n\n has a source object  \n\n\n\na\n\n\n{\\displaystyle a}\n\n and target object \n\n\n\nb\n\n\n{\\displaystyle b}\n\n.\nThe expression \n\n\n\nf\n:\na\n↦\nb\n\n\n{\\displaystyle f:a\\mapsto b}\n\n would be verbally stated as \"\n\n\n\nf\n\n\n{\\displaystyle f}\n\n is a morphism from a to b\".\nThe expression \n\n\n\n\nhom\n\n(\na\n,\nb\n)\n\n\n{\\displaystyle {\\text{hom}}(a,b)}\n\n – alternatively expressed as \n\n\n\n\n\nhom\n\n\n\nC\n\n\n\n(\na\n,\nb\n)\n\n\n{\\displaystyle {\\text{hom}}_{\\mathcal {C}}(a,b)}\n\n, \n\n\n\n\nmor\n\n(\na\n,\nb\n)\n\n\n{\\displaystyle {\\text{mor}}(a,b)}\n\n, or \n\n\n\n\n\nC\n\n\n(\na\n,\nb\n)\n\n\n{\\displaystyle {\\mathcal {C}}(a,b)}\n\n – denotes the hom-class of all morphisms from \n\n\n\na\n\n\n{\\displaystyle a}\n\n to \n\n\n\nb\n\n\n{\\displaystyle b}\n\n.[a]\nFrom the axioms, it can be proved that there is exactly one identity morphism for every object.\nRelations among morphisms (such as fg = h) are often depicted using commutative diagrams, with \"points\" (corners) representing objects and \"arrows\" representing morphisms.\n\nMorphisms can have any of the following properties. A morphism f : a → b is:\n\nEvery retraction is an epimorphism, and every section is a monomorphism. Furthermore, the following three statements are equivalent:\n\nFunctors are structure-preserving maps between categories. They can be thought of as morphisms in the category of all (small) categories.\n\nA (covariant) functor F from a category C to a category D, written F : C → D, consists of:\n\nsuch that the following two properties hold:\n\nA contravariant functor F: C → D is like a covariant functor, except that it \"turns morphisms around\" (\"reverses all the arrows\"). More specifically, every morphism f : x → y in C must be assigned to a morphism  F(f) : F(y) → F(x) in D. In other words, a contravariant functor acts as a covariant functor from the opposite category Cop to D.\n\nA natural transformation is a relation between two functors. Functors often describe \"natural constructions\" and natural transformations then describe \"natural homomorphisms\" between two such constructions. Sometimes two quite different constructions yield \"the same\" result; this is expressed by a natural isomorphism between the two functors.\n\nIf F and G are (covariant) functors between the categories C and D, then a natural transformation η from F to G associates to every object X in C a morphism ηX : F(X) → G(X) in D such that for every morphism f : X → Y in C, we have ηY ∘ F(f) = G(f) ∘ ηX; this means that the following diagram is commutative:\n\nThe two functors F and G are called naturally isomorphic if there exists a natural transformation from F to G such that ηX is an isomorphism for every object X in C.\n\nUsing the language of category theory, many areas of mathematical study can be categorized. Categories include sets, groups and topologies.\n\nEach category is distinguished by properties that all its objects have in common, such as the empty set or the product of two topologies, yet in the definition of a category, objects are considered atomic, i.e., we do not know whether an object A is a set, a topology, or any other abstract concept. Hence, the challenge is to define special objects without referring to the internal structure of those objects. To define the empty set without referring to elements, or the product topology without referring to open sets, one can characterize these objects in terms of their relations to other objects, as given by the morphisms of the respective categories. Thus, the task is to find universal properties that uniquely determine the objects of interest.\n\nNumerous important constructions can be described in a purely categorical way if the category limit can be developed and dualized to yield the notion of a colimit.\n\nIt is a natural question to ask: under which conditions can two categories be considered essentially the same, in the sense that theorems about one category can readily be transformed into theorems about the other category? The major tool one employs to describe such a situation is called equivalence of categories, which is given by appropriate functors between two categories. Categorical equivalence has found numerous applications in mathematics.\n\nThe definitions of categories and functors provide only the very basics of categorical algebra; additional important topics are listed below. Although there are strong interrelations between all of these topics, the given order can be considered as a guideline for further reading.\n\nMany of the above concepts, especially equivalence of categories, adjoint functor pairs, and functor categories, can be situated into the context of higher-dimensional categories. Briefly, if we consider a morphism between two objects as a \"process taking us from one object to another\", then higher-dimensional categories allow us to profitably generalize this by considering \"higher-dimensional processes\".\n\nFor example, a (strict) 2-category is a category together with \"morphisms between morphisms\", i.e., processes which allow us to transform one morphism into another. We can then \"compose\" these \"bimorphisms\" both horizontally and vertically, and we require a 2-dimensional \"exchange law\" to hold, relating the two composition laws. In this context, the standard example is Cat, the 2-category of all (small) categories, and in this example, bimorphisms of morphisms are simply natural transformations of morphisms in the usual sense. Another basic example is to consider a 2-category with a single object; these are essentially monoidal categories. Bicategories are a weaker notion of 2-dimensional categories in which the composition of morphisms is not strictly associative, but only associative \"up to\" an isomorphism.\n\nThis process can be extended for all natural numbers n, and these are called n-categories. There is even a notion of ω-category corresponding to the ordinal number ω.\n\nHigher-dimensional categories are part of the broader mathematical field of higher-dimensional algebra, a concept introduced by Ronald Brown.  For a conversational introduction to these ideas, see John Baez, 'A Tale of n-categories' (1996).\n\nIt should be observed first that the whole concept of a  category is essentially an auxiliary one; our basic concepts are essentially those of a functor and of a natural transformation [...]\nWhilst specific examples of functors and natural transformations had been given by Samuel Eilenberg and Saunders Mac Lane in a 1942 paper on group theory,[3] these concepts were introduced in a more general sense, together with the additional notion of categories, in a 1945 paper by the same authors[2] (who  discussed applications of category theory to the field of algebraic topology).[4] Their work was an important part of the transition from intuitive and geometric homology to homological algebra, Eilenberg and Mac Lane later writing that their goal was to understand natural transformations, which first required the definition of functors, then categories.\n\nStanislaw Ulam, and some writing on his behalf, have claimed that related ideas were current in the late 1930s in Poland.[citation needed] Eilenberg was Polish, and studied mathematics in Poland in the 1930s.[5] Category theory is also, in some sense, a continuation of the work of Emmy Noether (one of Mac Lane's teachers) in formalizing abstract processes;[6] Noether realized that understanding a type of mathematical structure requires understanding the processes that preserve that structure (homomorphisms).[citation needed] Eilenberg and Mac Lane introduced categories for understanding and formalizing the processes (functors) that relate topological structures to  algebraic structures (topological invariants) that characterize them.\n\nCategory theory was originally introduced for the need of homological algebra, and widely extended for the need of modern algebraic geometry (scheme theory). Category theory may be viewed as an extension of universal algebra, as the latter studies algebraic structures, and the former applies to any kind of mathematical structure and studies also the relationships between structures of different nature. For this reason, it is used throughout mathematics. Applications to mathematical logic and semantics (categorical abstract machine) came later.\n\nCertain categories called topoi (singular topos) can even serve as an alternative to axiomatic set theory as a foundation of mathematics. A topos can also be considered as a specific type of category with two additional topos axioms. These foundational applications of category theory have been worked out in fair detail as a basis for, and justification of, constructive mathematics. Topos theory is a form of abstract sheaf theory, with geometric origins, and leads to ideas such as pointless topology.\n\nCategorical logic is now a well-defined field based on type theory for intuitionistic logics, with applications in functional programming and domain theory, where a cartesian closed category is taken as a non-syntactic description of a lambda calculus. At the very least, category theoretic language clarifies what exactly these related areas have in common (in some abstract sense).\n\nCategory theory has been applied in other fields as well, see applied category theory. For example, John Baez has shown a link between Feynman diagrams in physics and monoidal categories.[7] Another application of category theory, more specifically topos theory, has been made in mathematical music theory, see for example the book The Topos of Music, Geometric Logic of Concepts, Theory, and Performance by Guerino Mazzola.\n\nMore recent efforts to introduce undergraduates to categories as a foundation for mathematics include those of William Lawvere and Rosebrugh (2003) and Lawvere and Stephen Schanuel (1997) and Mirroslav Yotov (2012).\n"
    },
    {
        "title": "Homological algebra",
        "content": "Homological algebra is the branch of mathematics that studies homology in a general algebraic setting. It is a relatively young discipline, whose origins can be traced to investigations in combinatorial topology (a precursor to algebraic topology) and abstract algebra (theory of modules and syzygies) at the end of the 19th century, chiefly by Henri Poincaré and David Hilbert. \n\nHomological algebra is the study of homological functors and the intricate algebraic structures that they entail; its development was closely intertwined with the emergence of category theory. A central concept is that of chain complexes, which can be studied through their homology and cohomology.\n\nHomological algebra affords the means to extract information contained in these complexes and present it in the form of homological invariants of rings, modules, topological spaces, and other \"tangible\" mathematical objects. A spectral sequence is a powerful tool for this.\n\nIt has played an enormous role in algebraic topology. Its influence has gradually expanded and presently includes commutative algebra, algebraic geometry, algebraic number theory, representation theory, mathematical physics, operator algebras, complex analysis, and the theory of partial differential equations. K-theory is an independent discipline which draws upon methods of homological algebra, as does the noncommutative geometry of Alain Connes.\n\nHomological algebra began to be studied in its most basic form in the 1800s as a branch of topology and in the 1940s became an independent subject with the study of objects such as the ext functor and the tor functor, among others.[1]\n\nThe notion of chain complex is central in homological algebra. An abstract chain complex is a sequence \n\n\n\n(\n\nC\n\n∙\n\n\n,\n\nd\n\n∙\n\n\n)\n\n\n{\\displaystyle (C_{\\bullet },d_{\\bullet })}\n\n of abelian groups and group homomorphisms, \nwith the property that the composition of any two consecutive maps is zero:\n\nThe elements of Cn are called n-chains and the homomorphisms dn are called the boundary maps or differentials. The chain groups Cn may be endowed with extra structure; for example, they may be vector spaces or modules over a fixed ring R. The differentials must preserve the extra structure if it exists; for example, they must be linear maps or homomorphisms of R-modules. For notational convenience, restrict attention to abelian groups (more correctly, to the category Ab of abelian groups); a celebrated theorem by Barry Mitchell implies the results will generalize to any abelian category. Every chain complex defines two further sequences of abelian groups, the cycles Zn = Ker dn and the boundaries Bn = Im dn+1, where Ker d and Im d denote the kernel and the image of d. Since the composition of two consecutive boundary maps is zero, these groups are embedded into each other as\n\nSubgroups of abelian groups are automatically normal; therefore we can define the nth homology group Hn(C) as the factor group of the n-cycles by the n-boundaries,\n\nA chain complex is called acyclic or an exact sequence if all its homology groups are zero.\n\nChain complexes arise in abundance in algebra and algebraic topology. For example, if X is a topological space then the singular chains Cn(X) are formal linear combinations of continuous maps from the standard n-simplex into X; if K is a simplicial complex then the simplicial chains Cn(K) are formal linear combinations of the n-simplices of K; if A = F/R is a presentation of an abelian group A by generators and relations, where F is a free abelian group spanned by the generators and R is the subgroup of relations, then letting C1(A) = R, C0(A) = F, and Cn(A) = 0 for all other n defines a sequence of abelian groups. In all these cases, there are natural differentials dn making  Cn into a chain complex, whose homology reflects the structure of the topological space X, the simplicial complex K, or the abelian group A. In the case of topological spaces, we arrive at the notion of singular homology, which plays a fundamental role in investigating the properties of such spaces, for example, manifolds.\n\nOn a philosophical level, homological algebra teaches us that certain chain complexes associated with algebraic or geometric objects (topological spaces, simplicial complexes, R-modules) contain a lot of valuable algebraic information about them, with the homology being only the most readily available part. On a technical level, homological algebra provides the tools for manipulating complexes and extracting this information. Here are two general illustrations.\n\nIn the context of group theory, a sequence\n\nof groups and group homomorphisms is called exact if the image of each homomorphism is equal to the kernel of the next:\n\nNote that the sequence of groups and homomorphisms may be either finite or infinite.\n\nA similar definition can be made for certain other algebraic structures.  For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.  More generally, the notion of an exact sequence makes sense in any category with kernels and cokernels.\n\nThe most common type of exact sequence is the short exact sequence. This is an exact sequence of the form\n\nwhere ƒ is a monomorphism and g is an epimorphism.  In this case, A is a subobject of B, and the corresponding quotient is isomorphic to C:\n\n(where  f(A) = im(f)).\n\nA short exact sequence of abelian groups may also be written as an exact sequence with five terms:\n\nwhere 0 represents the zero object, such as the trivial group or a zero-dimensional vector space.  The placement of the 0's forces ƒ to be a monomorphism and g to be an epimorphism (see below).\n\nA long exact sequence is an exact sequence indexed by the natural numbers.\n\nConsider the following commutative diagram in any abelian category (such as the category of abelian groups or the category of vector spaces over a given field) or in the category of groups.\n\n\n\nThe five lemma states that, if the rows are exact, m and p are isomorphisms, l is an epimorphism, and q is a monomorphism, then n is also an isomorphism.\n\nIn an abelian category (such as the category of abelian groups or the category of vector spaces over a given field), consider a commutative diagram:\n\n\n\nwhere the rows are exact sequences and 0 is the zero object.\nThen there is an exact sequence relating the kernels and cokernels of a, b, and c:\n\nFurthermore, if the morphism f is a monomorphism, then so is the morphism ker a → ker b, and if g' is an epimorphism, then so is coker b → coker c.\n\nIn mathematics, an abelian category is a category in which morphisms and objects can be added and in which kernels and cokernels exist and have desirable properties. The motivating prototype example of an abelian category is the category of abelian groups, Ab. The theory originated in a tentative attempt to unify several cohomology theories by Alexander Grothendieck. Abelian categories are very stable categories, for example they are regular and they satisfy the snake lemma. The class of Abelian categories is closed under several categorical constructions, for example, the category of chain complexes of an Abelian category, or the category of functors from a small category to an Abelian category are Abelian as well. These stability properties make them inevitable in homological algebra and beyond; the theory has major applications in algebraic geometry, cohomology and pure category theory. Abelian categories are named after Niels Henrik Abel.\n\nMore concretely, a category is abelian if\n\nSuppose we are given a covariant left exact functor F : A → B between two abelian categories A and B. If  0 → A → B → C → 0 is a short exact sequence in A, then applying F yields the exact sequence 0 → F(A) → F(B) → F(C) and one could ask how to continue this sequence to the right to form a long exact sequence. Strictly speaking, this question is ill-posed, since there are always numerous different ways to continue a given exact sequence to the right. But it turns out that (if A is \"nice\" enough) there is one canonical way of doing so, given by the right derived functors of F. For every i≥1, there is a functor RiF: A → B, and the above sequence continues like so: 0 → F(A) → F(B) → F(C) → R1F(A) → R1F(B) → R1F(C) → R2F(A) → R2F(B) → ... . From this we see that F is an exact functor if and only if R1F = 0; so in a sense the right derived functors of F measure \"how far\" F is from being exact.\n\nLet R be a ring and let ModR be the category of modules over R. Let B be in ModR and set T(B) = HomR(A,B), for fixed A in ModR. This is a left exact functor and thus has right derived functors RnT. The Ext functor is defined by\n\nThis can be calculated by taking any injective resolution\n\nand computing\n\nThen (RnT)(B) is the cohomology of this complex. Note that HomR(A,B) is excluded from the complex.\n\nAn alternative definition is given using the functor G(A)=HomR(A,B). For a fixed module B, this is a contravariant left exact functor, and thus we also have right derived functors RnG, and can define\n\nThis can be calculated by choosing any projective resolution\n\nand proceeding dually by computing\n\nThen (RnG)(A) is the cohomology of this complex. Again note that HomR(A,B) is excluded.\n\nThese two constructions turn out to yield isomorphic results, and so both may be used to calculate the Ext functor.\n\nSuppose R is a ring, and denoted by R-Mod the category of left R-modules and by Mod-R the category of right R-modules (if R is commutative, the two categories coincide). Fix a module B in R-Mod. For A in Mod-R, set T(A) = A⊗RB. Then T is a right exact functor from Mod-R to the category of abelian groups Ab (in the case when R is commutative, it is a right exact functor from Mod-R to Mod-R) and its left derived functors LnT are defined. We set\n\ni.e., we take a projective resolution\n\nthen remove the A term and tensor the projective resolution with B to get the complex\n\n(note that A⊗RB does not appear and the last arrow is just the zero map) and take the homology of this complex.\n\nFix an abelian category, such as a category of modules over a ring.  A spectral sequence is a choice of a nonnegative integer r0 and a collection of three sequences:\n\nA doubly graded spectral sequence has a tremendous amount of data to keep track of, but there is a common visualization technique which makes the structure of the spectral sequence clearer.  We have three indices, r, p, and q.  For each r, imagine that we have a sheet of graph paper.  On this sheet, we will take p to be the horizontal direction and q to be the vertical direction.  At each lattice point we have the object \n\n\n\n\nE\n\nr\n\n\np\n,\nq\n\n\n\n\n{\\displaystyle E_{r}^{p,q}}\n\n.\n\nIt is very common for n = p + q to be another natural index in the spectral sequence. n runs diagonally, northwest to southeast, across each sheet.  In the homological case, the differentials have bidegree (−r, r − 1), so they decrease n by one.  In the cohomological case, n is increased by one.  When r is zero, the differential moves objects one space down or up.  This is similar to the differential on a chain complex.  When r is one, the differential moves objects one space to the left or right.  When r is two, the differential moves objects just like a knight's move in chess.  For higher r, the differential acts like a generalized knight's move.\n\nA continuous map of topological spaces gives rise to a homomorphism between their nth homology groups for all n. This basic fact of algebraic topology finds a natural explanation through certain properties of chain complexes. Since it is very common to study\nseveral topological spaces simultaneously, in homological algebra one is led to simultaneous consideration of multiple chain complexes.\n\nA morphism between two chain complexes, \n\n\n\nF\n:\n\nC\n\n∙\n\n\n→\n\nD\n\n∙\n\n\n,\n\n\n{\\displaystyle F:C_{\\bullet }\\to D_{\\bullet },}\n\n is a family of homomorphisms of abelian groups \n\n\n\n\nF\n\nn\n\n\n:\n\nC\n\nn\n\n\n→\n\nD\n\nn\n\n\n\n\n{\\displaystyle F_{n}:C_{n}\\to D_{n}}\n\n that commute with the differentials, in the sense that \n\n\n\n\nF\n\nn\n−\n1\n\n\n∘\n\nd\n\nn\n\n\nC\n\n\n=\n\nd\n\nn\n\n\nD\n\n\n∘\n\nF\n\nn\n\n\n\n\n{\\displaystyle F_{n-1}\\circ d_{n}^{C}=d_{n}^{D}\\circ F_{n}}\n\n for all n. A morphism of chain complexes induces a morphism \n\n\n\n\nH\n\n∙\n\n\n(\nF\n)\n\n\n{\\displaystyle H_{\\bullet }(F)}\n\n of their homology groups, consisting of the homomorphisms \n\n\n\n\nH\n\nn\n\n\n(\nF\n)\n:\n\nH\n\nn\n\n\n(\nC\n)\n→\n\nH\n\nn\n\n\n(\nD\n)\n\n\n{\\displaystyle H_{n}(F):H_{n}(C)\\to H_{n}(D)}\n\n for all n. A morphism F is called a quasi-isomorphism if it induces an isomorphism on the nth homology for all n.\n\nMany constructions of chain complexes arising in algebra and geometry, including singular homology, have the following functoriality property: if two objects X and Y are connected by a map f, then the associated chain complexes are connected by a morphism \n\n\n\nF\n=\nC\n(\nf\n)\n:\n\nC\n\n∙\n\n\n(\nX\n)\n→\n\nC\n\n∙\n\n\n(\nY\n)\n,\n\n\n{\\displaystyle F=C(f):C_{\\bullet }(X)\\to C_{\\bullet }(Y),}\n\n and moreover, the composition \n\n\n\ng\n∘\nf\n\n\n{\\displaystyle g\\circ f}\n\n of maps f: X → Y and  g: Y → Z induces the morphism \n\n\n\nC\n(\ng\n∘\nf\n)\n:\n\nC\n\n∙\n\n\n(\nX\n)\n→\n\nC\n\n∙\n\n\n(\nZ\n)\n\n\n{\\displaystyle C(g\\circ f):C_{\\bullet }(X)\\to C_{\\bullet }(Z)}\n\n that coincides with the composition \n\n\n\nC\n(\ng\n)\n∘\nC\n(\nf\n)\n.\n\n\n{\\displaystyle C(g)\\circ C(f).}\n\n It follows that the homology groups \n\n\n\n\nH\n\n∙\n\n\n(\nC\n)\n\n\n{\\displaystyle H_{\\bullet }(C)}\n\n are functorial as well, so that morphisms between algebraic or topological objects give rise to compatible maps between their homology.\n\nThe following definition arises from a typical situation in algebra and topology. A triple consisting of three chain complexes \n\n\n\n\nL\n\n∙\n\n\n,\n\nM\n\n∙\n\n\n,\n\nN\n\n∙\n\n\n\n\n{\\displaystyle L_{\\bullet },M_{\\bullet },N_{\\bullet }}\n\n and two morphisms between them, \n\n\n\nf\n:\n\nL\n\n∙\n\n\n→\n\nM\n\n∙\n\n\n,\ng\n:\n\nM\n\n∙\n\n\n→\n\nN\n\n∙\n\n\n,\n\n\n{\\displaystyle f:L_{\\bullet }\\to M_{\\bullet },g:M_{\\bullet }\\to N_{\\bullet },}\n\n is called an exact triple, or a short exact sequence of complexes, and written as\n\nif for any n, the sequence\n\nis a short exact sequence of abelian groups. By definition, this means that fn is an injection, gn is a surjection, and Im fn =  Ker gn. One of the most basic theorems of homological algebra, sometimes known as the zig-zag lemma, states that, in this case, there is a long exact sequence in homology\n\nwhere the homology groups of L, M, and N cyclically follow each other, and δn are certain homomorphisms determined by f and g, called the connecting homomorphisms.  Topological manifestations of this theorem include the Mayer–Vietoris sequence and the long exact sequence for relative homology.\n\nCohomology theories have been defined for many different objects such as topological spaces, sheaves, groups, rings, Lie algebras, and C*-algebras. The study of modern algebraic geometry would be almost unthinkable without sheaf cohomology.\n\nCentral to homological algebra is the notion of exact sequence; these can be used to perform actual calculations. A classical tool of homological algebra is that of derived functor; the most basic examples are functors Ext and Tor.\n\nWith a diverse set of applications in mind, it was natural to try to put the whole subject on a uniform basis. There were several attempts before the subject settled down. An approximate history can be stated as follows:\n\nThese move from computability to generality.\n\nThe computational sledgehammer par excellence is the spectral sequence; these are essential in the Cartan-Eilenberg and Tohoku approaches where they are needed, for instance, to compute the derived functors of a composition of two functors.  Spectral sequences are less essential in the derived category approach, but still play a role whenever concrete computations are necessary.\n\nThere have been attempts at 'non-commutative' theories which extend first cohomology as torsors (important in Galois cohomology).\n\n\n"
    },
    {
        "title": "Goldbach's conjecture",
        "content": "Goldbach's conjecture is one of the oldest and best-known unsolved problems in number theory and all of mathematics. It states that every even natural number greater than 2 is the sum of two prime numbers.\n\nThe conjecture has been shown to hold for all integers less than 4×1018 but remains unproven despite considerable effort.\n\nOn 7 June 1742, the Prussian mathematician Christian Goldbach wrote a letter to Leonhard Euler (letter XLIII),[2] in which he proposed the following conjecture:\n\nGoldbach was following the now-abandoned convention of considering 1 to be a prime number,[3] so that a sum of units would be a sum of primes.\nHe then proposed a second conjecture in the margin of his letter, which implies the first:[4]\n\n... eine jede Zahl, die grösser ist als 2, ein aggregatum trium numerorum primorum sey.\nEvery integer greater than 2 can be written as the sum of three primes.\nEuler replied in a letter dated 30 June 1742[5] and reminded Goldbach of an earlier conversation they had had (\"... so Ew vormals mit mir communicirt haben ...\"), in which Goldbach had remarked that the first of those two conjectures would follow from the statement\n\nThis is in fact equivalent to his second, marginal conjecture.\nIn the letter dated 30 June 1742, Euler stated:[6][7]\n\nDass ... ein jeder numerus par eine summa duorum primorum sey, halte ich für ein ganz gewisses theorema, ungeachtet ich dasselbe nicht demonstriren kann.That ... every even integer is a sum of two primes, I regard as a completely certain theorem, although I cannot prove it.\nRené Descartes wrote that \"Every even number can be expressed as the sum of at most three primes.\"[8] The proposition is equivalent to Goldbach's conjecture, and Paul Erdős said that \"Descartes actually discovered this before Goldbach... but it is better that the conjecture was named for Goldbach because, mathematically speaking, Descartes was infinitely rich and Goldbach was very poor.\"[9]\n\nThe strong Goldbach conjecture is much more difficult than the weak Goldbach conjecture, which says that every odd integer greater than 5 is the sum of three primes. Using Vinogradov's method, Nikolai Chudakov,[10] Johannes van der Corput,[11] and Theodor Estermann[12] showed (1937–1938) that almost all even numbers can be written as the sum of two primes (in the sense that the fraction of even numbers up to some N which can be so written tends towards 1 as N increases). In 1930, Lev Schnirelmann proved that any natural number greater than 1 can be written as the sum of not more than C prime numbers, where C is an effectively computable constant; see Schnirelmann density.[13][14] Schnirelmann's constant is the lowest number C with this property. Schnirelmann himself obtained C < 800000. This result was subsequently enhanced by many authors, such as Olivier Ramaré, who in 1995 showed that every even number n ≥ 4 is in fact the sum of at most 6 primes. The best known result currently stems from the proof of the weak Goldbach conjecture by Harald Helfgott,[15] which directly implies that every even number n ≥ 4 is the sum of at most 4 primes.[16][17]\n\nIn 1924, Hardy and Littlewood showed under the assumption of the generalized Riemann hypothesis that the number of even numbers up to X violating the Goldbach conjecture is much less than X1⁄2 + c for small c.[18]\n\nIn 1948, using sieve theory methods, Alfréd Rényi showed that every sufficiently large even number can be written as the sum of a prime and an almost prime with at most K factors.[19] Chen Jingrun showed in 1973 using sieve theory that every sufficiently large even number can be written as the sum of either two primes, or a prime and a semiprime (the product of two primes).[20] See Chen's theorem for further information.\n\nIn 1975, Hugh Lowell Montgomery and Bob Vaughan showed that \"most\" even numbers are expressible as the sum of two primes. More precisely, they showed that there exist positive constants c and C such that for all sufficiently large numbers N, every even number less than N is the sum of two primes, with at most CN1 − c exceptions. In particular, the set of even integers that are not the sum of two primes has density zero.\n\nIn 1951, Yuri Linnik proved the existence of a constant K such that every sufficiently large even number is the sum of two primes and at most K powers of 2. János Pintz and Imre Ruzsa found in 2020 that K = 8 works.[21] Assuming the generalized Riemann hypothesis, K = 7 also works, as shown by Roger Heath-Brown and Jan-Christoph Schlage-Puchta in 2002.[22]\n\nA proof for the weak conjecture was submitted in 2013 by Harald Helfgott to Annals of Mathematics Studies series. Although the article was accepted, Helfgott decided to undertake the major modifications suggested by the referee. Despite several revisions, Helfgott's proof has not yet appeared in a peer-reviewed publication.[23][24][25] The weak conjecture is implied by the strong conjecture, as if n − 3 is a sum of two primes, then n is a sum of three primes. However, the converse implication and thus the strong Goldbach conjecture would remain unproven if Helfgott's proof is correct.\n\nFor small values of n, the strong Goldbach conjecture (and hence the weak Goldbach conjecture) can be verified directly. For instance, in 1938, Nils Pipping laboriously verified the conjecture up to n = 100000.[26] With the advent of computers, many more values of n have been checked; T. Oliveira e Silva ran a distributed computer search that has verified the conjecture for n ≤ 4×1018 (and double-checked up to 4×1017) as of 2013. One record from this search is that 3325581707333960528 is the smallest number that cannot be written as a sum of two primes where one is smaller than 9781.[27]\n\nGoldbach's Conjecture (Chinese: 哥德巴赫猜想) is the title of the biography of Chinese mathematician and number theorist Chen Jingrun, written by Xu Chi.\n\nThe conjecture is a central point in the plot of the 1992 novel Uncle Petros and Goldbach's Conjecture by Greek author Apostolos Doxiadis, in the short story \"Sixty Million Trillion Combinations\" by Isaac Asimov and also in the 2008 mystery novel No One You Know by Michelle Richmond.[28]\n\nGoldbach's conjecture is part of the plot of the 2007 Spanish film Fermat's Room.\n\nGoldbach's conjecture is featured as the main topic of research of actress Ella Rumpf's character Marguerite in the 2023 French-Swiss film Marguerite's Theorem.[29]\n\nEach of the three conjectures has a natural analog in terms of the modern definition of a prime, under which 1 is excluded. A modern version of the first conjecture is:\n\nA modern version of the marginal conjecture is:\n\nAnd a modern version of Goldbach's older conjecture of which Euler reminded him is:\n\nThese modern versions might not be entirely equivalent to the corresponding original statements. For example, if there were an even integer N = p + 1 larger than 4, for p a prime, that could not be expressed as the sum of two primes in the modern sense, then it would be a counterexample to the modern version of the third conjecture (without being a counterexample to the original version). The modern version is thus probably stronger (but in order to confirm that, one would have to prove that the first version, freely applied to any positive even integer n, could not possibly rule out the existence of such a specific counterexample N). In any case, the modern statements have the same relationships with each other as the older statements did. That is, the second and third modern statements are equivalent, and either implies the first modern statement.\n\nThe third modern statement (equivalent to the second) is the form in which the conjecture is usually expressed today. It is also known as the \"strong\", \"even\", or \"binary\" Goldbach conjecture. A weaker form of the second modern statement, known as \"Goldbach's weak conjecture\", the \"odd Goldbach conjecture\", or the \"ternary Goldbach conjecture\", asserts that \n\nStatistical considerations that focus on the probabilistic distribution of prime numbers present informal evidence in favour of the conjecture (in both the weak and strong forms) for sufficiently large integers: the greater the integer, the more ways there are available for that number to be represented as the sum of two or three other numbers, and the more \"likely\" it becomes that at least one of these representations consists entirely of primes.\n\nA very crude version of the heuristic probabilistic argument (for the strong form of the Goldbach conjecture) is as follows. The prime number theorem asserts that an integer m selected at random has roughly a ⁠1/ln m⁠ chance of being prime. Thus if n is a large even integer and m is a number between 3 and ⁠n/2⁠, then one might expect the probability of m and n − m simultaneously being prime to be ⁠1/ln m ln(n − m)⁠. If one pursues this heuristic, one might expect the total number of ways to write a large even integer n as the sum of two odd primes to be roughly\n\nSince ln n ≪ √n, this quantity goes to infinity as n increases, and one would expect that every large even integer has not just one representation as the sum of two primes, but in fact very many such representations.\n\nThis heuristic argument is actually somewhat inaccurate because it assumes that the events of m and n − m being prime are statistically independent of each other. For instance, if m is odd, then n − m is also odd, and if m is even, then n − m is even, a non-trivial relation because, besides the number 2, only odd numbers can be prime. Similarly, if n is divisible by 3, and m was already a prime other than 3, then n − m would also be coprime to 3 and thus be slightly more likely to be prime than a general number. Pursuing this type of analysis more carefully, G. H. Hardy and John Edensor Littlewood in 1923 conjectured (as part of their Hardy–Littlewood prime tuple conjecture) that for any fixed c ≥ 2, the number of representations of a large integer n as the sum of c primes n = p1 + ⋯ + pc with p1 ≤ ⋯ ≤ pc should be asymptotically equal to\n\nwhere the product is over all primes p, and γc,p(n) is the number of solutions to the equation n = q1 + ⋯ + qc mod p in modular arithmetic, subject to the constraints q1, …, qc ≠ 0 mod p. This formula has been rigorously proven to be asymptotically valid for c ≥ 3 from the work of Ivan Matveevich Vinogradov, but is still only a conjecture when c = 2.[citation needed] In the latter case, the above formula simplifies to 0 when n is odd, and to\n\nwhen n is even, where Π2 is Hardy–Littlewood's twin prime constant\n\nThis is sometimes known as the extended Goldbach conjecture. The strong Goldbach conjecture is in fact very similar to the twin prime conjecture, and the two conjectures are believed to be of roughly comparable difficulty.\n\nThe Goldbach partition function is the function that associates to each even integer the number of ways it can be decomposed into a sum of two primes. Its graph looks like a comet and is therefore called Goldbach's comet.[30]\n\nGoldbach's comet suggests tight upper and lower bounds on the number of representations of an even number as the sum of two primes, and also that the number of these representations depend strongly on the value modulo 3 of the number.\n\nAlthough Goldbach's conjecture implies that every positive integer greater than one can be written as a sum of at most three primes, it is not always possible to find such a sum using a greedy algorithm that uses the largest possible prime at each step. The Pillai sequence tracks the numbers requiring the largest number of primes in their greedy representations.[31]\n\nSimilar problems to Goldbach's conjecture exist in which primes are replaced by other particular sets of numbers, such as the squares:\n\nGoldbach's conjecture is used when studying computation complexity.[37]  The connection is made through the Busy Beaver function, where BB(n) is the maximum number of steps taken by any n state Turing machine that halts.  There is a 27-state Turing machine that halts if and only if Goldbach's conjecture is false.  Hence if BB(27) was known, and the Turing machine did not stop in that number of steps, it would be known to run forever and hence no counterexamples exist (which proves the conjecture true).  This is a completely impractical way to settle the conjecture; instead it is used to suggest that BB(27) will be very hard to compute, at least as difficult as settling the Goldbach conjecture.\n"
    },
    {
        "title": "Christian Goldbach",
        "content": "Christian Goldbach (/ˈɡoʊldbɑːk/ GOHLD-bahk, German: [ˈkʁɪsti̯a(ː)n ˈɡɔltbax]; 18 March 1690 – 20 November 1764) was a Prussian mathematician connected with some important research mainly in number theory; he also studied law and took an interest in and a role in the Russian court.[1][2] After traveling around Europe in his early life, he landed in Russia in 1725 as a professor at the newly founded Saint Petersburg Academy of Sciences.[3] Goldbach jointly led the Academy in 1737.[4] However, he relinquished duties in the Academy in 1742 and worked in the Russian Ministry of Foreign Affairs until his death in 1764.[4] He is remembered today for Goldbach's conjecture and the Goldbach–Euler Theorem.[1] He had a close friendship with famous mathematician Leonhard Euler, serving as inspiration for Euler's mathematical pursuits.[2]\n\nBorn in the Duchy of Prussia's capital Königsberg, part of Brandenburg-Prussia, Goldbach was the son of a pastor.[2] He studied at the Royal Albertus University.[2][5] After finishing his studies he went on long educational trips from 1710 to 1724 through Europe, visiting other German states, England, the Netherlands, Italy, and France, meeting with many famous mathematicians, such as Gottfried Leibniz, Leonhard Euler, and Nicholas I Bernoulli. These acquaintances started Goldbach's interest in mathematics.[6] He briefly attended Oxford University in 1713 and, while he was there, Goldbach studied mathematics with John Wallis and Isaac Newton.[3] Also, Goldbach's travels fostered his interest in philology, archaeology, metaphysics, ballistics, and medicine.[6] Between 1717 and 1724, Goldbach published his first few papers which, while minor, credited his mathematical ability. Back in Königsberg, he became acquainted with Georg Bernhard Bilfinger and Jakob Hermann.[2]\n\nGoldbach followed Bilfinger and Hermann to the newly opened St. Petersburg Academy of Sciences in 1725.[4] Christian Wolff had invited and had written recommendations for all the Germans who traveled to Saint Petersburg for the academy except Goldbach.[3] Goldbach wrote to the president-designate of the academy, petitioning for a position in the academy, using his past publications and knowledge in medicine and law as qualifications.[3][4] Goldbach was then hired to a five-year contract as a professor of mathematics and historian of the academy.[3][4] As historian of the academy, he recorded each academy meeting from the opening of the school in 1725 until January 1728.[4] Goldbach worked with famous mathematicians like Leonhard Euler, Daniel Bernoulli, Johann Bernoulli, and Jean le Rond d'Alembert.[5] Goldbach also played a part in Euler's decision to academically pursue mathematics instead of medicine, cementing mathematics as the premier research field of the academy in the 1730s.[3]\n\nIn 1728, when Peter II became Tsar of Russia, Goldbach became Peter II and Anna's, Peter II's cousin, tutor.[4] Peter II moved the Russian court from St. Petersburg to Moscow in 1729, so Goldbach followed him to Moscow.[2][4] Goldbach started a correspondence with Euler in 1729, in which some of Goldbach's most important mathematics contributions can be found.[2][5] Upon Peter II's death in 1730, Goldbach stopped teaching but continued to assist Empress Anna.[4] In 1732, Goldbach returned to the St. Petersburg Academy of Sciences and stayed in the Russian government when Anna moved the court back to St. Petersburg.[2][4] Upon return to the academy, Goldbach was named corresponding secretary.[3] With Goldbach's return, his friend Euler continued his teaching and research at the academy as well.[3] Then, in 1737, Goldbach and J.D. Schumacher took over the administration of the academy.[4] Also, Goldbach took on duty in Russian court under Empress Anna.[2][4] He managed to retain his influence in court after the death of Anna and the rule of Empress Elizabeth.[2] In 1742 he entered the Russian Ministry of Foreign Affairs, stepping away from the academy once more.[4] Goldbach was gifted land and increased salary for his good work and rise in the Russian government.[2] In 1760, Goldbach created new guidelines for the education of the royal children which would remain in place for 100 years.[2][4] He died on 20 November 1764, aged 74, in Moscow.\n\nChristian Goldbach was multilingual – he wrote a diary in German and Latin, his letters were written in German, Latin, French, and Italian and for official documents he used Russian, German and Latin.[7]\n\nGoldbach is most noted for his correspondence with Leibniz, Euler, and Bernoulli, especially in his 1742 letter to Euler stating his Goldbach's conjecture. He also studied and proved some theorems on perfect powers, such as the Goldbach–Euler theorem, and made several notable contributions to analysis.[1] He also proved a result concerning Fermat numbers that is called Goldbach's theorem.\n\nIt is Goldbach and Euler's correspondence that contains some of Goldbach's most important contributions to mathematics, specifically number theory.[2] Goldbach and Euler's friendship survived Goldbach's move to Moscow in 1728 and communication ensued.[3] Their correspondence spanned 196 letters over 35 years written in Latin, German, and French.[6] These letters spanned a wide range of topics, including various mathematics topics.[2] Goldbach was the leading influence on Euler's interest and work in number theory.[3] Most of the letters discuss Euler's research in number theory as well as differential calculus.[3] Until the late 1750s, Euler's correspondence on his number theory research was almost exclusively with Goldbach.[3]\n\nGoldbach's earlier mathematical work and ideas in letters to Euler directly influenced some of Euler's work. In 1729, Euler solved two problems pertaining to sequences which had stumped Goldbach.[3] Ensuingly, Euler outlined the solutions to Goldbach.[3] Also, in 1729 Goldbach closely approximated the Basel problem, which prompted Euler's interest and concurring breakthrough solution.[3] Goldbach, through his letters, kept Euler focused on number theory in the 1730s by discussing Fermat's conjecture with Euler.[3] Euler subsequently offered a proof to the conjecture, crediting Goldbach with introducing him to the subfield.[3] Euler proceeded to write 560 writings, published posthumously in four volumes of Opera omnia, with Goldbach's influence guiding some of the writings.[3] Goldbach's famous conjecture and his writings with Euler prove him to be one of a handful of mathematicians who understood complex number theory in light of Fermat's revolutionary ideas on the topic.[8]\n"
    },
    {
        "title": "Analytic number theory",
        "content": "In mathematics, analytic number theory is a branch of number theory that uses methods from mathematical analysis to solve problems about the integers.[1] It is often said to have begun with Peter Gustav Lejeune Dirichlet's 1837 introduction of Dirichlet L-functions to give the first proof of Dirichlet's theorem on arithmetic progressions.[1][2] It is well known for its results on prime numbers (involving the Prime Number Theorem and Riemann zeta function) and additive number theory (such as the Goldbach conjecture and Waring's problem).\n\nAnalytic number theory can be split up into two major parts, divided more by the type of problems they attempt to solve than fundamental differences in technique.[3]\n\nMuch of analytic number theory was inspired by the prime number theorem. Let π(x) be the prime-counting function that gives the number of primes less than or equal to x, for any real number x. For example, π(10) = 4 because there are four prime numbers (2, 3, 5 and 7) less than or equal to 10. The prime number theorem then states that x / ln(x) is a good approximation to π(x), in the sense that the limit of the quotient of the two functions π(x) and x / ln(x) as x approaches infinity is 1:\n\nknown as the asymptotic law of distribution of prime numbers.\n\nAdrien-Marie Legendre conjectured in 1797 or 1798 that π(a) is approximated by the function a/(A ln(a) + B), where A and B are unspecified constants. In the second edition of his book on number theory (1808) he then made a more precise conjecture, with A = 1 and B ≈ −1.08366. Carl Friedrich Gauss considered the same question: \"Im Jahr 1792 oder 1793\" ('in the year 1792 or 1793'), according to his own recollection nearly sixty years later in a letter to Encke (1849), he  wrote in his logarithm table (he was then 15 or 16) the short note \"Primzahlen unter \n\n\n\na\n(\n=\n∞\n)\n\n\na\n\nln\n⁡\na\n\n\n\n\n\n{\\displaystyle a(=\\infty ){\\frac {a}{\\ln a}}}\n\n\" ('prime numbers under \n\n\n\na\n(\n=\n∞\n)\n\n\na\n\nln\n⁡\na\n\n\n\n\n\n{\\displaystyle a(=\\infty ){\\frac {a}{\\ln a}}}\n\n'). But Gauss never published this conjecture. In 1838 Peter Gustav Lejeune Dirichlet came up with his own approximating function,  the logarithmic integral li(x) (under the slightly different form of a series, which he communicated to Gauss). Both Legendre's and Dirichlet's formulas imply the same conjectured asymptotic equivalence of π(x) and x / ln(x) stated above, although it turned out that Dirichlet's approximation is considerably better if one considers the differences instead of quotients.\n\nJohann Peter Gustav Lejeune Dirichlet is credited with the creation of analytic number theory,[6] a field in which he found several deep results and in proving them introduced some fundamental tools, many of which were later named after him. In 1837 he published Dirichlet's theorem on arithmetic progressions, using mathematical analysis concepts to tackle an algebraic problem and thus creating the branch of analytic number theory. In proving the theorem, he introduced the Dirichlet characters and L-functions.[6][7] In 1841 he generalized his arithmetic progressions theorem from integers to the ring of Gaussian integers \n\n\n\n\nZ\n\n[\ni\n]\n\n\n{\\displaystyle \\mathbb {Z} [i]}\n\n.[8]\n\nIn two papers from 1848 and 1850, the Russian mathematician Pafnuty L'vovich Chebyshev attempted to prove the asymptotic law of distribution of prime numbers. His work is notable for the use of the zeta function ζ(s) (for real values of the argument \"s\", as are works of Leonhard Euler, as early as 1737) predating Riemann's celebrated memoir of 1859, and he succeeded in proving a slightly weaker form of the asymptotic law, namely, that if the limit of π(x)/(x/ln(x)) as x goes to infinity exists at all, then it is necessarily equal to one.[9] He was able to prove unconditionally that this ratio is bounded above and below by two explicitly given constants near to 1 for all x.[10] Although Chebyshev's paper did not prove the Prime Number Theorem, his estimates for π(x) were strong enough for him to prove Bertrand's postulate that there exists a prime number between n and 2n for any integer n ≥ 2.\n\n\"…es ist sehr wahrscheinlich, dass alle Wurzeln reell sind. Hiervon wäre allerdings ein strenger Beweis zu wünschen; ich habe indess die Aufsuchung desselben nach einigen flüchtigen vergeblichen Versuchen vorläufig bei Seite gelassen, da er für den nächsten Zweck meiner Untersuchung entbehrlich schien.\"\"…it is very probable that all roots are real. Of course one would wish for a rigorous proof here; I have for the time being, after some fleeting vain attempts, provisionally put aside the search for this, as it appears dispensable for the next objective of my investigation.\"\n\nBernhard Riemann made some famous contributions to modern analytic number theory. In a single short paper (the only one he published on the subject of number theory), he investigated the Riemann zeta function and established its importance for understanding the distribution of prime numbers. He made a series of conjectures about properties of the zeta function, one of which is the well-known Riemann hypothesis.\n\nExtending the ideas of Riemann, two proofs of the prime number theorem were obtained independently by Jacques Hadamard and Charles Jean de la Vallée-Poussin and appeared in the same year (1896). Both proofs used methods from complex analysis, establishing as a main step of the proof that the Riemann zeta function ζ(s) is non-zero for all complex values of the variable s that have the form s = 1 + it with t > 0.[12]\n\nThe biggest technical change after 1950 has been the development of sieve methods,[13] particularly in multiplicative problems. These are combinatorial in nature, and quite varied. The extremal branch of combinatorial theory has in return been greatly influenced by the value placed in analytic number theory on quantitative upper and lower bounds. Another recent development is probabilistic number theory,[14] which uses methods from probability theory to estimate the distribution of number theoretic functions, such as how many prime divisors a number has.\n\nSpecifically, the breakthroughs by Yitang Zhang, James Maynard, Terence Tao and Ben Green have all used the Goldston–Pintz–Yıldırım method, which they originally used to prove that[15][16][17][18][19][20]\n\n\n\n\n\n\np\n\nn\n+\n1\n\n\n−\n\np\n\nn\n\n\n≥\no\n(\nlog\n⁡\n\np\n\nn\n\n\n)\n.\n\n\n{\\displaystyle p_{n+1}-p_{n}\\geq o(\\log p_{n}).}\n\n\n\nDevelopments within analytic number theory are often refinements of earlier techniques, which reduce the error terms and widen their applicability. For example, the circle method of Hardy and Littlewood was conceived as applying to power series near the unit circle in the complex plane; it is now thought of in terms of finite exponential sums (that is, on the unit circle, but with the power series truncated). The needs of Diophantine approximation are for auxiliary functions that are not generating functions—their coefficients are constructed by use of a pigeonhole principle—and involve several complex variables. The fields of Diophantine approximation and transcendence theory have expanded, to the point that the techniques have been applied to the Mordell conjecture.\n\nTheorems and results within analytic number theory tend not to be exact structural results about the integers, for which algebraic and geometrical tools are more appropriate. Instead, they give approximate bounds and estimates for various number theoretical functions, as the following examples illustrate.\n\nEuclid showed that there are infinitely many prime numbers. An important question is to determine the asymptotic distribution of the prime numbers; that is, a rough description of how many primes are smaller than a given number. Gauss, amongst others, after computing a large list of primes, conjectured that the number of primes less than or equal to a large number N is close to the value of the integral\n\n\n\n\n\n\n∫\n\n2\n\n\nN\n\n\n\n\n1\n\nlog\n⁡\nt\n\n\n\n\nd\nt\n.\n\n\n{\\displaystyle \\int _{2}^{N}{\\frac {1}{\\log t}}\\,dt.}\n\n\n\nIn 1859 Bernhard Riemann used complex analysis and a special meromorphic function now known as the Riemann zeta function to derive an analytic expression for the number of primes less than or equal to a real number x.  Remarkably, the main term in Riemann's formula was exactly the above integral, lending substantial weight to Gauss's conjecture.  Riemann found that the error terms in this expression, and hence the manner in which the primes are distributed, are closely related to the complex zeros of the zeta function. Using Riemann's ideas and by getting more information on the zeros of the zeta function, Jacques Hadamard and Charles Jean de la Vallée-Poussin managed to complete the proof of Gauss's conjecture. In particular, they proved that if \n\n\n\n\nπ\n(\nx\n)\n=\n(\n\nnumber of primes \n\n≤\nx\n)\n,\n\n\n{\\displaystyle \\pi (x)=({\\text{number of primes }}\\leq x),}\n\n\nthen\n\n\n\n\n\nlim\n\nx\n→\n∞\n\n\n\n\n\nπ\n(\nx\n)\n\n\nx\n\n/\n\nlog\n⁡\nx\n\n\n\n=\n1.\n\n\n{\\displaystyle \\lim _{x\\to \\infty }{\\frac {\\pi (x)}{x/\\log x}}=1.}\n\n\n\nThis remarkable result is what is now known as the prime number theorem. It is a central result in analytic number theory. Loosely speaking, it states that given a large number N, the number of primes less than or equal to N is about N/log(N).\n\nMore generally, the same question can be asked about the number of primes in any arithmetic progression a + nq for any integer n. In one of the first applications of analytic techniques to number theory, Dirichlet proved that any arithmetic progression with a and q coprime contains infinitely many primes. The prime number theorem can be generalised to this problem; letting \n\n\n\n\nπ\n(\nx\n,\na\n,\nq\n)\n=\n(\n\nnumber of primes \n\n≤\nx\n\n in the arithmetic progression \n\na\n+\nn\nq\n,\n \nn\n∈\n\nZ\n\n)\n,\n\n\n{\\displaystyle \\pi (x,a,q)=({\\text{number of primes }}\\leq x{\\text{ in the arithmetic progression }}a+nq,\\ n\\in \\mathbf {Z} ),}\n\n\nthen if a and q are coprime,\n\n\n\n\n\nlim\n\nx\n→\n∞\n\n\n\n\n\nπ\n(\nx\n,\na\n,\nq\n)\nϕ\n(\nq\n)\n\n\nx\n\n/\n\nlog\n⁡\nx\n\n\n\n=\n1\n,\n\n\n{\\displaystyle \\lim _{x\\to \\infty }{\\frac {\\pi (x,a,q)\\phi (q)}{x/\\log x}}=1,}\n\n\nwhere \n\n\n\nϕ\n\n\n{\\displaystyle \\phi }\n\n is the totient function.\n\nThere are also many deep and wide-ranging conjectures in number theory whose proofs seem too difficult for current techniques, such as the twin prime conjecture which asks whether there are infinitely many primes p such that p + 2 is prime. On the assumption of the Elliott–Halberstam conjecture it has been proven recently that there are infinitely many primes p such that p + k is prime for some positive even k at most 12. Also, it has been proven unconditionally (i.e. not depending on unproven conjectures) that there are infinitely many primes p such that p + k is prime for some positive even k at most 246.\n\nOne of the most important problems in additive number theory is Waring's problem, which asks whether it is possible, for any k ≥ 2, to write any positive integer as the sum of a bounded number of kth powers,\n\nThe case for squares, k = 2, was answered by Lagrange in 1770, who proved that every positive integer is the sum of at most four squares. The general case was proved by Hilbert in 1909, using algebraic techniques which gave no explicit bounds. An important breakthrough was the application of analytic tools to the problem by Hardy and Littlewood. These techniques are known as the circle method, and give explicit upper bounds for the function G(k), the smallest number of kth powers needed, such as Vinogradov's bound\n\nDiophantine problems are concerned with integer solutions to polynomial equations: one may study the distribution of solutions, that is, counting solutions according to some measure of \"size\" or height.\n\nAn important example is the Gauss circle problem, which asks for integers points (x y) which satisfy\n\nIn geometrical terms, given a circle centered about the origin in the plane with radius r, the problem asks how many integer lattice points lie on or inside the circle. It is not hard to prove that the answer is \n\n\n\nπ\n\nr\n\n2\n\n\n+\nE\n(\nr\n)\n\n\n{\\displaystyle \\pi r^{2}+E(r)}\n\n, where \n\n\n\nE\n(\nr\n)\n\n/\n\n\nr\n\n2\n\n\n→\n0\n\n\n{\\displaystyle E(r)/r^{2}\\to 0}\n\n as \n\n\n\nr\n→\n∞\n\n\n{\\displaystyle r\\to \\infty }\n\n.  Again, the difficult part and a great achievement of analytic number theory is obtaining specific upper bounds on the error term E(r).\n\nIt was shown by Gauss that \n\n\n\nE\n(\nr\n)\n=\nO\n(\nr\n)\n\n\n{\\displaystyle E(r)=O(r)}\n\n. In general, an O(r) error term would be possible with the unit circle (or, more properly, the closed unit disk) replaced by the dilates of any bounded planar region with piecewise smooth boundary.  Furthermore, replacing the unit circle by the unit square, the error term for the general problem can be as large as a linear function of r.  Therefore, getting an error bound of the form \n\n\n\nO\n(\n\nr\n\nδ\n\n\n)\n\n\n{\\displaystyle O(r^{\\delta })}\n\n\nfor some \n\n\n\nδ\n<\n1\n\n\n{\\displaystyle \\delta <1}\n\n in the case of the circle is a significant improvement.  The first to attain this was\nSierpiński in 1906, who showed \n\n\n\nE\n(\nr\n)\n=\nO\n(\n\nr\n\n2\n\n/\n\n3\n\n\n)\n\n\n{\\displaystyle E(r)=O(r^{2/3})}\n\n.  In 1915, Hardy and Landau each showed that one does not have \n\n\n\nE\n(\nr\n)\n=\nO\n(\n\nr\n\n1\n\n/\n\n2\n\n\n)\n\n\n{\\displaystyle E(r)=O(r^{1/2})}\n\n.  Since then the goal has been to show that for each fixed \n\n\n\nϵ\n>\n0\n\n\n{\\displaystyle \\epsilon >0}\n\n there exists a real number \n\n\n\nC\n(\nϵ\n)\n\n\n{\\displaystyle C(\\epsilon )}\n\n such that \n\n\n\nE\n(\nr\n)\n≤\nC\n(\nϵ\n)\n\nr\n\n1\n\n/\n\n2\n+\nϵ\n\n\n\n\n{\\displaystyle E(r)\\leq C(\\epsilon )r^{1/2+\\epsilon }}\n\n.\n\nIn 2000 Huxley showed[21] that \n\n\n\nE\n(\nr\n)\n=\nO\n(\n\nr\n\n131\n\n/\n\n208\n\n\n)\n\n\n{\\displaystyle E(r)=O(r^{131/208})}\n\n, which is the best published result.\n\nOne of the most useful tools in multiplicative number theory are Dirichlet series, which are functions of a complex variable defined by an infinite series of the form\n\nDepending on the choice of coefficients \n\n\n\n\na\n\nn\n\n\n\n\n{\\displaystyle a_{n}}\n\n, this series may converge everywhere, nowhere, or on some half plane. In many cases, even where the series does not converge everywhere, the holomorphic function it defines may be analytically continued to a meromorphic function on the entire complex plane. The utility of functions like this in multiplicative problems can be seen in the formal identity\n\nhence the coefficients of the product of two Dirichlet series are the multiplicative convolutions of the original coefficients. Furthermore, techniques such as partial summation and Tauberian theorems can be used to get information about the coefficients from analytic information about the Dirichlet series. Thus a common method for estimating a multiplicative function is to express it as a Dirichlet series (or a product of simpler Dirichlet series using convolution identities), examine this series as a complex function and then convert this analytic information back into information about the original function.\n\nEuler showed that the fundamental theorem of arithmetic implies (at least formally) the Euler product\n\nwhere the product is taken over all prime numbers p.\n\nEuler's proof of the infinity of prime numbers makes use of the divergence of the term at the left hand side for s = 1 (the so-called harmonic series), a purely analytic result. Euler was also the first to use analytical arguments for the purpose of studying properties of integers, specifically by constructing generating power series. This was the beginning of analytic number theory.[20]\n\nLater, Riemann considered this function for complex values of s and showed that this function can be extended to a meromorphic function on the entire plane with a simple pole at s = 1.  This function is now known as the Riemann Zeta function and is denoted by ζ(s).  There is a plethora of literature on this function and the function is a special case of the more general Dirichlet L-functions.\n\nAnalytic number theorists are often interested in the error of approximations such as the prime number theorem.  In this case, the error is smaller than x/log x.  Riemann's formula for π(x) shows that the error term in this approximation can be expressed in terms of the zeros of the zeta function. In his 1859 paper, Riemann conjectured that all the \"non-trivial\" zeros of ζ lie on the line \n\n\n\nℜ\n(\ns\n)\n=\n1\n\n/\n\n2\n\n\n{\\displaystyle \\Re (s)=1/2}\n\n but never provided a proof of this statement.  This famous and long-standing conjecture is known as the Riemann Hypothesis and has many deep implications in number theory; in fact, many important theorems have been proved under the assumption that the hypothesis is true.  For example, under the assumption of the Riemann Hypothesis, the error term in the prime number theorem is \n\n\n\nO\n(\n\nx\n\n1\n\n/\n\n2\n+\nε\n\n\n)\n\n\n{\\displaystyle O(x^{1/2+\\varepsilon })}\n\n.\n\nIn the early 20th century G. H. Hardy and Littlewood proved many results about the zeta function in an attempt to prove the Riemann Hypothesis.  In fact, in 1914,\nHardy proved that there were infinitely many zeros of the zeta function on the critical line\n\nThis led to several theorems describing the density of the zeros on the critical line.\n\nOn specialized aspects the following books have become especially well-known:\n\nCertain topics have not yet reached book form in any depth. Some examples are\n(i) Montgomery's pair correlation conjecture and the work that initiated from it,\n(ii) the new results of Goldston, Pintz and Yilidrim on small gaps between primes, and\n(iii) the Green–Tao theorem showing that arbitrarily long arithmetic progressions of primes exist.\n"
    },
    {
        "title": "Algebraic number theory",
        "content": "Ring homomorphisms\n\nAlgebraic structures\n\nRelated structures\n\nAlgebraic number theory\n\nNoncommutative algebraic geometry\n\nFree algebra\n\nClifford algebra\n\nAlgebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.\n\nThe beginnings of algebraic number theory can be traced to Diophantine equations,[1] named after the 3rd-century Alexandrian mathematician, Diophantus, who studied them and developed methods for the solution of some kinds of Diophantine equations. A typical Diophantine problem is to find two integers x and y such that their sum, and the sum of their squares, equal two given numbers A and B, respectively:\n\nDiophantine equations have been studied for thousands of years. For example, the solutions to the quadratic Diophantine equation  x2 + y2 = z2 are given by the Pythagorean triples, originally solved by the Babylonians (c. 1800 BC).[2] Solutions to linear Diophantine equations, such as 26x + 65y = 13, may be found using the Euclidean algorithm (c. 5th century BC).[3]\n\nDiophantus's major work was the Arithmetica, of which only a portion has survived.\n\nFermat's Last Theorem was first conjectured by Pierre de Fermat in 1637, famously in the margin of a copy of Arithmetica where he claimed he had a proof that was too large to fit in the margin. No successful proof was published until 1995 despite the efforts of countless mathematicians during the 358 intervening years. The unsolved problem stimulated the development of algebraic number theory in the 19th century and the proof of the modularity theorem in the 20th century.\n\nOne of the founding works of algebraic number theory, the Disquisitiones Arithmeticae (Latin: Arithmetical Investigations) is a textbook of number theory written in Latin[4] by Carl Friedrich Gauss in 1798 when Gauss was 21 and first published in 1801 when he was 24. In this book Gauss brings together results in number theory obtained by mathematicians such as Fermat, Euler, Lagrange and Legendre and adds important new results of his own. Before the Disquisitiones was published, number theory consisted of a collection of isolated theorems and conjectures. Gauss brought the work of his predecessors together with his own original work into a systematic framework, filled in gaps, corrected unsound proofs, and extended the subject in numerous ways.\n\nThe Disquisitiones was the starting point for the work of other nineteenth century European mathematicians including Ernst Kummer, Peter Gustav Lejeune Dirichlet and Richard Dedekind. Many of the annotations given by Gauss are in effect announcements of further research of his own, some of which remained unpublished. They must have appeared particularly cryptic to his contemporaries; we can now read them as containing the germs of the theories of L-functions and complex multiplication, in particular.\n\nIn a couple of papers in 1838 and 1839 Peter Gustav Lejeune Dirichlet proved the first class number formula, for quadratic forms (later refined by his student Leopold Kronecker). The formula, which Jacobi called a result \"touching the utmost of human acumen\", opened the way for similar results regarding more general number fields.[5] Based on his research of the structure of the unit group of quadratic fields, he proved the Dirichlet unit theorem, a fundamental result in algebraic number theory.[6]\n\nHe first used the pigeonhole principle, a basic counting argument, in the proof of a theorem in diophantine approximation, later named after him Dirichlet's approximation theorem. He published important contributions to Fermat's last theorem, for which he proved the cases n = 5 and n = 14, and to the biquadratic reciprocity law.[5] The Dirichlet divisor problem, for which he found the first results, is still an unsolved problem in number theory despite later contributions by other researchers.\n\nRichard Dedekind's study of Lejeune Dirichlet's work was what led him to his later study of algebraic number fields and ideals. In 1863, he published Lejeune Dirichlet's lectures on number theory as Vorlesungen über Zahlentheorie (\"Lectures on Number Theory\") about which it has been written that:\n\n\"Although the book is assuredly based on Dirichlet's lectures, and although Dedekind himself referred to the book throughout his life as Dirichlet's, the book itself was entirely written by Dedekind, for the most part after Dirichlet's death.\" (Edwards 1983)\n1879 and 1894 editions of the Vorlesungen included supplements introducing the notion of an ideal, fundamental to ring theory. (The word \"Ring\", introduced later by Hilbert, does not appear in Dedekind's work.) Dedekind defined an ideal as a subset of a set of numbers, composed of algebraic integers that satisfy polynomial equations with integer coefficients. The concept underwent further development in the hands of Hilbert and, especially, of Emmy Noether. Ideals generalize Ernst Eduard Kummer's ideal numbers, devised as part of Kummer's 1843 attempt to prove Fermat's Last Theorem.\n\nDavid Hilbert unified the field of algebraic number theory with his 1897 treatise Zahlbericht (literally \"report on numbers\"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers.[7] He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.\n\nHe made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.[8]\n\nEmil Artin established the Artin reciprocity law in a series of papers (1924; 1927; 1930). This law is a general theorem in number theory that forms a central part of global class field theory.[9] The term \"reciprocity law\" refers to a long line of more concrete number theoretic statements which it generalized, from the quadratic reciprocity law and the reciprocity laws of Eisenstein and Kummer to Hilbert's product formula for the norm symbol. Artin's result provided a partial solution to Hilbert's ninth problem.\n\nAround 1955, Japanese mathematicians Goro Shimura and Yutaka Taniyama observed a possible link between two apparently completely distinct, branches of mathematics, elliptic curves and modular forms. The resulting modularity theorem (at the time known as the Taniyama–Shimura conjecture) states that every elliptic curve is modular, meaning that it can be associated with a unique modular form.\n\nIt was initially dismissed as unlikely or highly speculative, but was taken more seriously when number theorist André Weil found evidence supporting it, yet no proof; as a result the \"astounding\"[10] conjecture was often known as the Taniyama–Shimura-Weil conjecture. It became a part of the Langlands program, a list of important conjectures needing proof or disproof.\n\nFrom 1993 to 1994, Andrew Wiles provided a proof of the modularity theorem for semistable elliptic curves, which, together with Ribet's theorem, provided a proof for Fermat's Last Theorem. Almost every mathematician at the time had previously considered both Fermat's Last Theorem and the Modularity Theorem either impossible or virtually impossible to prove, even given the most cutting-edge developments. Wiles first announced his proof in June 1993[11] in a version that was soon recognized as having a serious gap at a key point. The proof was corrected by Wiles, partly in collaboration with Richard Taylor, and the final, widely accepted version was released in September 1994, and formally published in 1995. The proof uses many techniques from algebraic geometry and number theory, and has many ramifications in these branches of mathematics. It also uses standard constructions of modern algebraic geometry, such as the category of schemes and Iwasawa theory, and other 20th-century techniques not available to Fermat.\n\nAn important property of the ring of integers is that it satisfies the fundamental theorem of arithmetic, that every (positive) integer has a factorization into a product of prime numbers, and this factorization is unique up to the ordering of the factors. This may no longer be true in the ring of integers O of an algebraic number field K.\n\nA prime element is an element p of O such that if p divides a product ab, then it divides one of the factors a or b. This property is closely related to primality in the integers, because any positive integer satisfying this property is either 1 or a prime number. However, it is strictly weaker. For example, −2 is not a prime number because it is negative, but it is a prime element. If factorizations into prime elements are permitted, then, even in the integers, there are alternative factorizations such as\n\nIn general, if u is a unit, meaning a number with a multiplicative inverse in O, and if p is a prime element, then up is also a prime element. Numbers such as p and up are said to be associate. In the integers, the primes p and −p are associate, but only one of these is positive. Requiring that prime numbers be positive selects a unique element from among a set of associated prime elements. When K is not the rational numbers, however, there is no analog of positivity. For example, in the Gaussian integers Z[i],[12] the numbers 1 + 2i and −2 + i are associate because the latter is the product of the former by i, but there is no way to single out one as being more canonical than the other. This leads to equations such as\n\nwhich prove that in Z[i], it is not true that factorizations are unique up to the order of the factors. For this reason, one adopts the definition of unique factorization used in unique factorization domains (UFDs). In a UFD, the prime elements occurring in a factorization are only expected to be unique up to units and their ordering.\n\nHowever, even with this weaker definition, many rings of integers in algebraic number fields do not admit unique factorization. There is an algebraic obstruction called the ideal class group. When the ideal class group is trivial, the ring is a UFD. When it is not, there is a distinction between a prime element and an irreducible element. An irreducible element x is an element such that if x = yz, then either y or z is a unit. These are the elements that cannot be factored any further. Every element in O admits a factorization into irreducible elements, but it may admit more than one. This is because, while all prime elements are irreducible, some irreducible elements may not be prime. For example, consider the ring Z[√-5].[13] In this ring, the numbers 3, 2 + √-5 and 2 - √-5 are irreducible. This means that the number 9 has two factorizations into irreducible elements,\n\nThis equation shows that 3 divides the product (2 + √-5)(2 - √-5) = 9. If 3 were a prime element, then it would divide 2 + √-5 or 2 - √-5, but it does not, because all elements divisible by 3 are of the form 3a + 3b√-5. Similarly, 2 + √-5 and 2 - √-5 divide the product 32, but neither of these elements divides 3 itself, so neither of them are prime. As there is no sense in which the elements 3, 2 + √-5 and 2 - √-5 can be made equivalent, unique factorization fails in Z[√-5]. Unlike the situation with units, where uniqueness could be repaired by weakening the definition, overcoming this failure requires a new perspective.\n\nIf I is an ideal in O, then there is always a factorization\n\nwhere each \n\n\n\n\n\n\np\n\n\n\ni\n\n\n\n\n{\\displaystyle {\\mathfrak {p}}_{i}}\n\n is a prime ideal, and where this expression is unique up to the order of the factors. In particular, this is true if I is the principal ideal generated by a single element. This is the strongest sense in which the ring of integers of a general number field admits unique factorization. In the language of ring theory, it says that rings of integers are Dedekind domains.\n\nWhen O is a UFD, every prime ideal is generated by a prime element. Otherwise, there are prime ideals which are not generated by prime elements. In Z[√-5], for instance, the ideal (2, 1 + √-5) is a prime ideal which cannot be generated by a single element.\n\nHistorically, the idea of factoring ideals into prime ideals was preceded by Ernst Kummer's introduction of ideal numbers. These are numbers lying in an extension field E of K. This extension field is now known as the Hilbert class field. By the principal ideal theorem, every prime ideal of O generates a principal ideal of the ring of integers of E. A generator of this principal ideal is called an ideal number. Kummer used these as a substitute for the failure of unique factorization in cyclotomic fields. These eventually led Richard Dedekind to introduce a forerunner of ideals and to prove unique factorization of ideals.\n\nAn ideal which is prime in the ring of integers in one number field may fail to be prime when extended to a larger number field. Consider, for example, the prime numbers. The corresponding ideals pZ are prime ideals of the ring Z. However, when this ideal is extended to the Gaussian integers to obtain pZ[i], it may or may not be prime. For example, the factorization 2 = (1 + i)(1 − i) implies that\n\nnote that because 1 + i = (1 − i) ⋅ i, the ideals generated by 1 + i and 1 − i are the same. A complete answer to the question of which ideals remain prime in the Gaussian integers is provided by Fermat's theorem on sums of two squares. It implies that for an odd prime number p, pZ[i] is a prime ideal if p ≡ 3 (mod 4) and is not a prime ideal if p ≡ 1 (mod 4). This, together with the observation that the ideal (1 + i)Z[i] is prime, provides a complete description of the prime ideals in the Gaussian integers. Generalizing this simple result to more general rings of integers is a basic problem in algebraic number theory. Class field theory accomplishes this goal when K is an abelian extension of Q (that is, a Galois extension with abelian Galois group).\n\nUnique factorization fails if and only if there are prime ideals that fail to be principal. The object which measures the failure of prime ideals to be principal is called the ideal class group. Defining the ideal class group requires enlarging the set of ideals in a ring of algebraic integers so that they admit a group structure. This is done by generalizing ideals to fractional ideals. A fractional ideal is an additive subgroup J of K which is closed under multiplication by elements of O, meaning that xJ ⊆ J if x ∈ O. All ideals of O are also fractional ideals. If I and J are fractional ideals, then the set IJ of all products of an element in I and an element in J is also a fractional ideal. This operation makes the set of non-zero fractional ideals into a group. The group identity is the ideal (1) = O, and the inverse of J is a (generalized) ideal quotient:\n\nThe principal fractional ideals, meaning the ones of the form Ox where x ∈ K×, form a subgroup of the group of all non-zero fractional ideals. The quotient of the group of non-zero fractional ideals by this subgroup is the ideal class group. Two fractional ideals I and J represent the same element of the ideal class group if and only if there exists an element x ∈ K such that xI = J. Therefore, the ideal class group makes two fractional ideals equivalent if one is as close to being principal as the other is. The ideal class group is generally denoted Cl K, Cl O, or Pic O (with the last notation identifying it with the Picard group in algebraic geometry).\n\nThe number of elements in the class group is called the class number of K. The class number of Q(√-5) is 2. This means that there are only two ideal classes, the class of principal fractional ideals, and the class of a non-principal fractional ideal such as (2, 1 + √-5).\n\nThe ideal class group has another description in terms of divisors. These are formal objects which represent possible factorizations of numbers. The divisor group Div K is defined to be the free abelian group generated by the prime ideals of O. There is a group homomorphism from K×, the non-zero elements of K up to multiplication, to Div K. Suppose that x ∈ K satisfies\n\nThen div x is defined to be the divisor\n\nThe kernel of div is the group of units in O, while the cokernel is the ideal class group. In the language of homological algebra, this says that there is an exact sequence of abelian groups (written multiplicatively),\n\nSome number fields, such as Q(√2), can be specified as subfields of the real numbers. Others, such as Q(√−1), cannot. Abstractly, such a specification corresponds to a field homomorphism K → R or K → C. These are called real embeddings and complex embeddings, respectively.\n\nA real quadratic field Q(√a), with a ∈ Q, a > 0, and a not a perfect square, is so-called because it admits two real embeddings but no complex embeddings. These are the field homomorphisms which send √a to √a and to −√a, respectively. Dually, an imaginary quadratic field Q(√−a) admits no real embeddings but admits a conjugate pair of complex embeddings. One of these embeddings sends √−a to √−a, while the other sends it to its complex conjugate, −√−a.\n\nConventionally, the number of real embeddings of K is denoted r1, while the number of conjugate pairs of complex embeddings is denoted r2. The signature of K is the pair (r1, r2). It is a theorem that r1 + 2r2 = d, where d is the degree of K.\n\nConsidering all embeddings at once determines a function \n\n\n\nM\n:\nK\n→\n\n\nR\n\n\n\nr\n\n1\n\n\n\n\n⊕\n\n\nC\n\n\n\nr\n\n2\n\n\n\n\n\n\n{\\displaystyle M\\colon K\\to \\mathbf {R} ^{r_{1}}\\oplus \\mathbf {C} ^{r_{2}}}\n\n, or equivalently \n\n\n\nM\n:\nK\n→\n\n\nR\n\n\n\nr\n\n1\n\n\n\n\n⊕\n\n\nR\n\n\n2\n\nr\n\n2\n\n\n\n\n.\n\n\n{\\displaystyle M\\colon K\\to \\mathbf {R} ^{r_{1}}\\oplus \\mathbf {R} ^{2r_{2}}.}\n\n\nThis is called the Minkowski embedding.\n\nThe subspace of the codomain fixed by complex conjugation is a real vector space of dimension d called Minkowski space. Because the Minkowski embedding is defined by field homomorphisms, multiplication of elements of K by an element x ∈ K corresponds to multiplication by a diagonal matrix in the Minkowski embedding. The dot product on Minkowski space corresponds to the trace form \n\n\n\n⟨\nx\n,\ny\n⟩\n=\nTr\n⁡\n(\nx\ny\n)\n\n\n{\\displaystyle \\langle x,y\\rangle =\\operatorname {Tr} (xy)}\n\n.\n\nThe image of O under the Minkowski embedding is a d-dimensional lattice. If B is a basis for this lattice, then det BTB is the discriminant of O. The discriminant is denoted Δ or D. The covolume of the image of O is \n\n\n\n\n\n\n|\n\nΔ\n\n|\n\n\n\n\n\n{\\displaystyle {\\sqrt {|\\Delta |}}}\n\n.\n\nReal and complex embeddings can be put on the same footing as prime ideals by adopting a perspective based on valuations. Consider, for example, the integers. In addition to the usual absolute value function |·| : Q → R, there are p-adic absolute value functions |·|p : Q → R, defined for each prime number p, which measure divisibility by p. Ostrowski's theorem states that these are all possible absolute value functions on Q (up to equivalence). Therefore, absolute values are a common language to describe both the real embedding of Q and the prime numbers.\n\nA place of an algebraic number field is an equivalence class of absolute value functions on K. There are two types of places. There is a \n\n\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {p}}}\n\n-adic absolute value for each prime ideal \n\n\n\n\n\np\n\n\n\n\n{\\displaystyle {\\mathfrak {p}}}\n\n of O, and, like the p-adic absolute values, it measures divisibility. These are called finite places. The other type of place is specified using a real or complex embedding of K and the standard absolute value function on R or C. These are infinite places. Because absolute values are unable to distinguish between a complex embedding and its conjugate, a complex embedding and its conjugate determine the same place. Therefore, there are r1 real places and r2 complex places. Because places encompass the primes, places are sometimes referred to as primes. When this is done, finite places are called finite primes and infinite places are called infinite primes. If v is a valuation corresponding to an absolute value, then one frequently writes \n\n\n\nv\n∣\n∞\n\n\n{\\displaystyle v\\mid \\infty }\n\n to mean that v is an infinite place and \n\n\n\nv\n∤\n∞\n\n\n{\\displaystyle v\\nmid \\infty }\n\n to mean that it is a finite place.\n\nConsidering all the places of the field together produces the adele ring of the number field. The adele ring allows one to simultaneously track all the data available using absolute values. This produces significant advantages in situations where the behavior at one place can affect the behavior at other places, as in the Artin reciprocity law.\n\nThere is a geometric analogy for places at infinity which holds on the function fields of curves. For example, let \n\n\n\nk\n=\n\n\nF\n\n\nq\n\n\n\n\n{\\displaystyle k=\\mathbb {F} _{q}}\n\n and \n\n\n\nX\n\n/\n\nk\n\n\n{\\displaystyle X/k}\n\n be a smooth, projective, algebraic curve. The function field \n\n\n\nF\n=\nk\n(\nX\n)\n\n\n{\\displaystyle F=k(X)}\n\n has many absolute values, or places, and each corresponds to a point on the curve. If \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is the projective completion of an affine curve \n\n\n\n\n\n\nX\n^\n\n\n\n⊂\n\n\nA\n\n\nn\n\n\n\n\n{\\displaystyle {\\hat {X}}\\subset \\mathbb {A} ^{n}}\n\n then the points in \n\n\n\nX\n−\n\n\n\nX\n^\n\n\n\n\n\n{\\displaystyle X-{\\hat {X}}}\n\n correspond to the places at infinity. Then, the completion of \n\n\n\nF\n\n\n{\\displaystyle F}\n\n at one of these points gives an analogue of the \n\n\n\np\n\n\n{\\displaystyle p}\n\n-adics.\n\n\nFor example, if \n\n\n\nX\n=\n\n\nP\n\n\n1\n\n\n\n\n{\\displaystyle X=\\mathbb {P} ^{1}}\n\n then its function field is isomorphic to \n\n\n\nk\n(\nt\n)\n\n\n{\\displaystyle k(t)}\n\n where \n\n\n\nt\n\n\n{\\displaystyle t}\n\n is an indeterminant and the field \n\n\n\nF\n\n\n{\\displaystyle F}\n\n is the field of fractions of polynomials in \n\n\n\nt\n\n\n{\\displaystyle t}\n\n. Then, a place \n\n\n\n\nv\n\np\n\n\n\n\n{\\displaystyle v_{p}}\n\n at a point \n\n\n\np\n∈\nX\n\n\n{\\displaystyle p\\in X}\n\n measures the order of vanishing or the order of a pole of a fraction of polynomials \n\n\n\np\n(\nx\n)\n\n/\n\nq\n(\nx\n)\n\n\n{\\displaystyle p(x)/q(x)}\n\n at the point \n\n\n\np\n\n\n{\\displaystyle p}\n\n. For example, if \n\n\n\np\n=\n[\n2\n:\n1\n]\n\n\n{\\displaystyle p=[2:1]}\n\n, so on the affine chart \n\n\n\n\nx\n\n1\n\n\n≠\n0\n\n\n{\\displaystyle x_{1}\\neq 0}\n\n this corresponds to the point \n\n\n\n2\n∈\n\n\nA\n\n\n1\n\n\n\n\n{\\displaystyle 2\\in \\mathbb {A} ^{1}}\n\n, the valuation \n\n\n\n\nv\n\n2\n\n\n\n\n{\\displaystyle v_{2}}\n\n measures the order of vanishing of \n\n\n\np\n(\nx\n)\n\n\n{\\displaystyle p(x)}\n\n minus the order of vanishing of \n\n\n\nq\n(\nx\n)\n\n\n{\\displaystyle q(x)}\n\n at \n\n\n\n2\n\n\n{\\displaystyle 2}\n\n. The function field of the completion at the place \n\n\n\n\nv\n\n2\n\n\n\n\n{\\displaystyle v_{2}}\n\n is then \n\n\n\nk\n(\n(\nt\n−\n2\n)\n)\n\n\n{\\displaystyle k((t-2))}\n\n which is the field of power series in the variable \n\n\n\nt\n−\n2\n\n\n{\\displaystyle t-2}\n\n, so an element is of the form\n\n\n\n\n\n\n\n\n\n\na\n\n−\nk\n\n\n(\nt\n−\n2\n\n)\n\n−\nk\n\n\n+\n⋯\n+\n\na\n\n−\n1\n\n\n(\nt\n−\n2\n\n)\n\n−\n1\n\n\n+\n\na\n\n0\n\n\n+\n\na\n\n1\n\n\n(\nt\n−\n2\n)\n+\n\na\n\n2\n\n\n(\nt\n−\n2\n\n)\n\n2\n\n\n+\n⋯\n\n\n\n\n\n\n=\n\n∑\n\nn\n=\n−\nk\n\n\n∞\n\n\n\na\n\nn\n\n\n(\nt\n−\n2\n\n)\n\nn\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&a_{-k}(t-2)^{-k}+\\cdots +a_{-1}(t-2)^{-1}+a_{0}+a_{1}(t-2)+a_{2}(t-2)^{2}+\\cdots \\\\&=\\sum _{n=-k}^{\\infty }a_{n}(t-2)^{n}\\end{aligned}}}\n\n\nfor some \n\n\n\nk\n∈\n\nN\n\n\n\n{\\displaystyle k\\in \\mathbb {N} }\n\n. For the place at infinity, this corresponds to the function field \n\n\n\nk\n(\n(\n1\n\n/\n\nt\n)\n)\n\n\n{\\displaystyle k((1/t))}\n\n which are power series of the form\n\n\n\n\n\n∑\n\nn\n=\n−\nk\n\n\n∞\n\n\n\na\n\nn\n\n\n(\n1\n\n/\n\nt\n\n)\n\nn\n\n\n\n\n{\\displaystyle \\sum _{n=-k}^{\\infty }a_{n}(1/t)^{n}}\n\n\nThe integers have only two units, 1 and −1. Other rings of integers may admit more units. The Gaussian integers have four units, the previous two as well as ±i. The Eisenstein integers Z[exp(2πi / 3)] have six units. The integers in real quadratic number fields have infinitely many units. For example, in Z[√3], every power of 2 + √3 is a unit, and all these powers are distinct.\n\nIn general, the group of units of O, denoted O×, is a finitely generated abelian group. The fundamental theorem of finitely generated abelian groups therefore implies that it is a direct sum of a torsion part and a free part. Reinterpreting this in the context of a number field, the torsion part consists of the roots of unity that lie in O. This group is cyclic. The free part is described by Dirichlet's unit theorem. This theorem says that rank of the free part is r1 + r2 − 1. Thus, for example, the only fields for which the rank of the free part is zero are Q and the imaginary quadratic fields. A more precise statement giving the structure of O× ⊗Z Q as a Galois module for the Galois group of K/Q is also possible.[14]\n\nThe free part of the unit group can be studied using the infinite places of K. Consider the function\n\nwhere v varies over the infinite places of K and |·|v is the absolute value associated with v. The function L is a homomorphism from K× to a real vector space. It can be shown that the image of O× is a lattice that spans the hyperplane defined by \n\n\n\n\nx\n\n1\n\n\n+\n⋯\n+\n\nx\n\n\nr\n\n1\n\n\n+\n\nr\n\n2\n\n\n\n\n=\n0.\n\n\n{\\displaystyle x_{1}+\\cdots +x_{r_{1}+r_{2}}=0.}\n\n The covolume of this lattice is the regulator of the number field. One of the simplifications made possible by working with the adele ring is that there is a single object, the idele class group, that describes both the quotient by this lattice and the ideal class group.\n\nThe Dedekind zeta function of a number field, analogous to the Riemann zeta function, is an analytic object which describes the behavior of prime ideals in K. When K is an abelian extension of Q, Dedekind zeta functions are products of Dirichlet L-functions, with there being one factor for each Dirichlet character. The trivial character corresponds to the Riemann zeta function. When K is a Galois extension, the Dedekind zeta function is the Artin L-function of the regular representation of the Galois group of K, and it has a factorization in terms of irreducible Artin representations of the Galois group.\n\nThe zeta function is related to the other invariants described above by the class number formula.\n\nCompleting a number field K at a place w gives a complete field. If the valuation is Archimedean, one obtains R or C, if it is non-Archimedean and lies over a prime p of the rationals, one obtains a finite extension \n\n\n\n\nK\n\nw\n\n\n\n/\n\n\n\nQ\n\n\np\n\n\n:\n\n\n{\\displaystyle K_{w}/\\mathbf {Q} _{p}:}\n\n a complete, discrete valued field with finite residue field. This process simplifies the arithmetic of the field and allows the local study of problems. For example, the Kronecker–Weber theorem can be deduced easily from the analogous local statement. The philosophy behind the study of local fields is largely motivated by geometric methods. In algebraic geometry, it is common to study varieties locally at a point by localizing to a maximal ideal. Global information can then be recovered by gluing together local data. This spirit is adopted in algebraic number theory. Given a prime in the ring of algebraic integers in a number field, it is desirable to study the field locally at that prime. Therefore, one localizes the ring of algebraic integers to that prime and then completes the fraction field much in the spirit of geometry.\n\nOne of the classical results in algebraic number theory is that the ideal class group of an algebraic number field K is finite. This is a consequence of Minkowski's theorem since there are only finitely many Integral ideals with norm less than a fixed positive integer[15] page 78. The order of the class group is called the class number, and is often denoted by the letter h.\n\nDirichlet's unit theorem provides a description of the structure of the multiplicative group of units O× of the ring of integers O. Specifically, it states that O× is isomorphic to G × Zr, where G is the finite cyclic group consisting of all the roots of unity in O, and r = r1 + r2 − 1 (where r1 (respectively, r2) denotes the number of real embeddings (respectively, pairs of conjugate non-real embeddings) of K). In other words, O× is a finitely generated abelian group of rank r1 + r2 − 1 whose torsion consists of the roots of unity in O.\n\nIn terms of the Legendre symbol, the law of quadratic reciprocity for positive odd primes states\n\nA reciprocity law is a generalization of the law of quadratic reciprocity.\n\nThere are several different ways to express reciprocity laws. The early reciprocity laws found in the 19th century were usually expressed in terms of a power residue symbol (p/q) generalizing the quadratic reciprocity symbol, that describes when a prime number is an nth power residue modulo another prime, and gave a relation between (p/q) and (q/p). Hilbert reformulated the reciprocity laws as saying that a product over p of Hilbert symbols (a,b/p), taking values in roots of unity, is equal to 1. Artin's reformulated reciprocity law states that the Artin symbol from ideals (or ideles) to elements of a Galois group is trivial on a certain subgroup. Several more recent generalizations express reciprocity laws using cohomology of groups or representations of adelic groups or algebraic K-groups, and their relationship with the original quadratic reciprocity law can be hard to see.\n\nThe class number formula relates many important invariants of a number field to a special value of its Dedekind zeta function.\n\nAlgebraic number theory interacts with many other mathematical disciplines. It uses tools from homological algebra. Via the analogy of function fields vs. number fields, it relies on techniques and ideas from algebraic geometry. Moreover, the study of higher-dimensional schemes over Z instead of number rings is referred to as arithmetic geometry. Algebraic number theory is also used in the study of arithmetic hyperbolic 3-manifolds.\n"
    },
    {
        "title": "Geometry of numbers",
        "content": "Geometry of numbers is the part of number theory which uses geometry for the study of algebraic numbers. Typically, a ring of algebraic integers is viewed as a lattice in \n\n\n\n\n\nR\n\n\nn\n\n\n,\n\n\n{\\displaystyle \\mathbb {R} ^{n},}\n\n and the study of these lattices provides fundamental information on algebraic numbers.[1] Hermann Minkowski (1896) initiated this line of research at the age of 26 in his work The Geometry of Numbers.[2]\n\nThe geometry of numbers has a close relationship with other fields of mathematics, especially functional analysis and Diophantine approximation, the problem of finding rational numbers  that  approximate an irrational quantity.[3]\n\nSuppose that \n\n\n\nΓ\n\n\n{\\displaystyle \\Gamma }\n\n is a lattice in \n\n\n\nn\n\n\n{\\displaystyle n}\n\n-dimensional Euclidean space \n\n\n\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{n}}\n\n and \n\n\n\nK\n\n\n{\\displaystyle K}\n\n is a convex centrally symmetric body.\nMinkowski's theorem, sometimes called Minkowski's first theorem, states that if \n\n\n\nvol\n⁡\n(\nK\n)\n>\n\n2\n\nn\n\n\nvol\n⁡\n(\n\n\nR\n\n\nn\n\n\n\n/\n\nΓ\n)\n\n\n{\\displaystyle \\operatorname {vol} (K)>2^{n}\\operatorname {vol} (\\mathbb {R} ^{n}/\\Gamma )}\n\n, then \n\n\n\nK\n\n\n{\\displaystyle K}\n\n contains a nonzero vector in \n\n\n\nΓ\n\n\n{\\displaystyle \\Gamma }\n\n.\n\nThe successive minimum \n\n\n\n\nλ\n\nk\n\n\n\n\n{\\displaystyle \\lambda _{k}}\n\n is defined to be the inf of the numbers \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n\n such that \n\n\n\nλ\nK\n\n\n{\\displaystyle \\lambda K}\n\n contains \n\n\n\nk\n\n\n{\\displaystyle k}\n\n linearly independent vectors of \n\n\n\nΓ\n\n\n{\\displaystyle \\Gamma }\n\n.\nMinkowski's theorem on successive minima, sometimes called Minkowski's second theorem, is a strengthening of his first theorem and states that[4]\n\nIn 1930–1960 research on the geometry of numbers was conducted by many number theorists (including Louis Mordell, Harold Davenport and Carl Ludwig Siegel). In recent years, Lenstra, Brion, and Barvinok have developed combinatorial theories that enumerate the lattice points in some convex bodies.[5]\n\nIn the geometry of numbers, the subspace theorem was obtained by Wolfgang M. Schmidt in 1972.[6] It states that if n is a positive integer, and L1,...,Ln are linearly independent linear forms in n variables with algebraic coefficients and if ε>0 is any given real number, then the non-zero integer points x in n coordinates with\n\nlie in a finite number of proper subspaces of Qn.\n\nMinkowski's geometry of numbers had a profound influence on functional analysis. Minkowski proved that symmetric convex bodies induce norms in finite-dimensional vector spaces. Minkowski's theorem was generalized to topological vector spaces by Kolmogorov, whose theorem states that the symmetric convex sets that are closed and bounded generate the topology of a Banach space.[7]\n\nResearchers continue to study generalizations to star-shaped sets and other non-convex sets.[8]\n"
    },
    {
        "title": "Diophantine equation",
        "content": "\n\nIn mathematics, a Diophantine equation is an equation, typically a polynomial equation in two or more unknowns with integer coefficients, for which only integer solutions are of interest. A linear Diophantine equation equates to a constant the sum of two or more monomials, each of degree one.  An exponential Diophantine equation is one in which unknowns can appear in exponents.\n\nDiophantine problems have fewer equations than unknowns and involve finding integers that solve simultaneously all equations. As such systems of equations define algebraic curves, algebraic surfaces, or, more generally, algebraic sets, their study is a part of algebraic geometry that is called Diophantine geometry.\n\nThe word Diophantine refers to the Hellenistic mathematician of the 3rd century, Diophantus of Alexandria, who made a study of such equations and was one of the first mathematicians to introduce symbolism into algebra. The mathematical study of Diophantine problems that Diophantus initiated is now called Diophantine analysis.\n\nWhile individual equations present a kind of puzzle and have been considered throughout history, the formulation of general theories of Diophantine equations (beyond the case of linear and quadratic equations) was an achievement of the twentieth century.\n\nIn the following Diophantine equations, w, x, y, and z are the unknowns and the other letters are given constants:\n\nThe simplest linear Diophantine equation takes the form \n\n\n\n\na\nx\n+\nb\ny\n=\nc\n,\n\n\n{\\displaystyle ax+by=c,}\n\n \nwhere a, b and c are given integers. The solutions are described by the following theorem:\n\nProof: If d is this greatest common divisor, Bézout's identity asserts the existence of integers e and f such that ae + bf = d. If c is a multiple of d, then c = dh for some integer h, and (eh, fh) is a solution. On the other hand, for every pair of integers x and y, the greatest common divisor d of a and b divides ax + by. Thus, if the equation has a solution, then c must be a multiple of d. If a = ud and b = vd, then for every solution (x, y), we have \n\n\n\n\n\n\n\n\na\n(\nx\n+\nk\nv\n)\n+\nb\n(\ny\n−\nk\nu\n)\n\n\n\n=\na\nx\n+\nb\ny\n+\nk\n(\na\nv\n−\nb\nu\n)\n\n\n\n\n\n\n=\na\nx\n+\nb\ny\n+\nk\n(\nu\nd\nv\n−\nv\nd\nu\n)\n\n\n\n\n\n\n=\na\nx\n+\nb\ny\n,\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}a(x+kv)+b(y-ku)&=ax+by+k(av-bu)\\\\&=ax+by+k(udv-vdu)\\\\&=ax+by,\\end{aligned}}}\n\n \nshowing that (x + kv, y − ku) is another solution. Finally, given two solutions such that \n\n\n\n\na\n\nx\n\n1\n\n\n+\nb\n\ny\n\n1\n\n\n=\na\n\nx\n\n2\n\n\n+\nb\n\ny\n\n2\n\n\n=\nc\n,\n\n\n{\\displaystyle ax_{1}+by_{1}=ax_{2}+by_{2}=c,}\n\n \none deduces that \n\n\n\nu\n(\n\nx\n\n2\n\n\n−\n\nx\n\n1\n\n\n)\n+\nv\n(\n\ny\n\n2\n\n\n−\n\ny\n\n1\n\n\n)\n=\n0.\n\n\n{\\displaystyle u(x_{2}-x_{1})+v(y_{2}-y_{1})=0.}\n\n \nAs u and v are coprime, Euclid's lemma shows that v divides x2 − x1, and thus that there exists an integer k such that both \n\n\n\n\n\nx\n\n2\n\n\n−\n\nx\n\n1\n\n\n=\nk\nv\n,\n\n\ny\n\n2\n\n\n−\n\ny\n\n1\n\n\n=\n−\nk\nu\n.\n\n\n{\\displaystyle x_{2}-x_{1}=kv,\\quad y_{2}-y_{1}=-ku.}\n\n \nTherefore, \n\n\n\n\n\nx\n\n2\n\n\n=\n\nx\n\n1\n\n\n+\nk\nv\n,\n\n\ny\n\n2\n\n\n=\n\ny\n\n1\n\n\n−\nk\nu\n,\n\n\n{\\displaystyle x_{2}=x_{1}+kv,\\quad y_{2}=y_{1}-ku,}\n\n\nwhich completes the proof.\n\nThe Chinese remainder theorem describes an important class of linear Diophantine systems of equations: let \n\n\n\n\nn\n\n1\n\n\n,\n…\n,\n\nn\n\nk\n\n\n\n\n{\\displaystyle n_{1},\\dots ,n_{k}}\n\n be k pairwise coprime integers greater than one, \n\n\n\n\na\n\n1\n\n\n,\n…\n,\n\na\n\nk\n\n\n\n\n{\\displaystyle a_{1},\\dots ,a_{k}}\n\n be k arbitrary integers, and N be the product \n\n\n\n\nn\n\n1\n\n\n⋯\n\nn\n\nk\n\n\n.\n\n\n{\\displaystyle n_{1}\\cdots n_{k}.}\n\n The Chinese remainder theorem asserts that the following linear Diophantine system has exactly one solution \n\n\n\n(\nx\n,\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nk\n\n\n)\n\n\n{\\displaystyle (x,x_{1},\\dots ,x_{k})}\n\n such that 0 ≤ x < N, and that the other solutions are obtained by adding to x a multiple of N:\n\n\n\n\n\n\n\n\nx\n\n\n\n=\n\na\n\n1\n\n\n+\n\nn\n\n1\n\n\n\n\nx\n\n1\n\n\n\n\n\n\n\n\n\n\n⋮\n\n\n\n\nx\n\n\n\n=\n\na\n\nk\n\n\n+\n\nn\n\nk\n\n\n\n\nx\n\nk\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}x&=a_{1}+n_{1}\\,x_{1}\\\\&\\;\\;\\vdots \\\\x&=a_{k}+n_{k}\\,x_{k}\\end{aligned}}}\n\n\n\nMore generally, every system of linear Diophantine equations may be solved by computing the Smith normal form of its matrix, in a way that is similar to the use of the reduced row echelon form to solve a system of linear equations over a field. Using matrix notation every system of linear Diophantine equations may be written\n\n\n\n\nA\nX\n=\nC\n,\n\n\n{\\displaystyle AX=C,}\n\n\nwhere A is an m × n matrix of integers, X is an n × 1 column matrix of unknowns and C is an m × 1 column matrix of integers.\n\nThe computation of the Smith normal form of A provides two unimodular matrices (that is matrices that are invertible over the integers and have ±1 as determinant) U and V of respective dimensions m × m and n × n, such that the matrix\n\n\n\n\nB\n=\n[\n\nb\n\ni\n,\nj\n\n\n]\n=\nU\nA\nV\n\n\n{\\displaystyle B=[b_{i,j}]=UAV}\n\n\nis such that bi,i is not zero for i not greater than some integer k, and all the other entries are zero. The system to be solved may thus be rewritten as\n\n\n\n\nB\n(\n\nV\n\n−\n1\n\n\nX\n)\n=\nU\nC\n.\n\n\n{\\displaystyle B(V^{-1}X)=UC.}\n\n\nCalling yi the entries of V−1X and di those of D = UC, this leads to the system\n\n\n\n\n\n\n\n\n\n\nb\n\ni\n,\ni\n\n\n\ny\n\ni\n\n\n=\n\nd\n\ni\n\n\n,\n\n1\n≤\ni\n≤\nk\n\n\n\n\n\n0\n\ny\n\ni\n\n\n=\n\nd\n\ni\n\n\n,\n\nk\n<\ni\n≤\nn\n.\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&b_{i,i}y_{i}=d_{i},\\quad 1\\leq i\\leq k\\\\&0y_{i}=d_{i},\\quad k<i\\leq n.\\end{aligned}}}\n\n\n\nThis system is equivalent to the given one in the following sense: A column matrix of integers x is a solution of the given system if and only if x = Vy for some column matrix of integers y such that By = D.\n\nIt follows that the system has a solution if and only if bi,i divides di for i ≤ k and di = 0 for i > k. If this condition is fulfilled, the solutions of the given system are\n\n\n\n\nV\n\n\n\n[\n\n\n\n\n\n\nd\n\n1\n\n\n\nb\n\n1\n,\n1\n\n\n\n\n\n\n\n\n⋮\n\n\n\n\n\n\n\nd\n\nk\n\n\n\nb\n\nk\n,\nk\n\n\n\n\n\n\n\n\n\nh\n\nk\n+\n1\n\n\n\n\n\n\n⋮\n\n\n\n\n\nh\n\nn\n\n\n\n\n\n]\n\n\n\n,\n\n\n{\\displaystyle V\\,{\\begin{bmatrix}{\\frac {d_{1}}{b_{1,1}}}\\\\\\vdots \\\\{\\frac {d_{k}}{b_{k,k}}}\\\\h_{k+1}\\\\\\vdots \\\\h_{n}\\end{bmatrix}}\\,,}\n\n\nwhere hk+1, …, hn are arbitrary integers.\n\nHermite normal form may also be used for solving systems of linear Diophantine equations. However, Hermite normal form does not directly provide the solutions; to get the solutions from the Hermite normal form, one has to successively solve several linear equations. Nevertheless, Richard Zippel wrote that the Smith normal form \"is somewhat more than is actually needed to solve linear diophantine equations. Instead of reducing the equation to diagonal form, we only need to make it triangular, which is called the Hermite normal form. The Hermite normal form is substantially easier to compute than the Smith normal form.\"[6]\n\nInteger linear programming amounts to finding some integer solutions (optimal in some sense) of linear systems that include also inequations.  Thus systems of linear Diophantine equations are basic in this context, and textbooks on integer programming usually have a treatment of systems of linear Diophantine equations.[7]\n\nA homogeneous Diophantine equation is a Diophantine equation that is defined by a homogeneous polynomial. A typical such equation is the equation of Fermat's Last Theorem\n\nAs a homogeneous polynomial in n indeterminates defines a hypersurface in the projective space of dimension n − 1, solving a homogeneous Diophantine equation is the same as finding the rational points of a projective hypersurface.\n\nSolving a homogeneous Diophantine equation is generally a very difficult problem, even in the simplest non-trivial case of three indeterminates (in the case of two indeterminates the problem is equivalent with testing if a rational number is the dth power of another rational number). A witness of the difficulty of the problem is Fermat's Last Theorem (for d > 2, there is no integer solution of the above equation), which needed more than three centuries of mathematicians' efforts before being solved.\n\nFor degrees higher than three, most known results are theorems asserting that there are no solutions (for example Fermat's Last Theorem) or that the number of solutions is finite (for example Falting's theorem). \n\nFor the degree three, there are general solving methods, which work on almost all equations that are encountered in practice, but no algorithm is known that works for every cubic equation.[8]\n\nHomogeneous Diophantine equations of degree two are easier to solve. The standard solving method proceeds in two steps. One has first to find one solution, or to prove that there is no solution. When a solution has been found, all solutions are then deduced.\n\nFor proving that there is no solution, one may reduce the equation modulo p. For example, the Diophantine equation\n\ndoes not have any other solution than the trivial solution (0, 0, 0). In fact, by dividing x, y, and z by their greatest common divisor, one may suppose that they are coprime. The squares modulo 4 are congruent to 0 and 1. Thus the left-hand side of the equation is congruent to 0, 1, or 2, and the right-hand side is congruent to 0 or 3. Thus the equality may be obtained only if x, y, and z are all even, and are thus not coprime. Thus the only solution is the trivial solution (0, 0, 0). This shows that there is no rational point on a circle of radius \n\n\n\n\n\n3\n\n\n,\n\n\n{\\displaystyle {\\sqrt {3}},}\n\n centered at the origin.\n\nMore generally, the Hasse principle allows deciding whether a homogeneous Diophantine equation of degree two has an integer solution, and computing a solution if there exist. \n\nIf a non-trivial integer solution is known, one may produce all other solutions in the following way.\n\nLet \n\nbe a homogeneous Diophantine equation, where \n\n\n\nQ\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle Q(x_{1},\\ldots ,x_{n})}\n\n is a quadratic form (that is, a homogeneous polynomial of degree 2), with integer coefficients. The trivial solution is the solution where all \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n are zero. If \n\n\n\n(\n\na\n\n1\n\n\n,\n…\n,\n\na\n\nn\n\n\n)\n\n\n{\\displaystyle (a_{1},\\ldots ,a_{n})}\n\n is a non-trivial integer solution of this equation, then \n\n\n\n\n(\n\n\na\n\n1\n\n\n,\n…\n,\n\na\n\nn\n\n\n\n)\n\n\n\n{\\displaystyle \\left(a_{1},\\ldots ,a_{n}\\right)}\n\n are the homogeneous coordinates of a rational point of the hypersurface defined by Q. Conversely, if  \n\n\n\n\n(\n\n\n\n\np\n\n1\n\n\nq\n\n\n,\n…\n,\n\n\n\np\n\nn\n\n\nq\n\n\n\n)\n\n\n\n{\\textstyle \\left({\\frac {p_{1}}{q}},\\ldots ,{\\frac {p_{n}}{q}}\\right)}\n\n are homogeneous coordinates of a rational point of this hypersurface, where \n\n\n\nq\n,\n\np\n\n1\n\n\n,\n…\n,\n\np\n\nn\n\n\n\n\n{\\displaystyle q,p_{1},\\ldots ,p_{n}}\n\n are integers, then \n\n\n\n\n(\n\n\np\n\n1\n\n\n,\n…\n,\n\np\n\nn\n\n\n\n)\n\n\n\n{\\displaystyle \\left(p_{1},\\ldots ,p_{n}\\right)}\n\n is an integer solution of the Diophantine equation. Moreover, the integer solutions that define a given rational point are all sequences of the form \n\nwhere k is any integer, and d is the greatest common divisor of the \n\n\n\n\np\n\ni\n\n\n.\n\n\n{\\displaystyle p_{i}.}\n\n\n\nIt follows that solving the Diophantine equation \n\n\n\nQ\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n=\n0\n\n\n{\\displaystyle Q(x_{1},\\ldots ,x_{n})=0}\n\n is completely reduced to finding the rational points of the corresponding projective hypersurface.\n\nLet now \n\n\n\nA\n=\n\n(\n\n\na\n\n1\n\n\n,\n…\n,\n\na\n\nn\n\n\n\n)\n\n\n\n{\\displaystyle A=\\left(a_{1},\\ldots ,a_{n}\\right)}\n\n be an integer solution of the equation \n\n\n\nQ\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n=\n0.\n\n\n{\\displaystyle Q(x_{1},\\ldots ,x_{n})=0.}\n\n As Q is a polynomial of degree two, a line passing through A crosses the hypersurface at a single other point, which is rational if and only if the line is rational (that is, if the line is defined by rational parameters). This allows parameterizing the hypersurface by the lines passing through A, and the rational points are those that are obtained from rational lines, that is, those that correspond to rational values of the parameters.\n\nMore precisely, one may proceed as follows. \n\nBy permuting the indices, one may suppose, without loss of generality that \n\n\n\n\na\n\nn\n\n\n≠\n0.\n\n\n{\\displaystyle a_{n}\\neq 0.}\n\n Then one may pass to the affine case by considering the affine hypersurface defined by \n\nwhich has the rational point\n\nIf this rational point is a singular point, that is if all partial derivatives are zero at R, all lines passing through R are contained in the hypersurface, and one has a cone. The change of variables \n\ndoes not change the rational points, and transforms q into a homogeneous polynomial in n − 1 variables. In this case, the problem may thus be solved by applying the method to an equation with fewer variables.\n\nIf the polynomial q is a product of linear polynomials (possibly with non-rational coefficients), then it defines two hyperplanes. The intersection of these hyperplanes is a rational flat, and contains rational singular points. This case is thus a special instance of the preceding case.\n\nIn the general case, consider the parametric equation of a line passing through R:\n\nSubstituting this in q, one gets a polynomial of degree two in x1, that is zero for x1 = r1. It is thus divisible by x1 – r1. The quotient is linear in x1, and may be solved for expressing x1 as a quotient of two polynomials of degree at most two in \n\n\n\n\nt\n\n2\n\n\n,\n…\n,\n\nt\n\nn\n−\n1\n\n\n,\n\n\n{\\displaystyle t_{2},\\ldots ,t_{n-1},}\n\n with integer coefficients:\n\nSubstituting this in the expressions for \n\n\n\n\nx\n\n2\n\n\n,\n…\n,\n\nx\n\nn\n−\n1\n\n\n,\n\n\n{\\displaystyle x_{2},\\ldots ,x_{n-1},}\n\n one gets, for i = 1, …, n − 1,\n\nwhere \n\n\n\n\nf\n\n1\n\n\n,\n…\n,\n\nf\n\nn\n\n\n\n\n{\\displaystyle f_{1},\\ldots ,f_{n}}\n\n are polynomials of degree at most two with integer coefficients.\n\nThen, one can return to the homogeneous case. Let, for i = 1, …, n, \n\nbe the homogenization of \n\n\n\n\nf\n\ni\n\n\n.\n\n\n{\\displaystyle f_{i}.}\n\n These quadratic polynomials with integer coefficients form a parameterization of the projective hypersurface defined by Q:\n\nA point of the projective hypersurface defined by Q is rational if and only if it may be obtained from rational values of \n\n\n\n\nt\n\n1\n\n\n,\n…\n,\n\nt\n\nn\n−\n1\n\n\n.\n\n\n{\\displaystyle t_{1},\\ldots ,t_{n-1}.}\n\n As \n\n\n\n\nF\n\n1\n\n\n,\n…\n,\n\nF\n\nn\n\n\n\n\n{\\displaystyle F_{1},\\ldots ,F_{n}}\n\n are homogeneous polynomials, the point is not changed if all ti are multiplied by the same rational number. Thus, one may suppose that \n\n\n\n\nt\n\n1\n\n\n,\n…\n,\n\nt\n\nn\n−\n1\n\n\n\n\n{\\displaystyle t_{1},\\ldots ,t_{n-1}}\n\n are coprime integers. It follows that the integer solutions of the Diophantine equation are exactly the sequences \n\n\n\n(\n\nx\n\n1\n\n\n,\n…\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle (x_{1},\\ldots ,x_{n})}\n\n where, for i = 1, ..., n,\n\nwhere k is an integer, \n\n\n\n\nt\n\n1\n\n\n,\n…\n,\n\nt\n\nn\n−\n1\n\n\n\n\n{\\displaystyle t_{1},\\ldots ,t_{n-1}}\n\n are coprime integers, and d is the greatest common divisor of the n integers \n\n\n\n\nF\n\ni\n\n\n(\n\nt\n\n1\n\n\n,\n…\n,\n\nt\n\nn\n−\n1\n\n\n)\n.\n\n\n{\\displaystyle F_{i}(t_{1},\\ldots ,t_{n-1}).}\n\n\n\nOne could hope that the coprimality of the ti, could imply that d = 1. Unfortunately this is not the case, as shown in the next section.\n\nThe equation \n\nis probably the first homogeneous Diophantine equation of degree two that has been studied. Its solutions are the Pythagorean triples. This is also the homogeneous equation of the unit circle. In this section, we show how the above method allows retrieving Euclid's formula for generating Pythagorean triples.\n\nFor retrieving exactly Euclid's formula, we start from the solution (−1, 0, 1), corresponding to the point (−1, 0) of the unit circle. A line passing through this point may be parameterized by its slope:\n\nPutting this in the circle equation \n\none gets \n\nDividing by x + 1, results in\n\nwhich is easy to solve in x:\n\nIt follows\n\nHomogenizing as described above one gets all solutions as \n\nwhere k is any integer, s and t are coprime integers, and d is the greatest common divisor of the three numerators. In fact, d = 2 if s and t are both odd, and d = 1 if one is odd and the other is even.\n\nThe primitive triples are the solutions where k = 1 and s > t > 0.\n\nThis description of the solutions differs slightly from Euclid's formula because Euclid's formula considers only the solutions such that x, y, and z are all positive, and does not distinguish between two triples that differ by the exchange of x and y,\n\nThe questions asked in Diophantine analysis include:\n\nThese traditional problems often lay unsolved for centuries, and mathematicians gradually came to understand their depth (in some cases), rather than treat them as puzzles.\n\nThe given information is that a father's age is 1 less than twice that of his son, and that the digits AB making up the father's age are reversed in the son's age (i.e. BA). This leads to the equation 10A + B = 2(10B + A) − 1, thus 19B − 8A = 1. Inspection gives the result A = 7, B = 3, and thus AB equals 73 years and BA equals 37 years. One may easily show that there is not any other solution with A and B positive integers less than 10.\n\nMany well known puzzles in the field of recreational mathematics lead to diophantine equations.  Examples include the cannonball problem, Archimedes's cattle problem and the monkey and the coconuts.\n\nIn 1637, Pierre de Fermat scribbled on the margin of his copy of Arithmetica: \"It is impossible to separate a cube into two cubes, or a fourth power into two fourth powers, or in general, any power higher than the second into two like powers.\" Stated in more modern language, \"The equation an + bn = cn has no solutions for any n higher than 2.\" Following this, he wrote: \"I have discovered a truly marvelous proof of this proposition, which this margin is too narrow to contain.\" Such a proof eluded mathematicians for centuries, however, and as such his statement became famous as Fermat's Last Theorem. It was not until 1995 that it was proven by the British mathematician Andrew Wiles.\n\nIn 1657, Fermat attempted to solve the Diophantine equation 61x2 + 1 = y2 (solved by Brahmagupta over 1000 years earlier). The equation was eventually solved by Euler in the early 18th century, who also solved a number of other Diophantine equations. The smallest solution of this equation in positive integers is x = 226153980, y = 1766319049 (see Chakravala method).\n\nIn 1900, David Hilbert proposed the solvability of all Diophantine equations as the tenth of his fundamental problems. In 1970, Yuri Matiyasevich solved it negatively, building on work of Julia Robinson, Martin Davis, and Hilary Putnam to prove that a general algorithm for solving all Diophantine equations cannot exist.\n\nDiophantine geometry, is the application of techniques from algebraic geometry which considers equations that also have a geometric meaning. The central idea of Diophantine geometry is that of a rational point, namely a solution to a polynomial equation or a system of polynomial equations, which is a vector in a prescribed field K, when K is not algebraically closed.\n\nThe oldest general method for solving a Diophantine equation—or for proving that there is no solution— is the method of infinite descent, which was introduced by Pierre de Fermat. Another general method is the Hasse principle that uses modular arithmetic modulo all prime numbers for finding the solutions. Despite many improvements these methods cannot solve most Diophantine equations.\n\nThe difficulty of solving Diophantine equations is illustrated by Hilbert's tenth problem, which was  set in 1900 by David Hilbert; it was to find an algorithm to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution. Matiyasevich's theorem implies that such an algorithm cannot exist.\n\nDuring the 20th century, a new approach has been deeply explored, consisting of using algebraic geometry. In fact, a Diophantine equation can be viewed as the equation of an hypersurface, and the solutions of the equation are the points of the hypersurface that have integer coordinates.\n\nThis approach led eventually to the proof by Andrew Wiles in 1994 of Fermat's Last Theorem, stated without proof around 1637. This is another illustration of the difficulty of solving Diophantine equations.\n\nAn example of an infinite Diophantine equation is:\n\n\n\n\nn\n=\n\na\n\n2\n\n\n+\n2\n\nb\n\n2\n\n\n+\n3\n\nc\n\n2\n\n\n+\n4\n\nd\n\n2\n\n\n+\n5\n\ne\n\n2\n\n\n+\n⋯\n,\n\n\n{\\displaystyle n=a^{2}+2b^{2}+3c^{2}+4d^{2}+5e^{2}+\\cdots ,}\n\n\nwhich can be expressed as \"How many ways can a given integer n be written as the sum of a square plus twice a square plus thrice a square and so on?\" The number of ways this can be done for each n forms an integer sequence. Infinite Diophantine equations are related to theta functions and infinite dimensional lattices. This equation always has a solution for any positive n.[9] Compare this to:\n\n\n\n\nn\n=\n\na\n\n2\n\n\n+\n4\n\nb\n\n2\n\n\n+\n9\n\nc\n\n2\n\n\n+\n16\n\nd\n\n2\n\n\n+\n25\n\ne\n\n2\n\n\n+\n⋯\n,\n\n\n{\\displaystyle n=a^{2}+4b^{2}+9c^{2}+16d^{2}+25e^{2}+\\cdots ,}\n\n\nwhich does not always have a solution for positive n.\n\nIf a Diophantine equation has as an additional variable or variables occurring as exponents, it is an exponential Diophantine equation. Examples include:\n\nA general theory for such equations is not available; particular cases such as Catalan's conjecture and Fermat's Last Theorem have been tackled. However, the majority are solved via ad-hoc methods such as Størmer's theorem or even trial and error.\n"
    },
    {
        "title": "Transcendental number theory",
        "content": "\n\nRing homomorphisms\n\nAlgebraic structures\n\nRelated structures\n\nAlgebraic number theory\n\nNoncommutative algebraic geometry\n\nFree algebra\n\nClifford algebra\n\nTranscendental number theory is a branch of number theory that investigates transcendental numbers (numbers that are not solutions of any polynomial equation with rational coefficients), in both qualitative and quantitative ways.\n\nThe fundamental theorem of algebra tells us that if we have a non-constant polynomial with rational coefficients (or equivalently, by clearing denominators, with integer coefficients) then that polynomial will have a root in the complex numbers.  That is, for any non-constant polynomial \n\n\n\nP\n\n\n{\\displaystyle P}\n\n with rational coefficients there will be a complex number \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n such that \n\n\n\nP\n(\nα\n)\n=\n0\n\n\n{\\displaystyle P(\\alpha )=0}\n\n.  Transcendence theory is concerned with the converse question: given a complex number \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n, is there a polynomial \n\n\n\nP\n\n\n{\\displaystyle P}\n\n with rational coefficients such that \n\n\n\nP\n(\nα\n)\n=\n0\n?\n\n\n{\\displaystyle P(\\alpha )=0?}\n\n If no such polynomial exists then the number is called transcendental.\n\nMore generally the theory deals with algebraic independence of numbers.  A set of numbers {α1, α2, …, αn} is called algebraically independent over a field K if there is no non-zero polynomial P in n variables with coefficients in K such that P(α1, α2, …, αn) = 0.  So working out if a given number is transcendental is really a special case of algebraic independence where n = 1 and the field K is the field of rational numbers.\n\nA related notion is whether there is a closed-form expression for a number, including exponentials and logarithms as well as algebraic operations. There are various definitions of \"closed-form\", and questions about closed-form can often be reduced to questions about transcendence.\n\nUse of the term transcendental to refer to an object that is not algebraic dates back to the seventeenth century, when Gottfried Leibniz proved that the sine function was not an algebraic function.[1]  The question of whether certain classes of numbers could be transcendental dates back to 1748[2] when Euler asserted[3] that the number logab was not algebraic for rational numbers a and b provided b is not of the form b = ac for some rational c.\n\nEuler's assertion was not proved until the twentieth century, but almost a hundred years after his claim Joseph Liouville did manage to prove the existence of numbers that are not algebraic, something that until then had not been known for sure.[4]  His original papers on the matter in the 1840s sketched out arguments using simple continued fractions to construct transcendental numbers.  Later, in the 1850s, he gave a necessary condition for a number to be algebraic, and thus a sufficient condition for a number to be transcendental.[5]  This transcendence criterion was not strong enough to be necessary too, and indeed it fails to detect that the number e is transcendental.  But his work did provide a larger class of transcendental numbers, now known as Liouville numbers in his honour.\n\nLiouville's criterion essentially said that algebraic numbers cannot be very well approximated by rational numbers.  So if a number can be very well approximated by rational numbers then it must be transcendental.  The exact meaning of \"very well approximated\" in Liouville's work relates to a certain exponent.  He showed that if α is an algebraic number of degree d ≥ 2 and ε is any number greater than zero, then the expression\n\ncan be satisfied by only finitely many rational numbers p/q.  Using this as a criterion for transcendence is not trivial, as one must check whether there are infinitely many solutions p/q for every d ≥ 2.\n\nIn the twentieth century work by Axel Thue,[6] Carl Siegel,[7] and Klaus Roth[8] reduced the exponent in Liouville's work from d + ε to d/2 + 1 + ε, and finally, in 1955, to 2 + ε.  This result, known as the Thue–Siegel–Roth theorem, is ostensibly the best possible, since if the exponent 2 + ε is replaced by just 2 then the result is no longer true.  However, Serge Lang conjectured an improvement of Roth's result; in particular he conjectured that q2+ε in the denominator of the right-hand side could be reduced to \n\n\n\n\nq\n\n2\n\n\n(\nlog\n⁡\nq\n\n)\n\n1\n+\nϵ\n\n\n\n\n{\\displaystyle q^{2}(\\log q)^{1+\\epsilon }}\n\n.\n\nRoth's work effectively ended the work started by Liouville, and his theorem allowed mathematicians to prove the transcendence of many more numbers, such as the Champernowne constant.  The theorem is still not strong enough to detect all transcendental numbers, though, and many famous constants including e and π either are not or are not known to be very well approximable in the above sense.[9]\n\nFortunately other methods were pioneered in the nineteenth century to deal with the algebraic properties of e, and consequently of π through Euler's identity.  This work centred on use of the so-called auxiliary function.  These are functions which typically have many zeros at the points under consideration.  Here \"many zeros\" may mean many distinct zeros, or as few as one zero but with a high multiplicity, or even many zeros all with high multiplicity.  Charles Hermite used auxiliary functions that approximated the functions \n\n\n\n\ne\n\nk\nx\n\n\n\n\n{\\displaystyle e^{kx}}\n\n for each natural number \n\n\n\nk\n\n\n{\\displaystyle k}\n\n in order to prove the transcendence of \n\n\n\ne\n\n\n{\\displaystyle e}\n\n in 1873.[10]  His work was built upon by Ferdinand von Lindemann in the 1880s[11] in order to prove that eα is transcendental for nonzero algebraic numbers α.  In particular this proved that π is transcendental since eπi is algebraic, and thus answered in the negative the problem of antiquity as to whether it was possible to square the circle.  Karl Weierstrass developed their work yet further and eventually proved the Lindemann–Weierstrass theorem in 1885.[12]\n\nIn 1900 David Hilbert posed his famous collection of problems.  The seventh of these, and one of the hardest in Hilbert's estimation, asked about the transcendence of numbers of the form ab where a and b are algebraic, a is not zero or one, and b is irrational.  In the 1930s Alexander Gelfond[13] and Theodor Schneider[14] proved that all such numbers were indeed transcendental using a non-explicit auxiliary function whose existence was granted by Siegel's lemma.  This result, the Gelfond–Schneider theorem, proved the transcendence of numbers such as eπ and the Gelfond–Schneider constant.\n\nThe next big result in this field occurred in the 1960s, when Alan Baker made progress on a problem posed by Gelfond on linear forms in logarithms.  Gelfond himself had managed to find a non-trivial lower bound for the quantity\n\nwhere all four unknowns are algebraic, the αs being neither zero nor one and the βs being irrational.  Finding similar lower bounds for the sum of three or more logarithms had eluded Gelfond, though.  The proof of Baker's theorem contained such bounds, solving Gauss' class number problem for class number one in the process.  This work won Baker the Fields medal for its uses in solving Diophantine equations.  From a purely transcendental number theoretic viewpoint, Baker had proved that if α1, ..., αn are algebraic numbers, none of them zero or one, and β1, ..., βn are algebraic numbers such that 1, β1, ..., βn are linearly independent over the rational numbers, then the number\n\nis transcendental.[15]\n\nIn the 1870s, Georg Cantor started to develop set theory and, in 1874, published a paper proving that the algebraic numbers could be put in one-to-one correspondence with the set of natural numbers, and thus that the set of transcendental numbers must be uncountable.[16]  Later, in 1891, Cantor used his more familiar diagonal argument to prove the same result.[17]  While Cantor's result is often quoted as being purely existential and thus unusable for constructing a single transcendental number,[18][19] the proofs in both the aforementioned papers give methods to construct transcendental numbers.[20]\n\nWhile Cantor used set theory to prove the plenitude of transcendental numbers, a recent development has been the use of model theory in attempts to prove an unsolved problem in transcendental number theory.  The problem is to determine the transcendence degree of the field\n\nfor complex numbers x1, ..., xn that are linearly independent over the rational numbers.  Stephen Schanuel conjectured that the answer is at least n, but no proof is known.  In 2004, though, Boris Zilber published a paper that used model theoretic techniques to create a structure that behaves very much like the complex numbers equipped with the operations of addition, multiplication, and exponentiation.  Moreover, in this abstract structure Schanuel's conjecture does indeed hold.[21]  Unfortunately it is not yet known that this structure is in fact the same as the complex numbers with the operations mentioned; there could exist some other abstract structure that behaves very similarly to the complex numbers but where Schanuel's conjecture doesn't hold.  Zilber did provide several criteria that would prove the structure in question was C, but could not prove the so-called Strong Exponential Closure axiom.  The simplest case of this axiom has since been proved,[22] but a proof that it holds in full generality is required to complete the proof of the conjecture.\n\nA typical problem in this area of mathematics is to work out whether a given number is transcendental. Cantor used a cardinality argument to show that there are only countably many algebraic numbers, and hence almost all numbers are transcendental.  Transcendental numbers therefore represent the typical case; even so, it may be extremely difficult to prove that a given number is transcendental (or even simply irrational).\n\nFor this reason transcendence theory often works towards a more quantitative approach.  So given a particular complex number α one can ask how close α is to being an algebraic number.  For example, if one supposes that the number α is algebraic then can one show that it must have very high degree or a minimum polynomial with very large coefficients?  Ultimately if it is possible to show that no finite degree or size of coefficient is sufficient then the number must be transcendental.  Since a number α is transcendental if and only if P(α) ≠ 0 for every non-zero polynomial P with integer coefficients, this problem can be approached by trying to find lower bounds of the form\n\nwhere the right hand side is some positive function depending on some measure A of the size of the coefficients of P, and its degree d, and such that these lower bounds apply to all P ≠ 0. Such a bound is called a transcendence measure.\n\nThe case of d = 1 is that of \"classical\" diophantine approximation asking for lower bounds for\n\nThe methods of transcendence theory and diophantine approximation have much in common: they both use the auxiliary function concept.\n\nThe Gelfond–Schneider theorem was the major advance in transcendence theory in the period 1900–1950. In the 1960s the method of Alan Baker on linear forms in logarithms of algebraic numbers reanimated transcendence theory, with applications to numerous classical problems and diophantine equations.\n\nKurt Mahler in 1932 partitioned the transcendental numbers into 3 classes, called S, T, and U.[23]  Definition of these classes draws on an extension of the idea of a Liouville number (cited above).\n\nOne way to define a Liouville number is to consider how small a given real number x makes linear polynomials |qx − p| without making them exactly 0.  Here p, q are integers with |p|, |q| bounded by a positive integer H.\n\nLet \n\n\n\nm\n(\nx\n,\n1\n,\nH\n)\n\n\n{\\displaystyle m(x,1,H)}\n\n be the minimum non-zero absolute value these polynomials take and take:\n\nω(x, 1) is often called the measure of irrationality of a real number x. For rational numbers, ω(x, 1) = 0 and is at least 1 for irrational real numbers. A Liouville number is defined to have infinite measure of irrationality. Roth's theorem says that irrational real algebraic numbers have measure of irrationality 1.\n\nNext consider the values of polynomials at a complex number x, when these polynomials have integer coefficients, degree at most n, and height at most H, with n, H being positive integers.\n\nLet \n\n\n\nm\n(\nx\n,\nn\n,\nH\n)\n\n\n{\\displaystyle m(x,n,H)}\n\n be the minimum non-zero absolute value such polynomials take at \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and take:\n\nSuppose this is infinite for some minimum positive integer n.  A complex number x in this case is called a U number of degree n.\n\nNow we can define \n\nω(x) is often called the measure of transcendence of x. If the ω(x, n) are bounded, then ω(x) is finite, and x is called an S number.  If the ω(x, n) are finite but unbounded, x is called a T number. x is algebraic if and only if ω(x) = 0.\n\nClearly the Liouville numbers are a subset of the U numbers.  William LeVeque in 1953 constructed U numbers of any desired degree.[24] The Liouville numbers and hence the U numbers are uncountable sets. They are sets of measure 0.[25]\n\nT numbers also comprise a set of measure 0.[26] It took about 35 years to show their existence. Wolfgang M. Schmidt in 1968 showed that examples exist.  However, almost all complex numbers are S numbers.[27] Mahler proved that the exponential function sends all non-zero algebraic numbers to S numbers:[28][29] this shows that e is an S number and gives a proof of the transcendence of π.  This number π is known not to be a U number.[30]  Many other transcendental numbers remain unclassified.\n\nTwo numbers x, y are called algebraically dependent if there is a non-zero polynomial P in two indeterminates with integer coefficients such that P(x, y) = 0. There is a powerful theorem that two complex numbers that are algebraically dependent belong to the same Mahler class.[24][31]  This allows construction of new transcendental numbers, such as the sum of a Liouville number with e or π.\n\nThe symbol S probably stood for the name of Mahler's teacher Carl Ludwig Siegel, and T and U are just the next two letters.\n\nJurjen Koksma in 1939 proposed another classification based on approximation by algebraic numbers.[23][32]\n\nConsider the approximation of a complex number x by algebraic numbers of degree ≤ n and height ≤ H. Let α be an algebraic number of this finite set such that |x − α| has the minimum positive value. Define ω*(x, H, n) and ω*(x, n) by:\n\nIf for a smallest positive integer n, ω*(x, n) is infinite, x is called a U*-number of degree n.\n\nIf the ω*(x, n) are bounded and do not converge to 0, x is called an S*-number,\n\nA number x is called an A*-number if the ω*(x, n) converge to 0.\n\nIf the ω*(x, n) are all finite but unbounded, x is called a T*-number,\n\nKoksma's and Mahler's classifications are equivalent in that they divide the transcendental numbers into the same classes.[32] The A*-numbers are the algebraic numbers.[27]\n\nLet\n\nIt can be shown that the nth root of λ (a Liouville number) is a U-number of degree n.[33]\n\nThis construction can be improved to create an uncountable family of U-numbers of degree n. Let Z be the set consisting of every other power of 10 in the series above for λ. The set of all subsets of Z is uncountable. Deleting any of the subsets of Z from the series for λ creates uncountably many distinct Liouville numbers, whose nth roots are U-numbers of degree n.\n\nThe supremum of the sequence {ω(x, n)} is called the type. Almost all real numbers are S numbers of type 1, which is minimal for real S numbers. Almost all complex numbers are S numbers of type 1/2, which is also minimal. The claims of almost all numbers were conjectured by Mahler and in 1965 proved by Vladimir Sprindzhuk.[34]\n\nWhile the Gelfond–Schneider theorem proved that a large class of numbers was transcendental, this class was still countable.  Many well-known mathematical constants are still not known to be transcendental, and in some cases it is not even known whether they are rational or irrational.  A partial list can be found here.\n\nA major problem in transcendence theory is showing that a particular set of numbers is algebraically independent rather than just showing that individual elements are transcendental.  So while we know that e and π are transcendental that doesn't imply that e + π is transcendental, nor other combinations of the two (except eπ, Gelfond's constant, which is known to be transcendental).  Another major problem is dealing with numbers that are not related to the exponential function.  The main results in transcendence theory tend to revolve around e and the logarithm function, which means that wholly new methods tend to be required to deal with numbers that cannot be expressed in terms of these two objects in an elementary fashion.\n\nSchanuel's conjecture would solve the first of these problems somewhat as it deals with algebraic independence and would indeed confirm that e + π is transcendental.  It still revolves around the exponential function, however, and so would not necessarily deal with numbers such as Apéry's constant or the Euler–Mascheroni constant.  Another extremely difficult unsolved problem is the so-called constant or identity problem.[35]\n"
    },
    {
        "title": "Line (geometry)",
        "content": "\n\nIn geometry, a straight line, usually abbreviated line, is an infinitely long object with no width, depth, or curvature, an idealization of such physical objects as a straightedge, a taut string, or a ray of light. Lines are spaces of dimension one, which may be embedded in spaces of dimension two, three, or higher. The word line may also refer, in everyday life, to a line segment, which is a part of a line delimited by two points (its endpoints).\n\nEuclid's Elements defines a straight line as a \"breadthless length\" that \"lies evenly with respect to the points on itself\", and introduced several postulates as basic unprovable properties on which the rest of geometry was established. Euclidean line and Euclidean geometry are terms introduced to avoid confusion with generalizations introduced since the end of the 19th century, such as non-Euclidean, projective, and affine geometry.\n\nIn the Greek deductive geometry of Euclid's Elements, a general line (now called a curve) is defined as a \"breadthless length\", and a straight line (now called a line segment) was defined as a line \"which lies evenly with the points on itself\".[1]: 291  These definitions appeal to readers' physical experience, relying on terms that are not themselves defined, and the definitions are never explicitly referenced in the remainder of the text. In modern geometry, a line is usually either taken as a primitive notion with properties given by axioms,[1]: 95  or else defined as a set of points obeying a linear relationship, for instance when real numbers are taken to be primitive and geometry is established analytically in terms of numerical coordinates.\n\nIn an axiomatic formulation of Euclidean geometry, such as that of Hilbert (modern mathematicians added to Euclid's original axioms to fill perceived logical gaps),[1]: 108  a line is stated to have certain properties that relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect at most at one point.[1]: 300  In two dimensions (i.e., the Euclidean plane), two lines that do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not.\n\nOn a Euclidean plane, a line can be represented as a boundary between two regions.[2]: 104  Any collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.\n\nIn three-dimensional space, a first degree equation in the variables x, y, and z defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in n-dimensional space n−1 first-degree equations in the n coordinate variables define a line under suitable conditions.\n\nIn more general Euclidean space, Rn (and analogously in every other affine space), the line L passing through two different points a and b is the subset\n\n\n\n\nL\n=\n\n{\n\n(\n1\n−\nt\n)\n\na\n+\nt\nb\n∣\nt\n∈\n\nR\n\n\n}\n\n.\n\n\n{\\displaystyle L=\\left\\{(1-t)\\,a+tb\\mid t\\in \\mathbb {R} \\right\\}.}\n\n\nThe direction of the line is from a reference point a (t = 0) to another point b (t = 1), or in other words, in the direction of the vector b − a. Different choices of a and b can yield the same line.\n\nThree or more points are said to be collinear if they lie on the same line. If three points are not collinear, there is exactly one plane that contains them.\n\nIn affine coordinates, in n-dimensional space the points X = (x1, x2, ..., xn), Y = (y1, y2, ..., yn), and Z = (z1, z2, ..., zn) are collinear if the matrix\n\n\n\n\n\n\n[\n\n\n\n1\n\n\n\nx\n\n1\n\n\n\n\n\nx\n\n2\n\n\n\n\n⋯\n\n\n\nx\n\nn\n\n\n\n\n\n\n1\n\n\n\ny\n\n1\n\n\n\n\n\ny\n\n2\n\n\n\n\n⋯\n\n\n\ny\n\nn\n\n\n\n\n\n\n1\n\n\n\nz\n\n1\n\n\n\n\n\nz\n\n2\n\n\n\n\n⋯\n\n\n\nz\n\nn\n\n\n\n\n\n]\n\n\n\n\n{\\displaystyle {\\begin{bmatrix}1&x_{1}&x_{2}&\\cdots &x_{n}\\\\1&y_{1}&y_{2}&\\cdots &y_{n}\\\\1&z_{1}&z_{2}&\\cdots &z_{n}\\end{bmatrix}}}\n\n\nhas a rank less than 3. In particular, for three points in the plane (n = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.\n\nEquivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, k points in a plane are collinear if and only if any (k–1) pairs of points have the same pairwise slopes.\n\nIn Euclidean geometry, the Euclidean distance d(a,b) between two points a and b may be used to express the collinearity between three points by:[3][4]\n\nHowever, there are other notions of distance (such as the Manhattan distance) for which this property is not true.\n\nIn the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.\n\nIn Euclidean geometry, all lines are congruent, meaning that every line can be obtained by moving a specific line. However, lines may play special roles with respect to other geometric objects and can be classified according to that relationship. \n\nFor instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:\n\nIn the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.\n\nFor more general algebraic curves, lines could also be:\n\nWith respect to triangles we have:\n\nFor a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.[7]\n\nFor a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.\n\nParallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other—every point that is on either one of them is also on the other.\n\nPerpendicular lines are lines that intersect at right angles.[8]\n\nIn three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.\n\nIn synthetic geometry, the concept of a line is often considered as a primitive notion,[1]: 95  meaning it is not being defined by using other concepts, but  it is defined by the properties, called  axioms, that it must satisfy.[9]\n\nHowever, the axiomatic definition of a line does not explain the relevance of the concept and is often too abstract for beginners. So, the definition is often replaced or completed by a mental image or intuitive description that allows understanding what is a line. Such descriptions are sometimes referred to as definitions, but are not true definitions since they cannot used in mathematical proofs. The \"definition\" of line in Euclid's Elements falls into this category;[1]: 95  and is never used in proofs of theorems.\n\nLines in a Cartesian plane or, more generally, in affine coordinates, are characterized by linear equations. More precisely, every line \n\n\n\nL\n\n\n{\\displaystyle L}\n\n (including vertical lines) is the set of all points whose coordinates (x, y) satisfy a linear equation; that is,\n\n\n\n\nL\n=\n{\n(\nx\n,\ny\n)\n∣\na\nx\n+\nb\ny\n=\nc\n}\n,\n\n\n{\\displaystyle L=\\{(x,y)\\mid ax+by=c\\},}\n\n\nwhere a, b and c are fixed real numbers (called coefficients) such that a and b are not both zero. Using this form, vertical lines correspond to equations with b = 0.\n\nOne can further suppose either c = 1 or c = 0, by dividing everything by c if it is not zero.\n\nThere are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. The above form is sometimes called the standard form. If the constant term is put on the left, the equation becomes\n\n\n\n\na\nx\n+\nb\ny\n−\nc\n=\n0\n,\n\n\n{\\displaystyle ax+by-c=0,}\n\n\nand this is sometimes called the general form of the equation. However, this terminology is not universally accepted, and many authors do not distinguish these two forms.\n\nThese forms are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.\n\nThe equation of the line passing through two different points \n\n\n\n\nP\n\n0\n\n\n(\n\nx\n\n0\n\n\n,\n\ny\n\n0\n\n\n)\n\n\n{\\displaystyle P_{0}(x_{0},y_{0})}\n\n and \n\n\n\n\nP\n\n1\n\n\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n\n\n{\\displaystyle P_{1}(x_{1},y_{1})}\n\n may be written as\n\n\n\n\n(\ny\n−\n\ny\n\n0\n\n\n)\n(\n\nx\n\n1\n\n\n−\n\nx\n\n0\n\n\n)\n=\n(\n\ny\n\n1\n\n\n−\n\ny\n\n0\n\n\n)\n(\nx\n−\n\nx\n\n0\n\n\n)\n.\n\n\n{\\displaystyle (y-y_{0})(x_{1}-x_{0})=(y_{1}-y_{0})(x-x_{0}).}\n\n\nIf x0 ≠ x1, this equation may be rewritten as\n\n\n\n\ny\n=\n(\nx\n−\n\nx\n\n0\n\n\n)\n\n\n\n\n\ny\n\n1\n\n\n−\n\ny\n\n0\n\n\n\n\n\nx\n\n1\n\n\n−\n\nx\n\n0\n\n\n\n\n\n+\n\ny\n\n0\n\n\n\n\n{\\displaystyle y=(x-x_{0})\\,{\\frac {y_{1}-y_{0}}{x_{1}-x_{0}}}+y_{0}}\n\n\nor\n\n\n\n\ny\n=\nx\n\n\n\n\n\ny\n\n1\n\n\n−\n\ny\n\n0\n\n\n\n\n\nx\n\n1\n\n\n−\n\nx\n\n0\n\n\n\n\n\n+\n\n\n\n\nx\n\n1\n\n\n\ny\n\n0\n\n\n−\n\nx\n\n0\n\n\n\ny\n\n1\n\n\n\n\n\nx\n\n1\n\n\n−\n\nx\n\n0\n\n\n\n\n\n\n.\n\n\n{\\displaystyle y=x\\,{\\frac {y_{1}-y_{0}}{x_{1}-x_{0}}}+{\\frac {x_{1}y_{0}-x_{0}y_{1}}{x_{1}-x_{0}}}\\,.}\n\nIn two dimensions, the equation for non-vertical lines is often given in the slope–intercept form:\n\n\n\n\n\ny\n=\nm\nx\n+\nb\n\n\n{\\displaystyle y=mx+b}\n\n\nwhere:\n\nThe slope of the line through points \n\n\n\nA\n(\n\nx\n\na\n\n\n,\n\ny\n\na\n\n\n)\n\n\n{\\displaystyle A(x_{a},y_{a})}\n\n and \n\n\n\nB\n(\n\nx\n\nb\n\n\n,\n\ny\n\nb\n\n\n)\n\n\n{\\displaystyle B(x_{b},y_{b})}\n\n, when \n\n\n\n\nx\n\na\n\n\n≠\n\nx\n\nb\n\n\n\n\n{\\displaystyle x_{a}\\neq x_{b}}\n\n, is given by \n\n\n\nm\n=\n(\n\ny\n\nb\n\n\n−\n\ny\n\na\n\n\n)\n\n/\n\n(\n\nx\n\nb\n\n\n−\n\nx\n\na\n\n\n)\n\n\n{\\displaystyle m=(y_{b}-y_{a})/(x_{b}-x_{a})}\n\n and the equation of this line can be written \n\n\n\ny\n=\nm\n(\nx\n−\n\nx\n\na\n\n\n)\n+\n\ny\n\na\n\n\n\n\n{\\displaystyle y=m(x-x_{a})+y_{a}}\n\n.\n\nAs a note, lines in three dimensions may also be described as the simultaneous solutions of two linear equations\n\n\n\n\n\na\n\n1\n\n\nx\n+\n\nb\n\n1\n\n\ny\n+\n\nc\n\n1\n\n\nz\n−\n\nd\n\n1\n\n\n=\n0\n\n\n{\\displaystyle a_{1}x+b_{1}y+c_{1}z-d_{1}=0}\n\n\n\n\n\n\n\na\n\n2\n\n\nx\n+\n\nb\n\n2\n\n\ny\n+\n\nc\n\n2\n\n\nz\n−\n\nd\n\n2\n\n\n=\n0\n\n\n{\\displaystyle a_{2}x+b_{2}y+c_{2}z-d_{2}=0}\n\n\nsuch that \n\n\n\n(\n\na\n\n1\n\n\n,\n\nb\n\n1\n\n\n,\n\nc\n\n1\n\n\n)\n\n\n{\\displaystyle (a_{1},b_{1},c_{1})}\n\n and \n\n\n\n(\n\na\n\n2\n\n\n,\n\nb\n\n2\n\n\n,\n\nc\n\n2\n\n\n)\n\n\n{\\displaystyle (a_{2},b_{2},c_{2})}\n\n are not proportional (the relations \n\n\n\n\na\n\n1\n\n\n=\nt\n\na\n\n2\n\n\n,\n\nb\n\n1\n\n\n=\nt\n\nb\n\n2\n\n\n,\n\nc\n\n1\n\n\n=\nt\n\nc\n\n2\n\n\n\n\n{\\displaystyle a_{1}=ta_{2},b_{1}=tb_{2},c_{1}=tc_{2}}\n\n imply \n\n\n\nt\n=\n0\n\n\n{\\displaystyle t=0}\n\n). This follows since in three dimensions a single linear equation typically describes a plane and a line is what is common to two distinct intersecting planes.\n\nParametric equations are also used to specify lines, particularly in those in three dimensions or more because in more than two dimensions lines cannot be described by a single linear equation.\n\nIn three dimensions lines are frequently described by parametric equations:\n\n\n\n\n\n\n\n\nx\n\n\n\n=\n\nx\n\n0\n\n\n+\na\nt\n\n\n\n\ny\n\n\n\n=\n\ny\n\n0\n\n\n+\nb\nt\n\n\n\n\nz\n\n\n\n=\n\nz\n\n0\n\n\n+\nc\nt\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}x&=x_{0}+at\\\\y&=y_{0}+bt\\\\z&=z_{0}+ct\\end{aligned}}}\n\n\nwhere:\n\nParametric equations for lines in higher dimensions are similar in that they are based on the specification of one point on the line and a direction vector.\n\nThe normal form (also called the Hesse normal form,[10] after the German mathematician Ludwig Otto Hesse), is based on the normal segment for a given line, which is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin. The normal form of the equation of a straight line on the plane is given by:\n\n\n\n\nx\ncos\n⁡\nφ\n+\ny\nsin\n⁡\nφ\n−\np\n=\n0\n,\n\n\n{\\displaystyle x\\cos \\varphi +y\\sin \\varphi -p=0,}\n\n\nwhere \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n is the angle of inclination of the normal segment (the oriented angle from the unit vector of the x-axis to this segment), and p is the (positive) length of the normal segment. The normal form can be derived from the standard form \n\n\n\na\nx\n+\nb\ny\n=\nc\n\n\n{\\displaystyle ax+by=c}\n\n by dividing all of the coefficients by\n\n\n\n\n\n\n\na\n\n2\n\n\n+\n\nb\n\n2\n\n\n\n\n.\n\n\n{\\displaystyle {\\sqrt {a^{2}+b^{2}}}.}\n\n\nand also multiplying through by \n\n\n\n−\n1\n\n\n{\\displaystyle -1}\n\n if \n\n\n\nc\n<\n0.\n\n\n{\\displaystyle c<0.}\n\n\n\nUnlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n and p, to be specified. If p > 0, then \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n is uniquely defined modulo 2π. On the other hand, if the line is through the origin (c = p = 0), one drops the c/|c| term to compute \n\n\n\nsin\n⁡\nφ\n\n\n{\\displaystyle \\sin \\varphi }\n\n and \n\n\n\ncos\n⁡\nφ\n\n\n{\\displaystyle \\cos \\varphi }\n\n, and it follows that \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n is only defined modulo π.\n\nThe vector equation of the line through points A and B is given by \n\n\n\n\nr\n\n=\n\nO\nA\n\n+\nλ\n\n\nA\nB\n\n\n\n{\\displaystyle \\mathbf {r} =\\mathbf {OA} +\\lambda \\,\\mathbf {AB} }\n\n (where λ is a scalar).\n\nIf a is vector OA and b is vector OB, then the equation of the line can be written: \n\n\n\n\nr\n\n=\n\na\n\n+\nλ\n(\n\nb\n\n−\n\na\n\n)\n\n\n{\\displaystyle \\mathbf {r} =\\mathbf {a} +\\lambda (\\mathbf {b} -\\mathbf {a} )}\n\n.\n\nA ray starting at point A is described by limiting λ. One ray is obtained if λ ≥ 0, and the opposite ray comes from λ ≤ 0.\n\nIn a Cartesian plane, polar coordinates (r, θ) are related to Cartesian coordinates by the parametric equations:[11]\n\n\n\nx\n=\nr\ncos\n⁡\nθ\n,\n\ny\n=\nr\nsin\n⁡\nθ\n.\n\n\n{\\displaystyle x=r\\cos \\theta ,\\quad y=r\\sin \\theta .}\n\n\n\nIn polar coordinates, the equation of a line not passing through the origin—the point with coordinates (0, 0)—can be written\n\n\n\n\nr\n=\n\n\np\n\ncos\n⁡\n(\nθ\n−\nφ\n)\n\n\n\n,\n\n\n{\\displaystyle r={\\frac {p}{\\cos(\\theta -\\varphi )}},}\n\n\nwith r > 0 and \n\n\n\nφ\n−\nπ\n\n/\n\n2\n<\nθ\n<\nφ\n+\nπ\n\n/\n\n2.\n\n\n{\\displaystyle \\varphi -\\pi /2<\\theta <\\varphi +\\pi /2.}\n\n Here, p is the (positive) length of the line segment perpendicular to the line and delimited by the origin and the line, and \n\n\n\nφ\n\n\n{\\displaystyle \\varphi }\n\n is the (oriented) angle from the x-axis to this segment.\n\nIt may be useful to express the equation in terms of the angle \n\n\n\nα\n=\nφ\n+\nπ\n\n/\n\n2\n\n\n{\\displaystyle \\alpha =\\varphi +\\pi /2}\n\n between the x-axis and the line. In this case, the equation becomes\n\n\n\n\nr\n=\n\n\np\n\nsin\n⁡\n(\nθ\n−\nα\n)\n\n\n\n,\n\n\n{\\displaystyle r={\\frac {p}{\\sin(\\theta -\\alpha )}},}\n\n\nwith r > 0 and \n\n\n\n0\n<\nθ\n<\nα\n+\nπ\n.\n\n\n{\\displaystyle 0<\\theta <\\alpha +\\pi .}\n\n\n\nThese equations can be derived from the normal form of the line equation by setting \n\n\n\nx\n=\nr\ncos\n⁡\nθ\n,\n\n\n{\\displaystyle x=r\\cos \\theta ,}\n\n and \n\n\n\ny\n=\nr\nsin\n⁡\nθ\n,\n\n\n{\\displaystyle y=r\\sin \\theta ,}\n\n and then applying the angle difference identity for sine or cosine.\n\nThese equations can also be proven geometrically by applying right triangle definitions of sine and cosine to the right triangle that has a point of the line and the origin as vertices, and the line and its perpendicular through the origin as sides.\n\nThe previous forms do not apply for a line passing through the origin, but a simpler formula can be written: the polar coordinates \n\n\n\n(\nr\n,\nθ\n)\n\n\n{\\displaystyle (r,\\theta )}\n\n of the points of a line passing through the origin and making an angle of \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n with the x-axis, are the pairs \n\n\n\n(\nr\n,\nθ\n)\n\n\n{\\displaystyle (r,\\theta )}\n\n such that \n\n\n\n\nr\n≥\n0\n,\n\n\nand\n\n\nθ\n=\nα\n\n\nor\n\n\nθ\n=\nα\n+\nπ\n.\n\n\n{\\displaystyle r\\geq 0,\\qquad {\\text{and}}\\quad \\theta =\\alpha \\quad {\\text{or}}\\quad \\theta =\\alpha +\\pi .}\n\n\n\nIn modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.\n\nWhen a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry, a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries, a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.\n\nIn many models of projective geometry, the representation of a line rarely conforms to the notion of the \"straight curve\" as it is visualised in Euclidean geometry. In elliptic geometry we see a typical example of this.[1]: 108  In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.\n\nThe \"shortness\" and \"straightness\" of a line, interpreted as the property that the distance along the line between any two of its points is minimized (see triangle inequality), can be generalized and leads to the concept of geodesics in metric spaces.\n\nGiven a line and any point A on it, we may consider A as decomposing this line into two parts.\nEach such part is called a ray and the point A is called its initial point. It is also known as half-line (sometimes, a half-axis if it plays a distinct role, e.g., as part of a coordinate axis). It is a one-dimensional half-space. The point A is considered to be a member of the ray.[a] Intuitively, a ray consists of those points on a line passing through A and proceeding indefinitely, starting at A, in one direction only along the line. However, in order to use this concept of a ray in proofs a more precise definition is required.\n\nGiven distinct points A and B, they determine a unique ray with initial point A. As two points define a unique line, this ray consists of all the points between A and B (including A and B) and all the points C on the line through A and B such that B is between A and C.[12] This is, at times, also expressed as the set of all points C on the line determined by A and B such that A is not between B and C.[13] A point D, on the line determined by A and B but not in the ray with initial point A determined by B, will determine another ray with initial point A. With respect to the AB ray, the AD ray is called the opposite ray.\n\nThus, we would say that two different points, A and B, define a line and a decomposition of this line into the disjoint union of an open segment (A, B) and two rays, BC and AD (the point D is not drawn in the diagram, but is to the left of A on the line AB). These are not opposite rays since they have different initial points.\n\nIn Euclidean geometry two rays with a common endpoint form an angle.[14]\n\nThe definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.\n\nA line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points. Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment. Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these, if they are coplanar and either do not intersect or are collinear.\n\nA point on number line corresponds to a real number and vice versa.[15] Usually, integers are evenly spaced on the line, with positive numbers are on the right, negative numbers on the left. As an extension to the concept, an imaginary line representing imaginary numbers can be drawn perpendicular to the number line at zero.[16] The two lines forms the complex plane, a geometrical representation of the set of complex numbers.\n"
    },
    {
        "title": "Angle",
        "content": "In Euclidean geometry, an angle or plane angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.[1]\nTwo intersecting curves may also define an angle, which is the angle of the rays lying tangent to the respective curves at their point of intersection. Angles are also formed by the intersection of two planes; these are called dihedral angles. \nIn any case, the resulting angle lies in a plane (spanned by the two rays or perpendicular to the line of plane-plane intersection).\n\nThe magnitude of an angle is called an angular measure or simply \"angle\". Two different angles may have the same measure, as in an isosceles triangle. \"Angle\" also denotes the angular sector, the infinite region of the plane bounded by the sides of an angle.[2][3][a]\n\nAngle of rotation is a measure conventionally defined as the ratio of a circular arc length to its radius, and may be a negative number. In the case of an ordinary angle, the arc is centered at the vertex and delimited by the sides. In the case of an angle of rotation, the arc is centered at the center of the rotation and delimited by any other point and its image after the rotation.\n\nThe word angle comes from the Latin word angulus, meaning \"corner\". Cognate words include the Greek ἀγκύλος (ankylοs) meaning \"crooked, curved\" and the English word \"ankle\". Both are connected with the Proto-Indo-European root *ank-, meaning \"to bend\" or \"bow\".[7]\n\nEuclid defines a plane angle as the inclination to each other, in a plane, of two lines that meet each other and do not lie straight with respect to each other. According to the Neoplatonic metaphysician Proclus, an angle must be either a quality, a quantity, or a relationship. The first concept, angle as quality, was used by Eudemus of Rhodes, who regarded an angle as a deviation from a straight line; the second, angle as quantity, by Carpus of Antioch, who regarded it as the interval or space between the intersecting lines; Euclid adopted the third: angle as a relationship.[8]\n\nIn mathematical expressions, it is common to use Greek letters (α, β, γ, θ, φ, . . . ) as variables denoting the size of some angle[9] (the symbol π is typically not used for this purpose to avoid confusion with the constant denoted by that symbol). Lower case Roman letters (a, b, c, . . . ) are also used. In contexts where this is not confusing, an angle may be denoted by the upper case Roman letter denoting its vertex. See the figures in this article for examples.\n\nThe three defining points may also identify angles in geometric figures. For example, the angle with vertex A formed by the rays AB and AC (that is, the half-lines from point A through points B and C) is denoted ∠BAC or \n\n\n\n\n\n\n\nB\nA\nC\n\n^\n\n\n\n\n\n{\\displaystyle {\\widehat {\\rm {BAC}}}}\n\n. Where there is no risk of confusion, the angle may sometimes be referred to by a single vertex alone (in this case, \"angle A\").\n\nIn other ways, an angle denoted as, say, ∠BAC might refer to any of four angles: the clockwise angle from B to C about A, the anticlockwise angle from B to C about A, the clockwise angle from C to B about A, or the anticlockwise angle from C to B about A, where the direction in which the angle is measured determines its sign (see § Signed angles). However, in many geometrical situations, it is evident from the context that the positive angle less than or equal to 180 degrees is meant, and in these cases, no ambiguity arises. Otherwise, to avoid ambiguity, specific conventions may be adopted so that, for instance, ∠BAC always refers to the anticlockwise (positive) angle from B to C about A and ∠CAB the anticlockwise (positive) angle from C to B about A.\n\nThere is some common terminology for angles, whose measure is always non-negative (see § Signed angles):\n\nThe names, intervals, and measuring units are shown in the table below:\n\nWhen two straight lines intersect at a point, four angles are formed. Pairwise, these angles are named according to their location relative to each other.\n\nThe equality of vertically opposite angles is called the vertical angle theorem. Eudemus of Rhodes attributed the proof to Thales of Miletus.[14][15] The proposition showed that since both of a pair of vertical angles are supplementary to both of the adjacent angles, the vertical angles are equal in measure. According to a historical note,[15] when Thales visited Egypt, he observed that whenever the Egyptians drew two intersecting lines, they would measure the vertical angles to make sure that they were equal. Thales concluded that one could prove that all vertical angles are equal if one accepted some general notions such as:\n\nWhen two adjacent angles form a straight line, they are supplementary. Therefore, if we assume that the measure of angle A equals x, the measure of angle C would be 180° − x. Similarly, the measure of angle D would be 180° − x. Both angle C and angle D have measures equal to 180° − x and are congruent. Since angle B is supplementary to both angles C and D, either of these angle measures may be used to determine the measure of Angle B. Using the measure of either angle C or angle D, we find the measure of angle B to be 180° − (180° − x) = 180° − 180° + x = x. Therefore, both angle A and angle B have measures equal to x and are equal in measure.\n\nA transversal is a line that intersects a pair of (often parallel) lines and is associated with exterior angles, interior angles, alternate exterior angles, alternate interior angles, corresponding angles, and consecutive interior angles.[16]\n\nThe angle addition postulate states that if B is in the interior of angle AOC, then\n\n\n\n\n\nm\n∠\n\nA\nO\nC\n\n=\nm\n∠\n\nA\nO\nB\n\n+\nm\n∠\n\nB\nO\nC\n\n\n\n{\\displaystyle m\\angle \\mathrm {AOC} =m\\angle \\mathrm {AOB} +m\\angle \\mathrm {BOC} }\n\n\n\nI.e., the measure of the angle AOC is the sum of the measure of angle AOB and the measure of angle BOC.\n\nThree special angle pairs involve the summation of angles:\n\n\nThe adjective complementary is from the Latin complementum, associated with the verb complere, \"to fill up\". An acute angle is \"filled up\" by its complement to form a right angle.\n\nThe difference between an angle and a right angle is termed the complement of the angle.[18]\n\nIf angles A and B are complementary, the following relationships hold: \n\n\n\n\n\n\n\n\n\nsin\n\n2\n\n\n⁡\nA\n+\n\nsin\n\n2\n\n\n⁡\nB\n=\n1\n\n\n\n\ncos\n\n2\n\n\n⁡\nA\n+\n\ncos\n\n2\n\n\n⁡\nB\n=\n1\n\n\n\n\n\ntan\n⁡\nA\n=\ncot\n⁡\nB\n\n\n\nsec\n⁡\nA\n=\ncsc\n⁡\nB\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&\\sin ^{2}A+\\sin ^{2}B=1&&\\cos ^{2}A+\\cos ^{2}B=1\\\\[3pt]&\\tan A=\\cot B&&\\sec A=\\csc B\\end{aligned}}}\n\n\n\n(The tangent of an angle equals the cotangent of its complement, and its secant equals the cosecant of its complement.)\n\nThe prefix \"co-\" in the names of some trigonometric ratios refers to the word \"complementary\".\n\nIf the two supplementary angles are adjacent (i.e., have a common vertex and share just one side), their non-shared sides form a straight line. Such angles are called a linear pair of angles.[20] However, supplementary angles do not have to be on the same line and can be separated in space. For example, adjacent angles of a parallelogram are supplementary, and opposite angles of a cyclic quadrilateral (one whose vertices all fall on a single circle) are supplementary.\n\nIf a point P is exterior to a circle with center O, and if the tangent lines from P touch the circle at points T and Q, then ∠TPQ and ∠TOQ are supplementary.\n\nThe sines of supplementary angles are equal. Their cosines and tangents (unless undefined) are equal in magnitude but have opposite signs.\n\nIn Euclidean geometry, any sum of two angles in a triangle is supplementary to the third because the sum of the internal angles of a triangle is a straight angle.\n\n\n\nThe difference between an angle and a complete angle is termed the explement of the angle or conjugate of an angle.\n\nThe size of a geometric angle is usually characterized by the magnitude of the smallest rotation that maps one of the rays into the other. Angles of the same size are said to be equal congruent or equal in measure.\n\nIn some contexts, such as identifying a point on a circle or describing the orientation of an object in two dimensions relative to a reference orientation, angles that differ by an exact multiple of a full turn are effectively equivalent. In other contexts, such as identifying a point on a spiral curve or describing an object's cumulative rotation in two dimensions relative to a reference orientation, angles that differ by a non-zero multiple of a full turn are not equivalent.\n\nTo measure an angle θ, a circular arc centered at the vertex of the angle is drawn, e.g., with a pair of compasses. The ratio of the length s of the arc by the radius r of the circle is the number of radians in the angle:[25]\n\n\n\n\nθ\n=\n\n\ns\nr\n\n\n\n\nr\na\nd\n\n.\n\n\n{\\displaystyle \\theta ={\\frac {s}{r}}\\,\\mathrm {rad} .}\n\n\nConventionally, in mathematics and the SI, the radian is treated as being equal to the dimensionless unit 1, thus being normally omitted.\n\nThe angle expressed by another angular unit may then be obtained by multiplying the angle by a suitable conversion constant of the form ⁠k/2π⁠, where k is the measure of a complete turn expressed in the chosen unit (for example, k = 360° for degrees or 400 grad for gradians):\n\n\n\n\n\nθ\n=\n\n\nk\n\n2\nπ\n\n\n\n⋅\n\n\ns\nr\n\n\n.\n\n\n{\\displaystyle \\theta ={\\frac {k}{2\\pi }}\\cdot {\\frac {s}{r}}.}\n\n\n\nThe value of θ thus defined is independent of the size of the circle: if the length of the radius is changed, then the arc length changes in the same proportion, so the ratio s/r is unaltered.[nb 1]\n\nThroughout history, angles have been measured in various units. These are known as angular units, with the most contemporary units being the degree ( ° ), the radian (rad), and the gradian (grad), though many others have been used throughout history.[27] Most units of angular measurement are defined such that one turn (i.e., the angle subtended by the circumference of a circle at its centre) is equal to n units, for some whole number n. Two exceptions are the radian (and its decimal submultiples) and the diameter part.\n\nIn the International System of Quantities, an angle is defined as a dimensionless quantity, and in particular, the radian unit is dimensionless. This convention impacts how angles are treated in dimensional analysis.\n\nThe following table lists some units used to represent angles.\n\n[29] It is ⁠1/256⁠ of a turn.[28]\n\nPlane angle may be defined as θ = s/r, where θ is the magnitude in radians of the subtended angle, s is circular arc length, and r is radius. One radian corresponds to the angle for which s = r, hence 1 radian = 1 m/m = 1.[33] However, rad is only to be used to express angles, not to express ratios of lengths in general.[34] A similar calculation using the area of a circular sector θ = 2A/r2 gives 1 radian as 1 m2/m2 = 1.[35] The key fact is that the radian is a dimensionless unit equal to 1. In SI 2019, the SI radian is defined accordingly as 1 rad = 1.[36] It is a long-established practice in mathematics and across all areas of science to make use of rad = 1.[37][38]\n\nGiacomo Prando writes \"the current state of affairs leads inevitably to ghostly appearances and disappearances of the radian in the dimensional analysis of physical equations\".[39] For example, an object hanging by a string from a pulley will rise or drop by y = rθ centimetres, where r is the magnitude of the radius of the pulley in centimetres and θ is the magnitude of the angle through which the pulley turns in radians. When multiplying r by θ, the unit radian does not appear in the product, nor does the unit centimetre—because both factors are magnitudes (numbers). Similarly in the formula for the angular velocity of a rolling wheel, ω = v/r, radians appear in the units of ω but not on the right hand side.[40] Anthony French calls this phenomenon \"a perennial problem in the teaching of mechanics\".[41] Oberhofer says that the typical advice of ignoring radians during dimensional analysis and adding or removing radians in units according to convention and contextual knowledge is \"pedagogically unsatisfying\".[42]\n\nIn 1993 the American Association of Physics Teachers Metric Committee specified that the radian should explicitly appear in quantities only when different numerical values would be obtained when other angle measures were used, such as in the quantities of angle measure (rad), angular speed (rad/s), angular acceleration (rad/s2), and torsional stiffness (N⋅m/rad), and not in the quantities of torque (N⋅m) and angular momentum (kg⋅m2/s).[43]\n\nAt least a dozen scientists between 1936 and 2022 have made proposals to treat the radian as a base unit of measurement for a base quantity (and dimension) of \"plane angle\".[44][45][46] Quincey's review of proposals outlines two classes of proposal. The first option changes the unit of a radius to meters per radian, but this is incompatible with dimensional analysis for the area of a circle, πr2. The other option is to introduce a dimensional constant. According to Quincey this approach is \"logically rigorous\" compared to SI, but requires \"the modification of many familiar mathematical and physical equations\".[47] A dimensional constant for angle is \"rather strange\" and the difficulty of modifying equations to add the dimensional constant is likely to preclude widespread use.[46]\n\nIn particular, Quincey identifies Torrens' proposal to introduce a constant η equal to 1 inverse radian (1 rad−1) in a fashion similar to the introduction of the constant ε0.[47][b] With this change the formula for the angle subtended at the center of a circle, s = rθ, is modified to become s = ηrθ, and the Taylor series for the sine of an angle θ becomes:[46][48]\n\n\n\n\nSin\n⁡\nθ\n=\nsin\n⁡\n \nx\n=\nx\n−\n\n\n\nx\n\n3\n\n\n\n3\n!\n\n\n\n+\n\n\n\nx\n\n5\n\n\n\n5\n!\n\n\n\n−\n\n\n\nx\n\n7\n\n\n\n7\n!\n\n\n\n+\n⋯\n=\nη\nθ\n−\n\n\n\n(\nη\nθ\n\n)\n\n3\n\n\n\n\n3\n!\n\n\n\n+\n\n\n\n(\nη\nθ\n\n)\n\n5\n\n\n\n\n5\n!\n\n\n\n−\n\n\n\n(\nη\nθ\n\n)\n\n7\n\n\n\n\n7\n!\n\n\n\n+\n⋯\n,\n\n\n{\\displaystyle \\operatorname {Sin} \\theta =\\sin \\ x=x-{\\frac {x^{3}}{3!}}+{\\frac {x^{5}}{5!}}-{\\frac {x^{7}}{7!}}+\\cdots =\\eta \\theta -{\\frac {(\\eta \\theta )^{3}}{3!}}+{\\frac {(\\eta \\theta )^{5}}{5!}}-{\\frac {(\\eta \\theta )^{7}}{7!}}+\\cdots ,}\n\n\nwhere \n\n\n\nx\n=\nη\nθ\n=\nθ\n\n/\n\n\nrad\n\n\n\n{\\displaystyle x=\\eta \\theta =\\theta /{\\text{rad}}}\n\n is the angle in radians.\nThe capitalized function Sin is the \"complete\" function that takes an argument with a dimension of angle and is independent of the units expressed,[48] while sin is the traditional function on pure numbers which assumes its argument is a dimensionless number in radians.[49] The capitalised symbol \n\n\n\nSin\n\n\n{\\displaystyle \\operatorname {Sin} }\n\n can be denoted \n\n\n\nsin\n\n\n{\\displaystyle \\sin }\n\n if it is clear that the complete form is meant.[46][50]\n\nCurrent SI can be considered relative to this framework as a natural unit system where the equation η = 1 is assumed to hold, or similarly, 1 rad = 1. This radian convention allows the omission of η in mathematical formulas.[51]\n\nIt is frequently helpful to impose a convention that allows positive and negative angular values to represent orientations and/or rotations in opposite directions or \"sense\" relative to some reference.\n\nIn a two-dimensional Cartesian coordinate system, an angle is typically defined by its two sides, with its vertex at the origin. The initial side is on the positive x-axis, while the other side or terminal side is defined by the measure from the initial side in radians, degrees, or turns, with positive angles representing rotations toward the positive y-axis and negative angles representing rotations toward the negative y-axis. When Cartesian coordinates are represented by standard position, defined by the x-axis rightward and the y-axis upward, positive rotations are anticlockwise, and negative cycles are clockwise.\n\nIn many contexts, an angle of −θ is effectively equivalent to an angle of \"one full turn minus θ\". For example, an orientation represented as −45° is effectively equal to an orientation defined as 360° − 45° or 315°. Although the final position is the same, a physical rotation (movement) of −45° is not the same as a rotation of 315° (for example, the rotation of a person holding a broom resting on a dusty floor would leave visually different traces of swept regions on the floor).\n\nIn three-dimensional geometry, \"clockwise\" and \"anticlockwise\" have no absolute meaning, so the direction of positive and negative angles must be defined in terms of an orientation, which is typically determined by a normal vector passing through the angle's vertex and perpendicular to the plane in which the rays of the angle lie.\n\nIn navigation, bearings or azimuth are measured relative to north. By convention, viewed from above, bearing angles are positive clockwise, so a bearing of 45° corresponds to a north-east orientation. Negative bearings are not used in navigation, so a north-west orientation corresponds to a bearing of 315°.\n\nFor an angular unit, it is definitional that the angle addition postulate holds. Some quantities related to angles where the angle addition postulate does not hold include:\n\nThe angle between a line and a curve (mixed angle) or between two intersecting curves (curvilinear angle) is defined to be the angle between the tangents at the point of intersection. Various names (now rarely, if ever, used) have been given to particular cases:—amphicyrtic (Gr. ἀμφί, on both sides, κυρτός, convex) or cissoidal (Gr. κισσός, ivy), biconvex; xystroidal or sistroidal (Gr. ξυστρίς, a tool for scraping), concavo-convex; amphicoelic (Gr. κοίλη, a hollow) or angulus lunularis, biconcave.[58]\n\nThe ancient Greek mathematicians knew how to bisect an angle (divide it into two angles of equal measure) using only a compass and straightedge but could only trisect certain angles. In 1837, Pierre Wantzel showed that this construction could not be performed for most angles.\n\nIn the Euclidean space, the angle θ between two Euclidean vectors u and v is related to their dot product and their lengths by the formula\n\n\n\n\n\n\nu\n\n⋅\n\nv\n\n=\ncos\n⁡\n(\nθ\n)\n\n‖\n\nu\n\n‖\n\n\n‖\n\nv\n\n‖\n\n.\n\n\n{\\displaystyle \\mathbf {u} \\cdot \\mathbf {v} =\\cos(\\theta )\\left\\|\\mathbf {u} \\right\\|\\left\\|\\mathbf {v} \\right\\|.}\n\n\n\nThis formula supplies an easy method to find the angle between two planes (or curved surfaces) from their normal vectors and between skew lines from their vector equations.\n\nTo define angles in an abstract real inner product space, we replace the Euclidean dot product ( · ) by the inner product \n\n\n\n⟨\n⋅\n,\n⋅\n⟩\n\n\n{\\displaystyle \\langle \\cdot ,\\cdot \\rangle }\n\n, i.e.\n\n\n\n\n\n⟨\n\nu\n\n,\n\nv\n\n⟩\n=\ncos\n⁡\n(\nθ\n)\n \n\n‖\n\nu\n\n‖\n\n\n‖\n\nv\n\n‖\n\n.\n\n\n{\\displaystyle \\langle \\mathbf {u} ,\\mathbf {v} \\rangle =\\cos(\\theta )\\ \\left\\|\\mathbf {u} \\right\\|\\left\\|\\mathbf {v} \\right\\|.}\n\n\n\nIn a complex inner product space, the expression for the cosine above may give non-real values, so it is replaced with\n\n\n\n\n\nRe\n⁡\n\n(\n\n⟨\n\nu\n\n,\n\nv\n\n⟩\n\n)\n\n=\ncos\n⁡\n(\nθ\n)\n\n‖\n\nu\n\n‖\n\n\n‖\n\nv\n\n‖\n\n.\n\n\n{\\displaystyle \\operatorname {Re} \\left(\\langle \\mathbf {u} ,\\mathbf {v} \\rangle \\right)=\\cos(\\theta )\\left\\|\\mathbf {u} \\right\\|\\left\\|\\mathbf {v} \\right\\|.}\n\n\n\nor, more commonly, using the absolute value, with\n\n\n\n\n\n\n|\n\n⟨\n\nu\n\n,\n\nv\n\n⟩\n\n|\n\n=\n\n|\n\ncos\n⁡\n(\nθ\n)\n\n|\n\n\n‖\n\nu\n\n‖\n\n\n‖\n\nv\n\n‖\n\n.\n\n\n{\\displaystyle \\left|\\langle \\mathbf {u} ,\\mathbf {v} \\rangle \\right|=\\left|\\cos(\\theta )\\right|\\left\\|\\mathbf {u} \\right\\|\\left\\|\\mathbf {v} \\right\\|.}\n\n\n\nThe latter definition ignores the direction of the vectors. It thus describes the angle between one-dimensional subspaces \n\n\n\nspan\n⁡\n(\n\nu\n\n)\n\n\n{\\displaystyle \\operatorname {span} (\\mathbf {u} )}\n\n and \n\n\n\nspan\n⁡\n(\n\nv\n\n)\n\n\n{\\displaystyle \\operatorname {span} (\\mathbf {v} )}\n\n spanned by the vectors \n\n\n\n\nu\n\n\n\n{\\displaystyle \\mathbf {u} }\n\n and \n\n\n\n\nv\n\n\n\n{\\displaystyle \\mathbf {v} }\n\n correspondingly.\n\nThe definition of the angle between one-dimensional subspaces \n\n\n\nspan\n⁡\n(\n\nu\n\n)\n\n\n{\\displaystyle \\operatorname {span} (\\mathbf {u} )}\n\n and \n\n\n\nspan\n⁡\n(\n\nv\n\n)\n\n\n{\\displaystyle \\operatorname {span} (\\mathbf {v} )}\n\n given by\n\n\n\n\n\n\n|\n\n⟨\n\nu\n\n,\n\nv\n\n⟩\n\n|\n\n=\n\n|\n\ncos\n⁡\n(\nθ\n)\n\n|\n\n\n‖\n\nu\n\n‖\n\n\n‖\n\nv\n\n‖\n\n\n\n{\\displaystyle \\left|\\langle \\mathbf {u} ,\\mathbf {v} \\rangle \\right|=\\left|\\cos(\\theta )\\right|\\left\\|\\mathbf {u} \\right\\|\\left\\|\\mathbf {v} \\right\\|}\n\n\n\nin a Hilbert space can be extended to subspaces of finite dimensions. Given two subspaces \n\n\n\n\n\nU\n\n\n\n\n{\\displaystyle {\\mathcal {U}}}\n\n, \n\n\n\n\n\nW\n\n\n\n\n{\\displaystyle {\\mathcal {W}}}\n\n with \n\n\n\ndim\n⁡\n(\n\n\nU\n\n\n)\n:=\nk\n≤\ndim\n⁡\n(\n\n\nW\n\n\n)\n:=\nl\n\n\n{\\displaystyle \\dim({\\mathcal {U}}):=k\\leq \\dim({\\mathcal {W}}):=l}\n\n, this leads to a definition of \n\n\n\nk\n\n\n{\\displaystyle k}\n\n angles called canonical or principal angles between subspaces.\n\nIn Riemannian geometry, the metric tensor is used to define the angle between two tangents. Where U and V are tangent vectors and gij are the components of the metric tensor G,\n\n\n\n\n\ncos\n⁡\nθ\n=\n\n\n\n\ng\n\ni\nj\n\n\n\nU\n\ni\n\n\n\nV\n\nj\n\n\n\n\n\n|\n\n\ng\n\ni\nj\n\n\n\nU\n\ni\n\n\n\nU\n\nj\n\n\n\n|\n\n\n|\n\n\ng\n\ni\nj\n\n\n\nV\n\ni\n\n\n\nV\n\nj\n\n\n\n|\n\n\n\n\n.\n\n\n{\\displaystyle \\cos \\theta ={\\frac {g_{ij}U^{i}V^{j}}{\\sqrt {\\left|g_{ij}U^{i}U^{j}\\right|\\left|g_{ij}V^{i}V^{j}\\right|}}}.}\n\n\n\nA hyperbolic angle is an argument of a hyperbolic function just as the circular angle is the argument of a circular function. The comparison can be visualized as the size of the openings of a hyperbolic sector and a circular sector since the areas of these sectors correspond to the angle magnitudes in each case.[59] Unlike the circular angle, the hyperbolic angle is unbounded. When the circular and hyperbolic functions are viewed as infinite series in their angle argument, the circular ones are just alternating series forms of the hyperbolic functions. This comparison of the two series corresponding to functions of angles was described by Leonhard Euler in Introduction to the Analysis of the Infinite (1748).\n\nIn geography, the location of any point on the Earth can be identified using a geographic coordinate system. This system specifies the latitude and longitude of any location in terms of angles subtended at the center of the Earth, using the equator and (usually) the Greenwich meridian as references.\n\nIn astronomy, a given point on the celestial sphere (that is, the apparent position of an astronomical object) can be identified using any of several astronomical coordinate systems, where the references vary according to the particular system. Astronomers measure the angular separation of two stars by imagining two lines through the center of the Earth, each intersecting one of the stars. The angle between those lines and the angular separation between the two stars can be measured.\n\nIn both geography and astronomy, a sighting direction can be specified in terms of a vertical angle such as altitude /elevation with respect to the horizon as well as the azimuth with respect to north.\n\nAstronomers also measure objects' apparent size as an angular diameter. For example, the full moon has an angular diameter of approximately 0.5° when viewed from Earth. One could say, \"The Moon's diameter subtends an angle of half a degree.\" The small-angle formula can convert such an angular measurement into a distance/size ratio.\n\nOther astronomical approximations include:\n\nThese measurements depend on the individual subject, and the above should be treated as rough rule of thumb approximations only.\n\nIn astronomy, right ascension and declination are usually measured in angular units, expressed in terms of time, based on a 24-hour day.\n\n This article incorporates text from a publication now in the public domain: Chisholm, Hugh, ed. (1911), \"Angle\", Encyclopædia Britannica, vol. 2 (11th ed.), Cambridge University Press, p. 14\n"
    }
]